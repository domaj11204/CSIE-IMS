@online{200902113Going,
  title = {[2009.02113] {{Going Beyond T-SNE}}: {{Exposing}} \textbackslash texttt\{whatlies\} in {{Text Embeddings}}},
  url = {https://arxiv.org/abs/2009.02113},
  urldate = {2024-03-22},
  langid = {english},
  annotation = {titleTranslation: [2009.02113] 超越 T-SNE：在文本嵌入中暴露 \textbackslash texttt\{whatlies\}},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\P6ZKYKW3\\[2009.02113] Going Beyond T-SNE Exposing texttt .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\VNJMYRFF\\2009.html}
}

@online{220407496Improving,
  title = {[2204.07496] {{Improving Passage Retrieval}} with {{Zero-Shot Question Generation}}},
  url = {https://arxiv.org/abs/2204.07496},
  urldate = {2024-03-16},
  langid = {english},
  keywords = {IR,問答系統,未整理,資料集},
  annotation = {titleTranslation: [2204.07496]透過零樣本問題產生改進段落檢索},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\9Q7T4XEN\\[2204.07496] Improving Passage Retrieval with Zero.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\4RHCW7JQ\\2204.html}
}

@online{240113256UniMSRAG,
  title = {[2401.13256] {{UniMS-RAG}}: {{A Unified Multi-source Retrieval-Augmented Generation}} for {{Personalized Dialogue Systems}}},
  url = {https://arxiv.org/abs/2401.13256},
  urldate = {2024-04-11},
  langid = {english},
  keywords = {待讀,未整理,重要},
  annotation = {titleTranslation: [2401.13256] UniMS-RAG：個人化對話系統的統一多源檢索增強生成},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\DQNM5JBJ\\[2401.13256] UniMS-RAG A Unified Multi-source Ret.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\VRBSPV5M\\2401.html}
}

@article{于琦2017基于本体的中医医案知识服务与共享系统构建研究,
  title = {基于本体的中医医案知识服务与共享系统构建研究},
  author = {{于琦} and {李敬华} and {李宗友} and {王映辉} and {孙晓峰} and {于彤} and {高宏杰} and {田野} and {徐丽丽}},
  date = {2017-01},
  journaltitle = {中国数字医学},
  volume = {12},
  number = {5},
  pages = {103--105},
  langid = {chinese},
  keywords = {No DOI found},
  annotation = {titleTranslation: 基於本體的中醫醫案知識服務與共享系統構建研究},
  file = {C:\Users\BlackCat\Zotero\storage\MR4AHC39\于琦 等。 - 2017 - 基于本体的中医医案知识服务与共享系统构建研究.pdf}
}

@article{郭明阳2020辨,
  title = {辨证论治思想产生的时代背景及局限性},
  author = {{郭明阳} and {罗勇} and {呼永河}},
  date = {2020},
  journaltitle = {西南军医},
  volume = {22},
  number = {3},
  pages = {290--291},
  publisher = {中国人民解放军西部战区总医院中医科, 四川 成都,610083},
  issn = {1672-7193},
  abstract = {R2-09\%R228; 本文详细探讨了"辨证论治"这一重要中医诊疗概念产生的背景与局限性,以供中医教学中,讲解相关概念时参考.},
  copyright = {Copyright © Wanfang Data Co. Ltd. All Rights Reserved.},
  langid = {chinese},
  keywords = {No DOI found},
  annotation = {titleTranslation: 明確證據論治理思想產生的時代背景及局限性\\
abstractTranslation:  R2-09\%R228；本文詳細探討了“辨證論治”這一重要中醫診療概念產生的背景與局限性，以供中醫教學中、講解相關概念時參考。},
  file = {C:\Users\BlackCat\Zotero\storage\ILBSUR7I\郭明阳 等。 - 2020 - 辨证论治思想产生的时代背景及局限性.pdf}
}

@article{a.a.belozerovSemanticWebTechnologies2022,
  title = {Semantic {{Web Technologies}}: {{Issues}} and {{Possible Ways}} of {{Development}}},
  shorttitle = {Semantic {{Web Technologies}}},
  author = {{A. A. Belozerov} and {V. V. Klimov}},
  date = {2022-01-01},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  series = {2022 {{Annual International Conference}} on {{Brain-Inspired Cognitive Architectures}} for {{Artificial Intelligence}}: {{The}} 13th {{Annual Meeting}} of the {{BICA Society}}},
  volume = {213},
  pages = {617--622},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2022.11.112},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050922018117},
  urldate = {2023-09-22},
  abstract = {Semantic digital technologies are as useful as they are difficult to implement. During its existence, the concept of the Semantic Web has still found very limited application and extremely slow development. The reasons for this are serious differences in the structure of hypertext and the Semantic Web as a superstructure above it, and the rules that circumscribed the development of the internet. Semantic search engines still do not implement semantic search in its pure form: various data organization schemes and data visualization formats make it a little easier to analyze facts, add them to databases, but do not make knowledge bases out of the latter, do not allow working with facts as with knowledge. In addition, the user interfaces of such systems tend to be either very complex or counter-intuitive. Nevertheless, there is every reason to believe that with the use of modern technologies, the ideas of semantization of the web and its content can already be improved and gradually implemented: for this it is necessary to get rid of some false expectations of semantic technologies, to direct efforts towards the development of web native formats of semantic markup and try to take advantage of the increasingly common voice input of household devices.},
  langid = {english},
  keywords = {FOAF,Hybrid Web,Linked Data,Logical Inference,Markup Language,Microformats,Ontology,OWL,RDF,RSS,Semantic Network,Semantic Search,Semantic Technology,Semantic Web,URI,已整理,待讀,知識管理,重要},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 語意網路技術：問題與可能的發展途徑\\
abstractTranslation:  語意數位技術很有用，但實施起來卻很困難。在其存在期間，語義網的概念的應用仍然非常有限且發展極其緩慢。原因在於超文本的結構與作為其上層建築的語意網路之間的嚴重差異，以及限制網路發展的規則。語義搜尋引擎仍然沒有以純粹的形式實現語義搜尋：各種資料組織方案和資料視覺化格式使分析事實、將它們添加到資料庫變得更容易，但不能從後者中創建知識庫，不允許像對待知識一樣對待事實。此外，這類系統的使用者介面往往非常複雜或違反直覺。儘管如此，我們完全有理由相信，隨著現代技術的運用，網路及其內容的語義化理念已經可以得到完善並逐步實施：為此，有必要擺脫對語義技術的一些錯誤期望，致力於開發語義標記的網路原生格式，並嘗試利用日益常見的家用設備語音輸入。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\X4P24GE9\\Belozerov 與 Klimov - 2022 - Semantic Web Technologies Issues and Possible Way.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\RZ4AYDEM\\S1877050922018117.html}
}

@article{a.m.cohenSurveyCurrentWork2005,
  title = {A Survey of Current Work in Biomedical Text Mining},
  author = {{A. M. Cohen}},
  date = {2005-01-01},
  journaltitle = {Briefings in Bioinformatics},
  shortjournal = {Briefings in Bioinformatics},
  volume = {6},
  number = {1},
  pages = {57--71},
  issn = {1467-5463, 1477-4054},
  doi = {10.1093/bib/6.1.57},
  url = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/6.1.57},
  urldate = {2022-09-19},
  langid = {english},
  annotation = {459 citations (Crossref) [2024-03-26]\\
824 citations (Semantic Scholar/DOI) [2022-10-26]\\
titleTranslation: 生物醫學文本挖掘當前工作的調查},
  file = {C:\Users\BlackCat\Zotero\storage\G2WIRDHC\Cohen - 2005 - A survey of current work in biomedical text mining.pdf}
}

@article{abbateInvestigatingHealthcareTransition2023,
  title = {Investigating {{Healthcare}} 4.0 {{Transition Through}} a {{Knowledge Management Perspective}}},
  author = {Abbate, Stefano and Centobelli, Piera and Cerchione, Roberto and Oropallo, Eugenio and Riccio, Emanuela},
  date = {2023-09},
  journaltitle = {IEEE Transactions on Engineering Management},
  volume = {70},
  number = {9},
  pages = {3297--3310},
  issn = {1558-0040},
  doi = {10.1109/TEM.2022.3200889},
  url = {https://ieeexplore.ieee.org/abstract/document/9893347},
  urldate = {2024-01-18},
  abstract = {The impact of technological innovation on the healthcare sector is becoming increasingly significant, and the number of studies exploring this topic is rising rapidly. However, studies on the digital transition of healthcare services are still a challenge, both from a theoretical and managerial perspective. In this context, knowledge management discipline can guide the evolving environment to cover this research gap. Indeed, the digital transition is transforming how healthcare professionals access data, handle information, and manage knowledge by adopting 4.0 enabling technologies. Thus, drawing on the SECI model, this study aims to investigate the healthcare sector's technological innovation through a knowledge management perspective, and evaluate the impact of 4.0 technologies on knowledge creation processes (i.e., socialization, externalization, combination, internalization) in the healthcare domain. Finally, the article investigates the critical areas of the digital transition, and the future research directions that remain to be addressed.},
  eventtitle = {{{IEEE Transactions}} on {{Engineering Management}}},
  langid = {english},
  keywords = {SECI,SLR,survey,已整理,略讀,知識管理,醫療},
  annotation = {14 citations (Crossref) [2024-03-26]\\
abstractTranslation:  科技創新對醫療保健產業的影響日益顯著，探索此主題的研究數量也迅速增加。然而，無論從理論或管理角度來看，醫療服務數位轉型的研究仍然是一個挑戰。在這種背景下，知識管理學科可以指導不斷發展的環境來彌補這個研究空白。事實上，數位轉型正在透過採用 4.0 支援技術來改變醫療保健專業人員存取資料、處理資訊和管理知識的方式。因此，本研究借鑒SECI模型，從知識管理的角度考察醫療健康領域的技術創新，評估4.0技術對醫療健康領域知識創造過程（即社會化、外化、組合、內化）的影響。 。最後，本文探討了數位轉型的關鍵領域以及有待解決的未來研究方向。\\
titleTranslation: 從知識管理的角度研究醫療保健 4.0 轉型},
  note = {使用SLR方法做文獻回顧。
\par
分析目前醫療領域的知識管理與SECI的關聯性},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\JGYDSPAR\\Abbate 等。 - 2023 - Investigating Healthcare 4.0 Transition Through a .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\QJYCYCX8\\9893347.html}
}

@article{abderahmanrejebChartingPresentFuture2022,
  title = {Charting {{Past}}, {{Present}}, and {{Future Research}} in the {{Semantic Web}} and {{Interoperability}}},
  author = {{Abderahman Rejeb} and {John G. Keogh} and {Wayne Martindale} and {Damion Dooley} and {Edward Smart} and {Steven Simske} and {Samuel Fosso Wamba} and {John G. Breslin} and {Kosala Yapa Bandara} and {Subhasis Thakur} and {Kelly Liu} and {Bridgette Crowley} and {Sowmya Desaraju} and {Angela Ospina} and {Horia Bradau}},
  date = {2022},
  journaltitle = {Future internet},
  volume = {14},
  number = {6},
  pages = {161-},
  publisher = {MDPI AG},
  location = {Basel},
  issn = {1999-5903},
  doi = {10.3390/fi14060161},
  abstract = {Huge advances in peer-to-peer systems and attempts to develop the semantic web have revealed a critical issue in information systems across multiple domains: the absence of semantic interoperability. Today, businesses operating in a digital environment require increased supply-chain automation, interoperability, and data governance. While research on the semantic web and interoperability has recently received much attention, a dearth of studies investigates the relationship between these two concepts in depth. To address this knowledge gap, the objective of this study is to conduct a review and bibliometric analysis of 3511 Scopus-registered papers on the semantic web and interoperability published over the past two decades. In addition, the publications were analyzed using a variety of bibliometric indicators, such as publication year, journal, authors, countries, and institutions. Keyword co-occurrence and co-citation networks were utilized to identify the primary research hotspots and group the relevant literature. The findings of the review and bibliometric analysis indicate the dominance of conference papers as a means of disseminating knowledge and the substantial contribution of developed nations to the semantic web field. In addition, the keyword co-occurrence network analysis reveals a significant emphasis on semantic web languages, sensors and computing, graphs and models, and linking and integration techniques. Based on the co-citation clustering, the Internet of Things, semantic web services, ontology mapping, building information modeling, bioinformatics, education and e-learning, and semantic web languages were identified as the primary themes contributing to the flow of knowledge and the growth of the semantic web and interoperability field. Overall, this review substantially contributes to the literature and increases scholars’ and practitioners’ awareness of the current knowledge composition and future research directions of the semantic web field.},
  langid = {english},
  keywords = {Bibliometrics,Bioinformatics,Blockchain,Building management systems,Cloud computing,Clustering,Collaboration,Data mining,Distance learning,Fault diagnosis,Fuzzy logic,Information systems,Internet of Things,Interoperability,Knowledge,Languages,Network analysis,Ontology,Semantic web,semantic web services,Semantics,Software,Supply chains,Survey,Technology,Web services,World Wide Web,已整理,待讀,語意網},
  annotation = {7 citations (Crossref) [2024-03-26]\\
titleTranslation: 繪製語意網和互通性的過去、現在和未來研究\\
abstractTranslation:  對等系統的巨大進步和開發語義網路的嘗試揭示了跨多個領域的資訊系統的一個關鍵問題：缺乏語義互通性。如今，在數位環境中營運的企業需要提高供應鏈自動化、互通性和資料治理。雖然語義網和互通性的研究最近受到了廣泛關注，但缺乏深入研究這兩個概念之間關係的研究。為了解決這一知識差距，本研究的目的是對過去 20 年來發表的 3511 篇 Scopus 註冊論文進行回顧和文獻計量分析，內容涉及語義網和互通性。此外，還使用各種文獻計量指標對出版物進行了分析，例如出版年份、期刊、作者、國家和機構。利用關鍵字共現和共引網絡來確定主要研究熱點並對相關文獻進行分組。審查和文獻計量分析的結果表明會議論文作為知識傳播手段的主導地位以及已開發國家對語義網路領域的重大貢獻。此外，關鍵字共現網路分析揭示了對語義網路語言、感測器和計算、圖形和模型以及連結和整合技術的重視。基於同被引聚類，物聯網、語義網路服務、本體映射、建築資訊模型、生物資訊學、教育和電子學習以及語義網路語言被確定為有助於知識流動和知識流動的主要主題。語義網和互通性領域的發展。總體而言，這篇綜述對文獻做出了重大貢獻，並提高了學者和實務工作者對語義網路領域當前知識構成和未來研究方向的認識。},
  file = {C:\Users\BlackCat\Zotero\storage\BPSYYFBF\Rejeb et al. - 2022 - Charting Past, Present, and Future Research in the.pdf}
}

@inproceedings{adithyamsDatasetMultitaskMultiview2023,
  title = {A {{Dataset}} and {{Multi-task Multi-view Approach}} for {{Question-Answering}} with the {{Dual Perspectives}} of {{Text}} and {{Knowledge}}},
  booktitle = {2023 15th {{International Conference}} on {{Computer}} and {{Automation Engineering}} ({{ICCAE}})},
  author = {{Adithya MS} and {Mohsin Ahmed} and {Mihir Madhusudan Kestur} and {A Sai Chaithanya} and {Bhaskarjyothi Das}},
  date = {2023-03},
  pages = {296--301},
  issn = {2154-4360},
  doi = {10.1109/ICCAE56788.2023.10111327},
  url = {https://ieeexplore.ieee.org/document/10111327},
  urldate = {2023-11-23},
  abstract = {Question-answering (QA) systems are important tools for extracting information from large datasets and providing accurate and relevant answers to user queries. Two of the most widely studied and built QA systems are Natural Language Question Answering (NLQA) and Knowledge Graph Question Answering (KGQA). NLQA relies on sequence learning algorithms, which have limitations on the length of input they can handle, while KGQA relies on the Subject-Predicate-Object (SPO) tuple representation of data, which may not always be available in the knowledge graph. In this paper, we present a novel approach for addressing these challenges by utilizing the structural information from the Knowledge Graph (KG) and the semantic information from the Natural Language Context. Due to the lack of a dataset to enable this approach, we propose the creation of a multi-view dataset - MTL-QA, specifically designed for multi-task learning. We also present a multi-task learning approach to jointly train NLQA and KGQA models and demonstrate the effectiveness on the proposed MTL-QA dataset.},
  eventtitle = {2023 15th {{International Conference}} on {{Computer}} and {{Automation Engineering}} ({{ICCAE}})},
  langid = {english},
  keywords = {KBQA,NLQA,問答系統,回收,已整理,機器學習,知識圖譜},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 文字和知識雙重視角的資料集和多任務多視圖問答方法},
  note = {開發了一個結合NLQA和KBQA兩者的資料集並提出了新的訓練方法。
\par
本方法和研究比較沒有關聯，但NLQA和KBQA的說明可能會有用。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\69N6GXIH\\MS et al. - 2023 - A Dataset and Multi-task Multi-view Approach for Q.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\CM5VWBIW\\10111327.html}
}

@inproceedings{adityagouravPersonalizationStrategiesEndtoEnd2021,
  title = {Personalization {{Strategies}} for {{End-to-End Speech Recognition Systems}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {{Aditya Gourav} and {Linda Liu} and {Ankur Gandhe} and {Yile Gu} and {Guitang Lan} and {Xiangyang Huang} and {Shashank Kalmane} and {Gautam Tiwari} and {Denis Filimonov} and {Ariya Rastrow} and {Andreas Stolcke} and {Ivan Bulyko}},
  date = {2021-06},
  pages = {7348--7352},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9413962},
  url = {https://ieeexplore.ieee.org/document/9413962},
  urldate = {2023-10-30},
  abstract = {The recognition of personalized content, such as contact names, remains a challenging problem for end-to-end speech recognition systems. In this work, we demonstrate how first- and second-pass rescoring strategies can be leveraged together to improve the recognition of such words. Following previous work, we use a shallow fusion approach to bias towards recognition of personalized content in the first-pass decoding. We show that such an approach can improve personalized content recognition by up to 16\% with minimum degradation on the general use case. We describe a fast and scalable algorithm that enables our biasing models to remain at the word-level, while applying the biasing at the subword level. This has the advantage of not requiring the biasing models to be dependent on any subword symbol table. We also describe a novel second-pass de-biasing approach: used in conjunction with a first-pass shallow fusion that optimizes on oracle WER, we can achieve an additional 14\% improvement on personalized content recognition, and even improve accuracy for the general use case by up to 2.5\%.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  langid = {english},
  keywords = {個人化語音辨識,已整理,語音辨識},
  annotation = {17 citations (Crossref) [2024-03-26]\\
titleTranslation: 端對端語音辨識系統的個人化策略},
  note = {比較像個人化用詞而非個人化微調},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\7ZFVY3SL\\Gourav 等。 - 2021 - Personalization Strategies for End-to-End Speech R.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\UWKKU6LL\\9413962.html}
}

@inproceedings{agarwalRememberingWhatYou2017,
  title = {Remembering What You Said: {{Semantic}} Personalized Memory for Personal Digital Assistants},
  shorttitle = {Remembering What You Said},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Agarwal, Vipul and Khan, Omar Zia and Sarikaya, Ruhi},
  date = {2017-03},
  pages = {5835--5839},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2017.7953275},
  url = {https://ieeexplore.ieee.org/document/7953275},
  urldate = {2023-12-03},
  abstract = {Personal digital assistants are designed to assist users in easy information retrieval or execute the tasks they are interested in. The conversational medium implies an additional level of intelligence but typically these systems do not support any reference to the user's past interactions. We propose a domain-agnostic approach that enables the system to address queries referring to the past by using an information retrieval approach to rank various entities for a given query. We also add semantic enrichment to the recall process by augmenting the entities with information from a knowledge graph and leverage that in the retrieval process. We mined user interactions for the Cortana digital assistant to extract queries with location and business entities and show that our technique can achieve an accuracy of 89.8\% for such recall queries.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  langid = {english},
  keywords = {已整理,數位助理,機器學習},
  annotation = {4 citations (Crossref) [2024-03-26]\\
titleTranslation: 記住你所說的話：個人數字助理的語意個性化記憶},
  note = {藉由知識圖譜幫助數位助理瞭解過去查詢的資料。關於問題的描述、論文的架構值得參考。
\par
可以寫在未來展望或相關應用中。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\QIE2GQTA\\Agarwal et al. - 2017 - Remembering what you said Semantic personalized m.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\75JKK4TQ\\7953275.html}
}

@article{aghaeiQuestionAnsweringKnowledge2022,
  title = {Question {{Answering Over Knowledge Graphs}}: {{A Case Study}} in {{Tourism}}},
  shorttitle = {Question {{Answering Over Knowledge Graphs}}},
  author = {Aghaei, Sareh and Raad, Elie and Fensel, Anna},
  date = {2022},
  journaltitle = {IEEE Access},
  volume = {10},
  pages = {69788--69801},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3187178},
  url = {https://ieeexplore.ieee.org/abstract/document/9810255},
  urldate = {2023-12-01},
  abstract = {Over the recent years, a large number of knowledge graphs (KGs) have been developed to store and present small and medium enterprises’ data in the form of subject-predicate-object triples. The KGs are not easily accessible to end-users because they need an understanding of query languages and KGs’ structures. To assist end-users in accessing these KGs, question answering over KGs targets to provide answers for natural language questions (NLQs). This paper proposes an approach to answer questions over small and medium scaled KGs based on graph isomorphism in two phases: (1) offline phase and (2) semantic parsing phase. In the offline phase, a semi-automated solution is proposed to generate NLQs and their answers, which are used to train machine learning models employed in the next phase. In the semantic parsing phase, a given input NLQ is mapped into a query pattern according to its grammatical structure. Each query pattern contains some slots that need to be filled with corresponding entities, classes and relations from the KG. While string and semantic similarity metrics are applied to identify the entities and classes, the probability distribution of the relations is used to extract the relations. The Cartesian product of the identified entities, classes and relations is utilized to fill the slots, derive SPARQL queries and finally retrieve the answers. To evaluate the proposed approach, we use SalzburgerLand KG, a real KG describing touristic entities of the region of Salzburg, Austria. Our results show that the approach improves the end-to-end user experience in terms of interactive question answering and performance.},
  eventtitle = {{{IEEE Access}}},
  langid = {english},
  keywords = {問答系統,待整理},
  annotation = {5 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於知識圖的問答：旅遊業案例研究},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\S7E38NXF\\Aghaei 等。 - 2022 - Question Answering Over Knowledge Graphs A Case S.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\I3FUNAN6\\9810255.html}
}

@online{agrawalCanKnowledgeGraphs2024,
  title = {Can {{Knowledge Graphs Reduce Hallucinations}} in {{LLMs}}? : {{A Survey}}},
  shorttitle = {Can {{Knowledge Graphs Reduce Hallucinations}} in {{LLMs}}?},
  author = {Agrawal, Garima and Kumarage, Tharindu and Alghamdi, Zeyad and Liu, Huan},
  date = {2024-03-15},
  eprint = {2311.07914},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.07914},
  url = {http://arxiv.org/abs/2311.07914},
  urldate = {2024-04-16},
  abstract = {The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,LLM,Survey,可解釋性,已整理,文獻,機器學習,知識圖譜,重要},
  annotation = {titleTranslation: 知識圖譜可以減少法學碩士的幻覺嗎？ ： 一項調查\\
abstractTranslation:  當代的法學碩士很容易產生幻覺，這主要源自於模型中的知識差距。為了解決這一關鍵限制，研究人員採用多種策略透過整合外部知識來增強法學碩士，旨在減少幻覺並提高推理準確性。在這些策略中，利用知識圖作為外部資訊來源已經證明了有希望的結果。在這項調查中，我們全面回顧了法學碩士中這些基於知識圖譜的增強技術，並著重於它們在減輕幻覺方面的功效。我們有系統地將這些方法分為三個總體組，提供方法比較和效能評估。最後，本調查探討了與這些技術相關的當前趨勢和挑戰，並概述了這一新興領域未來研究的潛在途徑。},
  note = {Comment: Accepted Paper in NAACL 2024},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\7D4BU82U\\Agrawal 等。 - 2024 - Can Knowledge Graphs Reduce Hallucinations in LLMs.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\ZSMKZKQS\\2311.html}
}

@article{agunghadhiatmaScientificPaperRecommendation2023,
  title = {A {{Scientific Paper Recommendation Framework Based}} on {{Multi-Topic Communities}} and {{Modified PageRank}}},
  author = {{Agung Hadhiatma} and {Azhari Azhari} and {Yohanes Suyanto}},
  date = {2023},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {11},
  pages = {25303--25317},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3251189},
  url = {https://ieeexplore.ieee.org/document/10056944/},
  urldate = {2023-10-23},
  abstract = {Personalized PageRank is a variant of PageRank, widely developed for citation recommendation. However, the personalized PageRank that works with a vast amount and rich scholarly data still results in information overload. Sometimes, junior scholars still need help to arrange queries quickly because of limited domain knowledge. Senior researchers need reference papers regarding a similar topic they intend to search for and related topics as a new insight. In this research, scientific citation recommendation aims to find the most influential papers with similar and related topics. Related topic papers in serendipitous perspectives are reference papers that are novel, diversified and unexpected to a user. The unexpectedness of recommended papers can be papers with different topics to queries but still relevant. To accomplish these challenges, we propose a framework of scientific citation recommendation with serendipitous perspectives. The framework includes feature extraction of an academic citation network, selection of multi-topic communities, and ranking papers in the selected multi-topic communities by modified PageRank. Papers in the chosen communities tend to link to similar and related papers. Modified PageRank is an extension of personalized PageRank, which works on multi-topic communities and manuscript queries. The experiments reveal that the proposed models outperform some models of personalized PageRank and some models of Content-Based Filtering. The multi-topic communities-based models work more effectively than the baselines if they run in a large dataset since the topic communities become more cohesive.},
  langid = {english},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於多主題社群與改進PageRank的科技論文推薦框架\\
abstractTranslation:  個人化 PageRank 是 PageRank 的變體，廣泛開髮用於引文推薦。然而，與大量豐富的學術數據一起使用的個人化PageRank仍然會導致資訊過載。有時，初級學者由於領域知識有限，仍需要協助快速安排查詢。高級研究人員需要關於他們打算搜尋的類似主題和相關主題的參考論文作為新的見解。在這項研究中，科學引文推薦旨在尋找具有相似和相關主題的最有影響力的論文。偶然視角的相關主題論文是新穎的、多元的、令使用者意想不到的參考論文。推薦論文的意外之處可能是與查詢主題不同但仍相關的論文。為了應對這些挑戰，我們提出了一個具有偶然視角的科學引文推薦框架。該框架包括學術引文網絡的特徵提取、多主題社區的選擇以及通過修改後的 PageRank 對所選多主題社區中的論文進行排名。所選社區中的論文往往會連結到類似和相關的論文。 Modified PageRank 是個人化 PageRank 的擴展，適用於多主題社群和手稿查詢。實驗表明，所提出的模型優於一些個人化 PageRank 模型和一些基於內容的過濾模型。如果在大型資料集中運行，基於多主題社群的模型比基準模型更有效，因為主題社群變得更有凝聚力。},
  note = {[TLDR] A framework of scientific citation recommendation with serendipitous perspectives is proposed, which includes feature extraction of an academic citation network, selection of multi-topic communities, and ranking papers in the selected multi- topic communities by modified PageRank.}
}

@article{ahmadchaddadSurveyExplainableAI2023,
  title = {Survey of {{Explainable AI Techniques}} in {{Healthcare}}},
  author = {{Ahmad Chaddad} and {Jihao Peng} and {Jian Xu} and {Ahmed Bouridane}},
  date = {2023-01},
  journaltitle = {Sensors},
  volume = {23},
  number = {2},
  pages = {634},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s23020634},
  url = {https://www.mdpi.com/1424-8220/23/2/634},
  urldate = {2023-10-25},
  abstract = {Artificial intelligence (AI) with deep learning models has been widely applied in numerous domains, including medical imaging and healthcare tasks. In the medical field, any judgment or decision is fraught with risk. A doctor will carefully judge whether a patient is sick before forming a reasonable explanation based on the patient’s symptoms and/or an examination. Therefore, to be a viable and accepted tool, AI needs to mimic human judgment and interpretation skills. Specifically, explainable AI (XAI) aims to explain the information behind the black-box model of deep learning that reveals how the decisions are made. This paper provides a survey of the most recent XAI techniques used in healthcare and related medical imaging applications. We summarize and categorize the XAI types, and highlight the algorithms used to increase interpretability in medical imaging topics. In addition, we focus on the challenging XAI problems in medical applications and provide guidelines to develop better interpretations of deep learning models using XAI concepts in medical image and text analysis. Furthermore, this survey provides future directions to guide developers and researchers for future prospective investigations on clinical topics, particularly on applications with medical imaging.},
  issue = {2},
  langid = {english},
  keywords = {deep learning,explainable AI,medical imaging,radiomics,可解釋性,已整理,機器學習,醫療},
  annotation = {77 citations (Crossref) [2024-03-26]\\
titleTranslation: 醫療保健中可解釋的人工智慧技術調查},
  file = {C:\Users\BlackCat\Zotero\storage\X2Z2VQF7\Chaddad et al. - 2023 - Survey of Explainable AI Techniques in Healthcare.pdf}
}

@article{aidanhoganKnowledgeGraphs2021,
  title = {Knowledge {{Graphs}}},
  author = {{Aidan Hogan} and {Eva Blomqvist} and {Michael Cochez} and {Claudia D’amato} and {Gerard De Melo} and {Claudio Gutierrez} and {Sabrina Kirrane} and {José Emilio Labra Gayo} and {Roberto Navigli} and {Sebastian Neumaier} and {Axel-Cyrille Ngonga Ngomo} and {Axel Polleres} and {Sabbir M. Rashid} and {Anisa Rula} and {Lukas Schmelzeisen} and {Juan Sequeda} and {Steffen Staab} and {Antoine Zimmermann}},
  date = {2021-07-02},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {54},
  number = {4},
  pages = {71:1--71:37},
  issn = {0360-0300},
  doi = {10.1145/3447772},
  url = {https://dl.acm.org/doi/10.1145/3447772},
  urldate = {2023-08-03},
  abstract = {In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.},
  langid = {english},
  keywords = {graph algorithms,graph databases,graph neural networks,graph query languages,rule mining,shapes,嵌入,已讀,文獻,知識圖譜,知識推理,知識本體,規則挖掘},
  annotation = {386 citations (Crossref) [2024-03-26]\\
137 citations (Semantic Scholar/DOI) [2023-08-11]\\
titleTranslation: 知識圖譜\\
abstractTranslation:  在本文中，我們對知識圖譜進行了全面的介紹，知識圖譜最近在需要利用多樣化、動態、大規模數據集合的場景中引起了業界和學術界的極大關注。在一些開場白之後，我們激發並對比了各種基於圖的數據模型，以及用於查詢和驗證知識圖的語言。我們解釋如何結合演繹和歸納技術來表示和提取知識。我們總結了知識圖的未來高水平研究方向。},
  file = {C:\Users\BlackCat\Zotero\storage\H288C7FY\Hogan et al. - 2021 - Knowledge Graphs.pdf}
}

@article{aKnowledgeManagementDigital2023,
  title = {Knowledge {{Management}} in the {{Digital Age}}: {{Harnessing Information}} and {{Innovation}} with {{Knowledge Management Systems}}},
  shorttitle = {Knowledge {{Management}} in the {{Digital Age}}},
  author = {A, Nurnaninsih and B, Ahmad Muktamar and Muthmainah, Hanifah Nurul},
  date = {2023-08-28},
  journaltitle = {The Eastasouth Journal of Information System and Computer Science},
  volume = {1},
  number = {01},
  pages = {25--34},
  issn = {3025-566X},
  doi = {10.58812/esiscs.v1i01.131},
  url = {https://esj.eastasouth-institute.com/index.php/esiscs/article/view/131},
  urldate = {2024-01-17},
  abstract = {In the dynamic landscape of the digital age, effective knowledge management has become paramount for organizations aiming to harness information and drive innovation. This research paper delves into the intricate interplay between knowledge management, information leveraging, and innovation within the context of the digital era. Leveraging the power of bibliometric analysis, this study examines the trends, influential authors, key concepts, and research gaps in the field. Knowledge Management Systems (KMS) emerge as crucial tools, facilitating the storage, sharing, and creation of knowledge. By systematically analyzing scholarly literature, this research contributes to a comprehensive understanding of knowledge management's evolving role in the digital age, shedding light on its implications for both theory and practice.},
  issue = {01},
  langid = {english},
  keywords = {Digital Age,Information,Innovation,Knowledge Management,Knowledge Management Systems,未整理,略讀,知識管理},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 數位時代的知識管理：透過知識管理系統利用資訊和創新\\
abstractTranslation:  在數位時代的動態格局中，有效的知識管理對於旨在利用資訊和推動創新的組織來說變得至關重要。本研究論文深入探討了數位時代背景下知識管理、資訊利用和創新之間複雜的相互作用。本研究利用文獻計量分析的力量，探討了該領域的趨勢、有影響力的作者、關鍵概念和研究差距。知識管理系統 (KMS) 成為促進知識儲存、分享和創建的重要工具。透過系統性分析學術文獻，本研究有助於全面了解知識管理在數位時代不斷演變的作用，揭示其對理論和實踐的影響。},
  note = {好像只有注重知識管理的相關研究或文獻數量},
  file = {C:\Users\BlackCat\Zotero\storage\2LW7K44A\A 等。 - 2023 - Knowledge Management in the Digital Age Harnessin.pdf}
}

@article{akramUndergraduateGraduateStudents2023,
  title = {Undergraduate and {{Graduate Students}}’ {{Challenges}}: {{A Qualitative Study}} with {{ONDAS Framework Across Multiple Disciplines}} and {{Innovative Research Methodologies}}},
  shorttitle = {Undergraduate and {{Graduate Students}}’ {{Challenges}}},
  author = {Akram, Omar and Franco, Daniel and Lee, Apina},
  date = {2023-10-03},
  journaltitle = {The Qualitative Report},
  volume = {28},
  number = {10},
  pages = {2887--2915},
  issn = {1052-0147},
  doi = {10.46743/2160-3715/2023.6679},
  url = {https://nsuworks.nova.edu/tqr/vol28/iss10/5},
  langid = {english},
  keywords = {ONDAS,已整理,待讀,研究流程,重要},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 本科生和研究生面臨的挑戰：利用 ONDAS 框架進行跨學科和創新研究方法的定性研究},
  note = {提出一個ONDAS框架來幫助解決研究生的研究問題。
\par
提到研究學(Researchology)這個名詞。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\NVR2H5GX\\Akram 等。 - 2023 - Undergraduate and Graduate Students’ Challenges A.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\ATD7Z53I\\5.html}
}

@article{al-sharafiUnderstandingImpactKnowledge2023,
  title = {Understanding the Impact of Knowledge Management Factors on the Sustainable Use of {{AI-based}} Chatbots for Educational Purposes Using a Hybrid {{SEM-ANN}} Approach},
  author = {Al-Sharafi, Mohammed A. and Al-Emran, Mostafa and Iranmanesh, Mohammad and Al-Qaysi, Noor and Iahad, Noorminshah A. and Arpaci, Ibrahim},
  date = {2023-12-15},
  journaltitle = {Interactive Learning Environments},
  volume = {31},
  number = {10},
  pages = {7491--7510},
  publisher = {Routledge},
  issn = {1049-4820},
  doi = {10.1080/10494820.2022.2075014},
  url = {https://doi.org/10.1080/10494820.2022.2075014},
  urldate = {2024-01-17},
  abstract = {Artificial intelligence (AI)-based chatbots have received considerable attention during the last few years. However, little is known concerning what affects their use for educational purposes. This research, therefore, develops a theoretical model based on extracting constructs from the expectation confirmation model (ECM) (expectation confirmation, perceived usefulness, and satisfaction), combined with the knowledge management (KM) factors (knowledge sharing, knowledge acquisition, and knowledge application) to understand the sustainable use of chatbots. The developed model was then tested based on data collected through an online survey from 448 university students who used chatbots for learning purposes. Contrary to the prior literature that mainly relied on structural equation modeling (SEM) techniques, the empirical data were analyzed using a hybrid SEM-artificial neural network (SEM-ANN) approach. The hypotheses testing results reinforced all the suggested hypotheses in the developed model. The sensitivity analysis results revealed that knowledge application has the most considerable effect on the sustainable use of chatbots with 96.9\% normalized importance, followed by perceived usefulness (70.7\%), knowledge acquisition (69.3\%), satisfaction (61\%), and knowledge sharing (19.6\%). Deriving from these results, the study highlighted a number of practical implications that benefit developers, designers, service providers, and instructors.},
  langid = {english},
  keywords = {artificial neural network,conversational agents,sustainability,使用者研究,問答系統,已整理,教育},
  annotation = {47 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用混合 SEM-ANN 方法了解知識管理因素對教育目的可持續使用基於人工智慧的聊天機器人的影響\\
abstractTranslation:  基於人工智慧 (AI) 的聊天機器人在過去幾年中受到了廣泛關注。然而，人們對影響其教育用途的因素知之甚少。因此，本研究發展了一個基於從期望確認模型（ECM）（期望確認、感知有用性和滿意度）中提取構造的理論模型，並結合知識管理（KM）因素（知識共享、知識獲取和知識應用程式）以了解聊天機器人的可持續使用。然後，根據對 448 名使用聊天機器人進行學習的大學生進行的線上調查收集的數據，對開發的模型進行了測試。與主要依賴結構方程式建模 (SEM) 技術的現有文獻相反，使用混合 SEM-人工神經網路 (SEM-ANN) 方法對經驗數據進行分析。假設檢定結果強化了所發展模型中所有建議的假設。敏感度分析結果顯示，知識應用對聊天機器人的永續使用影響最大，標準化重要性為96.9\%，其次是感知有用性（70.7\%）、知識獲取（69.3\%）、滿意度（61\%）和知識共享（19.6\%）。根據這些結果，研究強調了許多有利於開發人員、設計師、服務提供者和講師的實際意義。},
  note = {本研究注重聊天機器人在教育、團體情境下的可用性。
\par
結合知識管理與ECM模型，探討在團體背景下有那些因素影響教育相關的可用性。
\par
使用者研究證明，聊天機器人對教育是否有幫助會影響使用者滿意度。
\par
ECM模型對系統評估可能會有幫助，已經記錄到筆記中。},
  file = {D:\Paper\Al-Sharafi et al. - 2023 - Understanding the impact of knowledge management f.pdf}
}

@article{alanaziQuestionAnsweringSystems2021,
  title = {Question {{Answering Systems}}: {{A Systematic Literature Review}}},
  shorttitle = {Question {{Answering Systems}}},
  author = {Alanazi, Sarah and Mohamed, Nazar and Jarajreh, Mutsam and Algarni, Saad},
  date = {2021-01-01},
  journaltitle = {International Journal of Advanced Computer Science and Applications},
  shortjournal = {International Journal of Advanced Computer Science and Applications},
  volume = {12},
  doi = {10.14569/IJACSA.2021.0120359},
  abstract = {Question answering systems (QAS) are developed to answer questions presented in natural language by extracting the answer. The development of QAS is aimed at making the Web more suited to human use by eliminating the need to sift through a lot of search results manually to determine the correct answer to a question. Accordingly, the aim of this study was to provide an overview of the current state of QAS research. It also aimed at highlighting the key limitations and gaps in the existing body of knowledge relating to QAS. Furthermore, it intended to identify the most effective methods utilized in the design of QAS. The systematic review of literature research method was selected as the most appropriate methodology for studying the research topic. This method differs from the conventional literature review as it is more comprehensive and objective. Based on the findings, QAS is a highly active area of research, with scholars taking diverse approaches in the development of their systems. Some of the limitations observed in these studies encompass the focused nature of current QAS, weaknesses associated with models that are used as building blocks for QAS, the need for standard datasets and question formats hence limiting the applicability of the QAS in practical settings, and the failure of researchers to examine their QAS solutions comprehensively. The most effective methods for designing QAS include focusing on syntax and context, utilizing word encoding and knowledge systems, leveraging deep learning, and using elements such as machine learning and artificial intelligence. Going forward, modular designs ought to be encouraged to foster collaboration in the creation of QAS.},
  langid = {english},
  keywords = {Review,問答系統,已整理,微讀},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 問答系統：系統性文獻綜述},
  note = {有些問題的定義對論文有點幫助，但大體來說沒什麼新資訊。
\par
研究2018\textasciitilde 2020間的文獻},
  file = {C:\Users\BlackCat\Zotero\storage\WAZZUKF9\Alanazi 等。 - 2021 - Question Answering Systems A Systematic Literatur.pdf}
}

@article{alastairp.conwayEnhancingDesignDialogue2013,
  title = {Enhancing the Design Dialogue: An Architecture to Document Engineering Design Activities},
  shorttitle = {Enhancing the Design Dialogue},
  author = {{Alastair P. Conway} and {William J. Ion}},
  date = {2013-02-01},
  journaltitle = {Journal of Engineering Design},
  volume = {24},
  number = {2},
  pages = {140--164},
  publisher = {Taylor \& Francis},
  issn = {0954-4828},
  doi = {10.1080/09544828.2012.690859},
  url = {https://doi.org/10.1080/09544828.2012.690859},
  urldate = {2023-10-31},
  abstract = {This paper charts the development of a system architecture designed to address the challenges associated with creating accurate and re-usable records of synchronous design activities. It begins by describing the context of through-life support of engineering products, then presents the Knowledge Enhanced Notes system development work undertaken and provides direction for future research work in this area. An empirical research approach was adopted for this work incorporating 11 experimental episodes, ethnographic studies and case-based evaluation of the developed system. The approach and development of the system architecture within this research build upon and extend existing research in the area of knowledge and information capture. The proposed system architecture is proven to enhance the record of engineering design activities, demonstrating that the implementation of software-based tools can have a positive impact on the creation of a more accurate and complete record of activities. This research is focused on one category of design activities – synchronous; therefore, future research that focuses on asynchronous working, leading to an overall enhancement of design working, is needed. While the focus of the research was to aid the creation and through-life support of large, complex engineering products, the solution is entirely generic in its application to synchronous activities.},
  langid = {english},
  keywords = {knowledge and information capture,process documentation,synchronous working,through-life knowledge usage,實驗筆記,已整理,被引用},
  annotation = {4 citations (Crossref) [2024-03-26]\\
titleTranslation: 加強設計對話：記錄工程設計活動的架構\\
abstractTranslation:  本文描繪了系統架構的開發過程，該架構旨在解決與創建同步設計活動的準確且可重複使用的記錄相關的挑戰。它首先描述工程產品的整個生命週期支援的背景，然後介紹所進行的知識增強註釋系統開發工作，並為該領域的未來研究工作提供方向。這項工作採用了實證研究方法，結合了 11 個實驗片段、民族誌研究和對所開發系統的基於案例的評估。本研究中系統架構的方法和開發建立在知識和資訊擷取領域的現有研究的基礎上並對其進行了擴展。事實證明，所提出的系統架構可以增強工程設計活動的記錄，這表明基於軟體的工具的實施可以對創建更準確和完整的活動記錄產生積極影響。這項研究的重點是一類設計活動—同步；因此，未來的研究需要著重於非同步工作，從而全面增強設計工作。雖然研究的重點是幫助大型複雜工程產品的創建和終身支持，但該解決方案在同步活動的應用中是完全通用的。},
  file = {D\:\\Paper\\Conway and Ion - 2013 - Enhancing the design dialogue an architecture to  2.pdf;D\:\\Paper\\Enhancing the design dialogue an architecture to document engineering design activities.pdf}
}

@online{alecradfordRobustSpeechRecognition2022,
  title = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  author = {{Alec Radford} and {Jong Wook Kim} and {Tao Xu} and {Greg Brockman} and {Christine McLeavey} and {Ilya Sutskever}},
  date = {2022-12-06},
  eprint = {2212.04356},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2212.04356},
  url = {http://arxiv.org/abs/2212.04356},
  urldate = {2023-09-14},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  langid = {english},
  pubstate = {preprint},
  keywords = {完整公開,已整理,機器學習,語音辨識,重要},
  annotation = {titleTranslation: 透過大規模弱監督實現穩健語音識別\\
abstractTranslation:  我們研究了經過簡單訓練來預測網路上大量音訊轉錄的語音處理系統的功能。當擴展到 680,000 小時的多語言和多任務監督時，生成的模型可以很好地推廣到標準基準，並且通常可以與先前的完全監督結果競爭，但在零樣本傳輸設定中無需任何微調。與人類相比，模型的準確性和穩健性接近人類。我們正在發布模型和推理程式碼，作為進一步進行穩健語音處理工作的基礎。},
  note = {OpenAI開發的Whisper。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\7RA35UV3\\Radford 等。 - 2022 - Robust Speech Recognition via Large-Scale Weak Sup.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\KGKPA59J\\2212.html}
}

@article{alicebrenonClassifyingEncyclopediaArticles2022,
  title = {Classifying Encyclopedia Articles: {{Comparing}} Machine and Deep Learning Methods and Exploring Their Predictions},
  shorttitle = {Classifying Encyclopedia Articles},
  author = {{Alice Brenon} and {Ludovic Moncla} and {Katherine McDonough}},
  date = {2022-11-01},
  journaltitle = {Data \& Knowledge Engineering},
  shortjournal = {Data \& Knowledge Engineering},
  volume = {142},
  pages = {102098},
  issn = {0169-023X},
  doi = {10.1016/j.datak.2022.102098},
  url = {https://www.sciencedirect.com/science/article/pii/S0169023X22000891},
  urldate = {2023-09-13},
  abstract = {This article presents a comparative study of supervised classification approaches applied to the automatic classification of encyclopedia articles written in French. Our dataset includes all ~70k text articles from Diderot and d’Alembert’s Encyclopédie (1751-72). In a two-task experiment we test combinations of (1) text vectorization methods (bags-of-words and word embeddings) and (2) traditional Machine Learning and newer Deep Learning classification methods (including transformer architectures). In addition to evaluating each approach, we review the results quantitatively and qualitatively. The best model obtains an average F-score of 86\% for 38 classes. Using network analysis, we highlight the difficulty of labeling semantically close classes. We also discuss misclassifications in order to understand the relationship between content and different ways of ordering knowledge. We openly release all code and results, and data is available on request.11https://gitlab.liris.cnrs.fr/geode/EDdA-Classification.},
  langid = {english},
  keywords = {BERT,Computational humanities,Deep learning,Encyclopedias,Networks,Supervised machine learning,已整理,文本分類,機器學習,知識分類},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 將百科全書文章分類：比較機器和深度學習方法並探索它們的預測\\
abstractTranslation:  本文介紹了應用於法語百科全書文章自動分類的監督分類方法的比較研究。我們的資料集包括狄德羅和達朗貝爾百科全書 (1751-72) 中的所有 70k 篇文字文章。在一項雙任務實驗中，我們測試了 (1) 文本向量化方法（詞袋和詞嵌入）和 (2) 傳統機器學習和較新的深度學習分類方法（包括 Transformer 架構）的組合。除了評估每種方法之外，我們還定量和定性地審查結果。最佳模型在 38 個類別中獲得的平均 F 分數為 86\%。使用網絡分析，我們強調了標記語義上接近的類別的困難。我們也討論錯誤分類，以理解內容與不同知識排序方式之間的關係。我們公開發布所有程式碼和結果，並可根據要求提供數據。11https://gitlab.liris.cnrs.fr/geode/EDdA-Classification。},
  note = {為古老的百科全書做文本分類。使用BERT的結果最好。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\B4BB96XW\\Brenon 等。 - 2022 - Classifying encyclopedia articles Comparing machi.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\KKUUJLGM\\S0169023X22000891.html}
}

@article{alirezarahimiValidatingOntologybasedAlgorithm2014,
  title = {Validating an Ontology-Based Algorithm to Identify Patients with {{Type}} 2 {{Diabetes Mellitus}} in {{Electronic Health Records}}},
  author = {{Alireza Rahimi} and {Siaw-Teng Liaw} and {Jane Taggart} and {Pradeep Ray} and {Hairong Yu}},
  date = {2014-10-01},
  journaltitle = {International Journal of Medical Informatics},
  shortjournal = {International Journal of Medical Informatics},
  volume = {83},
  number = {10},
  pages = {768--778},
  issn = {1386-5056},
  doi = {10.1016/j.ijmedinf.2014.06.002},
  url = {https://www.sciencedirect.com/science/article/pii/S1386505614001038},
  urldate = {2023-04-19},
  abstract = {Background Improving healthcare for people with chronic conditions requires clinical information systems that support integrated care and information exchange, emphasizing a semantic approach to support multiple and disparate Electronic Health Records (EHRs). Using a literature review, the Australian National Guidelines for Type 2 Diabetes Mellitus (T2DM), SNOMED-CT-AU and input from health professionals, we developed a Diabetes Mellitus Ontology (DMO) to diagnose and manage patients with diabetes. This paper describes the manual validation of the DMO-based approach using real world EHR data from a general practice (n=908 active patients) participating in the electronic Practice Based Research Network (ePBRN). Method The DMO-based algorithm to query, using Semantic Protocol and RDF Query Language (SPARQL), the structured fields in the ePBRN data repository were iteratively tested and refined. The accuracy of the final DMO-based algorithm was validated with a manual audit of the general practice EHR. Contingency tables were prepared and Sensitivity and Specificity (accuracy) of the algorithm to diagnose T2DM measured, using the T2DM cases found by manual EHR audit as the gold standard. Accuracy was determined with three attributes – reason for visit (RFV), medication (Rx) and pathology (path) – singly and in combination. Results The Sensitivity and Specificity of the algorithm were 100\% and 99.88\% with RFV; 96.55\% and 98.97\% with Rx; and 15.6\% and 98.92\% with Path. This suggests that Rx and Path data were not as complete or correct as the RFV for this general practice, which kept its RFV information complete and current for diabetes. However, the completeness is good enough for this purpose as confirmed by the very small relative deterioration of the accuracy (Sensitivity and Specificity of 97.67\% and 99.18\%) when calculated for the combination of RFV, Rx and Path. The manual EHR audit suggested that the accuracy of the algorithm was influenced by data quality such as incorrect data due to mistaken units of measurement and unavailable data due to non-documentation or documented in the wrong place or progress notes, problems with data extraction, encryption and data management errors. Conclusion This DMO-based algorithm is sufficiently accurate to support a semantic approach, using the RFV, Rx and Path to define patients with T2DM from EHR data. However, the accuracy can be compromised by incomplete or incorrect data. The extent of compromise requires further study, using ontology-based and other approaches.},
  langid = {english},
  keywords = {Diabetes Mellitus,Electronic Health Records,Ontology,SPARQL,Type 2,Validation studies},
  annotation = {24 citations (Crossref) [2024-03-26]\\
51 citations (Semantic Scholar/DOI) [2023-04-20]\\
abstractTranslation:  背景 改善慢性病患者的醫療保健需要支持綜合護理和信息交換的臨床信息系統，強調語義方法來支持多個不同的電子健康記錄 (EHR)。通過文獻綜述、澳大利亞國家 2 型糖尿病 (T2DM) 指南、SNOMED-CT-AU 以及衛生專業人員的意見，我們開發了糖尿病本體論 (DMO) 來診斷和管理糖尿病患者。本文描述了使用來自參與電子實踐研究網絡 (ePBRN) 的全科診所（n=908 名活躍患者）的真實 EHR 數據對基於 DMO 的方法進行手動驗證。方法基於DMO的查詢算法，使用語義協議和RDF查詢語言(SPARQL)，對ePBRN數據存儲庫中的結構化字段進行迭代測試和細化。最終基於 DMO 的算法的準確性通過對一般實踐 EHR 的手動審核進行了驗證。使用手動 EHR 審核發現的 T2DM 病例作為黃金標準，準備了列聯表，並測量了診斷 T2DM 算法的敏感性和特異性（準確性）。準確性由三個屬性決定——就診原因 (RFV)、藥物 (Rx) 和病理學 (路徑)——單獨或組合。結果 RFV算法的靈敏度和特異度分別為100\%和99.88\%； Rx 為 96.55\% 和 98.97\%；路徑為 15.6\% 和 98.92\%。這表明 Rx 和 Path 數據不如該全科實踐的 RFV 完整或正確，後者保持了糖尿病 RFV 信息的完整性和最新性。然而，對於此目的來說，完整性已經足夠好了，在計算RFV、Rx 和Path 的組合時，精度相對惡化非常小（靈敏度和特異性分別為97.67\% 和99.18\%），這證實了這一點。手動EHR 審核表明，算法的準確性受到數據質量的影響，例如由於測量單位錯誤而導致的錯誤數據，以及由於無文檔或記錄在錯誤的位置或進度註釋而導致的數據不可用、數據提取、加密問題和數據管理錯誤。結論 這種基於 DMO 的算法足夠準確，可以支持語義方法，使用 RFV、Rx 和 Path 從 EHR 數據中定義 T2DM 患者。然而，不完整或不正確的數據可能會影響準確性。妥協的程度需要使用基於本體的方法和其他方法進行進一步研究。\\
titleTranslation: 驗證基於本體的算法以識別電子健康記錄中的 2 型糖尿病患者},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\9VXSDD3X\\Rahimi 等。 - 2014 - Validating an ontology-based algorithm to identify.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\VL5QEEQ3\\S1386505614001038.html}
}

@article{alkholyQuestionAnsweringSystems2018,
  title = {Question {{Answering Systems}}: {{Analysis}} and {{Survey}}},
  shorttitle = {Question {{Answering Systems}}},
  author = {Alkholy, Eman and Haggag, Mohamed and Aboutabl, Amal},
  date = {2018-12-31},
  journaltitle = {International Journal of Computer Science \& Engineering Survey},
  shortjournal = {International Journal of Computer Science \& Engineering Survey},
  volume = {09},
  pages = {1--13},
  doi = {10.5121/ijcses.2018.9601},
  abstract = {computing environment In real world that using a computer to answer questions has been a human dream since the beginning of the digital era, Question-answering systems are referred to as intelligent systems ,that can be used to provide responses for the questions being asked by the user based on certain facts or rules stored in the knowledge base it can generate answers of questions asked in natural , so this survey paper provides an overview on what Question-Answering is and its system ,as well as the previous related research with respect to approaches that were followed. were followed.},
  langid = {english},
  keywords = {Survey,問答系統,已整理,微讀},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 問答系統：分析與調查},
  note = {幾乎完全沒用，對於論文的整理使用表格，因此難以翻譯。
\par
一篇2018年的問答系統survey，詳細說明從20世紀開始關於問答系統的發展及分類，最後簡單說明系統的驗證方式。
\par
大部分內容都是本來就知道的，可能大概或許勉強能作為ˊ問答系統說明的引用來源。},
  file = {C:\Users\BlackCat\Zotero\storage\4GP37MEK\Alkholy 等。 - 2018 - Question Answering Systems Analysis and Survey.pdf}
}

@inproceedings{ammarConstructionLiteratureGraph2018,
  title = {Construction of the {{Literature Graph}} in {{Semantic Scholar}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 3 ({{Industry Papers}})},
  author = {Ammar, Waleed and Groeneveld, Dirk and Bhagavatula, Chandra and Beltagy, Iz and Crawford, Miles and Downey, Doug and Dunkelberger, Jason and Elgohary, Ahmed and Feldman, Sergey and Ha, Vu and Kinney, Rodney and Kohlmeier, Sebastian and Lo, Kyle and Murray, Tyler and Ooi, Hsu-Han and Peters, Matthew and Power, Joanna and Skjonsberg, Sam and Wang, Lucy Lu and Wilhelm, Chris and Yuan, Zheng and family=Zuylen, given=Madeleine, prefix=van, useprefix=true and Etzioni, Oren},
  editor = {Bangalore, Srinivas and Chu-Carroll, Jennifer and Li, Yunyao},
  date = {2018-06},
  pages = {84--91},
  publisher = {Association for Computational Linguistics},
  location = {New Orleans - Louisiana},
  doi = {10.18653/v1/N18-3011},
  url = {https://aclanthology.org/N18-3011},
  urldate = {2023-11-30},
  abstract = {We describe a deployed scalable system for organizing published scientific literature into a heterogeneous graph to facilitate algorithmic manipulation and discovery. The resulting literature graph consists of more than 280M nodes, representing papers, authors, entities and various interactions between them (e.g., authorships, citations, entity mentions). We reduce literature graph construction into familiar NLP tasks (e.g., entity extraction and linking), point out research challenges due to differences from standard formulations of these tasks, and report empirical results for each task. The methods described in this paper are used to enable semantic features in www.semanticscholar.org.},
  eventtitle = {{{NAACL-HLT}} 2018},
  langid = {english},
  keywords = {未整理},
  annotation = {128 citations (Crossref) [2024-03-26]\\
titleTranslation: 語意學者文獻圖譜建構},
  file = {C:\Users\BlackCat\Zotero\storage\ARUIR7MH\Ammar et al. - 2018 - Construction of the Literature Graph in Semantic S.pdf}
}

@inproceedings{an-ziyenPersonalKnowledgeBase2019,
  title = {Personal {{Knowledge Base Construction}} from {{Text-based Lifelogs}}},
  booktitle = {Proceedings of the 42nd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {{An-Zi Yen} and {Hen-Hsen Huang} and {Hsin-Hsi Chen}},
  year = {7 月 18, 2019},
  series = {{{SIGIR}}'19},
  pages = {185--194},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3331184.3331209},
  url = {https://dl.acm.org/doi/10.1145/3331184.3331209},
  urldate = {2023-09-16},
  abstract = {Previous work on lifelogging focuses on life event extraction from image, audio, and video data via wearable sensors. In contrast to wearing an extra camera to record daily life, people are used to log their life on social media platforms. In this paper, we aim to extract life events from textual data shared on Twitter and construct personal knowledge bases of individuals. The issues to be tackled include (1) not all text descriptions are related to life events, (2) life events in a text description can be expressed explicitly or implicitly, (3) the predicates in the implicit events are often absent, and (4) the mapping from natural language predicates to knowledge base relations may be ambiguous. A joint learning approach is proposed to detect life events in tweets and extract event components including subjects, predicates, objects, and time expressions. Finally, the extracted information is transformed to knowledge base facts. The evaluation is performed on a collection of lifelogs from 18 Twitter users. Experimental results show our proposed system is effective in life event extraction, and the constructed personal knowledge bases are expected to be useful to memory recall applications.},
  isbn = {978-1-4503-6172-9},
  langid = {english},
  keywords = {/unread,life event detection,lifelogging,personal knowledge base construction,social media,已整理},
  annotation = {15 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於文本的生活日誌建立個人知識庫\\
abstractTranslation:  先前有關生活記錄的工作主要集中於透過穿戴式感測器從影像、音訊和視訊資料中提取生活事件。與額外佩戴相機來記錄日常生活相比，人們習慣在社群媒體平台上記錄自己的生活。在本文中，我們的目標是從 Twitter 上共享的文字資料中提取生活事件並建立個人的個人知識庫。要解決的問題包括（1）並非所有文本描述都與生活事件相關，（2）文本描述中的生活事件可以顯式或隱式表達，（3）隱式事件中的謂詞往往不存在，以及（ 4）從自然語言謂詞到知識庫關係的映射可能是不明確的。提出了一種聯合學習方法來偵測推文中的生活事件並提取事件組件，包括主詞、謂詞、受詞和時間表達。最後，將提取的資訊轉化為知識庫事實。該評估是針對 18 位 Twitter 用戶的生活日誌集合進行的。實驗結果表明，我們提出的系統在生活事件提取方面是有效的，並且所建構的個人知識庫有望對記憶回憶應用有用。},
  file = {C:\Users\BlackCat\Zotero\storage\D6LFY8U8\Yen 等。 - 2019 - Personal Knowledge Base Construction from Text-bas.pdf}
}

@inproceedings{an-ziyenPersonalKnowledgeBase2021,
  title = {Personal {{Knowledge Base Construction}} from {{Multimodal Data}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Multimedia Retrieval}}},
  author = {{An-Zi Yen} and {Chia-Chung Chang} and {Hen-Hsen Huang} and {Hsin-Hsi Chen}},
  year = {9 月 1, 2021},
  series = {{{ICMR}} '21},
  pages = {496--500},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3460426.3463589},
  url = {https://dl.acm.org/doi/10.1145/3460426.3463589},
  urldate = {2023-09-15},
  abstract = {With the passage of time, people often have misty memories of their past experiences. Information recall support for people by collecting personal lifelogs is emerging. Recently, people tend to record their daily life via filming Video Weblog (VLog), which contains visual and audio data. These large scale multimodal data can be used to support information recall service that enables users to query their past experiences. The challenging issue is the semantic gap between the visual concept and the textual query. In this paper, we aim to extract personal life events from vlogs shared on YouTube and construct a personal knowledge base (PKB) for individuals. A multitask learning model is proposed to extract the components of personal life events, such as subjects, predicates and objects. The evaluation is performed on a video collection from three YouTubers who are English native speakers. Experimental results show our model achieves promising performance.},
  isbn = {978-1-4503-8463-6},
  langid = {english},
  keywords = {/unread,個人知識,已整理,影像辨識,機器學習,重要},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 從多模態資料建立個人知識庫\\
abstractTranslation:  隨著時間的流逝，人們對過去的經驗往往記憶模糊。透過收集個人生活日誌來支持人們的資訊回憶的方式正在興起。近年來，人們傾向於透過拍攝包含視覺和音訊資料的視訊部落格（VLog）來記錄日常生活。這些大規模的多模態資料可用於支援資訊回憶服務，使用戶能夠查詢他們過去的經驗。具有挑戰性的問題是視覺概念和文字查詢之間的語義差距。在本文中，我們的目標是從 YouTube 上共享的影片部落格中提取個人生活事件，並為個人建立個人知識庫（PKB）。提出了一種多任務學習模型來提取個人生活事件的組成部分，例如主詞、謂詞和賓語。該評估是針對三位以英語為母語的 YouTuber 的影片集進行的。實驗結果顯示我們的模型取得了有希望的性能。},
  note = {由於影像中常常出現和字幕不同的東西，因此準確度不佳。},
  file = {C:\Users\BlackCat\Zotero\storage\ASA7PARA\Yen 等。 - 2021 - Personal Knowledge Base Construction from Multimod.pdf}
}

@inproceedings{an-ziyenTenQuestionsLifelog2021,
  title = {Ten {{Questions}} in {{Lifelog Mining}} and {{Information Recall}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Multimedia Retrieval}}},
  author = {{An-Zi Yen} and {Hen-Hsen Huang} and {Hsin-Hsi Chen}},
  year = {9 月 1, 2021},
  series = {{{ICMR}} '21},
  pages = {511--518},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3460426.3463607},
  url = {https://dl.acm.org/doi/10.1145/3460426.3463607},
  urldate = {2023-09-15},
  abstract = {With the advance of science and technology, people are used to recording their daily life events via writing blogs, uploading social media posts, taking photos, or filming videos. Such rich repository personal information is useful for supporting human living assistance, such as information recall service. The main challenges are how to store and manage personal knowledge from various sources, and how to provide support for people who may have difficulty recalling past experiences. In this position paper, we propose a research agenda on personal knowledge mining from various sources of lifelogs, personal knowledge base construction, and information recall for assisting people to recall their experiences. Ten research questions are formulated.},
  isbn = {978-1-4503-8463-6},
  langid = {english},
  keywords = {/unread,information recall,life event extraction,personal knowledge base construction,個人知識,已整理},
  annotation = {5 citations (Crossref) [2024-03-26]\\
titleTranslation: 生活日誌挖掘與資訊回憶中的十個問題\\
abstractTranslation:  隨著科技的進步，人們習慣透過撰寫部落格、上傳社群媒體貼文、拍照或拍攝影片來記錄日常生活事件。如此豐富的個人資訊儲存庫對於支援人類生活援助非常有用，例如資訊召回服務。主要挑戰是如何儲存和管理來自各種來源的個人知識，以及如何為可能難以回憶過去經歷的人提供支援。在這篇立場文件中，我們提出了一個關於從各種生活日誌來源進行個人知識挖掘、個人知識庫建構和幫助人們回憶經歷的資訊回憶的研究議程。制定了十個研究問題。},
  file = {C:\Users\BlackCat\Zotero\storage\KUXE7MPF\Yen 等。 - 2021 - Ten Questions in Lifelog Mining and Information Re.pdf}
}

@inproceedings{an-ziyenUnanswerableQuestionCorrection2022,
  title = {Unanswerable {{Question Correction}} and {{Explanation}} over {{Personal Knowledge Base}}},
  booktitle = {Proceedings of the 31st {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {{An-Zi Yen} and {Hen-Hsen Huang} and {Hsin-Hsi Chen}},
  year = {10 月 17, 2022},
  series = {{{CIKM}} '22},
  pages = {4645--4649},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3511808.3557717},
  url = {https://dl.acm.org/doi/10.1145/3511808.3557717},
  urldate = {2023-09-16},
  abstract = {Handling unanswerable questions in knowledge base question answering (KBQA) has been a focus in recent years. However, how to explain why a given question is unanswerable is rarely discussed. In this work, we seek not only to correct unanswerable questions based on a personal knowledge base, but also to explain the reason of the correction. We argue that different types of questions need heterogeneous subgraphs with different types of connections. We thus propose a heterogeneous subgraph aggregation network with a two-level attention mechanism to detect important entities and relations in subgraphs and attend to informative subgraphs for different questions. We conduct comprehensive experiments on five subgraphs and their combinations, with results that attest the effectiveness of incorporating heterogeneous subgraphs.},
  isbn = {978-1-4503-9236-5},
  langid = {english},
  keywords = {/unread,可解釋性,問答系統,已整理,機器學習,知識本體,重要},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 個人知識庫無法解答的問題修正及解釋\\
abstractTranslation:  處理知識庫問答（KBQA）中無法回答的問題一直是近年來的焦點。然而，如何解釋為什麼給定的問題無法回答卻很少被討論。在這項工作中，我們不僅尋求根據個人知識庫來糾正無法回答的問題，而且還解釋糾正的原因。我們認為不同類型的問題需要具有不同類型連結的異構子圖。因此，我們提出了一種具有兩級注意力機制的異構子圖聚合網絡，以檢測子圖中的重要實體和關係，並關注不同問題的資訊子圖。我們對五個子圖及其組合進行了全面的實驗，結果證明了合併異構子圖的有效性。},
  file = {C:\Users\BlackCat\Zotero\storage\QUTVATKA\Yen 等。 - 2022 - Unanswerable Question Correction and Explanation o.pdf}
}

@inproceedings{anandadasSemanticSegmentationMOOC2020,
  title = {Semantic Segmentation of {{MOOC}} Lecture Videos by Analyzing Concept Change in Domain Knowledge Graph},
  booktitle = {International {{Conference}} on {{Asian Digital Libraries}}},
  author = {{Ananda Das} and {Partha Pratim Das}},
  date = {2020},
  pages = {55--70},
  publisher = {Springer},
  langid = {english},
  keywords = {No DOI found,已整理,影像分析,機器學習,知識圖譜,語意分析},
  annotation = {titleTranslation: 透過分析領域知識圖譜中的概念變化進行MOOC講座影片的語意分割}
}

@inproceedings{andreasharrerVisualizingWikisupportedKnowledge2008,
  title = {Visualizing Wiki-Supported Knowledge Building: Co-Evolution of Individual and Collective Knowledge},
  shorttitle = {Visualizing Wiki-Supported Knowledge Building},
  booktitle = {Proceedings of the 4th {{International Symposium}} on {{Wikis}}},
  author = {{Andreas Harrer} and {Johannes Moskaliuk} and {Joachim Kimmerle} and {Ulrike Cress}},
  year = {9 月 8, 2008},
  series = {{{WikiSym}} '08},
  pages = {1--9},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1822258.1822284},
  url = {https://doi.org/10.1145/1822258.1822284},
  urldate = {2023-09-11},
  abstract = {It is widely accepted that wikis are valuable tools for successful collaborative knowledge building. In this paper, we describe how processes of knowledge building with wikis may be visualized, citing Wikipedia as an example. The underlying theoretical basis of our paper is the framework for collaborative knowledge building with wikis, as introduced by Cress and Kimmerle [2], [3], [4]. This model describes collaborative knowledge building as a co-evolution of individual and collective knowledge, or of cognitive and social systems respectively. These co-evolutionary processes may be visualized graphically, applying methods from social network analysis, especially those methods that take dynamic changes into account [5], [18]. For this purpose, we have undertaken to analyze, on the one hand, the temporal development of an article in the German version of Wikipedia and related articles that are linked to this core article. On the other hand, we analyzed the temporal development of those users who worked on these articles. The resulting graphics show an analogous process, both with regard to the articles that refer to the core article and to the users involved. These results provide empirical support for the co-evolution model. Some implications of our findings and the potential for future research on collaborative knowledge building with wikis and on the application of social network analysis are discussed at the end of the article.},
  isbn = {978-1-60558-128-6},
  langid = {english},
  keywords = {co-evolution,collective knowledge,knowledge building,visualization,wikis},
  annotation = {8 citations (Crossref) [2024-03-26]\\
titleTranslation: 可視化維基支持的知識構建：個人和集體知識的共同進化\\
abstractTranslation:  人們普遍認為維基是成功協作知識構建的寶貴工具。在本文中，我們以維基百科為例，描述瞭如何可視化使用維基構建知識的過程。我們論文的底層理論基礎是 Cress 和 Kimmerle [2]、[3]、[4] 引入的利用 wiki 進行協作知識構建的框架。該模型將協作知識構建描述為個人知識和集體知識或認知系統和社會系統的共同進化。這些共同進化過程可以通過應用社交網絡分析方法，特別是那些考慮動態變化的方法，以圖形方式可視化[5]，[18]。為此，我們一方面分析了維基百科德語版中一篇文章以及與該核心文章鏈接的相關文章的時間發展。另一方面，我們分析了撰寫這些文章的用戶的時間發展。對於引用核心文章的文章和所涉及的用戶而言，生成的圖形顯示了類似的過程。這些結果為協同進化模型提供了實證支持。本文末尾討論了我們的研究結果的一些含義以及未來研究維基協作知識構建和社交網絡分析應用的潛力。},
  file = {C:\Users\BlackCat\Zotero\storage\7AM8EA5H\Harrer 等。 - 2008 - Visualizing wiki-supported knowledge building co-.pdf}
}

@inproceedings{andreasp.schmidtDesigningKnowledgeMaturing2014,
  title = {Designing for Knowledge Maturing: From Knowledge-Driven Software to Supporting the Facilitation of Knowledge Development},
  shorttitle = {Designing for Knowledge Maturing},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Knowledge Technologies}} and {{Data-driven Business}}},
  author = {{Andreas P. Schmidt} and {Christine Kunzmann}},
  year = {9 月 16, 2014},
  series = {I-{{KNOW}} '14},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2637748.2638421},
  url = {https://dl.acm.org/doi/10.1145/2637748.2638421},
  urldate = {2023-08-23},
  abstract = {近年來，通過將與客戶和目標環境的交互理解為一個持續的學習過程，軟件工程已經發生了轉變。結果是對變化的響應能力和以用戶為中心的設計。以類似的方式，知識和本體工程正在經歷根本性的變化，以承認它們是集體知識成熟過程的一部分。我們探討了三個例子：（i）職業指導中基於社交媒體的能力管理，（ii）姑息治療中多專業環境中以本體論為中心的反思，以及（iii）在全科醫生的實踐網絡中調整個人思維線。在此基礎上，我們提煉出知識成熟化設計和相關技術實現的四個層次。},
  isbn = {978-1-4503-2769-5},
  langid = {english},
  keywords = {design processes,knowledge engineering,knowledge management,knowledge maturing},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 知識成熟化設計：從知識驅動軟件到支持促進知識發展\\
abstractTranslation:  近來，通過將與客戶和目標環境的交互理解為一個持續的學習過程，軟件工程已經發生了轉變。結果是對響應能力的變化和以用戶為中心的設計。以類似的方式，知識和本體工程正在經歷根本性的變化，承認它們是集體知識成熟過程的一部分。 探討我們的三個例子：（i）基於社交媒體的能力管理的職業指導，（ii）息治療中多專業環境中本體論為中心思維的全面思考，以及(iii)在醫生的實踐網絡中調整個人路線。在此基礎上，我們提煉出知識成熟化設計和相關技術實現的四個層次。},
  file = {C:\Users\BlackCat\Zotero\storage\4QLX8T9R\Schmidt 與 Kunzmann - 2014 - Designing for knowledge maturing from knowledge-d.pdf}
}

@inproceedings{andreasschmidInfluenceAnnotationMedia2023,
  title = {Influence of {{Annotation Media}} on {{Proof-Reading Tasks}}},
  booktitle = {Proceedings of {{Mensch}} Und {{Computer}} 2023},
  author = {{Andreas Schmid} and {Marie Sautmann} and {Vera Wittmann} and {Florian Kaindl} and {Philipp Schauhuber} and {Philipp Gottschalk} and {Raphael Wimmer}},
  year = {9 月 3, 2023},
  series = {{{MuC}} '23},
  pages = {277--288},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3603555.3603572},
  url = {https://dl.acm.org/doi/10.1145/3603555.3603572},
  urldate = {2023-09-11},
  abstract = {Annotating and proof-reading documents are common tasks. Digital annotation tools provide easily searchable annotations and facilitate sharing documents and remote collaboration with others. On the other hand, advantages of paper, such as creative freedom and intuitive use, can get lost when annotating digitally. There is a large amount of research indicating that paper outperforms digital annotation tools in task time, error recall and task load. However, most research in this field is rather old and does not take into consideration increasing screen resolution and performance, as well as better input techniques in modern devices. We present three user studies comparing different annotation media in the context of proof-reading tasks. We found that annotating on paper is still faster and less stressful than with a PC or tablet computer, but the difference is significantly smaller with a state-of-the-art device. We did not find a difference in error recall, but the used medium has a strong influence on how users annotate.},
  isbn = {9798400707711},
  langid = {english},
  keywords = {annotaion,digitalization,proof-reading,使用者研究,回收},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 註釋媒體對校對任務的影響\\
abstractTranslation:  註釋和校對文檔是常見的任務。數字註釋工具提供易於搜索的註釋，並促進與他人共享文檔和遠程協作。另一方面，在進行數字註釋時，紙張的優勢（例如創作自由和直觀使用）可能會消失。大量研究表明，紙張在任務時間、錯誤回憶和任務負載方面優於數字註釋工具。然而，該領域的大多數研究都相當陳舊，並且沒有考慮提高屏幕分辨率和性能，以及現代設備中更好的輸入技術。我們提出了三項用戶研究，在校對任務的背景下比較不同的註釋媒體。我們發現，與使用 PC 或平板電腦相比，在紙上進行註釋仍然更快、壓力更小，但使用最先進的設備時，差異要小得多。我們沒有發現錯誤回憶方面存在差異，但所使用的媒介對用戶的註釋方式有很大影響。},
  note = {研究在現代高科技產品下，使用電子產品批改文件與紙本批改的差異。非研究範圍。},
  file = {C:\Users\BlackCat\Zotero\storage\5FBNETKU\Schmid 等。 - 2023 - Influence of Annotation Media on Proof-Reading Tas.pdf}
}

@article{andregomesreginoLinkMaintenanceIntegrity2021,
  title = {Link Maintenance for Integrity in Linked Open Data Evolution: {{Literature}} Survey and Open Challenges},
  shorttitle = {Link Maintenance for Integrity in Linked Open Data Evolution},
  author = {{Andre Gomes Regino} and {Julio Cesar Dos Reis} and {Rodrigo Bonacin} and {Ahsan Morshed} and {Timos Sellis}},
  editor = {{Oscar Corcho}},
  date = {2021-03-09},
  journaltitle = {Semantic Web},
  shortjournal = {SW},
  volume = {12},
  number = {3},
  pages = {517--541},
  issn = {22104968, 15700844},
  doi = {10.3233/SW-200398},
  url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-200398},
  urldate = {2023-09-28},
  abstract = {RDF data has been extensively deployed describing various types of resources in a structured way. Links between data elements described by RDF models stand for the core of Semantic Web. The rising amount of structured data published in public RDF repositories, also known as Linked Open Data, elucidates the success of the global and unified dataset proposed by the vision of the Semantic Web. Nowadays, semi-automatic algorithms build connections among these datasets by exploring a variety of methods. Interconnected open data demands automatic methods and tools to maintain their consistency over time. The update of linked data is considered as key process due to the evolutionary characteristic of such structured datasets. However, data changing operations might influence well-formed links, which turns difficult to maintain the consistencies of connections over time. In this article, we propose a thorough survey that provides a systematic review of the state of the art in link maintenance in linked open data evolution scenario. We conduct a detailed analysis of the literature for characterising and understanding methods and algorithms responsible for detecting, fixing and updating links between RDF data. Our investigation provides a categorisation of existing approaches as well as describes and discusses existing studies. The results reveal an absence of comprehensive solutions suited to fully detect, warn and automatically maintain the consistency of linked data over time.},
  langid = {english},
  keywords = {Link integrity,link maintenance,RDF evolution,未整理,重要},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 連結開放資料演化中完整性的連結維護：文獻調查與開放挑戰\\
abstractTranslation:  RDF 資料已被廣泛部署，以結構化方式描述各種類型的資源。 RDF模型所描述的資料元素之間的連結代表了語意Web的核心。公共 RDF 儲存庫中發布的結構化資料（也稱為連結開放資料）數量的不斷增加，闡明了語義 Web 願景提出的全球統一資料集的成功。如今，半自動演算法透過探索各種方法在這些數據集之間建立聯繫。互連的開放資料需要自動方法和工具來保持其隨時間的一致性。由於此類結構化資料集的演化特徵，連結資料的更新被認為是關鍵過程。然而，資料變更操作可能會影響格式良好的鏈接，這使得隨著時間的推移很難維持連接的一致性。在本文中，我們提出了一項全面的調查，對連結開放資料演進場景中連結維護的最新技術進行了系統性回顧。我們對文獻進行了詳細分析，以描述和理解負責檢測、修復和更新 RDF 數據之間連結的方法和演算法。我們的調查提供了現有方法的分類，並描述和討論了現有的研究。結果表明，缺乏適合完全檢測、警告和自動維護連結資料隨時間變化的一致性的綜合解決方案。},
  file = {C:\Users\BlackCat\Zotero\storage\D5I9LUHF\Regino et al. - 2021 - Link maintenance for integrity in linked open data.pdf}
}

@inproceedings{anniezaenenLocalTextualInference2005,
  title = {Local {{Textual Inference}}: {{Can}} It Be {{Defined}} or {{Circumscribed}}?},
  shorttitle = {Local {{Textual Inference}}},
  booktitle = {Proceedings of the {{ACL Workshop}} on {{Empirical Modeling}} of {{Semantic Equivalence}} and {{Entailment}}},
  author = {{Annie Zaenen} and {Lauri Karttunen} and {Richard Crouch}},
  date = {2005-06},
  pages = {31--36},
  publisher = {Association for Computational Linguistics},
  location = {Ann Arbor, Michigan},
  url = {https://aclanthology.org/W05-1206},
  urldate = {2023-09-19},
  langid = {english},
  keywords = {未整理,知識推理},
  annotation = {titleTranslation: 局部文本推理：它可以被定義或限制嗎？},
  file = {C:\Users\BlackCat\Zotero\storage\3RDFBHZB\Zaenen 等。 - 2005 - Local Textual Inference Can it be Defined or Circ.pdf}
}

@inproceedings{antoniozaitounOntoEvalAutomatedOntology2023,
  title = {{{OntoEval}}: An {{Automated Ontology Evaluation System}}},
  shorttitle = {{{OntoEval}}},
  booktitle = {Companion {{Proceedings}} of the {{ACM Web Conference}} 2023},
  author = {{Antonio Zaitoun} and {Tomer Sagi} and {Katja Hose}},
  year = {4 月 30, 2023},
  series = {{{WWW}} '23 {{Companion}}},
  pages = {82--85},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3543873.3587318},
  url = {https://dl.acm.org/doi/10.1145/3543873.3587318},
  urldate = {2023-09-15},
  abstract = {開發語義感知的 Web 服務需要全面且準確的本體。評估現有本體或對其進行調整是一項勞動密集型且複雜的任務，不存在自動化工具。儘管如此，在本文中，我們提出了一種旨在實現這一願景的工具，即，我們提出了一種用於本體自動評估的工具，它允許人們快速評估本體對某個領域的覆蓋範圍並識別本體結構中的具體問題。該工具根據從代表該領域的文本語料庫中派生的領域信息來評估給定本體的領域覆蓋率和父子關係的正確性。該工具提供了本體的整體統計和子圖的詳細分析。在演示中，},
  isbn = {978-1-4503-9419-2},
  langid = {english},
  keywords = {/unread,BERT,人機互動,已整理,文字處理,知識本體},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: OntoEval：自動化本體評估系統\\
abstractTranslation:  開發語意感知的Web服務需要全面且準確的本體。評估現有本體或對其進行調整是一個勞動密集且複雜的任務，不存在自動化工具。儘管如此，在本文中，我們提出了一個目的實現這個願景的工具，即，我們提出了一種用於本體自動評估的工具，它允許快速評估本體對某個領域人們的覆蓋範圍並識別本體結構中的具體問題。該工具根據代表該領域的文本語料庫中派生的領域資訊來評估給定本體的領域覆蓋率和該父子關係的正確性。工具提供了本體的整體統計和子圖的詳細分析。在演講中,},
  note = {通過領域語料庫(例如大量文獻)提取出實體，並以bert產生子圖，判斷待測ontology覆蓋了多少子圖。概讀，程式碼未公開。},
  file = {C:\Users\BlackCat\Zotero\storage\UK9K6QPY\Zaitoun 等。 - 2023 - OntoEval an Automated Ontology Evaluation System.pdf}
}

@article{aparajithansivanathanNovelDesignEngineering2017,
  title = {A Novel Design Engineering Review System with Searchable Content: Knowledge Engineering via Real-Time Multimodal Recording},
  shorttitle = {A Novel Design Engineering Review System with Searchable Content},
  author = {{Aparajithan Sivanathan} and {James M. Ritchie} and {Theodore Lim}},
  date = {2017},
  journaltitle = {JOURNAL OF ENGINEERING DESIGN},
  shortjournal = {J. Eng. Des.},
  volume = {28},
  number = {10-12},
  pages = {681--708},
  publisher = {Taylor \& Francis Ltd},
  location = {Abingdon},
  issn = {0954-4828, 1466-1837},
  doi = {10.1080/09544828.2017.1393655},
  url = {https://www.webofscience.com/api/gateway?GWVersion=2&SrcAuth=DOISource&SrcApp=WOS&KeyAID=10.1080%2F09544828.2017.1393655&DestApp=DOI&SrcAppSID=EUW1ED0B0Eh1M6RufxhQfBCxYjBK2&SrcJTitle=JOURNAL+OF+ENGINEERING+DESIGN&DestDOIRegistrantName=Informa+UK+%28Taylor+%26+Francis%29},
  urldate = {2023-10-16},
  abstract = {Cradle to grave product support has been a key issue in the engineering sector over many years, particularly because product engineering legacy knowledge is often lost during the product development process unless rigorously captured in some way. This is particularly the case during formal design reviews at any point during a product's life-cycle where engineering changes are not fully documented or where salient but important aspects of decision making are difficult to document explicitly. Though many software systems are available to support design reviews, they have not necessarily met the expectations of industry. Consequently, traditional knowledge capture methods tend to be time-consuming, costly and disruptive leading to many companies simply giving up on this crucial aspect of product development. This paper presents research carried out with regard to prototyping and testing a potential knowledge engineering capture and reuse solution, demonstrating real-time user-logging using virtual design environments focused on team-based design 'reviews'. Called the Virtual Aided Design Engineering Review (VADER) system, it provides millisecond precision time-phased knowledge capture in an automatic and unobtrusive manner. Both structured and unstructured data are synthesised via a ubiquitous integration and temporal synchronisation (UbiITS) framework that enables interactive information mapping, retrieval and mining. VADER's frontend includes a virtual reality based 3D model view display as a multiuser collaborative interface and an auxiliary web interface for concurrent access by multiple distributed users during product design discussions. Feedback from engineers using the system demonstrated that this concept is one which believe would substantially enhance their engineering task knowledge capture, rapid retrieval and reuse capability. It was also surmised that, if required, such a system can be extended throughout the whole product development process capturing individual and team-based engineers' inputs across the whole cradle-to-grave product life cycle. Also, due to its generic nature, this approach is not limited only to engineering applications or virtual environments but can potentially be used in other sectors using computer-based technologies of any kind.},
  langid = {english},
  pagetotal = {28},
  keywords = {cyber-physical systems,design review,industry 4.0,Knowledge capture,knowledge engineering,multimodal time synchronisation,PRIVACY,product lifecycle management,SUPPORT,user logging,WORKPLACE,已整理,待讀,微讀,數據隱私,文獻,知識管理,研究流程,被引用,重要},
  annotation = {15 citations (Crossref) [2024-03-26]\\
Web of Science ID: WOS:000416834900004\\
titleTranslation: 具有可搜尋內容的新穎設計工程評審系統：透過即時多模式記錄的知識工程},
  note = {開發一個結合CAD、錄音錄影、註解、多平台協作的產品生命週期管理系統。通過採訪七家廠商了解需求，並最終將產品交給廠商試用來驗證系統的可用性。包含許多設計細節但論文較長，可以之後再看?
\par
此外還考量到數據的隱私問題。相當值得研究。},
  file = {D:\Paper\Sivanathan et al. - 2017 - A novel design engineering review system with sear.pdf}
}

@article{ArchitecturesFrameworksModels2016,
  title = {On {{Architectures}}, {{Frameworks}}, and {{Models}} in {{Thesis Writing}} for {{Computer Science}} | {{Journal}} of {{IT}} in {{Asia}}},
  date = {2016-12-21},
  url = {https://publisher.unimas.my/ojs/index.php/JITA/article/view/318},
  urldate = {2023-10-20},
  abstract = {Thesis writing can be a very complicated, confusing, and exhausting process. This paper looks at the why, how and when in using architectures, frameworks and models for thesis writing in computer science and related domains, as well as achieving clarity in thesis writing in general, with illustrations and examples. The underlying objects in computer science are data and processes, and these are put together into systems and applications. Architectures and frameworks are used to places all the components required to implement systems and applications, while models are used to mimic behaviour, and can then be used to explain, control, and predict. Good expositions of the relevant architectures, frameworks and models in a computer science thesis (or project) would then go a long way in situating the project, emphaising the main contributions, as well as determining what would be left for future work – the basic ingredient for thesis writing (or project reports).},
  langid = {american},
  annotation = {titleTranslation: 论计算机科学论文写作中的架构、框架和模型 | 《亚洲信息技术》杂志\\
abstractTranslation:  论文写作可能是一个非常复杂、令人困惑和疲惫的过程。本文通过图解和示例，探讨了在计算机科学及相关领域的论文写作中使用架构、框架和模型的原因、方法和时机，以及如何在一般论文写作中做到清晰明了。计算机科学的基础对象是数据和流程，这些对象被组合成系统和应用程序。架构和框架用于放置实现系统和应用程序所需的所有组件，而模型则用于模仿行为，然后可用于解释、控制和预测。在计算机科学论文（或项目）中对相关架构、框架和模型进行良好的阐述，将大大有助于确定项目的位置，突出主要贡献，并确定未来工作的重点--这是论文写作（或项目报告）的基本要素。},
  file = {C:\Users\BlackCat\Zotero\storage\JWRGYBRQ\2016 - On Architectures, Frameworks, and Models in Thesis.pdf}
}

@inproceedings{arianiindrawatiKnowledgeGraphExploration2023,
  title = {A {{Knowledge Graph Exploration Method}} with {{No Prior Knowledge}}},
  booktitle = {Proceedings of the 2022 {{International Conference}} on {{Computer}}, {{Control}}, {{Informatics}} and {{Its Applications}}},
  author = {{Ariani Indrawati} and {Zaenal Akbar} and {Dwi Setyo Rini} and {Aris Yaman} and {Yulia Aris Kartika} and {Dadan Ridwan Saleh}},
  year = {2 月 27, 2023},
  series = {{{IC3INA}} '22},
  pages = {184--188},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3575882.3575918},
  url = {https://dl.acm.org/doi/10.1145/3575882.3575918},
  urldate = {2023-09-17},
  abstract = {Various sectors now widely adopt knowledge graphs to describe and share their organizational knowledge bases. Unfortunately, the majority of knowledge-sharing systems are designed for domain experts. Making it extremely difficult for a non-expert to understand the content and explore the graph. A solution to this issue is using a machine-assisted knowledge graph exploration approach. This research introduces a knowledge exploration method to systematically and efficiently navigate a knowledge graph. First, we modeled the knowledge graphs based on the existing common schema. Second, we created a search tree technique to navigate the knowledge graph efficiently. The algorithm solves the problem by determining the path of knowledge graph exploration. We evaluated the method using a knowledge base of morphological characteristics of Capsicum. The goal of graph exploration was to identify a Capsicum species correctly. As a result, the proposed mechanism can achieve high precision, even when the search’s starting point is unknown beforehand.},
  isbn = {978-1-4503-9790-2},
  langid = {english},
  keywords = {Capsicum,knowledge base,knowledge graph,ontology,人機互動,已整理,待讀,機器學習,知識圖譜},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 一種無先驗知識的知識圖探索方法\\
abstractTranslation:  現在各部門廣泛採用知識圖譜來描述和分享其組織知識庫。不幸的是，大多數知識共享系統都是為領域專家設計的。使得非專家很難理解內容並探索圖表。這個問題的解決方案是使用機器輔助的知識圖探索方法。本研究引進了一種知識探索方法來系統化、有效地導航知識圖譜。首先，我們根據現有的通用模式對知識圖進行建模。其次，我們創建了一種搜尋樹技術來有效地導航知識圖。該演算法透過確定知識圖譜探索路徑來解決問題。我們使用辣椒形態特徵的知識庫對該方法進行了評估。圖探索的目標是正確辨識辣椒種類。因此，即使搜尋的起點事先未知，所提出的機制也可以實現高精度。},
  file = {C:\Users\BlackCat\Zotero\storage\X27XTFYC\Indrawati 等。 - 2023 - A Knowledge Graph Exploration Method with No Prior.pdf}
}

@article{AssessingHiddenRisks2023,
  title = {Assessing {{Hidden Risks}} of {{LLMs}}: {{An Empirical Study}} on {{Robustness}}, {{Consistency}}, and {{Credibility}}},
  shorttitle = {Assessing {{Hidden Risks}} of {{LLMs}}},
  date = {2023-05-15},
  publisher = {arXiv (Cornell University)},
  doi = {10.48550/arxiv.2305.10235},
  url = {https://typeset.io/papers/assessing-hidden-risks-of-llms-an-empirical-study-on-1hbhb3bn},
  urldate = {2024-04-16},
  abstract = {The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite uncommon from this trendy community. Briefly, they are: (i)-the minor but inevitable error occurrence in the user-generated query input may, by chance, cause the LLM to respond unexpectedly; (ii)-LLMs possess poor consistency when processing semantically similar query input. In addition, as a side finding, we find that ChatGPT is still capable to yield the correct answer even when the input is polluted at an extreme level. While this phenomenon demonstrates the powerful memorization of the LLMs, it raises serious concerns about using such data for LLM-involved evaluation in academic development. To deal with it, we propose a novel index associated with a dataset that roughly decides the feasibility of using such data for LLM-involved evaluation. Extensive empirical studies are tagged to support the aforementioned claims.},
  langid = {english},
  keywords = {未整理},
  annotation = {titleTranslation: 評估法學碩士的隱藏風險：穩健性、一致性和可信度的實證研究\\
abstractTranslation:  最近流行的大型語言模型（LLM）為無限的領域帶來了重大影響，特別是透過其開放式生態系統，如 API、開源模型和插件。然而，隨著其廣泛部署，普遍缺乏對其隱藏的潛在風險進行深入討論和分析的研究。在這種情況下，我們打算進行一項初步但開創性的研究，涵蓋法學碩士系統的穩健性、一致性和可信度。鑑於法學碩士時代的大多數相關文獻都是未知的，我們提出了一種自動化工作流程，可以應對大量的查詢/回應。總體而言，我們對主流 LLM 進行了超過一百萬次查詢，包括 ChatGPT、LLaMA 和 OPT。我們工作流程的核心包括一個資料原語，然後是一個自動解釋器，該解釋器在不同的對抗性度量系統下評估這些法學碩士。結果，我們得出了幾個可能是不幸的結論，這些結論在這個時尚社群中是非常不常見的。簡而言之，它們是： (i)-使用者產生的查詢輸入中出現的輕微但不可避免的錯誤可能會偶然導致 LLM 意外回應； (ii)-LLM 在處理語意相似的查詢輸入時一致性較差。此外，作為一個側面發現，我們發現即使輸入受到極端程度的污染，ChatGPT 仍然能夠產生正確的答案。雖然這種現象證明了法學碩士的強大記憶力，但它引起了人們對將此類數據用於法學碩士涉及的學術發展評估的嚴重擔憂。為了解決這個問題，我們提出了一種與資料集相關的新穎索引，該索引粗略地決定了使用此類資料進行法學碩士相關評估的可行性。廣泛的實證研究被標記為支持上述主張。},
  file = {C:\Users\BlackCat\Zotero\storage\QXVB7FSU\2023 - Assessing Hidden Risks of LLMs An Empirical Study.pdf}
}

@article{atta-ur-rahmanNeurofuzzyApproachUser2019,
  title = {A {{Neuro-fuzzy}} Approach for User Behaviour Classification and Prediction},
  author = {{Atta-ur-Rahman} and {Sujata Dash} and {Ashish Kr. Luhach} and {Naveen Chilamkurti} and {Seungmin Baek} and {Yunyoung Nam}},
  date = {2019-11-21},
  journaltitle = {Journal of Cloud Computing},
  shortjournal = {Journal of Cloud Computing},
  volume = {8},
  number = {1},
  pages = {17},
  issn = {2192-113X},
  doi = {10.1186/s13677-019-0144-9},
  url = {https://doi.org/10.1186/s13677-019-0144-9},
  urldate = {2023-10-31},
  abstract = {Big data and cloud computing technology appeared on the scene as new trends due to the rapid growth of social media usage over the last decade. Big data represent the immense volume of complex data that show more details about behaviours, activities, and events that occur around the world. As a result, big data analytics needs to access diverse types of resources within a decreased response time to produce accurate and stable business experimentation that could help make brilliant decisions for organizations in real-time. These developments have spurred a revolutionary transformation in research, inventions, and business marketing. User behaviour analysis for classification and prediction is one of the hottest topics in data science. This type of analysis is performed for several purposes, such as finding users’ interests about a product (for marketing, e-commerce, etc.) or toward an event (elections, championships, etc.) and observing suspicious activities (security and privacy) based on their traits over the Internet. In this paper, a neuro-fuzzy approach for the classification and prediction of user behaviour is proposed. A dataset, composed of users’ temporal logs containing three types of information, namely, local machine, network and web usage logs, is targeted. To complement the analysis, each user’s 360-degree feedback is also utilized. Various rules have been implemented to address the company’s policy for determining the precise behaviour of a user, which could be helpful in managerial decisions. For prediction, a Gaussian Radial Basis Function Neural Network (GRBF-NN) is trained based on the example set generated by a Fuzzy Rule Based System (FRBS) and the 360-degree feedback of the user. The results are obtained and compared with other state-of-the-art schemes in the literature, and the scheme is found to be promising in terms of classification as well as prediction accuracy.},
  langid = {english},
  keywords = {360-degree feedback,Behaviour analysis,Big data,Classification,Cloud computing,FRBS,Neuro-fuzzy,Prediction,已整理,數學,模糊理論,被引用},
  annotation = {36 citations (Crossref) [2024-03-26]\\
titleTranslation: 用於使用者行為分類和預測的神經模糊方法},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\Y4HJG4HF\\Atta-ur-Rahman et al. - 2019 - A Neuro-fuzzy approach for user behaviour classifi.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\CI9LXSTX\\s13677-019-0144-9.html}
}

@article{auerSciQAScientificQuestion2023,
  title = {The {{SciQA Scientific Question Answering Benchmark}} for {{Scholarly Knowledge}}},
  author = {Auer, Sören and Barone, Dante A. C. and Bartz, Cassiano and Cortes, Eduardo G. and Jaradeh, Mohamad Yaser and Karras, Oliver and Koubarakis, Manolis and Mouromtsev, Dmitry and Pliukhin, Dmitrii and Radyush, Daniil and Shilin, Ivan and Stocker, Markus and Tsalapati, Eleni},
  date = {2023-05-04},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {13},
  number = {1},
  pages = {7240},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-33607-z},
  url = {https://www.nature.com/articles/s41598-023-33607-z},
  urldate = {2023-12-02},
  abstract = {Knowledge graphs have gained increasing popularity in the last decade in science and technology. However, knowledge graphs are currently relatively simple to moderate semantic structures that are mainly a collection of factual statements. Question answering (QA) benchmarks and systems were so far mainly geared towards encyclopedic knowledge graphs such as DBpedia and Wikidata. We present SciQA a scientific QA benchmark for scholarly knowledge. The benchmark leverages the Open Research Knowledge Graph (ORKG) which includes almost 170,000 resources describing research contributions of almost 15,000 scholarly articles from 709 research fields. Following a bottom-up methodology, we first manually developed a set of 100 complex questions that can be answered using this knowledge graph. Furthermore, we devised eight question templates with which we automatically generated further 2465 questions, that can also be answered with the ORKG. The questions cover a range of research fields and question types and are translated into corresponding SPARQL queries over the ORKG. Based on two preliminary evaluations, we show that the resulting SciQA benchmark represents a challenging task for next-generation QA systems. This task is part of the open competitions at the 22nd International Semantic Web Conference 2023 as the Scholarly Question Answering over Linked Data (QALD) Challenge.},
  issue = {1},
  langid = {english},
  keywords = {Computer science,Information technology,KBQA,ORKG,Scientific data,問答系統,學術,已整理,待讀,機器學習,知識圖譜,資料集,重要},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: SciQA 學術知識科學問答基準\\
abstractTranslation:  過去十年，知識圖在科學技術領域越來越受歡迎。然而，知識圖譜目前相對簡單到適度的語意結構，主要是事實陳述的集合。迄今為止，問答（QA）基準和系統主要面向百科全書式知識圖譜，例如 DBpedia 和 Wikidata。我們提出了 SciQA 學術知識的科學 QA 基準。本基準利用開放研究知識圖譜 (ORKG)，其中包括近 170,000 個資源，描述來自 709 個研究領域的近 15,000 篇學術文章的研究貢獻。按照自下而上的方法，我們首先手動開發了一組 100 個複雜問題，可以使用此知識圖來回答這些問題。此外，我們設計了 8 個問題模板，透過它們自動產生了另外 2465 個問題，這些問題也可以透過 ORKG 來回答。這些問題涵蓋了一系列研究領域和問題類型，並透過 ORKG 轉換為相應的 SPARQL 查詢。基於兩個初步評估，我們表明所得出的 SciQA 基準對於下一代 QA 系統來說是一項具有挑戰性的任務。該任務是 2023 年第 22 屆國際語意網會議公開競賽的一部分，即連結資料學術問答 (QALD) 挑戰賽。},
  file = {C:\Users\BlackCat\Zotero\storage\JXWM29Z9\Auer et al. - 2023 - The SciQA Scientific Question Answering Benchmark .pdf}
}

@online{badralkhamissiReviewLanguageModels2022,
  title = {A {{Review}} on {{Language Models}} as {{Knowledge Bases}}},
  author = {{Badr AlKhamissi} and {Millicent Li} and {Asli Celikyilmaz} and {Mona Diab} and {Marjan Ghazvininejad}},
  date = {2022-04-12},
  eprint = {2204.06031},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.06031},
  url = {http://arxiv.org/abs/2204.06031},
  urldate = {2023-09-15},
  abstract = {Recently, there has been a surge of interest in the NLP community on the use of pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have shown that LMs trained on a sufficiently large (web) corpus will encode a significant amount of knowledge implicitly in its parameters. The resulting LM can be probed for different kinds of knowledge and thus acting as a KB. This has a major advantage over traditional KBs in that this method requires no human supervision. In this paper, we present a set of aspects that we deem a LM should have to fully act as a KB, and review the recent literature with respect to those aspects.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,LLM,Review,已整理,知識管理,重要,預印本},
  annotation = {titleTranslation: 作為知識庫的語言模型綜述\\
abstractTranslation:  最近，NLP 社群對使用預訓練語言模型 (LM) 作為知識庫 (KB) 的興趣激增。研究人員表明，在足夠大的（網路）語料庫上訓練的語言模型將在其參數中隱式編碼大量知識。可以探究產生的 LM 來獲取不同類型的知識，從而充當 KB。與傳統知識庫相比，這種方法的一個主要優點是不需要人工監督。在本文中，我們提出了一系列我們認為 LM 應該完全充當知識庫的方面，並回顧了與這些方面相關的最新文獻。},
  note = {說明為甚麼LLM可以作為KB},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\TI78VXMA\\AlKhamissi 等。 - 2022 - A Review on Language Models as Knowledge Bases.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\I3HBYHRD\\2204.html}
}

@online{baekKnowledgeAugmentedLanguageModel2023,
  title = {Knowledge-{{Augmented Language Model Prompting}} for {{Zero-Shot Knowledge Graph Question Answering}}},
  author = {Baek, Jinheon and Aji, Alham Fikri and Saffari, Amir},
  date = {2023-06-07},
  eprint = {2306.04136},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.04136},
  url = {http://arxiv.org/abs/2306.04136},
  urldate = {2024-02-22},
  abstract = {Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user's question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48\% in average, across multiple LLMs of various sizes.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,LLM,問答系統,待讀,知識圖譜,重要},
  annotation = {titleTranslation: 知識增強語言模型提示零樣本知識圖問答\\
abstractTranslation:  大型語言模型 (LLM) 能夠根據預訓練期間儲存在參數中的內部知識來執行零樣本閉卷問答任務。然而，這種內化的知識可能是不充分和不正確的，這可能導致法學碩士產生事實上錯誤的答案。此外，微調法學碩士以更新他們的知識是昂貴的。為此，我們建議直接在法學碩士的輸入中增加知識。具體來說，我們首先根據問題及其相關事實之間的語義相似性從知識圖中檢索與輸入問題相關的事實。之後，我們以提示的形式將檢索到的事實添加到輸入問題中，然後將其轉發給法學碩士以產生答案。我們的框架知識增強語言模型 PromptING (KAPING) 不需要模型訓練，因此完全是零樣本。我們在知識圖問答任務上驗證了KAPING 框架的性能，該框架旨在根據知識圖上的事實回答用戶的問題，在該任務上，我們的框架在多個方面平均優於相關的零樣本基線高達48\%各種規模的法學碩士。},
  note = {知識獲取: 將問題中的三元組抽出
\par
知識表達: 將三元組轉成文本
\par
知識注入: 將知識圖譜中前N個三元組交給問答系統},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\ZMVT9UU7\\Baek et al. - 2023 - Knowledge-Augmented Language Model Prompting for Z.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\XQWYW6E4\\2306.html}
}

@online{bainWhisperXTimeAccurateSpeech2023,
  title = {{{WhisperX}}: {{Time-Accurate Speech Transcription}} of {{Long-Form Audio}}},
  shorttitle = {{{WhisperX}}},
  author = {Bain, Max and Huh, Jaesung and Han, Tengda and Zisserman, Andrew},
  date = {2023-07-11},
  eprint = {2303.00747},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2303.00747},
  url = {http://arxiv.org/abs/2303.00747},
  urldate = {2024-02-23},
  abstract = {Large-scale, weakly-supervised speech recognition models, such as Whisper, have demonstrated impressive results on speech recognition across domains and languages. However, their application to long audio transcription via buffered or sliding window approaches is prone to drifting, hallucination \& repetition; and prohibits batched transcription due to their sequential nature. Further, timestamps corresponding each utterance are prone to inaccuracies and word-level timestamps are not available out-of-the-box. To overcome these challenges, we present WhisperX, a time-accurate speech recognition system with word-level timestamps utilising voice activity detection and forced phoneme alignment. In doing so, we demonstrate state-of-the-art performance on long-form transcription and word segmentation benchmarks. Additionally, we show that pre-segmenting audio with our proposed VAD Cut \& Merge strategy improves transcription quality and enables a twelve-fold transcription speedup via batched inference.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Whisper,已整理,文獻,語音辨識,重要},
  annotation = {titleTranslation: WhisperX：長格式音訊的時間精確語音轉錄},
  note = {Comment: Accepted to INTERSPEECH 2023},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\H3K8JY2G\\Bain 等。 - 2023 - WhisperX Time-Accurate Speech Transcription of Lo.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\HN3GR3S5\\2303.html}
}

@inproceedings{bamfaceesayNTNUUnsupervisedKnowledge2015,
  title = {{{NTNU}}: {{An Unsupervised Knowledge Approach}} for {{Taxonomy Extraction}}},
  shorttitle = {{{NTNU}}},
  booktitle = {Proceedings of the 9th {{International Workshop}} on {{Semantic Evaluation}} ({{SemEval}} 2015)},
  author = {{Bamfa Ceesay} and {Wen Juan Hou}},
  date = {2015-06},
  pages = {938--943},
  publisher = {Association for Computational Linguistics},
  location = {Denver, Colorado},
  doi = {10.18653/v1/S15-2156},
  url = {https://aclanthology.org/S15-2156},
  urldate = {2023-09-19},
  eventtitle = {{{SemEval}} 2015},
  langid = {english},
  keywords = {未整理},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: NTNU：一種用於分類提取的無監督知識方法},
  file = {C:\Users\BlackCat\Zotero\storage\YBW6H3WB\Ceesay 與 Juan Hou - 2015 - NTNU An Unsupervised Knowledge Approach for Taxon.pdf}
}

@thesis{BaoWeiRenZhongYiMenZhenQianFuZhuXiTong2020,
  title = {中醫門診前輔助系統},
  author = {{鮑惟仁}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2020},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/693yq5},
  abstract = {台灣全民健康保險的實施促進醫院就診人次穩定逐年成長。就診人次的增加造成看診時間的縮減及看診品質的下降。為了因應這種穩定成長的趨勢，本論文開發一個中醫門診前輔助系統，來減緩看診品質的下降。利用病患候診的時間，中醫門診前輔助系統可用來預先收集病患的症狀。根據預先收集的症狀，中醫門診前輔助系統將為醫生自動產生初步的診斷結果及治療建議，並為病患自動產生初步的診斷結果及衛教資訊。有了初步的診斷結果、治療建議及衛教資訊，醫生和病患在非常有限的看診時間，就可以進行較高品質的看診。中醫門診前輔助系統使用回應式網頁設計技術，並奠基於本研究團隊先前開發的中醫診斷治療資訊系統。中醫門診前輔助系統已在大林慈濟醫院使用。初步效能評估顯示，在非常有限的看診時間，中醫門診前輔助系統對提高看診品質是有幫助的。},
  pagetotal = {52},
  keywords = {中醫,實驗室,已讀},
  file = {C:\Users\BlackCat\Zotero\storage\FUHJRR6S\鮑惟仁 - 2020 - 中醫門診前輔助系統.pdf}
}

@article{barbarablummerReducingPatronInformation2014,
  title = {Reducing {{Patron Information Overload}} in {{Academic Libraries}}},
  author = {{Barbara Blummer} and {Jeffrey M. Kenton}},
  date = {2014-04-03},
  journaltitle = {College \& Undergraduate Libraries},
  shortjournal = {College \& Undergraduate Libraries},
  volume = {21},
  number = {2},
  pages = {115--135},
  issn = {1069-1316, 1545-2530},
  doi = {10.1080/10691316.2014.906786},
  url = {http://www.tandfonline.com/doi/abs/10.1080/10691316.2014.906786},
  urldate = {2023-10-23},
  abstract = {Information overload interferes with our abilities to use information effectively. It represents a challenge in searching print as well as online collections. Academic librarians have developed strategies to help their patrons reduce the effects of information overload. This article provides best practices employed by academic librarians to reduce their users’ information overload. Five themes emerged in a review of the literature: information presentation, library instruction, user strategies, librarian roles, and software technologies. Information literacy instruction remains particularly important in combating information overload since the promotion of users’ research skills facilitates their search competencies. Research on information problem solving that focuses on metacognitive strategies may help decrease information overload among users searching digital libraries.},
  langid = {english},
  annotation = {16 citations (Crossref) [2024-03-26]\\
titleTranslation: 减轻学术图书馆读者的信息超载负担\\
abstractTranslation:  信息超载会影响我们有效利用信息的能力。它是搜索印刷品和在线馆藏的一个挑战。学术图书馆员已经制定了一些策略来帮助读者减轻信息超载的影响。本文介绍了学术图书馆员在减轻用户信息超载方面的最佳实践。在文献综述中出现了五个主题：信息呈现、图书馆指导、用户策略、图书馆员角色和软件技术。信息扫盲教学在应对信息超载方面仍然尤为重要，因为提高用户的研究技能有助于提高他们的搜索能力。以元认知策略为重点的信息问题解决研究可能有助于减轻数字图书馆用户的信息超载。},
  note = {[TLDR] Best practices employed by academic librarians to reduce their users’ information overload are provided, which help decrease information overload among users searching digital libraries.}
}

@inproceedings{bartuschAutomaticGenerationProvenance2018,
  title = {Automatic {{Generation}} of {{Provenance Metadata}} during {{Execution}} of {{Scientific Workflows}}.},
  booktitle = {{{IWSG}}},
  author = {Bartusch, Felix and Hanussek, Maximilian and Krüger, Jens},
  date = {2018},
  url = {https://ceur-ws.org/Vol-2357/paper8.pdf},
  urldate = {2024-01-17},
  langid = {english},
  keywords = {未整理},
  annotation = {titleTranslation: 在執行科學工作流程期間自動產生來源元資料。},
  note = {在執行時驗時通過紀錄程式記錄環境並存成xml檔。},
  file = {C:\Users\BlackCat\Zotero\storage\64ZH4BF4\Bartusch et al. - 2018 - Automatic Generation of Provenance Metadata during.pdf}
}

@incollection{bawdenInformationOverloadOverview2020,
  title = {Information {{Overload}}: {{An Overview}}},
  shorttitle = {Information {{Overload}}},
  author = {Bawden, D. and Robinson, L.},
  date = {2020-06-01},
  publisher = {Oxford University Press},
  location = {Oxford},
  doi = {10.1093/acrefore/9780190228637.013.1360},
  url = {https://openaccess.city.ac.uk/id/eprint/23544/},
  urldate = {2024-04-30},
  abstract = {For almost as long as there has been recorded information, there has been a perception that humanity has been overloaded by it. Concerns about 'too much to read' have been expressed for many centuries, and made more urgent since the arrival of ubiquitous digital information in the late twentieth century. The historical perspective is a necessary corrective to the often, and wrongly, held view that it is associated solely with the modern digital information environment, and with social media in particular. However, as society fully experiences Floridi's Fourth Revolution, and moves into hyper-history (with society dependent on, and defined by, information and communication technologies) and the infosphere (a information environment distinguished by a seamless blend of online and offline information actvity), individuals and societies are dependent on, and formed by, information in an unprecedented way, information overload needs to be taken more seriously than ever. Overload has been claimed to be both the major issue of our time, and a complete non-issue. It has been cited as an important factor in, inter alia, science, medicine, education, politics, governance, business and marketing, planning for smart cities, access to news, personal data tracking, home life, use of social media, and online shopping, and has even influenced literature The information overload phenomenon has been known by many different names, including: information overabundance, infobesity, infoglut, data smog, information pollution, information fatigue, social media fatigue, social media overload, information anxiety, library anxiety, infostress, infoxication, reading overload, communication overload, cognitive overload, information violence, and information assault. There is no single generally accepted definition, but it can best be understood as that situation which arises when there is so much relevant and potentially useful information available that it becomes a hindrance rather than a help. Its essential nature has not changed with changing technology, though its causes and proposed solutions have changed much. The best ways of avoiding overload, individually and socially, appear to lie in a variety of coping strategies, such as filtering, withdrawing, queuing, and 'satisficing'. Better design of information systems, effective personal information management, and the promotion of digital and media literacies, also have a part to play. Overload may perhaps best be overcome by seeking a mindful balance in consuming information, and in finding understanding.},
  isbn = {978-0-19-022863-7},
  langid = {english},
  keywords = {資訊超載},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\2B6WPK9R\\Bawden 與 Robinson - 2020 - Information Overload An Overview.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\VF4C3T4J\\23544.html}
}

@inproceedings{bengtgunnarmalmCitationPracticesFinal2022,
  title = {Citation {{Practices}} in {{Final Year Computer Science}} and {{Electrical Engineering Bachelor Theses}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Teaching}}, {{Assessment}} and {{Learning}} for {{Engineering}} ({{TALE}})},
  author = {{Bengt Gunnar Malm} and {Kjell Göran Hamrin}},
  date = {2022-02},
  pages = {65--70},
  issn = {2470-6698},
  doi = {10.1109/TALE54877.2022.00019},
  url = {https://ieeexplore.ieee.org/document/10148417},
  urldate = {2023-10-13},
  abstract = {This study investigates the citation practices of electrical engineering (EE) and computer science (CS) bachelor students. A citation category based coding scheme for content analysis was developed to study the academic writing genre, represented by final year thesis reports. The academic writing skill-set displayed by CS and EE bachelor students was gauged by interpretative content analysis of a corpus of full-text theses, retrieved from a university repository. Two research questions were addressed: 1) what are the citation practices, employed by two cohorts of final year bachelor students, in electrical engineering and computer science? 2) what use of academic writing genre conventions can be quantified through interpretative content analysis? The intention of the study was to categorize the purposes behind citation use in bachelor level theses. Both student cohorts displayed an exaggerated use of citations, deviating from fully mature academic writing practices in the CS and EE fields. Many factual statements of a general nature were supported by multiple and weakly motivated instances of citations to a limited set of sources. An apparent genre convention was to cite sources mainly in text segments of introductory or survey character. Hence the investigated bachelor level theses made little use of citations to support the choice of research question, method or in critical discussion of results.},
  eventtitle = {2022 {{IEEE International Conference}} on {{Teaching}}, {{Assessment}} and {{Learning}} for {{Engineering}} ({{TALE}})},
  annotation = {0 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\2UE9VIF7\Malm and Hamrin - 2022 - Citation Practices in Final Year Computer Science .pdf}
}

@inproceedings{benhutchinsonAccountabilityMachineLearning2021,
  title = {Towards {{Accountability}} for {{Machine Learning Datasets}}: {{Practices}} from {{Software Engineering}} and {{Infrastructure}}},
  shorttitle = {Towards {{Accountability}} for {{Machine Learning Datasets}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {{Ben Hutchinson} and {Andrew Smart} and {Alex Hanna} and {Emily Denton} and {Christina Greer} and {Oddur Kjartansson} and {Parker Barnes} and {Margaret Mitchell}},
  year = {3 月 1, 2021},
  series = {{{FAccT}} '21},
  pages = {560--575},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3442188.3445918},
  url = {https://dl.acm.org/doi/10.1145/3442188.3445918},
  urldate = {2023-09-13},
  abstract = {為機器學習提供動力的數據集經常被使用、共享和重用，但對其創建的審議過程卻知之甚少。隨著人工智能係統越來越多地用於高風險任務，系統開發和部署實踐必須進行調整，以解決模型開發數據在實踐中構建和使用的真正後果。這包括提高數據透明度，以及對開發數據時做出的決策負責。在本文中，我們介紹了一個嚴格的數據集開發透明度框架，以支持決策和問責制。該框架利用數據集開發的循環、基礎設施和工程性質來借鑒軟件開發生命週期的最佳實踐。數據開發生命週期的每個階段都會生成有助於改進溝通和決策的文檔，並引起人們對仔細數據工作的價值和必要性的關注。擬議的框架使數據集創建中經常被忽視的工作和決策變得可見，這是縮小人工智能問責差距的關鍵一步，也是與最近的審計流程工作相一致的關鍵/必要資源。},
  isbn = {978-1-4503-8309-7},
  keywords = {datasets,machine learning,requirements engineering,可解釋性,已整理,框架,監督式學習},
  annotation = {111 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\Z84HX8LY\Hutchinson 等。 - 2021 - Towards Accountability for Machine Learning Datase.pdf}
}

@inproceedings{bernhardwohlmacherOvercomingTechnologyCommunication2023,
  title = {Overcoming Technology and Communication Barriers in Intergenerational Communication with a Tangible Interface},
  booktitle = {Mensch Und {{Computer}} 2023},
  author = {{Bernhard Wohlmacher} and {Holger Klapperich} and {Fabian Mertl} and {Alina Huldtgren}},
  year = {9 月 3, 2023},
  series = {{{MuC}} '23},
  pages = {508--512},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3603555.3608554},
  url = {https://doi.org/10.1145/3603555.3608554},
  urldate = {2023-08-23},
  abstract = {Despite the higher uptake of digital media also in older age groups, there are still challenges for people of high ages and with cognitive or motoric problems, which prevent them from learning and using devices like smartphones or tablets. Tangible user interfaces provide an opportunity to connect these people to social media, thereby increasing their digital participation in society and reducing loneliness. In this paper, we discuss the challenges of intergenerational communication and research the opportunities affiliated with tangible technologies to overcome technology barriers for adults of high ages. We present a hybrid system for social media interactions between young smartphone users and their old relatives, who do not use mobile technologies. The system consists of a tangible user interface designed specifically for old adults that connects to a standard mobile messenger service. TAMI was designed in a user-centered process to fulfill the needs of older adults and tested in a longer-term field study with pairs of young and old relatives.},
  isbn = {9798400707711},
  langid = {english},
  keywords = {accessibility,enabling,intergenerational,tangible user interface,人機互動,回收},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 通過有形界面克服代際溝通中的技術和溝通障礙\\
abstractTranslation:  儘管數字媒體在老年人群中的使用率也較高，但對於高齡人群以及有認知或運動問題的人來說，仍然面臨著挑戰，這阻礙了他們學習和使用智能手機或平板電腦等設備。有形的用戶界面提供了將這些人連接到社交媒體的機會，從而增加他們對社會的數字參與並減少孤獨感。在本文中，我們討論了代際溝通的挑戰，並研究了與有形技術相關的機會，以克服高齡成年人的技術障礙。我們提出了一種混合系統，用於年輕智能手機用戶和不使用移動技術的年長親戚之間的社交媒體互動。該系統由專門為老年人設計的有形用戶界面組成，可連接到標準的移動通訊服務。 TAMI 的設計以用戶為中心，旨在滿足老年人的需求，並通過對年輕和年長親屬進行的長期實地研究進行了測試。},
  note = {開發一個系統用於讓老人能與年輕人的手機溝通。非研究範圍。},
  file = {C:\Users\BlackCat\Zotero\storage\B7IT6WRU\Wohlmacher 等。 - 2023 - Overcoming technology and communication barriers i.pdf}
}

@incollection{billcopeMapsMedicalReason2022,
  title = {Maps of {{Medical Reason}}: {{Applying Knowledge Graphs}} and {{Artificial Intelligence}} in {{Medical Education}} and {{Practice}}},
  shorttitle = {Maps of {{Medical Reason}}},
  booktitle = {Bioinformational {{Philosophy}} and {{Postdigital Knowledge Ecologies}}},
  author = {{Bill Cope} and {Mary Kalantzis} and {ChengXiang Zhai} and {Andrea Krussel} and {Duane Searsmith} and {Duncan Ferguson} and {Richard Tapping} and {Yerko Berrocal}},
  editor = {{Michael A. Peters} and {Petar Jandrić} and {Sarah Hayes}},
  date = {2022},
  series = {Postdigital {{Science}} and {{Education}}},
  pages = {133--159},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-95006-4_8},
  url = {https://doi.org/10.1007/978-3-030-95006-4_8},
  urldate = {2023-10-18},
  abstract = {This chapter explores the connections between the bio and the digital in the construction of ‘bioinformation’ and ‘biodigital convergence’. The site of examination of these connections is medical understandings of the body. Its focus is the notion of ontology in two related senses, philosophical and technical. The chapter considers the connections between, on the one hand, the immaterial understanding reflected in medical knowledge—in philosophical terms ‘the ideal’ or ideational—and on the other, the material, biological realities of bodies. In a technical sense, the chapter discusses medical ontologies in a computer science frame of reference, and the emergence in recent years of ‘knowledge graphs’ for their representation. On these philosophical and technical bases, the chapter goes on to discuss a research and development project in which the authors have been engaged, to develop a web-based knowledge graphing environment, with a wide range of potential sites of applications, one to support medical students in clinical case analysis, and the other to build medical logic visualizations to supplement electronic health records.},
  isbn = {978-3-030-95006-4},
  langid = {english},
  keywords = {Electronic health records,Knowledge graphs,Medical education,Medical informatics,已整理,知識本體,醫學,重要},
  annotation = {titleTranslation: 医学推理地图：在医学教育和实践中应用知识图谱和人工智能\\
abstractTranslation:  本章探讨了在构建 "生物信息 "和 "生物数字融合 "过程中生物与数字之间的联系。这些联系的研究对象是医学对身体的理解。其重点是哲学和技术两种相关意义上的本体概念。本章探讨了医学知识中反映的非物质理解--哲学术语 "理想 "或意识形态--与身体的物质和生物现实之间的联系。在技术意义上，本章讨论了计算机科学参考框架下的医学本体论，以及近年来出现的用于表示医学本体论的 "知识图谱"。在这些哲学和技术基础上，本章接着讨论了作者参与的一个研究与开发项目，该项目旨在开发一个基于网络的知识图谱环境，该环境具有广泛的潜在应用场所，其中一个用于支持医科学生进行临床病例分析，另一个用于建立医学逻辑可视化，以补充电子健康记录。},
  note = {描述知識本體如何應用於將醫學概念具體化。對於這樣的過程描述的非常詳細。},
  file = {C:\Users\BlackCat\Zotero\storage\LQ26W6T4\Cope et al. - 2022 - Maps of Medical Reason Applying Knowledge Graphs .pdf}
}

@article{bin-nashwanUseChatGPTAcademia2023,
  title = {Use of {{ChatGPT}} in Academia: {{Academic}} Integrity Hangs in the Balance},
  shorttitle = {Use of {{ChatGPT}} in Academia},
  author = {Bin-Nashwan, Saeed Awadh and Sadallah, Mouad and Bouteraa, Mohamed},
  date = {2023-11-01},
  journaltitle = {Technology in Society},
  shortjournal = {Technology in Society},
  volume = {75},
  pages = {102370},
  issn = {0160-791X},
  doi = {10.1016/j.techsoc.2023.102370},
  url = {https://www.sciencedirect.com/science/article/pii/S0160791X23001756},
  urldate = {2024-02-24},
  abstract = {In today's academic world, some academicians, researchers and students have begun employing Artificial Intelligence (AI) language models, e.g., ChatGPT, in completing a variety of academic tasks, including generating ideas, summarising literature, and essay writing. However, the use of ChatGPT in academic settings is a controversial issue, leading to a severe concern about academic integrity and AI-assisted cheating, while scholarly communities still lack clear principles on using such innovation in academia. Accordingly, this study aims to understand the motivations driving academics and researchers to use ChatGPT in their work, and specifically the role of academic integrity in making up adoption behavior. Based on 702 responses retrieved from users of ResearchGate and Academia.edu, we found that ChatGPT usage is positively shaped by time-saving feature, e-word of mouth, academic self-efficacy, academic self-esteem, and perceived stress. In contrast, peer influence and academic integrity had a negative effect on usage. Intriguingly, academic integrity-moderated interactions of time-saving, self-esteem and perceived stress on ChatGPT usage are found to be significantly positive. Therefore, we suggest that stakeholders, including academic institutions, publishers and AI language models' programmers, should work together to specify necessary guidelines for the ethical use of AI chatbots in academic work and research.},
  langid = {english},
  keywords = {Academia,Academic integrity,Artificial intelligence,ChatGPT,Plagiarism,Technology adoption,使用者研究,已整理,研究流程},
  annotation = {17 citations (Crossref) [2024-03-26]\\
titleTranslation: ChatGPT 在學術界的使用：學術誠信懸而未決},
  file = {D:\Paper\Use of ChatGPT in academia- Academic integrity hangs in the balance.pdf}
}

@article{bingcongxueKnowledgeGraphQuality2023,
  title = {Knowledge {{Graph Quality Management}}: {{A Comprehensive Survey}}},
  shorttitle = {Knowledge {{Graph Quality Management}}},
  author = {{Bingcong Xue} and {Lei Zou}},
  date = {2023-05},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {35},
  number = {5},
  pages = {4969--4988},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2022.3150080},
  url = {https://ieeexplore.ieee.org/document/9709663},
  urldate = {2023-11-23},
  abstract = {As a powerful expression of human knowledge in a structural form, knowledge graph (KG) has drawn great attention from both the academia and the industry and a large number of construction and application technologies have been proposed. Large-scale knowledge graphs such as DBpedia, YAGO and Wikidata are published and widely used in various tasks. However, most of them are far from perfect and have many quality issues. For example, they may contain inaccurate or outdated entries and do not cover enough facts, which limits their credibility and further utility. Data quality has a long research history in the field of traditional relational data and recently attracts more knowledge graph experts. In this paper, we provide a systematic and comprehensive review of the quality management on knowledge graphs, covering overall research topics about not only quality issues, dimentions and metrics, but also quality management processes from quality assessment and error detection, to error correction and KG completion. We categorize existing works in terms of target goals and used methods for better understanding. In the end, we discuss some key issues and possible directions on knowledge graph quality management for further research.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  langid = {english},
  keywords = {Survey,品質管理,已整理,知識圖譜},
  annotation = {25 citations (Crossref) [2024-03-26]\\
titleTranslation: 知識圖品質管理：綜合調查},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\SSSAHSMC\\Xue 與 Zou - 2023 - Knowledge Graph Quality Management A Comprehensiv.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\KRCP6DI6\\9709663.html}
}

@inproceedings{binyangResearchProblemsCountermeasures2011,
  title = {Research on {{Problems}} and {{Countermeasures}} of {{Undergraduation Thesis}} in {{Colleges}} and {{Universities}}},
  booktitle = {2011 {{International Conference}} on {{Control}}, {{Automation}} and {{Systems Engineering}} ({{CASE}})},
  author = {{Bin Yang}},
  date = {2011-07},
  pages = {1--3},
  doi = {10.1109/ICCASE.2011.5997819},
  url = {https://ieeexplore.ieee.org/document/5997819},
  urldate = {2023-10-13},
  abstract = {The undergraduation thesis is an important link and stage in the training of college students. But in recent years, there are many problems in the undergraduation thesis, such as "serious plagiarism", "improper topics", "lacking in novelty", and "deviating from practice". Why did those problems occur, because of lacking teachers, without enough time and the funding shortage. So we investigate the bottleneck of how to improve the quality of undergraduation thesis and bring forward countermeasures like improving curriculum, strengthening the guidance teachers, optimization the topics of the thesis, encouraging innovation, strengthening supervising system.},
  eventtitle = {2011 {{International Conference}} on {{Control}}, {{Automation}} and {{Systems Engineering}} ({{CASE}})},
  annotation = {0 citations (Crossref) [2024-03-26]},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\EMBB5ID9\\Yang - 2011 - Research on Problems and Countermeasures of Underg.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\RUEAU656\\5997819.html}
}

@inproceedings{bondarenkoCausalQABenchmarkCausal2022,
  title = {{{CausalQA}}: {{A Benchmark}} for {{Causal Question Answering}}},
  shorttitle = {{{CausalQA}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Bondarenko, Alexander and Wolska, Magdalena and Heindorf, Stefan and Blübaum, Lukas and Ngonga Ngomo, Axel-Cyrille and Stein, Benno and Braslavski, Pavel and Hagen, Matthias and Potthast, Martin},
  editor = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
  date = {2022-10},
  pages = {3296--3308},
  publisher = {International Committee on Computational Linguistics},
  location = {Gyeongju, Republic of Korea},
  url = {https://aclanthology.org/2022.coling-1.291},
  urldate = {2024-03-22},
  abstract = {At least 5\% of questions submitted to search engines ask about cause-effect relationships in some way. To support the development of tailored approaches that can answer such questions, we construct Webis-CausalQA-22, a benchmark corpus of 1.1 million causal questions with answers. We distinguish different types of causal questions using a novel typology derived from a data-driven, manual analysis of questions from ten large question answering (QA) datasets. Using high-precision lexical rules, we extract causal questions of each type from these datasets to create our corpus. As an initial baseline, the state-of-the-art QA model UnifiedQA achieves a ROUGE-L F1 score of 0.48 on our new benchmark.},
  eventtitle = {{{COLING}} 2022},
  langid = {english},
  keywords = {問答系統,未整理},
  annotation = {titleTranslation: CausalQA：因果問答的基準},
  file = {C:\Users\BlackCat\Zotero\storage\V8P4DTPE\Bondarenko 等。 - 2022 - CausalQA A Benchmark for Causal Question Answerin.pdf}
}

@inproceedings{bonifacioInParsUnsupervisedDataset2022,
  title = {{{InPars}}: {{Unsupervised Dataset Generation}} for {{Information Retrieval}}},
  shorttitle = {{{InPars}}},
  booktitle = {Proceedings of the 45th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Nogueira, Rodrigo},
  year = {7 月 7, 2022},
  series = {{{SIGIR}} '22},
  pages = {2387--2392},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3477495.3531863},
  url = {https://doi.org/10.1145/3477495.3531863},
  urldate = {2024-03-15},
  abstract = {The Information Retrieval (IR) community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models. In this work, we harness the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models finetuned solely on our synthetic datasets outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods. Code, models, and data are available at https://github.com/zetaalphavector/inpars.},
  isbn = {978-1-4503-8732-3},
  langid = {english},
  keywords = {few-shot models,generative models,IR,large language models,LLM,multi-stage ranking,question generation,synthetic datasets,問答系統,已整理,機器學習,資料集},
  annotation = {24 citations (Crossref) [2024-04-27]\\
titleTranslation: InPars：用於資訊檢索的無監督資料集生成\\
abstractTranslation:  資訊檢索 (IR) 社群最近見證了大型預訓練 Transformer 模型帶來的革命。這場革命的另一個關鍵因素是 MS MARCO 資料集，其規模和多樣性使得零樣本遷移學習能夠應用於各種任務。然而，並非所有 IR 任務和領域都能同樣受益於一個資料集。對各種 NLP 任務的廣泛研究表明，使用特定領域的訓練資料（而不是通用訓練資料）可以提高神經模型的表現。在這項工作中，我們利用大型預訓練語言模型的小樣本功能作為 IR 任務的合成資料產生器。我們表明，僅在我們的合成資料集上進行微調的模型優於 BM25 等強基線以及最近提出的自監督密集檢索方法。程式碼、模型和資料可在 https://github.com/zetaalphavector/inpars 取得。},
  file = {D:\Paper\InPars Unsupervised Dataset Generation for Information Retrieval.pdf}
}

@inproceedings{boruiwangVisualizingWebBrowsing2013,
  title = {Visualizing Web Browsing History with Barcode Chart},
  booktitle = {Adjunct {{Proceedings}} of the 26th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {{Borui Wang} and {Ningxia Zhang} and {Jianfeng Hu} and {Zheng Shen}},
  year = {10 月 8, 2013},
  series = {{{UIST}} '13 {{Adjunct}}},
  pages = {99--100},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2508468.2514729},
  url = {https://doi.org/10.1145/2508468.2514729},
  urldate = {2023-09-06},
  abstract = {Inspired by the DNA art, we introduce a data visualization technique called barcode chart, which uses color-illuminated stripes that resemble barcodes to visualize temporal data. Barcode chart excels at demonstrating high-level patterns in highly segmented temporal data, while retaining details in the data through interaction. We demonstrate Yogurt, a browser extension that implements barcode chart to visualize online browsing history. We conducted a user study and analyzed the effectiveness of using barcode chart for Yogurt in comparison with other applications. We conclude that barcode chart satisfies the need of visualizing the high-density and high-fragmentation nature of temporal data in Yogurt, and helps reveal online distraction and other web browsing patterns.},
  isbn = {978-1-4503-2406-9},
  langid = {english},
  keywords = {barcode chart,temporal data visualization,visualization,web history visualization},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用條形碼圖表可視化網頁瀏覽歷史記錄\\
abstractTranslation:  受 DNA 藝術的啟發，我們引入了一種稱為條形碼圖表的數據可視化技術，該技術使用類似於條形碼的彩色照明條紋來可視化時間數據。條形碼圖表擅長展示高度分段的時間數據中的高級模式，同時通過交互保留數據中的細節。我們演示了 Yogurt，這是一個瀏覽器擴展，它實現了條形碼圖表以可視化在線瀏覽歷史記錄。我們進行了一項用戶研究，並與其他應用程序相比，分析了使用酸奶條形碼圖表的有效性。我們得出的結論是，條形碼圖表滿足了可視化酸奶中時間數據的高密度和高碎片性質的需求，並有助於揭示在線乾擾和其他網絡瀏覽模式。},
  file = {C:\Users\BlackCat\Zotero\storage\8TLIZFVF\Wang 等。 - 2013 - Visualizing web browsing history with barcode char.pdf}
}

@article{boseLineageRetrievalScientific2005,
  title = {Lineage Retrieval for Scientific Data Processing: A Survey},
  shorttitle = {Lineage Retrieval for Scientific Data Processing},
  author = {Bose, Rajendra and Frew, James},
  year = {3 月 1, 2005},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {37},
  number = {1},
  pages = {1--28},
  issn = {0360-0300},
  doi = {10.1145/1057977.1057978},
  url = {https://dl.acm.org/doi/10.1145/1057977.1057978},
  urldate = {2024-01-17},
  abstract = {Scientific research relies as much on the dissemination and exchange of data sets as on the publication of conclusions. Accurately tracking the lineage (origin and subsequent processing history) of scientific data sets is thus imperative for the complete documentation of scientific work. Researchers are effectively prevented from determining, preserving, or providing the lineage of the computational data products they use and create, however, because of the lack of a definitive model for lineage retrieval and a poor fit between current data management tools and scientific software. Based on a comprehensive survey of lineage research and previous prototypes, we present a metamodel to help identify and assess the basic components of systems that provide lineage retrieval for scientific data products.},
  langid = {english},
  keywords = {audit,Data lineage,data provenance,scientific data,scientific workflow,survey},
  annotation = {264 citations (Crossref) [2024-03-26]\\
titleTranslation: 科學資料處理的譜系檢索：一項調查\\
abstractTranslation:  科學研究既依賴結論的發表，也依賴資料集的傳播和交換。因此，準確追蹤科學資料集的譜系（起源和後續處理歷史）對於科學工作的完整記錄至關重要。然而，由於缺乏用於譜系檢索的明確模型以及當前資料管理工具和科學軟體之間的不匹配，研究人員實際上無法確定、保存或提供他們使用和創建的計算資料產品的譜系。基於對譜系研究和先前原型的全面調查，我們提出了一個元模型，以幫助識別和評估為科學資料產品提供譜系檢索的系統的基本組件。},
  file = {C:\Users\BlackCat\Zotero\storage\5HI5P7EH\Bose and Frew - 2005 - Lineage retrieval for scientific data processing .pdf}
}

@article{boseLineageRetrievalScientific2005a,
  title = {Lineage Retrieval for Scientific Data Processing: A Survey},
  shorttitle = {Lineage Retrieval for Scientific Data Processing},
  author = {Bose, Rajendra and Frew, James},
  year = {3 月 1, 2005},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {37},
  number = {1},
  pages = {1--28},
  issn = {0360-0300},
  doi = {10.1145/1057977.1057978},
  url = {https://doi.org/10.1145/1057977.1057978},
  urldate = {2024-03-15},
  abstract = {Scientific research relies as much on the dissemination and exchange of data sets as on the publication of conclusions. Accurately tracking the lineage (origin and subsequent processing history) of scientific data sets is thus imperative for the complete documentation of scientific work. Researchers are effectively prevented from determining, preserving, or providing the lineage of the computational data products they use and create, however, because of the lack of a definitive model for lineage retrieval and a poor fit between current data management tools and scientific software. Based on a comprehensive survey of lineage research and previous prototypes, we present a metamodel to help identify and assess the basic components of systems that provide lineage retrieval for scientific data products.},
  langid = {english},
  keywords = {audit,Data lineage,data provenance,scientific data,scientific workflow},
  annotation = {264 citations (Crossref) [2024-03-26]\\
titleTranslation: 科學資料處理的譜系檢索：一項調查\\
abstractTranslation:  科學研究既依賴結論的發表，也依賴資料集的傳播和交換。因此，準確追蹤科學資料集的譜系（起源和後續處理歷史）對於科學工作的完整記錄至關重要。然而，由於缺乏用於譜系檢索的明確模型以及當前資料管理工具和科學軟體之間的不匹配，研究人員實際上無法確定、保存或提供他們使用和創建的計算資料產品的譜系。基於對譜系研究和先前原型的全面調查，我們提出了一個元模型，以幫助識別和評估為科學資料產品提供譜系檢索的系統的基本組件。},
  file = {D:\Paper\Lineage retrieval for scientific data processing a survey.pdf}
}

@article{bouzianeQuestionAnsweringSystems2015,
  title = {Question {{Answering Systems}}: {{Survey}} and {{Trends}}},
  shorttitle = {Question {{Answering Systems}}},
  author = {Bouziane, Abdelghani and Bouchiha, Djelloul and Doumi, Noureddine and Malki, Mimoun},
  date = {2015-01-01},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  series = {International {{Conference}} on {{Advanced Wireless Information}} and {{Communication Technologies}} ({{AWICT}} 2015)},
  volume = {73},
  pages = {366--375},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2015.12.005},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050915034663},
  urldate = {2024-03-26},
  abstract = {The need to query information content available in various formats including structured and unstructured data (text in natural language, semi-structured Web documents, structured RDF data in the semantic Web, etc.) has become increasingly important. Thus, Question Answering Systems (QAS) are essential to satisfy this need. QAS aim at satisfying users who are looking to answer a specific question in natural language. In this paper we survey various QAS. We give also statistics and analysis. This can clear the way and help researchers to choose the appropriate solution to their issue. They can see the insufficiency, so that they can propose new systems for complex queries. They can also adapt or reuse QAS techniques for specific research issues.},
  langid = {english},
  keywords = {Information Retrieval,Natural Language Processing (NLP),Semantic Web,SPARQL,Survey,問答系統,已整理,略讀},
  annotation = {82 citations (Crossref) [2024-03-26]\\
titleTranslation: 問答系統：調查和趨勢},
  note = {本調查相當粗略且舊(2015)，其中關於問答系統的定義勉強可以拿來用，其他部份則沒什麼用。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\24D8C9NC\\Bouziane 等。 - 2015 - Question Answering Systems Survey and Trends.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\36XP52A2\\S1877050915034663.html}
}

@inproceedings{boxuCNDBpediaNeverEndingChinese2017,
  title = {{{CN-DBpedia}}: {{A Never-Ending Chinese Knowledge Extraction System}}},
  shorttitle = {{{CN-DBpedia}}},
  booktitle = {Advances in {{Artificial Intelligence}}: {{From Theory}} to {{Practice}}},
  author = {{Bo Xu} and {Yong Xu} and {Jiaqing Liang} and {Chenhao Xie} and {Bin Liang} and {Wanyun Cui} and {Yanghua Xiao}},
  editor = {{Salem Benferhat} and {Karim Tabia} and {Moonis Ali}},
  date = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {428--438},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-60045-1_44},
  abstract = {Great efforts have been dedicated to harvesting knowledge bases from online encyclopedias. These knowledge bases play important roles in enabling machines to understand texts. However, most current knowledge bases are in English and non-English knowledge bases, especially Chinese ones, are still very rare. Many previous systems that extract knowledge from online encyclopedias, although are applicable for building a Chinese knowledge base, still suffer from two challenges. The first is that it requires great human efforts to construct an ontology and build a supervised knowledge extraction model. The second is that the update frequency of knowledge bases is very slow. To solve these challenges, we propose a never-ending Chinese Knowledge extraction system, CN-DBpedia, which can automatically generate a knowledge base that is of ever-increasing in size and constantly updated. Specially, we reduce the human costs by reusing the ontology of existing knowledge bases and building an end-to-end facts extraction model. We further propose a smart active update strategy to keep the freshness of our knowledge base with little human costs. The 164 million API calls of the published services justify the success of our system.},
  isbn = {978-3-319-60045-1},
  langid = {english},
  keywords = {Current Knowledge Base,Exist Knowledge Base,Human Cost,Knowledge Base,Natural Language Sentence,中文,知識圖譜,知識本體,重要},
  annotation = {120 citations (Crossref) [2024-03-26]\\
titleTranslation: CN-DBpedia：永無止境的中文知識擷取系統\\
abstractTranslation:  人們付出了巨大的努力來從線上百科全書中獲取知識庫。這些知識庫在使機器理解文字方面發揮著重要作用。然而，目前的知識庫大多是英文的，非英文的知識庫，尤其是中文的知識庫還很少。以前的許多從線上百科全書中提取知識的系統雖然適用於建立中文知識庫，但仍面臨兩個挑戰。首先是建構本體、建立有監督的知識抽取模型需要付出龐大的人力。二是知識庫的更新頻率很慢。為了解決這些挑戰，我們提出了一個永無止境的中文知識抽取系統CN-DBpedia，它可以自動產生一個規模不斷增加並不斷更新的知識庫。特別是，我們透過重複使用現有知識庫的本體並建立端到端的事實提取模型來降低人力成本。我們進一步提出了一種智慧主動更新策略，以很少的人力成本來維持知識庫的新鮮度。已發布服務的 1.64 億次 API 呼叫證明了我們系統的成功。},
  file = {C:\Users\BlackCat\Zotero\storage\58UZMKMS\Xu et al. - 2017 - CN-DBpedia A Never-Ending Chinese Knowledge Extra.pdf}
}

@inproceedings{boxuCrossLingualTypeInference2016,
  title = {Cross-{{Lingual Type Inference}}},
  booktitle = {Database {{Systems}} for {{Advanced Applications}}},
  author = {{Bo Xu} and {Yi Zhang} and {Jiaqing Liang} and {Yanghua Xiao} and {Seung-won Hwang} and {Wei Wang}},
  editor = {{Shamkant B. Navathe} and {Weili Wu} and {Shashi Shekhar} and {Xiaoyong Du} and {X. Sean Wang} and {Hui Xiong}},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {447--462},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-32025-0_28},
  abstract = {Entity typing is an essential task for constructing a knowledge base. However, many non-English knowledge bases fail to type their entities due to the absence of a reasonable local hierarchical taxonomy. Since constructing a widely accepted taxonomy is a hard problem, we propose to type these non-English entities with some widely accepted taxonomies in English, such as DBpedia, Yago and Freebase. We define this problem as cross-lingual type inference. In this paper, we present CUTE to type Chinese entities with DBpedia types. First we exploit the cross-lingual entity linking between Chinese and English entities to construct the training data. Then we propose a multi-label hierarchical classification algorithm to type these Chinese entities. Experimental results show the effectiveness and efficiency of our method.},
  isbn = {978-3-319-32025-0},
  langid = {english},
  keywords = {English Type,Entity Typing,Head Compound,Local Knowledge Base,Name Entity Recognition,中文,待讀,本體建立,無法取得,知識本體,重要},
  annotation = {13 citations (Crossref) [2024-03-26]\\
titleTranslation: 跨語言類型推斷\\
abstractTranslation:  實體類型是建構知識庫的一項重要任務。然而，由於缺乏合理的本地分層分類法，許多非英語知識庫無法鍵入其實體。由於建立廣泛接受的分類法是一個難題，因此我們建議使用一些廣泛接受的英語分類法對這些非英語實體進行分類，例如 DBpedia、Yago 和 Freebase。我們將這個問題定義為跨語言類型推理。在本文中，我們提出了 CUTE 來使用 DBpedia 類型來輸入中文實體。首先，我們利用中文和英文實體之間的跨語言實體連結來建立訓練資料。然後我們提出了一種多標籤層次分類演算法來對這些中文實體進行分類。實驗結果顯示了我們方法的有效性和效率。}
}

@inproceedings{boyachengConstructionTraditionalChinese2018,
  title = {Construction of Traditional {{Chinese}} Medicine {{Knowledge Graph}} Using {{Data Mining}} and {{Expert Knowledge}}},
  booktitle = {2018 {{International Conference}} on {{Network Infrastructure}} and {{Digital Content}} ({{IC-NIDC}})},
  author = {{Boya Cheng} and {Yuan Zhang} and {Dejun Cai} and {Wan Qiu} and {Dongxin Shi}},
  date = {2018-08},
  pages = {209--213},
  issn = {2575-4955},
  doi = {10.1109/ICNIDC.2018.8525665},
  abstract = {Knowledge Graph (KG) is a powerful tool for Medical Decision Support(MDS). In this paper, we propose a novel method to construct a knowledge graph of traditional Chinese medicine (TCM), which combines data mining on limited but typical electronic medical records (EMRs) with expert knowledge. In particular, we leverage data mining to mine the medical regulations, and then convert them into medical knowledge with the help of experts, and finally build the KG accordingly. The goal of data mining is to exclude infrequent patterns which are caused by interference in EMRs. This method takes a different view from knowledge extraction. It avoids over-intervention of experts and subjectivity resulting from experts' direct intervention. This method can also be applied to other professional fields whose training data includes foundation knowledge and empirical information. The constructed KG has been approved by expert evaluation.},
  eventtitle = {2018 {{International Conference}} on {{Network Infrastructure}} and {{Digital Content}} ({{IC-NIDC}})},
  langid = {english},
  keywords = {Complex systems,Complex Systems,Data mining,Data Mining,EMRs,Entropy,Expert Knowledge,Kidney,Knowledge Graph,Medical diagnostic imaging,Mutual information,Testing,traditional Chinese medicine,中醫,已整理,待讀,數據挖掘,知識本體,規則挖掘,重要,電子病歷},
  annotation = {9 citations (Crossref) [2024-03-26]\\
titleTranslation: 利用數據挖掘和專家知識構建中醫藥知識圖譜\\
abstractTranslation:  知識圖譜（KG）是醫療決策支持（MDS）的強大工具。在本文中，我們提出了一種構建中醫知識圖譜的新方法，它將有限但典型的電子病歷（EMR）的數據挖掘與專家知識相結合。具體來說，我們利用數據挖掘來挖掘醫療法規，然後在專家的幫助下將其轉化為醫學知識，最後構建相應的知識圖譜。數據挖掘的目標是排除由 EMR 干擾引起的不常見模式。該方法採用與知識提取不同的觀點。避免了專家的過度干預和專家直接干預帶來的主觀性。該方法也可以應用於訓練數據包括基礎知識和經驗信息的其他專業領域。所建幼兒園已通過專家評審。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\69XZLFL5\\Construction of traditional Chinese medicine Knowledge Graph using Data Mining and Expert Knowledge.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\MNKTYHPM\\Cheng 等。 - 2018 - Construction of traditional Chinese medicine Knowl.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\H7BHMIVZ\\8525665.html}
}

@article{bradyd.lundChattingChatGPTHow2023,
  title = {Chatting about {{ChatGPT}}: How May {{AI}} and {{GPT}} Impact Academia and Libraries?},
  shorttitle = {Chatting about {{ChatGPT}}},
  author = {{Brady D. Lund} and {Ting Wang}},
  date = {2023-01-01},
  journaltitle = {Library Hi Tech News},
  volume = {40},
  number = {3},
  pages = {26--29},
  publisher = {Emerald Publishing Limited},
  issn = {0741-9058},
  doi = {10.1108/LHTN-01-2023-0009},
  url = {https://doi.org/10.1108/LHTN-01-2023-0009},
  urldate = {2023-10-25},
  abstract = {Purpose This paper aims to provide an overview of key definitions related to ChatGPT, a public tool developed by OpenAI, and its underlying technology, Generative Pretrained Transformer (GPT). Design/methodology/approach This paper includes an interview with ChatGPT on its potential impact on academia and libraries. The interview discusses the benefits of ChatGPT such as improving search and discovery, reference and information services; cataloging and metadata generation; and content creation, as well as the ethical considerations that need to be taken into account, such as privacy and bias. Findings ChatGPT has considerable power to advance academia and librarianship in both anxiety-provoking and exciting new ways. However, it is important to consider how to use this technology responsibly and ethically, and to uncover how we, as professionals, can work alongside this technology to improve our work, rather than to abuse it or allow it to abuse us in the race to create new scholarly knowledge and educate future professionals. Originality/value This paper discusses the history and technology of GPT, including its generative pretrained transformer model, its ability to perform a wide range of language-based tasks and how ChatGPT uses this technology to function as a sophisticated chatbot.},
  langid = {english},
  keywords = {Academia,AI,ChatGPT,Generative pretrained transformer,GPT-3,Libraries,已整理,文獻,研究流程},
  annotation = {229 citations (Crossref) [2024-03-26]\\
titleTranslation: 聊ChatGPT：AI和GPT如何影響學術界和圖書館？\\
abstractTranslation:  目的 本文旨在概述與 OpenAI 開發的公共工具 ChatGPT 及其底層技術 Generative Pretrained Transformer (GPT) 相關的關鍵定義。設計/方法論/途徑 本文包含對 ChatGPT 的採訪，以了解其對學術界和圖書館的潛在影響。訪談討論了 ChatGPT 的好處，例如改進搜尋和發現、參考和資訊服務；編目和元資料生成；和內容創作，以及需要考慮的道德因素，例如隱私和偏見。研究結果 ChatGPT 有相當大的力量以令人焦慮和令人興奮的新方式推動學術界和圖書館事業的發展。然而，重要的是要考慮如何負責任地、合乎道德地使用這項技術，並揭示我們作為專業人士如何能夠與這項技術一起工作來改進我們的工作，而不是濫用它或允許它在競爭中濫用它。創造新的學術知識並教育未來的專業人士。原創性/價值 本文討論了 GPT 的歷史和技術，包括其生成式預訓練 Transformer 模型、執行各種基於語言的任務的能力以及 ChatGPT 如何使用該技術充當複雜的聊天機器人。},
  note = {很簡單的介紹。\\
可以用來說明ChatGPT要小心使用。},
  file = {C:\Users\BlackCat\Zotero\storage\A92AK5J8\Lund and Wang - 2023 - Chatting about ChatGPT how may AI and GPT impact .pdf}
}

@online{brejaSurveyWhyTypeQuestion2019,
  title = {A {{Survey}} on {{Why-Type Question Answering Systems}}},
  author = {Breja, Manvi and Jain, Sanjay Kumar},
  date = {2019-11-12},
  eprint = {1911.04879},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1911.04879},
  url = {http://arxiv.org/abs/1911.04879},
  urldate = {2024-03-26},
  abstract = {Search engines such as Google, Yahoo and Baidu yield information in the form of a relevant set of web pages according to the need of the user. Question Answering Systems reduce the time taken to get an answer, to a query asked in natural language by providing the one most relevant answer. To the best of our knowledge, major research in Why-type questions began in early 2000's and our work on Why-type questions can help explore newer avenues for fact-finding and analysis. The paper presents a survey on Why-type Question Answering System, details the architecture, the processes involved in the system and suggests further areas of research.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Survey,問答系統,未整理},
  annotation = {titleTranslation: Why型問答系統調查},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\XZK9MS26\\Breja 與 Jain - 2019 - A Survey on Why-Type Question Answering Systems.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\FML2DHXA\\1911.html}
}

@article{brettdrurySurveyExtractionApplications2022,
  title = {A Survey of the Extraction and Applications of Causal Relations},
  author = {{Brett Drury} and {Hugo Gonçalo Oliveira} and {Alneu de Andrade Lopes}},
  date = {2022-05},
  journaltitle = {Natural Language Engineering},
  volume = {28},
  number = {3},
  pages = {361--400},
  publisher = {Cambridge University Press},
  issn = {1351-3249, 1469-8110},
  doi = {10.1017/S135132492100036X},
  url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/survey-of-the-extraction-and-applications-of-causal-relations/8B43AC51BE1F0B53B82DA99997DBC7E6#access-block},
  urldate = {2023-09-12},
  abstract = {Causationin written natural language can express a strong relationship between events and facts. Causation in the written form can be referred to as a causal relation where a cause event entails the occurrence of an effect event. A cause and effect relationship is stronger than a correlation between events, and therefore aggregated causal relations extracted from large corpora can be used in numerous applications such as question-answering and summarisation to produce superior results than traditional approaches. Techniques like logical consequence allow causal relations to be used in niche practical applications such as event prediction which is useful for diverse domains such as security and finance. Until recently, the use of causal relations was a relatively unpopular technique because the causal relation extraction techniques were problematic, and the relations returned were incomplete, error prone or simplistic. The recent adoption of language models and improved relation extractors for natural language such as Transformer-XL (Dai et al. (2019). Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860) has seen a surge of research interest in the possibilities of using causal relations in practical applications. Until now, there has not been an extensive survey of the practical applications of causal relations; therefore, this survey is intended precisely to demonstrate the potential of causal relations. It is a comprehensive survey of the work on the extraction of causal relations and their applications, while also discussing the nature of causation and its representation in text.},
  langid = {english},
  keywords = {Causal relations,Cause identification,Event prediction,Information retrieval,Survey,情緒分析,無法取得,規則挖掘},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 因果關係的提取與應用綜述\\
abstractTranslation:  書面自然語言中的因果關係可以表達事件和事實之間的緊密關係。書面形式的因果關係可以指因果關係，其中原因事件必然導致結果事件的發生。因果關係比事件之間的相關性更強，因此從大型語料庫中提取的聚合因果關係可以用於多種應用，例如問答和總結，以產生比傳統方法更好的結果。邏輯結果等技術允許因果關係用於特定的實際應用，例如事件預測，這對於安全和金融等不同領域很有用。直到最近，因果關係的使用還是一種相對不受歡迎的技術，因為因果關係提取技術存在問題，並且返回的關係不完整、容易出錯或過於簡單。最近採用的語言模型和改進的自然語言關係提取器，例如 Transformer-XL（Dai 等人（2019）。Transformer-xl：超越固定長度上下文的注意力語言模型。arXiv 預印本 arXiv：1901.02860）已經看到了人們對在實際應用中使用因果關係的可能性的研究興趣激增。到目前為止，還沒有對因果關係的實際應用進行廣泛的調查。因此，這項調查的目的正是為了展示因果關係的潛力。它是對因果關係提取及其應用工作的全面概述，同時還討論了因果關係的本質及其在文本中的表示。},
  note = {對因果關係提取及其應用工作的全面概述，同時還討論了因果關係的本質及其在文本中的表示。}
}

@online{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2024-04-26},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,LLM,已整理,待讀,經典,重要},
  annotation = {abstractTranslation:  最近的工作已經證明，透過對大量文字進行預訓練，然後對特定任務進行微調，在許多 NLP 任務和基準測試中取得了巨大的成果。雖然在架構中通常與任務無關，但該方法仍需要針對特定任務的數千或數萬個範例的資料集進行微調。相較之下，人類通常只需幾個例子或簡單的指令就可以執行新的語言任務，而目前的 NLP 系統在很大程度上仍然難以做到這一點。在這裡，我們表明，擴展語言模型極大地提高了與任務無關的、少量的效能，有時甚至達到了與先前最先進的微調方法的競爭力。具體來說，我們訓練了 GPT-3，這是一種具有 1750 億個參數的自回歸語言模型，比任何以前的非稀疏語言模型多 10 倍，並在少數樣本設定中測試其效能。對於所有任務，應用 GPT-3 時無需任何梯度更新或微調，任務和少量演示純粹透過與模型的文字互動來指定。 GPT-3 在許多 NLP 資料集上實現了強大的效能，包括翻譯、問答和完形填空任務，以及一些需要即時推理或領域適應的任務，例如解讀單字、在文字中使用新單字。進行三位數算術。同時，我們也確定了一些 GPT-3 的少樣本學習仍然存在問題的資料集，以及一些 GPT-3 面臨與大型網路語料庫訓練相關的方法問題的資料集。最後，我們發現 GPT-3 可以產生人類評估者難以區分的新聞文章樣本和人類撰寫的文章。我們總體上討論了這項發現和 GPT-3 的更廣泛的社會影響。},
  note = {Comment: 40+32 pages},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\N496BUKW\\Brown 等。 - 2020 - Language Models are Few-Shot Learners.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\JW7LUNHD\\2005.html}
}

@inproceedings{caioc.vielRecordingMaybeLocationBased2018,
  title = {Recording {{Maybe}}? {{Location-Based Application}} for {{Detection}} and {{Capture}} of {{Ad-hoc Meetings}}},
  shorttitle = {Recording {{Maybe}}?},
  booktitle = {Proceedings of the 24th {{Brazilian Symposium}} on {{Multimedia}} and the {{Web}}},
  author = {{Caio C. Viel} and {Larissa C. Zimmermann} and {Kamila R. H. Rodrigues} and {Maria G. C. Pimentel}},
  year = {10 月 16, 2018},
  series = {{{WebMedia}} '18},
  pages = {53--60},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3243082.3243102},
  url = {https://doi.org/10.1145/3243082.3243102},
  urldate = {2023-09-08},
  abstract = {University campi, start-ups and conferences venue are places where there is a constant flow information exchanged between individuals. Part of this exchange takes place in formal, scheduled meetings, presentations, lectures and workshops. Another part, however, occurs in ad-hoc encounters such as coffee breaks, or when we meet colleagues or supervisors in corridors or at the canteen. Some have the habit of recording the information and decisions from formal encounters by taking notes on paper or capturing audio or video. However, this is not always possible in ad-hoc meetings, either because of lack of means to carry out the notes or because participants do not realize the need to record what was discussed, thus many information and decisions taken in ad-hoc meetings are lost. In this paper we present a case study that investigates how common is the occurrence of ad-hoc meetings in university campi, and proposes the "Context-Based Opportunistic Capture", a Capture \& Access technique that uses mobile devices to detect possible occurrences of meetings from user's close proximity and alerts them that a meeting may be held, and suggests that they use their devices to record the meeting in audio or video. As proof of concept we developed an Android application and a context server that were evaluated by means of a case study. Partial results indicate that ad-hoc meetings are indeed common, that these are not usually recorded and that the provision of an infrastructure for the capture and registration of these meetings proves feasible, especially for the aforementioned environments.},
  isbn = {978-1-4503-5867-5},
  langid = {english},
  keywords = {Ad-hoc Meeting,Capture & Access,Location-Based},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 錄音也許？用於檢測和捕獲臨時會議的基於位置的應用程序\\
abstractTranslation:  大學校園、初創企業和會議場所是個人之間不斷交換信息的地方。這種交流的一部分是通過正式的、預定的會議、演講、講座和研討會進行的。然而，另一部分發生在臨時的相遇中，例如喝咖啡休息時間，或者當我們在走廊或食堂遇到同事或主管時。有些人習慣通過在紙上做筆記或捕捉音頻或視頻來記錄正式會面的信息和決定。然而，這在臨時會議中並不總是可行，要么是因為缺乏進行記錄的手段，要么是因為參與者沒有意識到需要記錄所討論的內容，因此臨時會議中做出的許多信息和決定都被忽略了。丟失的。在本文中，我們提出了一個案例研究，調查了大學校園中臨時會議發生的普遍性，並提出了“基於上下文的機會捕獲”，這是一種使用移動設備檢測可能發生的會議的捕獲和訪問技術距離用戶很近，提醒他們可能要召開會議，並建議他們使用自己的設備以音頻或視頻形式錄製會議。作為概念驗證，我們開發了一個 Android 應用程序和一個上下文服務器，並通過案例研究對其進行了評估。部分結果表明，臨時會議確實很常見，這些會議通常不會被記錄，並且提供用於捕獲和登記這些會議的基礎設施被證明是可行的，特別是對於上述環境。}
}

@thesis{CaiYunTingHuXiDaoJiBingDeZhongYiMoHuBianZheng2021,
  title = {呼吸道疾病的中醫模糊辨證},
  author = {{蔡昀庭}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2021},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/qj778t},
  abstract = {中醫將一個疾病定義成一連串的證候。證候的診斷被中醫稱為辨證。中醫辨證問題可以被定義為模糊分類問題，透過計算機龐大的記憶能力及迅速的運算能力，客觀地辨識病人所屬的證候是可行的。本研究團隊之前已開發一個中醫辨證系統，根據病人症狀集合，運用模糊集合，計算該病人罹患各證候的歸屬度。本篇論文著重於提升呼吸道相關證候的辨證精準度，透過病因共有症狀群、病位共有症狀群、病因病位症狀群、舌診症狀群和脈診症狀群的分群方式，將舊的證候症狀分四群的方式，改為新的證候症狀分五群的方式。並將群內症狀用「且」及「或」的概念來進一步分組，建構新的模糊集合的歸屬函數。本論文也透過基線鑑別係數及臨床病例準確度來驗證新辨證系統的辨證效果，將呼吸道有關證候的基線鑑別係數從0.477提升至0.585，衛表證候的臨床準確率從89\%提升至100\%及臨床鑑別係數從0.34提升至0.40，肺證候的臨床準確率從76\%提升至96\%及臨床鑑別係數從0.31提升至0.36。},
  pagetotal = {58},
  keywords = {中醫,實驗室}
}

@thesis{CaiYuXinZhongYiTiZhiZuoWeiAiZhengCunHuoYuCeDeYanJiu2020,
  title = {中醫體質作為癌症存活預測的研究},
  author = {{蔡昱信}},
  namea = {{林迺衛}},
  nameatype = {collaborator},
  date = {2020},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/7827za},
  abstract = {本研究探討接受中醫治療的癌症病患在不同的中醫體質和癌因性疲憊上的存活率是否存在差異。首先從大林慈濟醫院取得2010年至2017年的癌症登記資料庫與中醫門診紀錄資料庫，接著在資料前置處理的過程刪除和調整不齊全的資料。 本研究首先利用第一種方法，Kaplan-Meier計算存活率並繪製存活曲線圖，確認存活曲線無相交後利用對數等級檢定Log-rank test探討不同組別在存活率上是否存在差異，最後利用皮爾森卡方檢定Pearson's chi-squared test對分組後的病患其類別變量進行獨立檢定，初步判斷是否有其他可能的影響因子造就對數等級檢定判斷失準。初步研究結果顯示平和體質的癌症患者其存活時間及存活率較好，氣鬱體質的癌症患者其存活時間與存活率較差。但是平和體質組在某些類別變量上和非平和體質組存在分佈上的差異。 接著本研究利用第二種方法COX等比例風險模型 cox proportional hazards model，把可能影響存活率的影響因子放入模型中，平衡各影響因子的死亡風險。最後得出的模型結果顯示氣虛且中重度癌因性疲憊會造就死亡風險顯著提高1.94倍，而陽虛且中重度癌因性疲憊會造就死亡風險顯著提高2.24倍。},
  pagetotal = {70},
  keywords = {中醫,實驗室}
}

@article{caoBuildingUsingPersonal2022,
  title = {Building and {{Using Personal Knowledge Graph}} to {{Improve Suicidal Ideation Detection}} on {{Social Media}}},
  author = {Cao, Lei and Zhang, Huijun and Feng, Ling},
  date = {2022},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {24},
  pages = {87--102},
  issn = {1941-0077},
  doi = {10.1109/TMM.2020.3046867},
  url = {https://ieeexplore.ieee.org/document/9308975},
  urldate = {2023-12-03},
  abstract = {A large number of individuals are suffering from suicidal ideation in the world. There are a number of causes behind why an individual might suffer from suicidal ideation. As the most popular platform for self-expression, emotion release, and personal interaction, individuals may exhibit a number of symptoms of suicidal ideation on social media. Nevertheless, challenges from both data and knowledge aspects remain as obstacles, constraining the social media-based detection performance. Data implicitness and sparsity make it difficult to discover the inner true intentions of individuals based on their posts. Inspired by psychological studies, we build and unify a high-level suicide-oriented knowledge graph with deep neural networks for suicidal ideation detection on social media. We further design a two-layered attention mechanism to explicitly reason and establish key risk factors to individual's suicidal ideation. The performance study on microblog and Reddit shows that: 1) with the constructed personal knowledge graph, the social media-based suicidal ideation detection can achieve over 93\% accuracy; and 2) among the six categories of personal factors, post, personality, and experience are the top-3 key indicators. Under these categories, posted text, stress level, stress duration, posted image, and ruminant thinking contribute to one's suicidal ideation detection.},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  langid = {english},
  keywords = {回收,已整理,機器學習,知識圖譜},
  annotation = {27 citations (Crossref) [2024-03-26]\\
titleTranslation: 建立和使用個人知識圖來改善社群媒體上的自殺意念偵測},
  note = {將個人資料、發言紀錄、社交關係等建立成知識圖譜，結合機器學習預測自殺想法。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\ZGL8THKL\\Cao et al. - 2022 - Building and Using Personal Knowledge Graph to Imp.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\ZW5M7V5W\\9308975.html}
}

@article{carlosbadenes-olmedoLessonsLearnedEnable2023,
  title = {Lessons Learned to Enable Question Answering on Knowledge Graphs Extracted from Scientific Publications: {{A}} Case Study on the Coronavirus Literature},
  shorttitle = {Lessons Learned to Enable Question Answering on Knowledge Graphs Extracted from Scientific Publications},
  author = {{Carlos Badenes-Olmedo} and {Oscar Corcho}},
  date = {2023-06-01},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {Journal of Biomedical Informatics},
  volume = {142},
  pages = {104382},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2023.104382},
  url = {https://www.sciencedirect.com/science/article/pii/S153204642300103X},
  urldate = {2023-09-27},
  abstract = {The article presents a workflow to create a question-answering system whose knowledge base combines knowledge graphs and scientific publications on coronaviruses. It is based on the experience gained in modeling evidence from research articles to provide answers to questions in natural language. The work contains best practices for acquiring scientific publications, tuning language models to identify and normalize relevant entities, creating representational models based on probabilistic topics, and formalizing an ontology that describes the associations between domain concepts supported by the scientific literature. All the resources generated in the domain of coronavirus are available openly as part of the Drugs4COVID initiative, and can be (re)-used independently or as a whole. They can be exploited by scientific communities conducting research related to SARS-CoV-2/COVID-19 and also by therapeutic communities, laboratories, etc., wishing to find and understand relationships between symptoms, drugs, active ingredients and their documentary evidence.},
  keywords = {Evidences,Knowledge graphs,NLP,Ontology,Question-answering,問答系統,實體抽取,已整理,正在讀,知識抽取,語意分析,醫學,重要},
  annotation = {1 citations (Crossref) [2024-03-26]},
  file = {D\:\\Paper\\Lessons learned to enable question answering on knowledge graphs extracted from scientific publications： A case study on the coronavirus literature.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\WEVT4M32\\S153204642300103X.html}
}

@article{carloscacciuttoloResearchThesisUndergraduate2023,
  title = {Research {{Thesis}} for {{Undergraduate Engineering Programs}} in the {{Digitalization Era}}: {{Learning Strategies}} and {{Responsible Research Conduct Road}} to a {{University Education}} 4.0 {{Paradigm}}},
  shorttitle = {Research {{Thesis}} for {{Undergraduate Engineering Programs}} in the {{Digitalization Era}}},
  author = {{Carlos Cacciuttolo} and {Yaneth Vásquez} and {Deyvis Cano} and {Fernando Valenzuela}},
  date = {2023-01},
  journaltitle = {Sustainability},
  volume = {15},
  number = {14},
  pages = {11206},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2071-1050},
  doi = {10.3390/su151411206},
  url = {https://www.mdpi.com/2071-1050/15/14/11206},
  urldate = {2023-10-18},
  abstract = {Many university students have doubts about how or where to start writing their undergraduate thesis work; it is common not to be clear how to identify a research problem or even towards which topic their research is oriented, and there are doubts about how to process and systematize so much information available in the era of digitalization. This article presents learning strategies to formulate a research thesis for engineering undergraduate programs, with an emphasis on the use of information and communication technologies (ICTs) toward a University Education 4.0 paradigm. The main themes and issues discussed in this article, carried out through learning strategies for students based on the scientific method, are the following: (i) Recommendations for choosing a research topic, (ii) Guidelines for problem identification and question research, (iii) Suggestions for choosing a thesis advisor, (iv) Instructions for finding suitable sources of information, (v) Structure of the table of contents for writing the thesis manuscript, and (vi) Indications for preparing an oral defense of thesis research. This article highlights recommendations and precautions directed towards professors and students with the use of the chatbot-type artificial intelligence (AI) tool called ChatGPT for the formulation of the thesis under a responsible conduct approach in research. This article concludes that the application of tutoring/guidance strategies between professors and students requires an adequate ethical use of information and communication technologies (ICTs) during the development of a research thesis to generate a comprehensive educational environment that encourages research and develops a sustainable learning process in the context of the University Education 4.0 paradigm.},
  issue = {14},
  langid = {english},
  keywords = {ChatGPT,education 4.0,engineering,ethical issues,information and communication technologies,professors,research,students,thesis,undergraduate program,university,已整理,概讀,研究流程,重要},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 数字化时代工程学本科课程的研究论文：学习策略与负责任的研究行为 通往大学教育 4.0 范式之路\\
abstractTranslation:  许多大学生对如何或从何处开始撰写本科毕业论文工作存在疑问；不清楚如何确定研究问题，甚至不知道自己的研究方向是什么是很常见的；对如何处理和系统化数字化时代的大量信息也存在疑问。本文介绍了为工程学本科课程撰写研究论文的学习策略，重点是如何利用信息和通信技术（ICTs）实现大学教育 4.0 范式。通过基于科学方法的学生学习策略，本文讨论的主要主题和问题如下：(i) 选择研究课题的建议，(ii) 发现问题和研究问题的指南，(iii) 选择论文指导教师的建议，(iv) 寻找合适信息来源的说明，(v) 撰写论文手稿的目录结构，以及 (vi) 准备论文研究口头答辩的说明。本文重点介绍了针对教授和学生的建议和注意事项，即使用名为 ChatGPT 的聊天机器人型人工智能（AI）工具，以负责任的研究行为方式撰写论文。本文的结论是，教授和学生之间的辅导/指导策略的应用要求在研究论文的撰写过程中充分合乎道德地使用信息和通信技术（ICTs），以便在大学教育 4.0 范式的背景下创造一个鼓励研究和发展可持续学习过程的综合教育环境。},
  note = {還算有幫助但幫助不大，可以做為參考來源。
\par
有一半的內容都再講永續發展，全部跳過。
\par
還算有一個可以用得研究方法。
\par
提到在現代的學生學習時有那些改變，以及這些改變對撰寫論文造成的影響。
\par
另外，關於整個撰寫流程，從主題的選定、教授的選擇到最終的口試，提出了過程中需要注意的項目。
\par
此外還有提到ChatGPT在現代的論文撰寫中能提供的幫助。
\par
還有終身學習、永續發展、正確的引用等相關主題。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\A74VMLRW\\Cacciuttolo et al. - 2023 - Research Thesis for Undergraduate Engineering Prog.pdf;D\:\\Paper\\Research Thesis for Undergraduate Engineering Programs in the Digitalization Era： Learning Strategies and Responsible Research Conduct Road to a University Education 4.0 Paradigm.pdf}
}

@article{carstenosterlundDocumentationAccessKnowledge2019,
  title = {Documentation and Access to Knowledge in Online Communities: {{Know}} Your Audience and Write Appropriately?},
  shorttitle = {Documentation and Access to Knowledge in Online Communities},
  author = {{Carsten Østerlund} and {Kevin Crowston}},
  date = {2019},
  journaltitle = {Journal of the Association for Information Science and Technology},
  volume = {70},
  number = {6},
  pages = {619--633},
  issn = {2330-1643},
  doi = {10.1002/asi.24152},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24152},
  urldate = {2023-09-13},
  abstract = {Virtual collaborations bring together people who must work together despite having varied access to and understanding of the work at hand. In many cases, the collaborations are technology supported, meaning that the work is done through shared documents. We develop a framework articulating the characteristics of documents supporting collaborators with access to asymmetric knowledge versus those with access to symmetric knowledge. Drawing on theories about document genre, boundary objects, and provenance, we hypothesize that documents supporting asymmetric collaborators are likely to articulate or prescribe their own (a) purpose, (b) context of use, (c) content and form, and (d) provenance in greater detail than documents supporting symmetric collaborators. We explore these hypotheses through content analysis of documents and instructions for documents from a variety of free/libre open-source projects (FLOSS). We present findings consistent with the hypotheses developed as well as results extending beyond our theory-derived assumptions. When participants have access to the same knowledge, the study suggests that prescriptions about the content of documents become less important compared with prescriptions about the context, provenance, and process of work. The study contributes with a dynamic perspective on communicative practices that consider an often-uneven distribution of knowledge in virtual collaborations.},
  langid = {english},
  keywords = {已整理,知識分享,調查},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 在線社區中的文檔和知識獲取：了解您的受眾並適當地寫作？\\
abstractTranslation:  虛擬協作將必須一起工作的人們聚集在一起，儘管他們對手頭的工作有不同的訪問和理解。在許多情況下，協作是由技術支持的，這意味著工作是通過共享文檔完成的。我們開發了一個框架，闡明支持協作者訪問非對稱知識與訪問對稱知識的文檔的特徵。借鑒有關文檔類型、邊界對象和出處的理論，我們假設支持不對稱協作者的文檔可能闡明或規定他們自己的（a）目的，（b）使用上下文，（c）內容和形式，以及（d） ）出處比支持對稱協作者的文檔更詳細。我們通過對來自各種免費/自由開源項目 (FLOSS) 的文檔和文檔的說明進行內容分析來探索這些假設。我們提出的發現與所提出的假設一致，並且結果超出了我們的理論推導的假設。研究表明，當參與者能夠獲得相同的知識時，與有關工作背景、出處和流程的規定相比，有關文檔內容的規定變得不那麼重要。該研究從動態角度對交流實踐做出了貢獻，考慮了虛擬協作中知識分佈往往不均勻的情況。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\L6HYTH2L\\Østerlund 與 Crowston - 2019 - Documentation and access to knowledge in online co.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\GQLAWJBM\\asi.html}
}

@inproceedings{cassiatrojahndossantosHowWriteGood2019,
  title = {How to Write a Good State of the Art: Should It Be the First Step of Your Thesis ?},
  shorttitle = {How to Write a Good State of the Art},
  author = {{Cássia Trojahn dos Santos}},
  date = {2019},
  url = {https://www.semanticscholar.org/paper/How-to-write-a-good-state-of-the-art%3A-should-it-be-Santos/3765178bbca50c268d05d47a0e942d63888d5e6a},
  urldate = {2023-10-20},
  abstract = {In scientific writing, the state of the art describes the current knowledge about the studied matter through the analysis of similar or related published work. It might provide a comprehensive overview of what has been done in the field and what should be further investigated, in order to help formulating the problems and hypothesis the thesis intends to address. Producing a good state of the art might be considered the main initial step of a PhD thesis. This is however a challenging task that involves analysing, comparing, evaluating and linking different sources (i.e., many hours of reading and content organisation). This task may be also considered an intimidating task that requires the help and guiding of the thesis supervisors. While starting a thesis by producing (good) state of the art is a nice starting point, this process is non linear and involves many iterations. It requires as well to be continuously and incrementally refreshed. The different types of state of the art (to be incorporated into a paper, a survey article or thesis), the steps of writing a (good) state of the art and some tips facilitating the process will be discussed.},
  eventtitle = {Rencontres Des {{Jeunes Chercheurs}} En {{Intelligence Artificielle}}},
  keywords = {No DOI found},
  note = {[TLDR] In scientific writing, the state of the art describes the current knowledge about the studied matter through the analysis of similar or related published work in order to help formulating the problems and hypothesis the thesis intends to address.}
}

@inproceedings{cassioreginatoGOFORGoalOrientedFramework2019,
  title = {{{GO-FOR}}: {{A Goal-Oriented Framework}} for {{Ontology Reuse}}},
  shorttitle = {{{GO-FOR}}},
  booktitle = {2019 {{IEEE}} 20th {{International Conference}} on {{Information Reuse}} and {{Integration}} for {{Data Science}} ({{IRI}})},
  author = {{Cássio Reginato} and {Jordana Salamon} and {Gabriel Nogueira} and {Monalessa Barcellos} and {Vítor Souza} and {Maxwell Monteiro}},
  date = {2019-07},
  pages = {99--106},
  doi = {10.1109/IRI.2019.00028},
  url = {https://ieeexplore.ieee.org/document/8843481},
  urldate = {2023-09-28},
  abstract = {Ontologies have been successfully used to assign semantics in the Semantic Web context and to enable integration of data from different systems or different sources. However, building ontologies is not a trivial task. Ontology reuse can help in this matter. The search and selection of ontologies to be reused should consider the alignment between their scope and the scope of the ontology being developed. In this paper, we discuss how goal modeling can be helpful in this context and we propose GO-FOR, a framework in which goals are the central elements to promote ontology reuse. We introduce goal-oriented ontology patterns as a new type of pattern to be applied to develop ontologies in a goal-oriented approach. Results of the use of GO-FOR to build an ontology used to integrate water quality data are also shown in this paper.},
  eventtitle = {2019 {{IEEE}} 20th {{International Conference}} on {{Information Reuse}} and {{Integration}} for {{Data Science}} ({{IRI}})},
  keywords = {本體建立,知識本體},
  annotation = {4 citations (Crossref) [2024-03-26]},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\AZPS99SM\\Reginato et al. - 2019 - GO-FOR A Goal-Oriented Framework for Ontology Reu.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\BZB9RJJV\\8843481.html}
}

@article{cecilenghuangchuaDerivingKnowledgeRepresentation2012,
  title = {Deriving Knowledge Representation Guidelines by Analyzing Knowledge Engineer Behavior},
  author = {{Cecil Eng Huang Chua} and {Veda C. Storey} and {Roger H. L. Chiang}},
  date = {2012-12-01},
  journaltitle = {Decision Support Systems},
  shortjournal = {Decision Support Systems},
  volume = {54},
  number = {1},
  pages = {304--315},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2012.05.038},
  url = {https://www.sciencedirect.com/science/article/pii/S0167923612001492},
  urldate = {2023-09-18},
  abstract = {Knowledge engineering research has focused on proposing knowledge acquisition techniques, developing and evaluating knowledge representation schemes and engineering tools, and testing and debugging knowledge-based systems. Few formal studies have been conducted on understanding the behaviors and roles of knowledge engineers. Applying the theory of mental models, this paper describes a think aloud verbal protocol study to determine an empirical basis for understanding: (1) how knowledge engineers extract domain knowledge from textual sources; and (2) the cognitive mechanisms by which they engage various knowledge representation schemes to represent that knowledge acquired. The results suggest that knowledge representation is not simply a translation of acquired knowledge to a knowledge representation. Instead, it is an iterative process of selective querying of acquired knowledge, and continuous refinement of a model leveraging, not only on acquired knowledge from domain experts, but also from the knowledge engineer. From the findings of empirical studies, a set of guidelines is derived to support the training and development of better knowledge representation schemes, representation processes, and knowledge engineering tools.},
  langid = {english},
  keywords = {Knowledge engineering,Knowledge representation,Problem behavior graph,Protocol analysis,Theory of mental models,使用者調查,已整理,本體建立,知識本體},
  annotation = {7 citations (Crossref) [2024-03-26]\\
abstractTranslation:  知識工程研究的重點是提出知識獲取技術、開發和評估知識表示方案和工程工具以及測試和調試基於知識的系統。很少有正式的研究來了解知識工程師的行為和角色。應用心智模型理論，本文描述了一種有聲思考言語協議研究，以確定理解的實證基礎：（1）知識工程師如何從文本源中提取領域知識； (2)他們採用各種知識表示方案來表示所獲得的知識的認知機制。結果表明，知識表示不僅僅是將獲得的知識轉化為知識表示。相反，它是一個迭代過程，選擇性查詢所獲得的知識，並不斷完善模型，不僅利用從領域專家那裡獲得的知識，還利用從知識工程師那裡獲得的知識。根據實證研究的結果，得出了一套指導方針來支持更好的知識表示方案、表示過程和知識工程工具的培訓和開發。\\
titleTranslation: 透過分析知識工程師行為得出知識表示指南},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\47MAGLSS\\Chua 等。 - 2012 - Deriving knowledge representation guidelines by an.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\7MJKKMBE\\S0167923612001492.html}
}

@article{cecilenghuangchuaKnowledgeRepresentationConceptual2012,
  title = {Knowledge {{Representation}}: {{A Conceptual Modeling Approach}}},
  shorttitle = {Knowledge {{Representation}}},
  author = {{Cecil Eng Huang Chua} and {Veda C. Storey} and {Roger H. Chiang}},
  date = {2012-01-01},
  journaltitle = {Journal of Database Management (JDM)},
  shortjournal = {JDM},
  volume = {23},
  number = {1},
  pages = {1--30},
  publisher = {IGI Global},
  issn = {1063-8016},
  doi = {10.4018/jdm.2012010101},
  url = {https://www.igi-global.com/article/knowledge-representation-conceptual-modeling-approach/www.igi-global.com/article/knowledge-representation-conceptual-modeling-approach/62030},
  urldate = {2023-09-18},
  abstract = {Substantial work in knowledge engineering has focused on eliciting knowledge and representing it in a computational form. However, before elicited knowledge can be represented, it must be integrated and transformed so the knowledge engineer can understand it. This research identifies the need to sep...},
  langid = {english},
  keywords = {無法取得,知識本體},
  annotation = {8 citations (Crossref) [2024-03-26]\\
abstractTranslation:  知識工程的實質工作集中在獲取知識並以計算形式表示它。然而，在表達知識之前，必須對其進行整合和轉換，以便知識工程師能夠理解它。這項研究確定了需要分離...\\
titleTranslation: 知識表示：概念建模方法}
}

@article{ChaiKaiJieJiYuQuKuaiLianDeZhongYiDianZiBingLiGaoXiaoChaXunFangFaYanJiu2021,
  title = {基于区块链的中医电子病历高效查询方法研究},
  author = {{柴凯杰} and {丁有伟} and {胡孔法}},
  date = {2021},
  journaltitle = {软件导刊},
  doi = {10.11907/rjdk.212129},
  abstract = {随着可信时代的到来,人们对中医电子病历安全,高效查询的要求越来越高.针对中医电子病历查询无法兼顾数据安全性与查询性能的问题,提出一种基于区块链的中医电子病历高效查询方法.在用区块链不可篡改,去中心化的特性保证中医电子病历安全性的同时,通过分片策略与索引技术提高了中医药数据查询性能,并且实现了中医药数据的定值查询与范围查询.安全性分析显示,利用区块链技术可有效保障中医电子病历数据的安全性.利用真实数据集对该方法进行测试,实验结果表明,基于两个分片的查询相比不分片情况下的查询性能大幅提升,同时使用区块链索引的查询显著优于没有索引的查询,验证了该方法的可行性.},
  keywords = {中醫,區塊鏈,回收,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\I52G98D5\柴凯杰 等。 - 2021 - 基于区块链的中医电子病历高效查询方法研究.pdf}
}

@online{ChangJianZhiShiTuPuXiLieIVDBpedia,
  title = {常见知识图谱系列 IV DBpedia CN\_DBpedia},
  url = {https://zhuanlan.zhihu.com/p/232121628},
  urldate = {2023-09-28},
  abstract = {这是常见知识图谱系列第四篇：主要介绍基于英文wikipedia和中文weikpedia构建知识库(DBpedia, CN\_DBpedia)的过程 常见知识图谱系列链接:常见知识图谱和概念图谱: https://zhuanlan.zhihu.com/p/214093805Yago and …},
  langid = {chinese},
  organization = {知乎专栏},
  keywords = {中文,文章,知識圖譜},
  annotation = {titleTranslation: 常見知識圖譜系列 IV DBpedia CN\_DBpedia\\
abstractTranslation:  這是常見知識圖譜系列第四篇：主要介紹基於中文wikipedia和中文weikpedia構建知識庫(DBpedia、CN\_DBpedia)的過程常見知識圖譜系列連結：常見知識圖譜和概念圖譜：https://zhuanlan.zhihu.com /p/214093805亞戈和...},
  file = {C:\Users\BlackCat\Zotero\storage\J4PCU2X3\232121628.html}
}

@article{changliuTextMiningbasedApproach2023,
  title = {A Text Mining-Based Approach for Understanding {{Chinese}} Railway Incidents Caused by Electromagnetic Interference},
  author = {{Chang Liu} and {Shiwu Yang}},
  date = {2023-01-01},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  shortjournal = {Engineering Applications of Artificial Intelligence},
  volume = {117},
  pages = {105598},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2022.105598},
  url = {https://www.sciencedirect.com/science/article/pii/S0952197622005887},
  urldate = {2023-09-18},
  abstract = {The high-speed railway is a deeply coupled system with strong and weak electrical equipment, while complex electromagnetic interference (EMI) consequently brings potential risks and hazards to signaling safety. Since the incident reports on signaling failure intrinsically reflect the generation and evolution mechanism of equipment failures, relying on text mining technology, this paper tries to extract failure-related entities and constructs a knowledge graph to clarify the negative impact of the on-site electromagnetic environment. Firstly, based on convolutional neural networks (CNN), a supervised deep learning model for Chinese text classification is established to generate a corpus containing only railway failures caused by EMI. Then, the bidirectional long short-term memory (BiLSTM) and bidirectional encoder representations from transformers (BERT) algorithms are adopted to build the named entity recognition (NER) model. A NER algorithm more suitable for Chinese text features is proposed through ensemble modeling, training verification, and comparative evaluation. Finally, the knowledge storage and visualization of relational graph construction based on the Neo4j database are realized according to the obtained failure-related entities. This knowledge topology network effectively explores the inherent relationship between EMI factors and railway safety, as well as provides support for improving the safety assessment and enhancing the anti-interference performance of the equipment.},
  langid = {english},
  keywords = {Deep learning,Electromagnetic interference,Knowledge graph,Railway safety,Text mining,中文,實體識別,已整理,待讀,知識圖譜,重要},
  annotation = {8 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於文本挖掘的方法了解電磁幹擾引起的中國鐵路事故\\
abstractTranslation:  高鐵是一個強弱電設備深度耦合的系統，複雜的電磁幹擾（EMI）對訊號安全帶來潛在的風險和危害。由於訊號故障事件報告本質上反映了設備故障的產生和演化機制，本文依托文本挖掘技術，嘗試提取故障相關實體並建立知識圖譜，闡明現場電磁環境的負面影響。首先，基於卷積神經網路（CNN），建立用於中文文本分類的監督深度學習模型，產生僅包含電磁幹擾引起的鐵路故障的語料庫。然後，採用雙向長短期記憶（BiLSTM）和來自變壓器的雙向編碼器表示（BERT）演算法來建立命名實體辨識（NER）模型。透過整合建模、訓練驗證和比較評估，提出了一種更適合中文文字特徵的NER演算法。最後根據所取得的故障相關實體，實現基於Neo4j資料庫的關係圖所建構的知識儲存與視覺化。此知識拓樸網絡有效探討了EMI因素與鐵路安全之間的內在關係，為完善安全評估、增強設備抗干擾性能提供支撐。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\F3HEYEME\\Liu 與 Yang - 2023 - A text mining-based approach for understanding Chi.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\KN6B588B\\S0952197622005887.html}
}

@inproceedings{changWebQAMultihopMultimodal2022,
  title = {{{WebQA}}: {{Multihop}} and {{Multimodal QA}}},
  shorttitle = {{{WebQA}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chang, Yingshan and Cao, Guihong and Narang, Mridu and Gao, Jianfeng and Suzuki, Hisami and Bisk, Yonatan},
  date = {2022-06},
  pages = {16474--16483},
  issn = {2575-7075},
  doi = {10.1109/CVPR52688.2022.01600},
  url = {https://ieeexplore.ieee.org/document/9879677},
  urldate = {2023-11-28},
  abstract = {Scaling Visual Question Answering (VQA) to the open-domain and multi-hop nature of web searches, requires fundamental advances in visual representation learning, knowledge aggregation, and language generation. In this work, we introduce WEBQA, a challenging new benchmark that proves difficult for large-scale state-of-the-art models which lack language groundable visual representations for novel objects and the ability to reason, yet trivial for humans. WebQA mirrors the way humans use the web: 1) Ask a question, 2) Choose sources to aggregate, and 3) Produce a fluent language response. This is the behavior we should be expecting from IoT devices and digital assistants. Existing work prefers to assume that a model can either reason about knowledge in images or in text. WebQA includes a secondary text-only QA task to ensure improved visual performance does not come at the cost of language understanding. Our challenge for the community is to create unified multimodal reasoning models that answer questions regardless of the source modality, moving us closer to digital assistants that not only query language knowledge, but also the richer visual online world.},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  langid = {english},
  keywords = {問答系統,已整理,機器學習},
  annotation = {8 citations (Crossref) [2024-03-26]\\
titleTranslation: WebQA：多跳與多模式 QA},
  note = {目標是解決圖像資料來源及多跳推理的問答問題。和研究方向較無相關。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\SGT7NJ2I\\Chang et al. - 2022 - WebQA Multihop and Multimodal QA.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\3ZN8YNKB\\9879677.html}
}

@article{ChangYuanHeng;DingYouWei;HuKongFaYiZhongJiYuQuKuaiLianDeZhongYiDianZiBingLiGongXiangFangFa2021,
  title = {一种基于区块链的中医电子病历共享方法},
  author = {{常源恒;丁有伟;胡孔法}},
  date = {2021},
  journaltitle = {软件导刊},
  volume = {20},
  number = {11},
  issn = {1672-7800},
  abstract = {随着大数据和人工智能等技术的发展与应用,基于中医电子病历的各类智慧医疗应用越来越多,这些应用需要对各个医院的中医电子病历进行共享与综合分析。当前中医电子病历共享主要采用云平台进行数据存储,并使用集中式的密钥分配与权限认证方法进行访问控制,容易因服务提供商的不可信导致数据泄漏及非法访问等安全问题。因此,提出一种基于区块链的中医电子病历安全共享方法,利用区块链的去中心化、可追溯、不可篡改等特性保证中医电子病历的安全存储、合法访问与访问存证,同时利用云平台超强的存储与计算能力构建弱中心化的区块链结构,以提高区块链的数据处理性能。通过理论分析与实验证明,在安全保障方面,该方案相比传统方案具有更强的安全性；在数据分享效率方面,每10个节点带来的延迟为200ms,具有一定可行性。},
  keywords = {No DOI found,中醫電子病歷,區塊鏈,回收,數據共享},
  file = {C:\Users\BlackCat\Zotero\storage\6AHJ8KFU\常源恒;丁有伟;胡孔法 - 2021 - 一种基于区块链的中医电子病历共享方法.pdf}
}

@inproceedings{chao-wenhsuanyuanInteractiveVisualExploration2023,
  title = {Interactive {{Visual Exploration}} of {{Knowledge Graphs}} with {{Embedding-based Guidance}}},
  booktitle = {Extended {{Abstracts}} of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {{Chao-Wen Hsuan Yuan} and {Tzu-Wei Yu} and {Jia-Yu Pan} and {Wen-Chieh Lin}},
  year = {4 月 19, 2023},
  series = {{{CHI EA}} '23},
  pages = {1--8},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3544549.3585596},
  url = {https://dl.acm.org/doi/10.1145/3544549.3585596},
  urldate = {2023-09-17},
  abstract = {Knowledge graphs have been commonly used to represent relationships between entities and utilized in the industry to enhance service qualities. As knowledge graphs integrate data from a variety of sources, they can also be useful references for human users. However, there is a lack of effective tools for data analysts to make the most of the rich information in knowledge graphs. Existing knowledge graph exploration systems are ineffective because they didn’t consider various users’ needs and the characteristics of knowledge graphs. Exploratory approaches specifically designed for uncovering and summarizing insights in knowledge graphs have not been well studied yet. In this paper, we propose KGScope that supports interactive visual explorations and provides embedding-based guidance to derive insights from knowledge graphs. We demonstrate KGScope with a usage scenario and assess its efficacy in supporting knowledge graph exploration with a user study. The results show that KGScope supports knowledge graph exploration effectively by providing useful information and aiding comprehensive exploration.},
  isbn = {978-1-4503-9422-2},
  langid = {english},
  keywords = {Interactive visual exploration,Knowledge graph,Knowledge graph embedding,人機互動,可視化,已整理,知識圖譜},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 具有基於嵌入的指導的知識圖的互動式視覺探索\\
abstractTranslation:  知識圖譜通常用於表示實體之間的關係，並在產業中用於提高服務品質。由於知識圖整合了各種來源的數據，因此它們也可以為人類使用者提供有用的參考。然而，數據分析師缺乏有效的工具來充分利用知識圖譜中豐富的資訊。現有的知識圖譜探索系統效率低下，因為它們沒有考慮到各種使用者的需求和知識圖譜的特徵。專門為揭示和總結知識圖中的見解而設計的探索性方法尚未得到充分研究。在本文中，我們提出了 KGScope，它支援互動式視覺探索並提供基於嵌入的指導以從知識圖譜中獲取見解。我們透過使用場景演示了 KGScope，並透過使用者研究評估了其在支援知識圖譜探索方面的功效。結果表明，KGScope 透過提供有用資訊並輔助綜合探索，有效支持知識圖譜探索。},
  file = {C:\Users\BlackCat\Zotero\storage\QHECQABE\Hsuan Yuan 等。 - 2023 - Interactive Visual Exploration of Knowledge Graphs.pdf}
}

@inproceedings{chaoshangTaxonomyConstructionUnseen2020,
  title = {Taxonomy {{Construction}} of {{Unseen Domains}} via {{Graph-based Cross-Domain Knowledge Transfer}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {{Chao Shang} and {Sarthak Dash} and {Md. Faisal Mahbub Chowdhury} and {Nandana Mihindukulasooriya} and {Alfio Gliozzo}},
  date = {2020-07},
  pages = {2198--2208},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.acl-main.199},
  url = {https://aclanthology.org/2020.acl-main.199},
  urldate = {2023-09-19},
  abstract = {Extracting lexico-semantic relations as graph-structured taxonomies, also known as taxonomy construction, has been beneficial in a variety of NLP applications. Recently Graph Neural Network (GNN) has shown to be powerful in successfully tackling many tasks. However, there has been no attempt to exploit GNN to create taxonomies. In this paper, we propose Graph2Taxo, a GNN-based cross-domain transfer framework for the taxonomy construction task. Our main contribution is to learn the latent features of taxonomy construction from existing domains to guide the structure learning of an unseen domain. We also propose a novel method of directed acyclic graph (DAG) generation for taxonomy construction. Specifically, our proposed Graph2Taxo uses a noisy graph constructed from automatically extracted noisy hyponym hypernym candidate pairs, and a set of taxonomies for some known domains for training. The learned model is then used to generate taxonomy for a new unknown domain given a set of terms for that domain. Experiments on benchmark datasets from science and environment domains show that our approach attains significant improvements correspondingly over the state of the art.},
  eventtitle = {{{ACL}} 2020},
  langid = {english},
  keywords = {未整理},
  annotation = {10 citations (Crossref) [2024-03-26]\\
titleTranslation: 透過基於圖的跨領域知識轉移來建構未見過的領域的分類法\\
abstractTranslation:  將詞彙語意關係提取為圖結構分類法（也稱為分類法建構）在各種 NLP 應用中都是有益的。最近，圖神經網路（GNN）在成功解決許多任務方面表現出了強大的功能。然而，還沒有嘗試利用 GNN 來創建分類法。在本文中，我們提出了 Graph2Taxo，這是一種用於分類建構任務的基於 GNN 的跨域傳輸框架。我們的主要貢獻是從現有領域學習分類法所建構的潛在特徵，以指導未知領域的結構學習。我們也提出了一種用於分類建構的有向無環圖（DAG）產生的新方法。具體來說，我們提出的 Graph2Taxo 使用由自動提取的噪聲下位詞上位詞候選對構建的噪聲圖，以及一些用於訓練的已知領域的一組分類法。然後，在給定該領域的一組術語的情況下，使用學習到的模型為新的未知領域產生分類法。科學和環境領域基準資料集的實驗表明，我們的方法相對於現有技術取得了顯著的改進。},
  file = {C:\Users\BlackCat\Zotero\storage\STSKWR9R\Shang 等。 - 2020 - Taxonomy Construction of Unseen Domains via Graph-.pdf}
}

@article{chaowuTaskdrivenCleaningPruning2023,
  title = {Task-Driven Cleaning and Pruning of Noisy Knowledge Graph},
  author = {{Chao Wu} and {Zeyu Zeng} and {Yajing Yang} and {Mao Chen} and {Xicheng Peng} and {Sannyuya Liu}},
  date = {2023-10-01},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {646},
  pages = {119406},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2023.119406},
  url = {https://www.sciencedirect.com/science/article/pii/S002002552300991X},
  urldate = {2023-09-18},
  abstract = {Many knowledge graphs, especially those that are collaboratively or automatically generated, are prone to noise and cross-domain entries, which can impede domain-specific applications. Existing methods for pruning inaccurate or out-of-domain information from knowledge graphs often rely on topological graph-pruning strategies. However, these approaches have two major drawbacks: they may discard logical structure and semantic information, and they allow multiple inheritance. To address these limitations, this study introduces KGPruning, which is a novel approach that can effectively clean and prune noisy knowledge graphs by guiding tasks with a given set of concepts and automatically generating a domain-specific taxonomy. Specifically, KGPruning employs a graph hierarchy inference method that is based on the Agony model to precisely identify and eliminate noisy entries while striving to preserve the underlying hierarchy of semantic relations as much as possible. Furthermore, to establish a tree-structured taxonomy, KGPruning integrates semantic relations and structural characteristics to effectively eliminate out-of-domain information and multiple inheritance. Through extensive experimental evaluations conducted on open benchmark datasets as well as large-scale real-world problems, the superior performance of KGPruning over state-of-the-art methods is demonstrated on the task of pruning noisy knowledge graphs.},
  langid = {english},
  keywords = {Knowledge graph pruning,Multiple inheritance,Noisy knowledge graph,Taxonomy,已整理,本體驗證,機器學習,知識分享,知識衝突},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 任務驅動的噪音知識圖譜清理與剪枝\\
abstractTranslation:  許多知識圖譜，尤其是那些協作或自動生成的知識圖譜，很容易出現雜訊和跨域條目，這可能會阻礙特定領域的應用程式。現有的從知識圖中修剪不準確或域外資訊的方法通常依賴拓樸圖修剪策略。然而，這些方法有兩個主要缺點：它們可能會丟棄邏輯結構和語義訊息，並且它們允許多重繼承。為了解決這些限制，本研究引入了 KGPruning，這是一種新穎的方法，可以透過使用給定的概念集來指導任務並自動生成特定於領域的分類法來有效地清理和修剪嘈雜的知識圖。具體來說，KGPruning 採用基於 Agony 模型的圖層次推理方法來精確識別和消除噪音條目，同時盡可能保留底層語義關係的層次結構。此外，為了建立樹狀結構的分類法，KGPruning整合了語意關係和結構特徵，有效消除了域外資訊和多重繼承。透過對開放基準資料集以及大規模現實世界問題進行廣泛的實驗評估，KGPruning 在修剪雜訊知識圖的任務上優於最先進的方法。},
  note = {藉由自動建立分類結構樹來幫助演算法修剪知識圖譜中不需要的部分(雜訊)。
\par
和研究方向較無關係。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\ATD4GKC5\\Wu et al. - 2023 - Task-driven cleaning and pruning of noisy knowledg.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\YQVI8RLF\\S002002552300991X.html}
}

@software{ChatchatspaceLangchainChatchat,
  title = {Chatchat-Space/{{Langchain-Chatchat}}},
  shorttitle = {Langchain-{{Chatchat}}},
  namea = {, chatchat-space},
  nameatype = {collaborator},
  url = {https://github.com/chatchat-space/Langchain-Chatchat}
}

@inproceedings{cheikhniangAppropriateGlobalOntology2011,
  title = {Appropriate Global Ontology Construction: A Domain-Reference-Ontology Based Approach},
  shorttitle = {Appropriate Global Ontology Construction},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Information Integration}} and {{Web-based Applications}} and {{Services}}},
  author = {{Cheikh Niang} and {Béatrice Bouchou} and {Moussa. Lo} and {Yacine Sam}},
  year = {12 月 5, 2011},
  series = {{{iiWAS}} '11},
  pages = {166--173},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2095536.2095566},
  url = {https://dl.acm.org/doi/10.1145/2095536.2095566},
  urldate = {2023-09-16},
  abstract = {Data integration involves combining data residing in different sources and providing users with a unified view of these data through what is called a "global schema". We address here the problem of automatic construction of this global schema in the semantic Web context, where data sources are annotated with ontologies. We aim in other words to automatically build a common vocabulary (ontology) that will serve as a shared conceptual level for several heterogeneous data sources needing to share their data in a specific application domain. We propose a solution based on the use of a domain reference ontology (or "background knowledge") as a mediation support.},
  isbn = {978-1-4503-0784-0},
  langid = {english},
  keywords = {/unread,data integration,已整理,知識分享,知識本體,語意分析},
  annotation = {2 citations (Crossref) [2024-03-26]\\
abstractTranslation:  數據整合涉及組合駐留在不同來源的數據，並透過所謂的「全局模式」為使用者提供這些數據的統一視圖。我們在這裡解決在語義 Web 上下文中自動建立此全域模式的問題，其中資料來源會以本體進行註解。換句話說，我們的目標是自動建立一個通用詞彙表（本體），它將作為需要在特定應用程式域中共享資料的多個異質資料來源的共享概念層級。我們提出了一個基於使用領域參考本體（或「背景知識」）作為中介支援的解決方案。\\
titleTranslation: 適當的全域本體建構：基於領域參考本體的方法},
  note = {研究如何使用本體來結合本體與本體},
  file = {C:\Users\BlackCat\Zotero\storage\DVG92H8C\Niang 等。 - 2011 - Appropriate global ontology construction a domain.pdf}
}

@thesis{ChenChangJunYiGeChaoChangZhiLingShuWeiXunHaoChuLiQiKeChongBiaoDeCBianYiQi2002,
  title = {{{一個超長指令數位訊號處理器可重標的C編譯器}}},
  author = {{陳昌浚}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2002},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/4579v2},
  abstract = {超長指令處理器必須藉由編譯器找出程式中可平行執行的指令，進而將它們排程為有效率的超長指令程式碼。LCC是一個針對傳統處理器的可重標C程式語言編譯器。LCC具有精簡且模組化的特性，但是尚缺乏支援超長指令處理器的功能。 本篇論文將以LCC為基礎，以精簡且模組化的方式修改LCC編譯器，來支援超長指令處理器。我們將分別加入一個資料相依性分析模組來找出程式中可平行執行的指令，以及一個指令排程模組來產生有效率的超長指令目的碼。我們研發的平台為德州儀器TMS320C6711處理器。我們也評比了我們所研發的編譯器與德州儀器所研發之編譯器。},
  pagetotal = {84}
}

@inproceedings{chenchenyeTextIncorporatingMetadata2021,
  title = {Beyond {{Text}}: {{Incorporating Metadata}} and {{Label Structure}} for {{Multi-Label Document Classification}} Using {{Heterogeneous Graphs}}},
  shorttitle = {Beyond {{Text}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {{Chenchen Ye} and {Linhai Zhang} and {Yulan He} and {Deyu Zhou} and {Jie Wu}},
  date = {2021-01},
  pages = {3162--3171},
  publisher = {Association for Computational Linguistics},
  location = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.253},
  url = {https://aclanthology.org/2021.emnlp-main.253},
  urldate = {2023-09-12},
  abstract = {Multi-label document classification, associating one document instance with a set of relevant labels, is attracting more and more research attention. Existing methods explore the incorporation of information beyond text, such as document metadata or label structure. These approaches however either simply utilize the semantic information of metadata or employ the predefined parent-child label hierarchy, ignoring the heterogeneous graphical structures of metadata and labels, which we believe are crucial for accurate multi-label document classification. Therefore, in this paper, we propose a novel neural network based approach for multi-label document classification, in which two heterogeneous graphs are constructed and learned using heterogeneous graph transformers. One is metadata heterogeneous graph, which models various types of metadata and their topological relations. The other is label heterogeneous graph, which is constructed based on both the labels' hierarchy and their statistical dependencies. Experimental results on two benchmark datasets show the proposed approach outperforms several state-of-the-art baselines.},
  eventtitle = {{{EMNLP}} 2021},
  langid = {english},
  keywords = {待讀,機器學習,知識分類},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 超越文本：使用異構圖結合元數據和標籤結構進行多標籤文檔分類\\
abstractTranslation:  多標籤文檔分類，將一個文檔實例與一組相關標籤相關聯，正在吸引越來越多的研究關注。現有方法探索文本之外的信息的合併，例如文檔元數據或標籤結構。然而，這些方法要么簡單地利用元數據的語義信息，要么採用預定義的父子標籤層次結構，忽略元數據和標籤的異構圖形結構，我們認為這對於準確的多標籤文檔分類至關重要。因此，在本文中，我們提出了一種新的基於神經網絡的多標籤文檔分類方法，其中使用異構圖轉換器構建和學習兩個異構圖。一是元數據異構圖，它對各種類型的元數據及其拓撲關係進行建模。另一種是標籤異構圖，它是基於標籤的層次結構及其統計依賴關係構建的。兩個基準數據集的實驗結果表明，所提出的方法優於幾個最先進的基線。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\5WM5LBCM\\Ye 等。 - 2021 - Beyond Text Incorporating Metadata and Label Stru.pdf;D\:\\download\\論文\\Beyond Text： Incorporating Metadata and Label Structure for Multi-Label Document Classification using Heterogeneous Graphs.pdf}
}

@online{chenDataJuicerOneStopData2023,
  title = {Data-{{Juicer}}: {{A One-Stop Data Processing System}} for {{Large Language Models}}},
  shorttitle = {Data-{{Juicer}}},
  author = {Chen, Daoyuan and Huang, Yilun and Ma, Zhijian and Chen, Hesen and Pan, Xuchen and Ge, Ce and Gao, Dawei and Xie, Yuexiang and Liu, Zhaoyang and Gao, Jinyang and Li, Yaliang and Ding, Bolin and Zhou, Jingren},
  date = {2023-12-20},
  eprint = {2309.02033},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.02033},
  url = {http://arxiv.org/abs/2309.02033},
  urldate = {2024-05-04},
  abstract = {The immense evolution in Large Language Models (LLMs) has underscored the importance of massive, heterogeneous, and high-quality data. A data recipe is a mixture of data from different sources for training LLMs, which plays a vital role in LLMs' performance. Existing open-source tools for LLM data processing are mostly tailored for specific data recipes. To continuously uncover the potential of LLMs, incorporate data from new sources, and improve LLMs' performance, we build a new system named Data-Juicer, with which we can efficiently generate diverse data recipes, explore different possibilities in forming data mixtures, and evaluate their effects on model performance. Different from traditional data-analytics pipelines, Data-Juicer faces some unique challenges. Firstly, the possible data sources for forming data recipes are truly heterogeneous and massive with various qualities. Secondly, it is extremely expensive to precisely evaluate data recipes' impact on LLMs' performance. Thirdly, the end users of Data-Juicer, model developers, need sufficient flexibility to configure and evaluate different data recipes. Data-Juicer features a fine-grained abstraction of pipelines for constructing data recipes, with over 50 built-in operators for easy composition and extension. By incorporating visualization and auto-evaluation capabilities, Data-Juicer enables a timely feedback loop for both LLM pre-training and fine-tuning. Further, Data-Juicer is optimized and integrated with ecosystems for LLM training, evaluation, and distributed computing. The data recipes derived with Data-Juicer gain notable improvements on state-of-the-art LLMs, by up to 7.45\% increase in averaged score across 16 LLM benchmarks and 17.5\% higher win rate in pair-wise GPT-4 evaluations. Our system, data recipes, and tutorials are released, calling for broader data-centric research on training and understanding LLMs.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Databases,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,未整理},
  annotation = {abstractTranslation:  大型語言模型 (LLM) 的巨大發展凸顯了大量、異質和高品質資料的重要性。資料配方是來自不同來源的資料的混合，用於訓練法學碩士，這對法學碩士的表現起著至關重要的作用。現有的 LLM 資料處理開源工具大多是針對特定資料配方量身打造的。為了不斷發掘LLM的潛力，整合新來源的數據，提高LLM的表現，我們建立了一個名為Data-Juicer的新系統，透過它我們可以有效地產生不同的數據配方，探索形成數據混合的不同可能性，並評估它們對模型性能的影響。與傳統的數據分析管道不同，Data-Juicer 面臨一些獨特的挑戰。首先，形成資料配方的可能資料來源確實是異質的、海量的、品質各異的。其次，精確評估資料配方對法學碩士績效的影響極為昂貴。第三，Data-Juicer的最終用戶，即模型開發人員，需要足夠的靈活性來配置和評估不同的資料配方。 Data-Juicer 具有用於建立資料配方的細粒度管道抽象，具有 50 多個內建運算符，可輕鬆組合和擴展。透過整合視覺化和自動評估功能，Data-Juicer 為 LLM 預訓練和微調提供了及時的回饋循環。此外，Data-Juicer 還針對 LLM 訓練、評估和分散式運算進行了最佳化並與生態系統整合。使用 Data-Juicer 得出的資料配方在最先進的 LLM 上獲得了顯著改進，16 個 LLM 基準的平均分數提高了 7.45\%，成對 GPT-4 評估的勝率提高了 17.5\%。我們的系統、數據配方和教程已發布，呼籲對法學碩士的培訓和理解進行更廣泛的以數據為中心的研究。\\
titleTranslation: Data-Juicer：大型語言模型的一站式資料處理系統},
  note = {Comment: 20 Pages, 10 figures, 9 tables. The system, data recipes, and demos are continuously maintained at https://github.com/alibaba/data-juicer},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\3JY8V56D\\Chen 等。 - 2023 - Data-Juicer A One-Stop Data Processing System for.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\MT62NXDM\\2309.html}
}

@online{chenEvaluatingEnhancingLarge2024,
  title = {Evaluating and {{Enhancing Large Language Models Performance}} in {{Domain-specific Medicine}}: {{Osteoarthritis Management}} with {{DocOA}}},
  shorttitle = {Evaluating and {{Enhancing Large Language Models Performance}} in {{Domain-specific Medicine}}},
  author = {Chen, Xi and You, MingKe and Wang, Li and Liu, WeiZhi and Fu, Yu and Xu, Jie and Zhang, Shaoting and Chen, Gang and Li, Kang and Li, Jian},
  date = {2024-01-19},
  eprint = {2401.12998},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.12998},
  url = {http://arxiv.org/abs/2401.12998},
  urldate = {2024-03-13},
  abstract = {The efficacy of large language models (LLMs) in domain-specific medicine, particularly for managing complex diseases such as osteoarthritis (OA), remains largely unexplored. This study focused on evaluating and enhancing the clinical capabilities of LLMs in specific domains, using osteoarthritis (OA) management as a case study. A domain specific benchmark framework was developed, which evaluate LLMs across a spectrum from domain-specific knowledge to clinical applications in real-world clinical scenarios. DocOA, a specialized LLM tailored for OA management that integrates retrieval-augmented generation (RAG) and instruction prompts, was developed. The study compared the performance of GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less effective in the specialized domain of OA management, particularly in providing personalized treatment recommendations. However, DocOA showed significant improvements. This study introduces a novel benchmark framework which assesses the domain-specific abilities of LLMs in multiple aspects, highlights the limitations of generalized LLMs in clinical contexts, and demonstrates the potential of tailored approaches for developing domain-specific medical LLMs.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,LLM,RAG,問答系統,已整理,醫學},
  annotation = {titleTranslation: 評估和增強大型語言模型在特定領域醫學中的表現：使用 DocOA 進行骨關節炎管理\\
abstractTranslation:  大語言模型 (LLM) 在特定領域醫學中的功效，特別是在治療骨關節炎 (OA) 等複雜疾病方面的功效，在很大程度上仍未被探索。這項研究的重點是評估和提高法學碩士在特定領域的臨床能力，以骨關節炎 (OA) 管理作為案例研究。開發了一個特定領域的基準框架，該框架對從特定領域知識到現實臨床場景中的臨床應用的法學碩士進行評估。 DocOA是專為OA管理量身定制的專業法學碩士，整合了檢索增強生成（RAG）和指示提示。該研究使用客觀和人工評估來比較 GPT-3.5、GPT-4 和專業助理 DocOA 的表現。結果顯示，GPT-3.5 和 GPT-4 等普通法學碩士在 OA 管理專業領域中效果較差，特別是在提供個人化治療建議方面。然而，DocOA 顯示出顯著的改進。本研究引入了一個新穎的基準框架，該框架從多個方面評估法學碩士的特定領域能力，強調了廣義法學碩士在臨床環境中的局限性，並展示了開發特定領域醫學法學碩士的定制方法的潛力。},
  note = {Comment: 16 Pages, 7 Figures},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\BPZDGVI9\\Chen 等。 - 2024 - Evaluating and Enhancing Large Language Models Per.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\UCP7TVKR\\2401.html}
}

@inproceedings{chengbiaoyangTemporalSemanticSearch2020,
  title = {A {{Temporal Semantic Search System}} for {{Traditional Chinese Medicine Based}} on {{Temporal Knowledge Graphs}}},
  booktitle = {Semantic {{Technology}}},
  author = {{Chengbiao Yang} and {Weizhuo Li} and {Xiaoping Zhang} and {Runshun Zhang} and {Guilin Qi}},
  editor = {{Xin Wang} and {Francesca A. Lisi} and {Guohui Xiao} and {Elena Botoeva}},
  date = {2020},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {13--20},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-15-3412-6_2},
  abstract = {Traditional Chinese medicine (TCM) is an important intangible cultural heritage of China. To enhance the services of TCM, many works focus on constructing various types of TCM knowledge graphs according to the concrete requirements such as information retrieval. However, most of them ignored several key issues. One is temporal information that is very important for TCM clinical diagnosis and treatment. For example, a herb needs to be boiled for different periods in different prescriptions, but existing methods cannot represent this temporal information very well. The other is that current TCM-based retrieval systems cannot effectively deal with the temporal intentions of search sentences, which leads to bad experiences for users in retrieval services. To solve these issues, we propose a new model tailored for TCM based on the temporal knowledge graph in this paper, which can effectively represent the clinical knowledge changing dynamically over time. Moreover, we implement a temporal semantic search system and employ reasoning rules based on our proposed model to complete the temporal intentions of search sentences. The preliminary result indicates that our system can obtain better results than existing methods in terms of precision.},
  isbn = {9789811534126},
  langid = {english},
  keywords = {Temporal intention,Temporal knowledge graph,Temporal semantic search,Traditional Chinese medicine,中醫,問答系統,回收,已整理,知識圖譜},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於時態知識圖的中醫時態語意搜尋系統},
  file = {C:\Users\BlackCat\Zotero\storage\5VT4XDPV\Yang et al. - 2020 - A Temporal Semantic Search System for Traditional .pdf}
}

@inproceedings{chenglinzengSciDGBenchmarkingScientific2023,
  title = {{{SciDG}}: {{Benchmarking Scientific Dynamic Graph Queries}}},
  shorttitle = {{{SciDG}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Scientific}} and {{Statistical Database Management}}},
  author = {{Chenglin Zeng} and {Chuan Hu} and {Huajin Wang} and {Zhihong Shen}},
  year = {8 月 27, 2023},
  series = {{{SSDBM}} '23},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3603719.3603724},
  url = {https://dl.acm.org/doi/10.1145/3603719.3603724},
  urldate = {2023-09-11},
  abstract = {Dynamic graphs are increasingly being utilized in domain knowledge modeling and large-scale scientific data management. Managing dynamic graph data requires a graph database system that can handle constantly changing volumes and data versions, while maintaining an acceptable query latency related to versioning. To understand how the design of storage structures affects database performance and assist scientific application developers in finding the optimal storage structure for their dynamic graph application scenarios, we have designed an easy-to-use benchmark framework called SciDG. We also conducted a study on the latencies of five fundamental version-related queries for various scientific application scenarios using SciDG. We evaluated the performance of databases based on three distinct storage principles: Sp-DB, Dp-DB, and Tp-DB. The experimental results indicate that SciDG is a valuable tool for assessing the strengths and weaknesses of different storage structures for dynamic graphs in various scenarios. Additionally, it assists scientists in selecting the most suitable dynamic graph database system for their work.},
  isbn = {9798400707469},
  langid = {english},
  keywords = {dynamic graph database,scientific data,version-related query,回收,知識圖譜,軟體測試},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: SciDG：科學動態圖查詢基準測試\\
abstractTranslation:  動態圖越來越多地應用於領域知識建模和大規模科學數據管理。管理動態圖數據需要一個圖數據庫系統，該系統可以處理不斷變化的捲和數據版本，同時保持與版本控制相關的可接受的查詢延遲。為了了解存儲結構的設計如何影響數據庫性能，並幫助科學應用程序開發人員找到適合其動態圖應用場景的最佳存儲結構，我們設計了一個易於使用的基準測試框架，稱為 SciDG。我們還使用 SciDG 對各種科學應用場景的五個基本版本相關查詢的延遲進行了研究。我們根據三種不同的存儲原則評估數據庫的性能：Sp-DB、Dp-DB 和 Tp-DB。實驗結果表明，SciDG 是評估各種場景下動態圖不同存儲結構的優缺點的有價值的工具。此外，它還可以幫助科學家選擇最適合其工作的動態圖數據庫系統。},
  note = {考量版本管控、效能等因素，圖數據應該要以恰當的方式儲存。本研究設計一個測試框架SciDG，實驗證明可以有效的推薦適合的圖數據系統或結構。超出研究範圍。},
  file = {C:\Users\BlackCat\Zotero\storage\N724QG5Z\Zeng 等。 - 2023 - SciDG Benchmarking Scientific Dynamic Graph Queri.pdf}
}

@article{chengmingliConstructionTraditionalChinese2023,
  title = {Construction of {{Traditional Chinese Medicine Knowledge Graph Question Answering System Based}} on {{Data Augmentation}}},
  author = {{Chengming Li} and {Jun Pan}},
  date = {2023-01},
  journaltitle = {World Scientific Research Journal},
  volume = {9},
  number = {1},
  pages = {74--83},
  publisher = {Boya Century Publishing Limited},
  issn = {2472-3703},
  doi = {10.6911/WSRJ.202301_9(1).0009},
  url = {https://www.airitilibrary.com/Publication/alDetailedMesh?docid=P20190709001-202301-202212280017-202212280017-74-83},
  urldate = {2023-09-15},
  abstract = {The field of Traditional Chinese Medicine (TCM) contains a large amount of knowledge data, but the organization of the data is very different from that required by modern information technology, and currently, the Chinese language domain lacks a knowledge graph related to the field of TCM. On the one hand, there are difficulties in constructing a high‐quality knowledge graph in the field of Chinese medicine; on the other hand, automated QA requires a system with strong natural language understanding, and there is a lack of a knowledge base QA training corpus in the field of Chinese medicine, and there are challenges in automated QA tasks based on deep learning. This paper proposes a study on the construction of QA system based on TCM knowledge graph. Firstly, a training corpus of question sentences is generated and automatically annotated based on the existing knowledge base, secondly, a BiLSTM‐CRF entity recognition model for question sentences and a BERT‐TextCNN intention recognition model are used to achieve semantic parsing of the question sentences, and finally, the knowledge base answer query is completed by converting natural language question sentences to query statements. The aim of this thesis is to solve the difficulties in the application of knowledge graphs in the field of TCM through the above‐mentioned research content, and to improve the natural language understanding capability of the automatic QA system based on knowledge graph through deep learning models, so as to finally build a QA system that can meet the needs of users and provide practical help in the popularization of knowledge in the field of TCM and the clinical application of TCM.},
  langid = {english},
  keywords = {Named entity recognition,Relation extraction,中醫,問答系統,已整理,機器學習,知識本體},
  annotation = {titleTranslation: 基於數據增強的中醫知識圖譜問答系統構建\\
abstractTranslation:  中醫藥領域蘊藏著大量的知識數據，但數據的組織方式與現代信息技術所要求的有很大差異，目前中文領域缺乏與中醫藥領域相關的知識圖譜。中醫。一方面，中醫藥領域高質量的知識圖譜構建存在困難；另一方面，自動化QA需要具有較強自然語言理解能力的系統，而中醫領域缺乏知識庫QA訓練語料庫，基於深度學習的自動化QA任務存在挑戰。本文提出基於中醫知識圖譜的問答系統構建研究。首先，基於現有知識庫生成問句訓練語料並自動標註，其次利用BiLSTM-CRF問句實體識別模型和BERT-TextCNN意圖識別模型實現問句語義解析，最後通過將自然語言疑問句轉換為查詢語句，完成知識庫答案查詢。本論文的目的是通過上述研究內容解決知識圖譜在中醫領域應用的難點，並通過深度學習模型提高基於知識圖譜的自動問答系統的自然語言理解能力從而最終構建一個能夠滿足用戶需求的問答系統，為中醫領域知識的普及和中醫臨床應用提供切實的幫助。},
  note = {根據中醫的結構化資料建立知識圖譜，並結合實體抽取及語義分析，做neo4j的自然語言搜尋。但所謂的結構化資料在論文中並沒有提供來源。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\GLM4DVCD\\Li 與 Pan - 2023 - Construction of Traditional Chinese Medicine Knowl.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\GRUXRCJU\\alDetailedMesh.html}
}

@article{ChengNanJiYuNLPJiShuHouJieGouHuaChuLiDeDianZiBingLiYingYong2021,
  title = {基于NLP技术后结构化处理的电子病历应用},
  author = {{程楠} and {侯豪} and {牛亚军} and {唐浩} and {杨旭} and {张现辉}},
  date = {2021},
  journaltitle = {河南医学研究},
  shortjournal = {Henan Medical Research},
  volume = {30},
  number = {24},
  pages = {4510--4513},
  doi = {10.3969/j.issn.1004-437x.2021.24.029},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5Eg9obnl4eWoyMDIxMjQwMjkaCDh2OW9ibHoy},
  urldate = {2022-08-14},
  abstract = {目的 探索电子病历后结构化命名实体识别率.方法 抽取1000份运行电子病历,利用自然语言处理(NLP)技术进行语义识别.结果 本次评测抽取了1000份入院记录,针对检查、临床表现、药品等8个实体类别进行分类抽取,抽取量较大,预计能取得较好的识别效果.结论 基于深度学习的方法可以利用包括词法分析、句法分析、语义分析、文档分析更多的特征,帮助临床进行更好的数据抽取服务.但中文电子病历因有其独特的语言特性,医疗行业习惯用语大量出现,包含一些以数字和单位表示的检查结果和英文缩写词,句子语法结构不完整,模式化较强等,造成中文病历实体总体识别难度较大.我们应该根据中文电子病历文本明显的子语言特点,一是加强},
  langid = {zh\_CN},
  keywords = {Henan Medical Research,人工智能,后结构化,张现辉,文本识别,牛亚军,电子病历,程楠,自然语言处理技术},
  annotation = {河南省人民医院 网络信息中心,河南 郑州 450003郑州市第一人民医院 信息科,河南 郑州 450004郑州大学第三附属医院 信息科,河南 郑州 450052郑州人民医院 信息科化建设和研发部,河南 郑州 450003中牟县卫生健康委员会,河南 郑州 451400河南省传染病医院 信息科,河南 郑州 450052\\
2021-09-16 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 基於NLP技術後重構處理的電子病歷應用\\
abstractTranslation:  目的探索電子病歷後構造命名實體識別率。方法抽取1000份運行電子病歷，利用自然語言處理（NLP）技術進行語義識別。結果本次影像抽取了1000份入院記錄，針對檢查、臨床表現、藥品對8個實體類別進行分類抽取，抽取量增加，近期能取得較好的識別效果。基於深度學習的結論可以利用包括詞法分析、句法分析、語義分析、文檔分析等更多的特徵，有助於臨床進行更好的數據抽取服務。但中文電子病歷因有其獨特的語言特性，醫療行業用語大量出現，包含一些習慣以數字和單位表示的檢查結果和英文縮寫詞，句子結構不完整，模式化由於中文電子病歷文本明顯的子語言特徵，一是加強。},
  file = {C:\Users\BlackCat\Zotero\storage\E2G3ZHFX\程 等。 - 2021 - 基于NLP技术后结构化处理的电子病历应用.pdf}
}

@online{ChenHengZhongWenYiXueWenDangFenCiJiGuanJianCiTiQuYanJiu,
  title = {中文医学文档分词及关键词提取研究},
  author = {{陈衡}},
  url = {https://xueshu.baidu.com/usercenter/paper/show?paperid=1r4g0040vh3y0gp0xs1e0vg0cq628891&site=xueshu_se},
  urldate = {2022-07-23},
  abstract = {目的：分詞和關鍵詞提取，是中文自然語言處理的基礎，本文通過對中文電子病歷文檔的分詞和關鍵詞提取研究，為電子病歷結構化研究提供條件[1]。對象：從省內某三甲醫院電子病歷系統中導出的170份腎內科電子病歷的現病史部分，約為59000個中文字。過程與方法：選擇分詞系統，利用ICTCLAS系統作為研究工具，通過加入醫學專業詞典以及一些必要的調整和人工干預，提高其在醫學領域的分詞準確率。結果：采用本方法的處理，分詞系統對現病史的分詞準確率有了顯著提高，達到90％。結論：以170份的現病史做樣本進行研究，結果基本達到預期，若擴大樣本容量，其結果能達到或高于本研究的結果，有一定的推廣意義。},
  langid = {chinese},
  keywords = {中文電子病歷,分詞},
  annotation = {abstractTranslation:  目的：分詞和關鍵詞提取，是中文自然語言處理的基礎，論文通過對中文電子病歷文檔的分詞和關鍵詞提取研究，為電子病歷形成研究提供[1]。對象：來自省內某三甲醫院電子病歷系統中導出的170份腎內科電子病歷的現病史部分，大約59000個中文字。過程與專業方法：選擇分詞系統，利用ICTCLAS系統作為研究工具，通過加入醫學詞典以及一些必要的調整和人工干預，提高其在醫學領域的分詞準確率。結果：採用本方法的處理，分詞系統對現病史的分詞準確率有了顯著提高，達到90\%。結論：以170份的現病史做樣本進行研究，結果基本達到預期，若擴大樣本容量，其結果能達到或本研究的結果，有一定的推廣意義。\\
titleTranslation: 中文醫學文檔分詞及關鍵詞提取研究},
  note = {使用ICPCLAS及Hylanda分詞，結合專有名詞字典等提升分詞準確度到90\%}
}

@online{chenHiQAHierarchicalContextual2024,
  title = {{{HiQA}}: {{A Hierarchical Contextual Augmentation RAG}} for {{Massive Documents QA}}},
  shorttitle = {{{HiQA}}},
  author = {Chen, Xinyue and Gao, Pengyu and Song, Jiangjiang and Tan, Xiaoyang},
  date = {2024-01-31},
  eprint = {2402.01767},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.01767},
  url = {http://arxiv.org/abs/2402.01767},
  urldate = {2024-03-28},
  abstract = {As language model agents leveraging external tools rapidly evolve, significant progress has been made in question-answering(QA) methodologies utilizing supplementary documents and the Retrieval-Augmented Generation (RAG) approach. This advancement has improved the response quality of language models and alleviates the appearance of hallucination. However, these methods exhibit limited retrieval accuracy when faced with massive indistinguishable documents, presenting notable challenges in their practical application. In response to these emerging challenges, we present HiQA, an advanced framework for multi-document question-answering (MDQA) that integrates cascading metadata into content as well as a multi-route retrieval mechanism. We also release a benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA demonstrates the state-of-the-art performance in multi-document environments.},
  langid = {english},
  pubstate = {preprint},
  keywords = {benchmark,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,RAG,問答系統,資料集},
  annotation = {titleTranslation: HiQA：用於海量文件 QA 的分層上下文增強 RAG\\
abstractTranslation:  隨著利用外部工具的語言模型代理的快速發展，利用補充文件和檢索增強生成（RAG）方法的問答（QA）方法已經取得了重大進展。這項進步提高了語言模型的反應品質並減輕了幻覺的出現。然而，這些方法在面對大量難以區分的文件時表現出有限的檢索精度，在實際應用中提出了顯著的挑戰。為了應對這些新出現的挑戰，我們提出了 HiQA，這是一個先進的多文檔問答 (MDQA) 框架，它將級聯元資料整合到內容中以及多路由檢索機制。我們也發布了一個名為 MasQA 的基準來評估和研究 MDQA。最後，HiQA 展示了多文檔環境中最先進的效能。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\RFUPYYTX\\Chen et al. - 2024 - HiQA A Hierarchical Contextual Augmentation RAG f.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\Y9E3DWGP\\Chen et al. - 2024 - HiQA A Hierarchical Contextual Augmentation RAG f.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\IVNRCLZJ\\2402.html}
}

@article{ChenJianQingJiYuZhongXiYiJieHeLinChuangSiWeiDeDianZiBingLiXiTongYanJiuYuYingYong2011,
  title = {基于中西医结合临床思维的电子病历系统研究与应用},
  author = {{陈建清} and {马祯一} and {包周超}},
  date = {2011},
  journaltitle = {电脑编程技巧与维护},
  number = {20},
  pages = {14--15},
  publisher = {嘉兴市中医医院,浙江 嘉兴},
  issn = {1006-4052},
  doi = {10.3969/j.issn.1006-4052.2011.20.005},
  abstract = {通过对中医电子病历系统的研究阐述,结合嘉兴市中医医院的实际应用,证明中医电子病历系统使病历书写更为标准和规范,检索使用更为方便,病历、病案管理水平显著提高。},
  langid = {chi},
  keywords = {電子病歷},
  annotation = {abstractTranslation:  通過對中醫電子病歷系統的研究闡述，結合嘉興市中醫醫院的實際應用，證明中醫電子病歷系統使病曆書寫更加規範和規範，檢索使用更加方便，病歷、病案管理水平顯著提高。\\
titleTranslation: 基於中西醫結合臨床思維的電子病歷系統研究與應用},
  file = {C:\Users\BlackCat\Zotero\storage\4F96WZQG\陈建清 马祯一 包周超 - 2011 - 基于中西医结合临床思维的电子病历系统研究与应用.pdf}
}

@article{ChenJieJiYuALBERTDeZhongWenYiLiaoBingLiMingMingShiTiShiBie2021,
  title = {基于ALBERT的中文医疗病历命名实体识别},
  author = {{陈杰} and {奚雪峰} and {皮洲} and {盛胜利} and {崔志明}},
  date = {2021},
  journaltitle = {南京师范大学学报（工程技术版）},
  shortjournal = {Journal of Nanjing Normal University(Engineering and Technology Edition)},
  volume = {21},
  number = {1},
  pages = {36--43},
  doi = {10.3969/j.issn.1672-1292.2021.01.006},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhduanNmZHh4Yi1nY2pzYjIwMjEwMTAwNhoIYTl3dXluajY%3D},
  urldate = {2022-08-14},
  abstract = {医疗病历命名实体识别的主要任务是将临床电子病历中的非结构化文本转化为结构化数据,进而为面向医疗领域任务开展的数据挖掘提供基础支撑.提出一种基于ALBERT模型融合学习的中文医疗病历命名实体识别模型.首先,采用人工标注方式扩展样本数据集,结合ALBERT模型对数据集进行微调;其次,采用双向长短记忆网络(BiLSTM)提取文本的全局特征;最后,基于条件随机场模型(CRF)命名实体的序列标记.在标准数据集上的实验结果表明,该方法进一步提高了医疗文本命名识别精度,减少了时间开销.},
  langid = {zh\_CN},
  keywords = {ALBERT,Journal of Nanjing Normal University(Engineering and Technology Edition),南京师范大学学报（工程技术版）,双向长短记忆网络,命名实体识别,奚雪峰,条件随机场,电子医疗病历,皮洲,盛胜利,陈杰},
  annotation = {苏州科技大学电子与信息工程学院,江苏 苏州215009苏州科技大学电子与信息工程学院,江苏 苏州215009;苏州智慧城市研究院,江苏 苏州215009Computer Science Department,Texas Tech University,Texas 79431,USA\\
国家自然科学基金 研究生科研创新项目~苏州市科技发展计划~江苏省"六大人才高峰"高层次人才项目\\
2021-04-02 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 基於ALBERT的中文醫療疾病歷命名實體識別\\
abstractTranslation:  醫療病歷命名實體識別的主要任務將臨床電子病歷中的非重構文本轉化為重構數據，進而為面向醫療領域任務開展融合的數據挖掘提供支撐基礎。提出一種基於ALBERT模型學習的中文醫療首先，採用人工標註方式擴展樣本數據集，結合ALBERT模型對數據集進行加權；其次，採用死亡長短記憶網絡（BiLSTM）提取文本的全局特徵；最後，基於條件隨機場模型（ CRF）命名實體的序列標記。在標準數據集上的實驗結果表明，該方法進一步提高了醫療文本命名識別的準確性，減少了所需時間。},
  file = {C:\Users\BlackCat\Zotero\storage\YJKJDK75\陈 等。 - 2021 - 基于ALBERT的中文医疗病历命名实体识别.pdf}
}

@article{ChenJuJiYuZhongYiYouShiBingZhongDeBianZhengLunZhiPingTai2017,
  title = {基于中医优势病种的辨证论治平台},
  author = {{陈菊} and {陶瑞卿} and {宋岚} and {赵姝婷} and {姜雪梅} and {谭星} and {黄玲} and {曹继忠} and {温川飙}},
  date = {2017},
  journaltitle = {时珍国医国药},
  volume = {28},
  number = {10},
  pages = {5},
  abstract = {如何发挥中医学自身方法论优势,融入到信息化时代,分享信息化的成果,提升中医辨证论治的水平和疗效,一直是中医药界关注的重点课题.文章介绍一种以优势病种知识为基础的可量化的中医辨证论治平台,以达到规范辅助中医优势病种辨证诊疗流程,生成具有中医特色电子病历,提高中医诊疗服务质量.平台构造336种优势病种知识库,在其基础上构造一个新的临床辨证论治辅助算法,通过获取病人症状及体质量化码以进行辨证分型,从而自动推荐治法与处方.在win7平台实现了该系统的原型,目前在71家县级中医医院进行应用,统计医院使用情况,中医优势病种诊疗的适应率达85\%左右,89.46\%以上的电子处方可实现智能辅助辨证,说明本系统的应用可行性.},
  keywords = {中醫,辨證},
  file = {C:\Users\BlackCat\Zotero\storage\WAV6RCUW\陈菊 等。 - 2017 - 基于中医优势病种的辨证论治平台.pdf}
}

@inproceedings{chenLargeLanguageModels2023,
  title = {Large {{Language Models}} Are Few(1)-Shot {{Table Reasoners}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EACL}} 2023},
  author = {Chen, Wenhu},
  editor = {Vlachos, Andreas and Augenstein, Isabelle},
  date = {2023-05},
  pages = {1120--1130},
  publisher = {Association for Computational Linguistics},
  location = {Dubrovnik, Croatia},
  doi = {10.18653/v1/2023.findings-eacl.83},
  url = {https://aclanthology.org/2023.findings-eacl.83},
  urldate = {2023-12-06},
  abstract = {Recent literature has shown that large language models (LLMs) are generally excellent few-shot reasoners to solve text reasoning tasks. However, the capability of LLMs on table reasoning tasks is yet to be explored. In this paper, we aim at understanding how well LLMs can perform table-related tasks with few-shot in-context learning. Specifically, we evaluated LLMs on popular table QA and fact verification datasets like WikiTableQuestion, FetaQA, TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning over table structures, though these models are not pre-trained on any table corpus. When combined with `chain of thoughts' prompting, LLMs can achieve very strong performance with only a 1-shot demonstration, even on par with some SoTA models. We show that LLMs are even more competent at generating comprehensive long-form answers on FetaQA than tuned T5-large. We further manually studied the reasoning chains elicited from LLMs and found that these reasoning chains are highly consistent with the underlying semantic form. We believe that LLMs can serve as a simple yet generic baseline for future research. The code and data are released in https://github.com/wenhuchen/TableCoT.},
  eventtitle = {Findings 2023},
  langid = {english},
  keywords = {LLM,prompt,已整理,待讀,重要},
  annotation = {1 citations (Crossref) [2024-04-27]\\
titleTranslation: 大型語言模型是很少（1）次的表推理器\\
abstractTranslation:  最近的文獻表明，大型語言模型（LLM）通常是解決文本推理任務的優秀小樣本推理器。然而，法學碩士在表格推理任務上的能力仍有待探索。在本文中，我們的目的是了解法學碩士如何透過少量情境學習來執行與表格相關的任務。具體來說，我們在流行的表QA 和事實驗證資料集（如WikiTableQuestion、FetaQA、TabFact 和FEVEROUS）上評估了LLM，發現LLM 能夠勝任表結構的複雜推理，儘管這些模型沒有在任何表語料庫上進行預訓練。當與「思想鏈」提示相結合時，LLM 只需 1-shot 演示即可獲得非常強大的效能，甚至可以與某些 SoTA 模型相媲美。我們表明，法學碩士比調整後的 T5-large 更有能力在 FetaQA 上產生全面的長格式答案。我們進一步手動研究了法學碩士得出的推理鏈，發現這些推理鏈與底層語義形式高度一致。我們相信法學碩士可以作為未來研究的簡單而通用的基線。程式碼和資料發佈於https://github.com/wenhuchen/TableCoT。},
  file = {C:\Users\BlackCat\Zotero\storage\MVVYB6YN\Chen - 2023 - Large Language Models are few(1)-shot Table Reason.pdf}
}

@thesis{ChenMinYuHeiXiangShiHanShiCengJiCeShiAnLiChanShengQiZhiShiShuDeChuLi2012,
  title = {黑箱式函式層級測試案例產生器之實數的處理},
  author = {{陳敏毓}},
  namea = {{林迺衛} and {Neiwei Lin}},
  nameatype = {collaborator},
  date = {2012},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/x5a644},
  abstract = {軟體測試是確保軟體品質最主要的方法。本研究團隊曾開發一個可處理Java整數型態的黑箱函式層級測試案例產生器，本論文以此為基礎將其擴充為可處理實數型態的測試案例產生器。軟體測試最困難的地方是測試案例的產生，測試案例包含測試輸入與預期輸出。奠基於限制邏輯程式，本論文可以使用一致的方法同時產生實數型態的測試輸入與預期輸出本工具先由規格讀取器將類別圖與物件規格語言轉換成函式限制式圖，並交由測試路徑產生器系統化地條列出限制式圖上的測試路徑，接著測試資料產生器將每一條測試路徑上的限制式轉換成一個限制邏輯程式的敘述式，進行實數型態的求解，其中需分三個求解步驟以確保測試輸入的正確性和預期輸出的精確度。最後測試類別產生器將這些實數型態的測試輸入與預期輸出轉換成 Java 測試函式並使用JUnit 測試架構自動執行產生的測試案例。},
  pagetotal = {79}
}

@thesis{ChenNanHongZhongYiZhengZhuangZhiShiBenTiDeJianGou2015,
  title = {中醫症狀知識本體的建構},
  author = {{陳南宏}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2015},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/8k2x3h},
  abstract = {中醫是世界古老醫學之一，幾千年來，積累了相當多的臨床經驗與文獻，形成了獨特的診療體系。知識本體為某一特定領域的模型，利於該領域知識的擷取及推論。一個知識本體包含該領域術語的定義，及術語之間的關係。藉由知識本體，資訊系統可以分享領域知識，並從事有系統的領域知識推論。因此，中醫知識本體的建構是中醫資訊化的基礎建設。 中醫症狀知識本體是中醫知識本體最基礎的一部分。中醫症狀知識本體的核心工作是收集及定義中醫症狀術語，這是一個非常浩大的工程，屬於中醫專業領域的重要工作。與此同時，中醫症狀知識本體的建構有賴於資訊專業領域的協助。因此，本論文是跨中醫及資訊專業領域的研究成果。在本論文建構的中醫症狀知識本體中，一個症狀可由兩個症狀核心屬性：症狀物件與症狀屬性，及兩個症狀附屬屬性：症狀程度與症狀期間所組成。 中醫症狀知識本體有許多應用。本論文利用中醫症狀知識本體建構一個中醫症狀詞庫，從事中醫症狀的標準化。一般中醫文獻中的症狀描述存在許多一義多詞或一詞多義的情況，中醫症狀知識本體中的症狀術語皆為一詞一義。透過中醫症狀知識本體，中醫症狀詞庫可以將多個一義多詞的同義症狀都對應到同一個一詞一義的症狀術語，也可以將一個一詞多義的症狀拆解成多個一詞一義的症狀術語。本論文也利用中醫症狀知識本體建構一個中醫辨證系統的雛型，從事中醫辨證的自動化。中醫症狀標準化是中醫辨證自動化的基礎，利於從事精確量化的中醫辨證。中醫症狀知識本體的建構必須經過長期的演化，才會漸趨正確與完整。希望本研究可以啟動一個正確與完整的中醫症狀知識本體的建構。},
  pagetotal = {128},
  file = {C:\Users\BlackCat\Zotero\storage\2D57BMCS\陳南宏 - 2015 - 中醫症狀知識本體的建構.pdf}
}

@article{ChenSuTaisu-taichenJianYiZhongYiZhenDuanChuFangZhuanJiaXiTongYanZhi2011,
  title = {簡易中醫診斷處方專家系統研製},
  author = {{陳甦臺(Su-Tai Chen)}},
  date = {2011},
  journaltitle = {中醫藥研究論叢},
  shortjournal = {中醫藥研究論叢},
  volume = {14},
  number = {1},
  pages = {161--174},
  publisher = {台北市中醫師公會},
  issn = {2311-1984},
  doi = {10.6516/tjtcm.201103_14(1).0015},
  url = {http://dx.doi.org/10.6516/TJTCM.201103_14(1).0015},
  abstract = {本研究的主要目的爲建構一個高效率簡易的診斷處方專家系統，輔助中醫師進行診斷處方作業，來朝向臨床決策支援系統的方向設計。系統以中醫診斷頸椎病證型的過程爲例，解釋如何建立專家系統資料庫，其中對頸椎病症狀的診斷與判定證型是由專家建立八十一個特徵症狀比重，並經Matlab語言作彈性模糊邏輯程式運算，以模擬中醫師之話斷。處方程式本身則偏重開放系統架構及系統介面之設計，以物件導向的觀點來規劃多媒體中醫系統的架構及功能流程，並使用統一化模式語言(visual basic)爲工具輔助系統分析與設計。實作上則以Access資料庫爲基礎，配合多媒體技術提供友善的病歷存取介面，在資料交換方面則利用XML文件做爲交換格式。藉由實際操作以驗證系統的準確性。此專家系統可以輔助臨床中醫師以減輕診斷處方的工作。},
  langid = {zh\_CN},
  keywords = {Cervical Vertebra Disease,Elastic fuzzy mathematics,中醫,專家系統,彈性模糊數學},
  annotation = {abstractTranslation:  本研究的主要目的為目前一個高效率簡易的診斷標籤專家系統，輔助中醫師進行診斷標籤作業，來診斷臨床決策支持系統的方向設計。系統以診斷中醫頸椎病證型的流程為例，解釋如何建立了專家系統資料庫，其中對頸椎病症狀的診斷與判定證型是由專家建立了八十個特徵症狀比重，並通過Matlab語言作彈性模糊圖案症狀，以模擬中醫師之話斷。本身則偏重開放系統架構及系統介面的設計，以對象導向的觀點來規劃多媒體中醫系統的架構及功能流程，並採用統一化模式語言（Visual Basic）為工具輔助系統分析與設計。以Access數據庫為基礎，支持多媒體技術提供友好的病歷訪問介面，在數據交換方面則採用XML文件做為交換格式。由實際操作來驗證系統的準確性。此專家系統輔助臨床中醫師以減弱診斷的工作。\\
titleTranslation: 簡易中醫診斷專家系統說明書}
}

@inproceedings{chenTestingYourQuestion2021,
  title = {Testing {{Your Question Answering Software}} via {{Asking Recursively}}},
  booktitle = {2021 36th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Chen, Songqiang and Jin, Shuo and Xie, Xiaoyuan},
  date = {2021-01},
  pages = {104--116},
  issn = {2643-1572},
  doi = {10.1109/ASE51524.2021.9678670},
  url = {https://ieeexplore.ieee.org/document/9678670},
  urldate = {2024-01-15},
  abstract = {Question Answering (QA) is an attractive and challenging area in NLP community. There are diverse algorithms being proposed and various benchmark datasets with different topics and task formats being constructed. QA software has also been widely used in daily human life now. However, current QA software is mainly tested in a reference-based paradigm, in which the expected outputs (labels) of test cases need to be annotated with much human effort before testing. As a result, neither the just-in-time test during usage nor the extensible test on massive unlabeled real-life data is feasible, which keeps the current testing of QA software from being flexible and sufficient. In this paper, we propose a method, qaAskeR, with three novel Metamorphic Relations for testing QA software. qaAskeR does not require the annotated labels but tests QA software by checking its behaviors on multiple recursively asked questions that are related to the same knowledge. Experimental results show that qaAskeR can reveal violations at over 80\% of valid cases without using any preannotated labels. Diverse answering issues, especially the limited generalization on question types across datasets, are revealed on a state-of-the-art QA algorithm.},
  eventtitle = {2021 36th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  langid = {english},
  keywords = {benchmark,問答系統,已整理,資料集},
  annotation = {11 citations (Crossref) [2024-03-26]\\
titleTranslation: 透過遞歸提問來測試您的問答軟體},
  file = {C:\Users\BlackCat\Zotero\storage\TTAKWGDE\9678670.html}
}

@article{chenTypedirectedSynthesisVisualizations2022,
  title = {Type-Directed Synthesis of Visualizations from Natural Language Queries},
  author = {Chen, Qiaochu and Pailoor, Shankara and Barnaby, Celeste and Criswell, Abby and Wang, Chenglong and Durrett, Greg and Dillig, Işil},
  year = {10 月 31, 2022},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {6},
  pages = {144:532--144:559},
  doi = {10.1145/3563307},
  url = {https://dl.acm.org/doi/10.1145/3563307},
  urldate = {2023-12-01},
  abstract = {We propose a new technique based on program synthesis for automatically generating visualizations from natural language queries. Our method parses the natural language query into a refinement type specification using the intents-and-slots paradigm and leverages type-directed synthesis to generate a set of visualization programs that are most likely to meet the user's intent. Our refinement type system captures useful hints present in the natural language query and allows the synthesis algorithm to reject visualizations that violate well-established design guidelines for the input data set. We have implemented our ideas in a tool called Graphy and evaluated it on NLVCorpus, which consists of 3 popular datasets and over 700 real-world natural language queries. Our experiments show that Graphy significantly outperforms state-of-the-art natural language based visualization tools, including transformer and rule-based ones.},
  issue = {OOPSLA2},
  langid = {english},
  keywords = {Data Visualization,Program Synthesis,Programming by Natural Languages,回收,已整理},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 來自自然語言查詢的可視化的類型定向合成},
  note = {研究目的：自然語言描述轉成圖結果，也就是根據描述自動查表並選擇恰當的表現方式。和研究較無相關},
  file = {C:\Users\BlackCat\Zotero\storage\G8DYGBLG\Chen 等。 - 2022 - Type-directed synthesis of visualizations from nat.pdf}
}

@thesis{ChenWeiZhenMoHuZhongYiShiZhengZhengHouBianZheng2018,
  title = {模糊中醫實證證候辨證},
  author = {{陳偉振}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2018},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/54f92b},
  abstract = {中醫辨證根據望，聞，問，切四診所蒐集的病人症狀，辨識病人的證候，即疾病的病因、病位及病性。中醫辨證是一個模糊分類問題。本論文運用計算機龐大的記憶能力及迅速的分析能力，研製一個基於模糊理論的中醫實證證候辨證系統。本論文先依據五本中醫診斷學典籍，歸納整理出47個中醫實證證候及其特徵症狀。本論文再運用模糊理論的歸屬函數，將證候的特徵症狀分為病因症狀、病位症狀、舌診症狀、及脈診症狀四個群組，進行模糊分類。我們也使用10個台灣期刊上的病例報告進行初步的系統評估，評估結果顯示辨證系統和病例報告的辨證結果吻合度相當高。 關鍵詞: 中醫辨證系統; 中醫實證證候; 模糊理論。},
  pagetotal = {156},
  keywords = {實驗室}
}

@thesis{ChenWenLingDianDingYuZhiShiBenTiDeZhongYiZhengZhuangCiKuXiTong2014,
  title = {奠定於知識本體的中醫症狀詞庫系統},
  author = {{陳玟伶}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2014},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/794vud},
  abstract = {中醫在長時間的歷史中，發展出「辨證論治」的特殊診治方式。透過四診─望、聞、問、切的方法診察病情，依據四診獲得的疾病資訊進行分析及判斷，即辨證；最後由辨證的資訊，確立治療方式，即為「論治」。 由於中醫辨證體系的龐雜，我們計畫運用計算機龐大的記憶能力及迅速的分析能力，研製一個中醫自動辨證系統，提供研究中醫辨證體系的平台。然而在中醫發展的過程中，因為時代背景、氣候、派別、習慣等因素，使得在古籍及文獻中充滿不同的症狀字詞及描述。沒有標準化的症狀字詞導致系統在辨證上難以作精確的判斷與區分。 因此我們先研製一個中醫症狀字詞庫，以解決中醫症狀字詞的標準化，未來可進一步作為中醫自動辨證系統的基礎建設。我們透過建立中醫症狀知識本體，來定義標準症狀，並建立症狀、症狀物件與症狀屬性之間的關係，使得標準症狀滿足一詞一義的標準。不過，一個中醫症狀字詞庫必須經過長期的使用與修訂，才會漸趨正確與完整。這個中醫症狀知識本體的建立，也能夠提供其他中醫資訊系統運用中醫症狀字詞做更有系統地分析、推論與管理。},
  pagetotal = {66},
  file = {C:\Users\BlackCat\Zotero\storage\Q39K3DR4\奠定於知識本體的中醫症狀詞庫系統.pdf}
}

@article{ChenXiangZhongYiGuanJianKangXinXiPingTaiDianZiBingLiXiTongJiBenGongNengGuiFanYanJiuJinZhan2021,
  title = {中医馆健康信息平台电子病历系统基本功能规范研究进展},
  author = {{陈翔} and {汪涛} and {李肖凤} and {郑昌锐} and {刘春} and {忻凌} and {于大江} and {宋伟}},
  date = {2021},
  journaltitle = {电脑知识与技术:学术版},
  abstract = {中医馆健康信息平台以服务公众为出发点,以深化医药卫生体制改革为着力点,以维护和增进全体人民健康为宗旨,运用了云计算,大数据,新一代互联网等信息技术,提升中医药各业务系统间的信息共享和业务协同能力.中医馆健康信息平台电子病历系统为借助中医馆健康平台在基层医疗卫生机构中医诊疗区(中医馆)内使用的电子病历系统,该文结合该具体应用场景,进行了中医馆健康信息平台电子病历系统基本功能规范标准的制定与研究工作,旨在推动中医馆健康信息平台电子病历系统的规范性应用.},
  keywords = {中醫,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\XADMTZ2J\陈翔 等。 - 2021 - 中医馆健康信息平台电子病历系统基本功能规范研究进展.pdf}
}

@thesis{ChenXinXianJiYuZhiShiBenTiDeZhongYiXuZhengBianZhengXiTong2016,
  title = {基於知識本體的中醫虛證辨證系統},
  author = {{陳信賢}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2016},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/e5nw66},
  abstract = {中醫透過長時間的演變，發展出「辨證論治」的特殊診治方式。透過望、聞、問、切四診的方法蒐集病人的症狀資訊，及依據症狀資訊進行分析及判斷疾病，即為「辨證」；再由辨證出的疾病資訊，確立治療方式，即為「論治」。 對於龐大及複雜的中醫辨證體系，中醫師在臨床診治時，通常不易具備完整的記憶，及進行全面的分析。本論文希望利用計算機龐大的記憶能力及迅速的分析能力，研製一套具備完整記憶及可進行全面分析的中醫辨證輔助系統，輔助中醫師的臨床診斷、辨證研究、及辨證教學。 本論文利用知識本體及模糊分類技術，實作一個以虛證證候為範疇的中醫辨證系統。本論文進行虛證證候定義的標準化，及虛證證候特徵症狀的標準化，並利用知識本體開發工具建構中醫症狀知識本體及中醫證候知識本體。本論文根據證候特徵症狀，利用模糊分類技術，分別定義各證候的歸屬函數。本論文也根據各證候的歸屬函數，針對43筆案例進行初步的系統評估。在大約1/7的案例中，系統所辨識的證候和案例所提供的證候不一致。但是根據中醫師的意見，大多數這些案例所提供的證候是有疑問的。這個評估結果也具體顯示中醫辨證體系的複雜性，及研製一個中醫辨證系統的重要性。},
  pagetotal = {129}
}

@inproceedings{chenyanDesignKnowledgeGraph2020,
  title = {Design of {{Knowledge Graph}} of {{Traditional Chinese Medicine Prescription}} and {{Knowledge Analysis}} of {{Implicit Relationship}}},
  booktitle = {Proceedings of the 1st {{International Symposium}} on {{Artificial Intelligence}} in {{Medical Sciences}}},
  author = {{Chen Yan} and {Gong Qingyue} and {Qiu Jingjing} and {Li Chenlin} and {Zeng Xing} and {Wu Haoyu} and {Wu Rixin} and {Zhang Meilin} and {Su Junkang} and {Hu Kongfa}},
  year = {12 月 4, 2020},
  series = {{{ISAIMS}} '20},
  pages = {56--63},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3429889.3429900},
  url = {https://dl.acm.org/doi/10.1145/3429889.3429900},
  urldate = {2023-09-14},
  abstract = {Purpose-As of March 2020, this paper collected 41 prescriptions for Covid-19 in official reports, and used machine learning and knowledge graph technology to discover the rules of common prescription medication for Covid-19 and the Chinese medicinal herb groups applicable to different groups of people to help the TCM prescriptions for the clinical treatment of Covid-19. Methods-The FP-Growth algorithm was used to analyze the properties, tastes, and meridian tropism of Chinese medicinal herb groups in prescription data, constructing a knowledge graph for Covid-19 description and mining the prescription medication rules. Results-In the 41 pieces of prescription, patients in medical observation period were treated with Chinese medicinal herbs that can relieve exterior syndromes and stop vomiting, such as Radix Saposhnikoviae, Semen Sojae Preparatum, etc. Those with mild common syndromes were treated with heat-clearing and detoxicating Chinese medicinal herbs, such as Rhizoma Coptidis, Radix Scutellariae, etc. Infected patients were treated with Chinese medicinal herbs such as Radix Glycyrrhizae, Herba Asari, etc., which can dissolve phlegm, relieve cough and invigorate the spleen, relieve pain, expel wind syndromes, relieve exterior symptoms and help digesting. Conclusion-Combining machine learning and knowledge graph technology to analyze the data characteristics of common prescriptions used by different populations can help analyze the core medication mechanism of TCM (Traditional Chinese Medicine) treatment of different populations with the same disease.},
  isbn = {978-1-4503-8860-3},
  langid = {english},
  keywords = {中醫,已整理,數據挖掘},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 中藥方劑知識圖譜設計及隱含關係知識分析\\
abstractTranslation:  目的-截至2020年3月，本文收集了官方報告中的41個Covid-19處方，利用機器學習和知識圖譜技術發現Covid-19常見處方用藥規律以及適用於不同人群的中藥組的人們幫助臨床治療Covid-19的中醫處方。方法－採用FP-Growth演算法分析處方資料中中藥組的性狀、性味、歸經等，建構Covid-19所描述的知識圖譜，挖掘處方用藥規則。結果-41張處方中,醫學觀察期患者均採用防風、淡豆豉等解表止嘔中藥治療,常見證候較輕者採用熱敷治療。清熱解毒的中藥，如黃連、黃芩等。感染患者用中藥治療，如甘草、細辛等，具有化痰、止咳、健脾、止痛的作用。 、祛風、祛表、助消化。結論－結合機器學習與知識圖譜技術，分析不同族群常用方劑的資料特徵，有助於分析中醫治療不同族群同病的核心用藥機制。},
  file = {C:\Users\BlackCat\Zotero\storage\I3CHXMIX\Yan 等。 - 2020 - Design of Knowledge Graph of Traditional Chinese M.pdf}
}

@article{ChenYiMoJiYuBertBiLSTMCRFMoXingDeDianZiBingLiYinSiXinXiShiBieFangFa2022,
  title = {基于Bert-BiLSTM-CRF模型的电子病历隐私信息识别方法},
  author = {{陈逸墨} and {叶辉} and {易珺} and {周华文} and {方丹丹} and {曹东}},
  date = {2022},
  journaltitle = {自动化与信息工程},
  shortjournal = {Automation \& Information Engineering},
  volume = {43},
  number = {2},
  pages = {35--40},
  doi = {10.3969/j.issn.1674-2605.2022.02.006},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhNnZHpkaHl4eGdjMjAyMjAyMDA2GghmemVhNjFucw%3D%3D},
  urldate = {2022-08-14},
  abstract = {随着电子病历数据开放共享的需求越来越大,电子病历去隐私性问题亟需解决.利用自然语言处理技术,提出一种基于Bert-BiLSTM-CRF模型的电子病历隐私信息识别方法.采用某三甲中医院的电子病历作为数据来源,结合当前公开的数据集进行训练,得到正确率为94.02％、召回率为94.25％、Fl为93.98％的中医电子病历隐私信息识别模型.与其他传统模型进行对比实验表明,Bert-BiLSTM-CRF模型能有效识别并保护电子病历中的隐私数据,有助于医疗数据的开放共享.},
  langid = {zh\_CN},
  keywords = {Automation & Information Engineering,Bert,双向长短时记忆网络,周华文,易珺,条件随机场,电子病历,自动化与信息工程,陈逸墨,隐私信息},
  annotation = {广州中医药大学医学信息工程学院,广东广州510006广东药科大学医药信息工程学院,广东广州510006\\
国家重点研发计划\\
2022-06-02 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 基於Bert-BiLSTM-CRF模型的電子病歷隱私信息識別方法\\
abstractTranslation:  隨著電子病歷數據開放共享的需求越來越大，電子病歷隱私問題急需解決。利用自然語言處理技術，提出一種基於Bert-BiLSTM-CRF模型的電子病歷隱私信息識別方法。三甲中醫院的電子病歷作為數據源，結合當前公開的數據集進行，得到正確率為94.02\%、召回率為94.25\%、Fl為93.98\%的中醫電子病歷隱私信息識別模型。與其他傳統模型進行對比實驗表明，Bert- BiLSTM-CRF模型能夠有效識別並保護電子病歷中的隱私數據，有助於醫療數據的開放共享。}
}

@article{chenyuanhuMultiTaskJointLearning2022,
  title = {Multi-{{Task Joint Learning Model}} for {{Chinese Word Segmentation}} and {{Syndrome Differentiation}} in {{Traditional Chinese Medicine}}},
  author = {{Chenyuan Hu} and {Shuoyan Zhang} and {Tianyu Gu} and {Zhuangzhi Yan} and {Jiehui Jiang}},
  date = {2022-01},
  journaltitle = {International Journal of Environmental Research and Public Health},
  volume = {19},
  number = {9},
  pages = {5601},
  issn = {1660-4601},
  doi = {10.3390/ijerph19095601},
  url = {https://www.mdpi.com/1660-4601/19/9/5601},
  urldate = {2022-08-03},
  abstract = {Evidence-based treatment is the basis of traditional Chinese medicine (TCM), and the accurate differentiation of syndromes is important for treatment in this context. The automatic differentiation of syndromes of unstructured medical records requires two important steps: Chinese word segmentation and text classification. Due to the ambiguity of the Chinese language and the peculiarities of syndrome differentiation, these tasks pose a daunting challenge. We use text classification to model syndrome differentiation for TCM, and use multi-task learning (MTL) and deep learning to accomplish the two challenging tasks of Chinese word segmentation and syndrome differentiation. Two classic deep neural networks—bidirectional long short-term memory (Bi-LSTM) and text-based convolutional neural networks (TextCNN)—are fused into MTL to simultaneously carry out these two tasks. We used our proposed method to conduct a large number of comparative experiments. The experimental comparisons showed that it was superior to other methods on both tasks. Our model yielded values of accuracy, specificity, and sensitivity of 0.93, 0.94, and 0.90, and 0.80, 0.82, and 0.78 on the Chinese word segmentation task and the syndrome differentiation task, respectively. Moreover, statistical analyses showed that the accuracies of the non-joint and joint models were both within the 95\% confidence interval, with pvalue {$<$} 0.05. The experimental comparison showed that our method is superior to prevalent methods on both tasks. The work here can help modernize TCM through intelligent differentiation.},
  langid = {english},
  keywords = {deep learning,joint learning,multi-task learning,syndrome differentiation},
  annotation = {13 citations (Crossref) [2024-03-26]\\
titleTranslation: 中醫分詞與辨證的多任務聯合學習模型\\
abstractTranslation:  循證治療是中醫的基礎，準確辨證對於治療具有重要意義。非結構化病歷的自動辨證需要兩個重要步驟：中文分詞和文本分類。由於漢語的歧義性和辨證論治的特殊性，這些任務提出了艱鉅的挑戰。我們使用文本分類來模擬中醫辨證，並使用多任務學習（MTL）和深度學習來完成中文分詞和辨證這兩個具有挑戰性的任務。兩種經典的深度神經網絡——雙向長短期記憶網絡（Bi-LSTM）和基於文本的捲積神經網絡（TextCNN）——被融合到 MTL 中以同時執行這兩項任務。我們使用我們提出的方法進行了大量的對比實驗。實驗比較表明，它在這兩項任務上均優於其他方法。我們的模型在中文分詞任務和辨證任務上的準確度、特異性和敏感性分別為 0.93、0.94 和 0.90，以及 0.80、0.82 和 0.78。此外，統計分析表明，非聯合模型和聯合模型的準確性均在 95\% 置信區間內，p 值 {$<$} 0.05。實驗比較表明，我們的方法在這兩項任務上都優於流行的方法。這裡的工作可以通過智能差異化來幫助實現中醫現代化。},
  file = {C:\Users\BlackCat\Zotero\storage\6Z6RTF9S\Hu et al. - 2022 - Multi-Task Joint Learning Model for Chinese Word Segmentation and Syndrome Differentiation in Traditional Chinese Medicine.pdf}
}

@thesis{ChenYuTingZhiYuanShiYongZheDingYiXingTaiDuoWeiZhenLieDeCeShiAnLiChanSheng2021,
  title = {支援使用者定義型態多維陣列的測試案例產生},
  author = {{陳昱廷}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2021},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/3cbaaq},
  abstract = {軟體測試的自動化，不僅能降低測試成本，還能提升軟體品質。本研究團隊之前已開發一個基於限制式的函式層級的黑箱測試案例產生系統。此系統可自動將軟體規格轉換成對應的限制邏輯程式，並透過執行限制邏輯程式，自動產生對應的測試案例。但是，此系統只支援整數型態與1維及2維的整數陣列型態。本論文擴充此系統，增加支援使用者自定義型態的多維陣列的測試案例產生。對於多維陣列型態部份，本論文在軟體規格語言中，新增多維陣列型態的定義方法。本論文也在限制邏輯程式中，重新實作多維陣列的結構與陣列元素索引的方法。對於使用者自定義型態部份，本論文在軟體規格語言中，改善使用者自定義型態函式呼叫的轉換。本論文也在限制邏輯程式中，引入物件扁平化的概念，來定義物件和其屬性的關係。初步的效能評估顯示新擴充的系統可以針對使用者自定義型態的多維陣列，在有效的時間產生高突變分數的測試案例集。},
  pagetotal = {218},
  keywords = {實驗室}
}

@thesis{ChenYuXiangWangYeYingYongChengShiDiZengShiHeiXiangCeShiAnLiChanShengQi2014,
  title = {網頁應用程式遞增式黑箱測試案例產生器},
  author = {{陳渝翔}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2014},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/k5mrdb},
  abstract = {軟體測試是確保軟體品質的主要方法。近代由於網際網路發展迅速，網頁應用程式也隨之興起，如何確保網頁應用程式的品質也成為一個重要議題。網頁應用程式具有高變動性及變動具有高區域性的特質，這些特質非常適合採用遞增式黑箱測試案例產生器。黑箱測試案例產生器依據程式規格自動產生測試案例，可以有效減緩高變動性所造成的影響。而遞增式測試案例產生器可以有效利用變動的高區域性來減輕產生及執行測試案例的成本。本論文實作一個網頁應用程式的遞增式黑箱測試案例產生器。本工具應用XML定義網頁應用程式的規格，並依據新舊版本的規格變動差異，自動產生需變動的測試案例，最後利用JWebUnit測試平台來執行所產生的測試案例。},
  pagetotal = {93}
}

@thesis{ChenZhengXiongTIVRYiGeHuDongShiDianHuaYuYinXiTongDeKaiFaGongJu2002,
  title = {{{TIVR}}:一個互動式電話語音系統的開發工具},
  author = {{陳正雄}},
  namea = {{林迺衛}},
  nameatype = {collaborator},
  date = {2002},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/q72y94},
  abstract = {摘    要 在傳統互動式電話語音回覆系統中，客戶只能使用簡單的電話按鍵來輸入資料。這個限制使得客戶和系統間，只能依靠非常原始的方式來互動。最近發展出來的VoiceXML，和諧地結合了語音識別及語音合成的功能。因此提供一個解決方案，可以大量地提昇客戶和系統間的互動方式。現在非常迫切需要的就是，提供一個非常容易使用且奠基於VoiceXML的互動式電話語音回覆系統的開發工具。這篇論文就是描述研製一個這樣的開發工具的成果。這個開發工具具備下列特點： 一.提供一個非常親切的圖形介面，可以讓系統開發者完全避免任 何程式的撰寫； 二.提供一個資料庫整合功能，可以在開發出來的系統中，非常容 易地容納和整合各種資料庫管理系統； 三.提供一組基本的對話範本，可以在開發出來的系統中，非常容 易地組合成各種複雜的對話； 四.提供一組簡單的頁面控制指令，可以在開發出來的系統中，非 常有彈性地在不同的互動頁面間遊走。},
  pagetotal = {79}
}

@thesis{ChenZongRenQiXiangYinZiYuAiZhengBingHuanZhongYiShiZhengDeFenXi2019,
  title = {氣象因子與癌症病患中醫實證的分析},
  author = {{陳宗仁}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2019},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/q2h44x},
  abstract = {中醫自古以來即極為重視氣象變化對健康的影響，並相信風、寒、暑、濕、燥、火等氣象因素會對身體產生相對應的證候，例如春季多風病，夏季多暑病，長夏多濕病，秋季多燥病，冬季多寒病。因此本研究針對雲嘉地區癌症病人，使用資料探勘技術計算罹患特定證候的人數變化與地區氣象變化的Pearson關聯係數，了解哪些氣候因子與中醫證候有顯著的相關性。並運用單因子變異數分析了解在對應的氣候因子等級下中醫證候的變化。研究結果可提供雲嘉地區醫療人員更進一步瞭解本地病患的證候特質。研究結果顯示心脈痺阻(氣滯)、肝火上炎與肝陽上亢等證候和均溫有正相關，肝火上炎證候和氣壓有負相關，及心脈痺阻(寒凝)證候和平均風速有負相關。},
  pagetotal = {37},
  keywords = {實驗室}
}

@online{chiangChatbotArenaOpen2024,
  title = {Chatbot {{Arena}}: {{An Open Platform}} for {{Evaluating LLMs}} by {{Human Preference}}},
  shorttitle = {Chatbot {{Arena}}},
  author = {Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E. and Stoica, Ion},
  date = {2024-03-06},
  eprint = {2403.04132},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.04132},
  url = {http://arxiv.org/abs/2403.04132},
  urldate = {2024-04-20},
  abstract = {Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \textbackslash url\{https://chat.lmsys.org\}.},
  langid = {english},
  pubstate = {preprint},
  keywords = {benchmark,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,LLM,已整理,待讀,重要},
  annotation = {abstractTranslation:  大型語言模型（LLM）解鎖了新的功能和應用程式；然而，評估與人類偏好的一致性仍面臨重大挑戰。為了解決這個問題，我們推出了 Chatbot Arena，一個根據人類偏好評估法學碩士的開放平台。我們的方法採用成對比較方法，並透過眾包利用不同使用者群體的輸入。該平台已運行數月，累積了超過 24 萬張選票。本文描述了該平台，分析了我們迄今為止收集的數據，並解釋了我們用於高效、準確地評估和排序模型的經過驗證的統計方法。我們確認，眾包問題足夠多樣化且具有區分性，並且眾包人類投票與專家評分者的投票非常一致。這些分析共同為 Chatbot Arena 的可信度奠定了堅實的基礎。由於其獨特的價值和開放性，Chatbot Arena 已成為引用最多的 LLM 排行榜之一，被領先的 LLM 開發人員和公司廣泛引用。我們的演示可在 \textbackslash url\{https://chat.lmsys.org\} 上公開取得。\\
titleTranslation: Chatbot Arena：根據人類偏好評估法學碩士的開放平台},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\QZZZ9LXJ\\Chiang 等。 - 2024 - Chatbot Arena An Open Platform for Evaluating LLM.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\HF643X4Z\\2403.html}
}

@inproceedings{chiaraceccariniSDGsYouHave2023,
  title = {{{SDGs Like You Have Never Seen Before}}!: {{Co-designing Data Visualization Tools}} with and for {{University Students}}},
  shorttitle = {{{SDGs Like You Have Never Seen Before}}!},
  booktitle = {Proceedings of the 2023 {{ACM Conference}} on {{Information Technology}} for {{Social Good}}},
  author = {{Chiara Ceccarini} and {Tommaso Zambon} and {Nicola De Luigi} and {Catia Prandi}},
  year = {9 月 6, 2023},
  series = {{{GoodIT}} '23},
  pages = {521--529},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3582515.3609577},
  url = {https://doi.org/10.1145/3582515.3609577},
  urldate = {2023-09-06},
  abstract = {Sustainability is currently a hot topic also inside universities. Every year, several universities publish an open report based on their efforts to achieve the 17 Sustainable Development Goals (SDGs). However, the university community is often unaware of these reports and does not know how an individual can contribute to sustainability inside and outside the university premises. To partly tackle that, we exploited a three-step process composed of interviews, data analysis, and co-design to understand which data are relevant in this context and what elements an interactive data visualization system should have to raise awareness of the actions performed by the university, and of what can be done by the university community to improve the overall sustainability of the campuses. In this paper, we present our experience in co-designing solutions with students of the University of Bologna. We finally extract some guidelines that can be exploited in a similar context to make university sustainability reporting an action toward a more sustainable future.},
  isbn = {9798400701160},
  langid = {english},
  keywords = {awareness,co-design,sustainability,university,使用者調查,回收},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 您以前從未見過的可持續發展目標！：與大學生共同設計數據可視化工具\\
abstractTranslation:  可持續發展目前也是大學內部的熱門話題。每年，一些大學都會根據其為實現 17 項可持續發展目標 (SDG) 所做的努力發布一份公開報告。然而，大學社區往往不知道這些報告，也不知道個人如何為大學校園內外的可持續發展做出貢獻。為了部分解決這個問題，我們採用了由訪談、數據分析和協同設計組成的三步流程，以了解哪些數據在此背景下相關，以及交互式數據可視化系統應具有哪些元素來提高人們對所執行操作的認識。大學，以及大學社區可以採取哪些措施來提高校園的整體可持續性。在本文中，我們介紹了與博洛尼亞大學的學生共同設計解決方案的經驗。我們最終提取了一些可以在類似背景下利用的指導方針，使大學可持續發展報告成為邁向更可持續未來的行動。},
  note = {只是永續發展與使用者調查},
  file = {C:\Users\BlackCat\Zotero\storage\SBJGN27V\Ceccarini 等。 - 2023 - SDGs Like You Have Never Seen Before! Co-designin.pdf}
}

@inproceedings{chiaraceccariniWhatDoesAir2023,
  title = {What {{Does Air Quality Sound Like}}? {{On Exploring}} the Impact of {{Data Sonification Versus Data Visualization}}},
  shorttitle = {What {{Does Air Quality Sound Like}}?},
  booktitle = {Proceedings of the 2023 {{ACM Conference}} on {{Information Technology}} for {{Social Good}}},
  author = {{Chiara Ceccarini} and {Gianni Tumedei} and {Catia Prandi}},
  year = {9 月 6, 2023},
  series = {{{GoodIT}} '23},
  pages = {510--516},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3582515.3609575},
  url = {https://doi.org/10.1145/3582515.3609575},
  urldate = {2023-09-06},
  abstract = {Air pollution is currently a hot topic and a significant threat that impacts not only climate change but also the health of individuals. As a matter of fact, it was estimated that it causes 6.7 million premature deaths annually, and, in 2021, 97\% of the urban population was exposed to particulate matter concentrations above the World Health Organization’s health-based guideline. Generally, individuals have no perception of the air quality around them and in the city where they live, and their knowledge of the subject depends on what they learn from newscasts or newspapers. However, increasing awareness of the topic can help them make more conscious choices. To partly tackle this problem, we developed a web-based prototype exploiting two modalities to communicate air quality data: data sonification (through audio) and data visualization (through animated video). With the aim of investigating the best communication modality that, eventually, can raise awareness on the topic, we performed a preliminary study. To anticipate some findings, we found out that the videos were considered less mentally demanding and less frustrating, while the sound was considered more pleasant. At the same time, while the videos required less time to be understood and communicated a more precise level of pollution, the audios were considered, on hand, more involving, making the users also feel more immersed in the experience, and, on the other hand, gave the possibility to concentrate on something else while experiencing the data.},
  isbn = {9798400701160},
  langid = {english},
  keywords = {air quality,data sonification,data visualization,人機互動,可視化,回收},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 空氣質量聽起來怎麼樣？探索數據可聽化與數據可視化的影響\\
abstractTranslation:  空氣污染是當前的熱門話題和重大威脅，不僅影響氣候變化，還影響個人健康。事實上，據估計，每年有 670 萬人過早死亡，而且到 2021 年，97\% 的城市人口所接觸的顆粒物濃度超過了世界衛生組織的健康指南。一般來說，個人對其周圍和所居住城市的空氣質量沒有任何感知，他們對該主題的了解取決於他們從新聞廣播或報紙中了解到的信息。然而，提高對該主題的認識可以幫助他們做出更有意識的選擇。為了部分解決這個問題，我們開發了一個基於網絡的原型，利用兩種方式來傳達空氣質量數據：數據聲音化（通過音頻）和數據可視化（通過動畫視頻）。為了調查最終可以提高人們對該主題的認識的最佳溝通方式，我們進行了初步研究。為了預測一些發現，我們發現這些視頻被認為對精神要求較低，也不那麼令人沮喪，而聲音被認為更令人愉悅。與此同時，雖然視頻需要更少的時間來理解並傳達更精確的污染水平，但音頻被認為一方面更具參與性，使用戶也感覺更沉浸在體驗中，另一方面一方面，讓我們可以在體驗數據的同時專注於其他事情。},
  note = {將空氣汙染數據轉成動畫及聲音，並調查其他人聽起來ˊ的感受。},
  file = {C:\Users\BlackCat\Zotero\storage\C8YKYA2U\Ceccarini 等。 - 2023 - What Does Air Quality Sound Like On Exploring the.pdf}
}

@article{christanearlgrantChallengeLongTermKnowledge2015,
  title = {A {{Challenge}} for {{Long-Term Knowledge Base Maintenance}}},
  author = {{Christan Earl Grant} and {Daisy Zhe Wang}},
  year = {6 月 3, 2015},
  journaltitle = {Journal of Data and Information Quality},
  shortjournal = {J. Data and Information Quality},
  volume = {6},
  number = {2-3},
  pages = {7:1--7:3},
  issn = {1936-1955},
  doi = {10.1145/2738044},
  url = {https://dl.acm.org/doi/10.1145/2738044},
  urldate = {2023-08-24},
  langid = {english},
  keywords = {databases,entity resolution,inference,Knowledge base,probabilistic knowledge base},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 長期知識庫維護的挑戰},
  file = {C:\Users\BlackCat\Zotero\storage\LYIXKYYQ\Grant 與 Wang - 2015 - A Challenge for Long-Term Knowledge Base Maintenan.pdf}
}

@inproceedings{christianclausnerNAMERichXML2023,
  title = {{{NAME}} – {{A Rich XML Format}} for {{Named Entity}} and {{Relation Tagging}}},
  booktitle = {Proceedings of the 7th {{International Workshop}} on {{Historical Document Imaging}} and {{Processing}}},
  author = {{Christian Clausner} and {Stefan Pletschacher} and {Apostolos Antonacopoulos}},
  year = {8 月 25, 2023},
  series = {{{HIP}} '23},
  pages = {91--96},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3604951.3605521},
  url = {https://doi.org/10.1145/3604951.3605521},
  urldate = {2023-09-11},
  abstract = {We present NAME XML, a schema for named entities and relations in documents. The standout features are: option to reference a variety of document formats (such as PAGE XML or plain text), support of entity hierarchies, custom entity types via ontologies, more expressivity due to disambiguation of base entities and entity attributes (e.g. “person” and “person name”), and relations between entities that can be directed or undirected. We describe the format in detail, show examples, and discuss real-world use cases.},
  isbn = {9798400708411},
  langid = {english},
  keywords = {回收},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: NAME – 用於命名實體和關係標記的豐富 XML 格式\\
abstractTranslation:  我們提出了 NAME XML，這是文檔中命名實體和關係的模式。突出的功能包括：引用各種文檔格式（例如PAGE XML 或純文本）的選項、支持實體層次結構、通過本體自定義實體類型、由於基本實體和實體屬性（例如“人”）的歧義消除而更具表現力和“人名”），以及可以有向或無向的實體之間的關係。我們詳細描述格式、展示示例並討論實際用例。},
  note = {開發一種基於XML的結構表示法……研究上應該不會用到這部分，而是採用owl或圖資料庫的形式。},
  file = {C:\Users\BlackCat\Zotero\storage\T8XRGQ8F\Clausner 等。 - 2023 - NAME – A Rich XML Format for Named Entity and Rela.pdf}
}

@inproceedings{christiangrevisseOntologyCoverageTool2018,
  title = {Ontology {{Coverage Tool}} and {{Document Browser}} for {{Learning Material Exploration}}},
  booktitle = {2018 {{Thirteenth International Conference}} on {{Digital Information Management}} ({{ICDIM}})},
  author = {{Christian Grévisse} and {Jeff Meder} and {Jean Botev} and {Steffen Rothkugel}},
  date = {2018-09},
  pages = {185--190},
  doi = {10.1109/ICDIM.2018.8847139},
  abstract = {Document collections in e-learning can cause issues to both learners and teachers. On one hand, inquiry from the vast corpus of available resources is non-trivial without adequate formulation support and semantic information. Implicit links between documents are hardly understood without a proper visualization. On the other hand, it is difficult for teachers to keep track of the topics covered by a large collection. In this paper, we present an ontology coverage tool and document browser for learning material exploration. Both learners and teachers can benefit from a visualization of an ontology and the documents related to the comprised concepts, overcoming the limitations of traditional file explorers. Guiding users through a visual query process, learners can quickly pinpoint relevant learning material. The visualization, which has been implemented as a web application using the D3.js JavaScript library, can be integrated into different e-learning applications to further enhance the workflow of learners. Finally, teachers are provided an overview of topic coverage within the collection.},
  eventtitle = {2018 {{Thirteenth International Conference}} on {{Digital Information Management}} ({{ICDIM}})},
  langid = {english},
  keywords = {/unread,Browsers,Data visualization,Document Browser,Document Collection Visualization,Electronic learning,Learning Material,Libraries,Ontologies,Ontology Coverage,Tools,Visual Query Support,Visualization,已整理,知識本體},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 用於學習材料探索的本體覆蓋工具和文件瀏覽器\\
abstractTranslation:  電子學習中的文件收集可能會給學習者和教師帶來問題。一方面，如果沒有足夠的表達支持和語義訊息，對大量可用資源的查詢就並非易事。如果沒有適當的可視化，很難理解文件之間的隱式連結。另一方面，教師很難追蹤大量館藏所涵蓋的主題。在本文中，我們提出了一種用於學習材料探索的本體覆蓋工具和文件瀏覽器。學習者和教師都可以從本體和與所包含概念相關的文件的視覺化中受益，克服了傳統文件瀏覽器的限制。透過視覺化查詢過程引導用戶，學習者可以快速找到相關的學習材料。此視覺化已使用 D3.js JavaScript 函式庫實現為 Web 應用程序，可整合到不同的電子學習應用程式中，以進一步增強學習者的工作流程。最後，向教師提供了館藏主題覆蓋範圍的概述。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\YS7RUJ5G\\Grévisse 等。 - 2018 - Ontology Coverage Tool and Document Browser for Le.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\6YYZPK4J\\8847139.html}
}

@inproceedings{christinaschneegassDesignLongtermMemory2021,
  title = {Design for {{Long-term Memory Augmentation}} in {{Personal Knowledge Management Applications}}},
  booktitle = {12th {{Augmented Human International Conference}}},
  author = {{Christina Schneegass} and {Yannik Wojcicki} and {Evangelos Niforatos}},
  year = {5 月 28, 2021},
  series = {{{AH2021}}},
  pages = {1--5},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3460881.3460931},
  url = {https://dl.acm.org/doi/10.1145/3460881.3460931},
  urldate = {2023-09-16},
  abstract = {In our digitized world, we increasingly rely on technology for remembering. Personal Knowledge Management Applications (PKMAs) help us save and organize relevant and worth-remembering digital information. Thus, PKMAs serve as an external memory prosthesis but with the innate risk of substituting our organic memory. Our results from an online survey (N = 58) on user motivation for PKMAs show that users rarely revisit their digitally saved content. As a result, any memory about previously obtained knowledge naturally attenuates over time. However, being able to recall from one’s organic memory is pivotal for creative processes such as brainstorming and the successful integration of new information. We propose endowing PKMAs with a memory-augmentation feature that periodically reminds one (e.g., on one’s mobile device) to revisit stored content for counteracting long-term forgetting. Periodic revisiting may consolidate the memory recall of the stored information, and thus the memory about saved information becomes gradually augmented. To address the specific requirements of reminders in PKMS, we conducted a follow-up focus group (N = 7) to discuss potential design features, in particular in regards to timing and presentation format. Ultimately, we elicit a set of design principles for future PKMAs that support memory augmentation.},
  isbn = {978-1-4503-9030-9},
  langid = {english},
  keywords = {/unread,memory augmentation,notification,Personal knowledge management applications,reminder,使用者調查,已整理,知識系統},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 個人知識管理應用程式中的長期記憶增強設計\\
abstractTranslation:  在我們的數位化世界中，我們越來越依賴科技來記憶。個人知識管理應用程式 (PKMA) 可協助我們保存和組織相關且值得記住的數位資訊。因此，PKMA 作為外在記憶假體，但具有取代我們的有機記憶的固有風險。我們對 PKMA 使用者動機的線上調查 (N = 58) 結果顯示，使用者很少重新造訪他們以數位方式保存的內容。結果，任何關於先前獲得的知識的記憶都會隨著時間的推移而自然減弱。然而，能夠回憶起一個人的有機記憶對於集思廣益和成功整合新資訊等創造性過程至關重要。我們建議賦予 PKMA 一種記憶增強功能，定期提醒人們（例如，在行動裝置上）重新存取儲存的內容，以防止長期遺忘。定期重溫可以鞏固已儲存資訊的記憶回憶，從而對已儲存資訊的記憶逐漸增強。為了解決 PKMS 中提醒的具體要求，我們進行了後續焦點小組 (N = 7) 來討論潛在的設計功能，特別是在時間和演示格式方面。最終，我們為未來支援記憶體增強的 PKMA 提出了一套設計原則。},
  note = {發現大部分人都不會回顧系統中舊的知識，造成遺忘。},
  file = {C:\Users\BlackCat\Zotero\storage\HCJPIFJ7\Schneegass 等。 - 2021 - Design for Long-term Memory Augmentation in Person.pdf}
}

@incollection{chuanliTCMinerHighPerformance2004,
  title = {{{TCMiner}}: {{A High Performance Data Mining System}} for {{Multi-dimensional Data Analysis}} of {{Traditional Chinese Medicine Prescriptions}}},
  shorttitle = {{{TCMiner}}},
  booktitle = {Conceptual {{Modeling}} for {{Advanced Application Domains}}},
  author = {{Chuan Li} and {Changjie Tang} and {Jing Peng} and {Jianjun Hu} and {Lingming Zeng} and {Xiaoxiong Yin} and {Yongguang Jiang} and {Juan Liu}},
  editor = {{Shan Wang} and {Katsumi Tanaka} and {Shuigeng Zhou} and {Tok-Wang Ling} and {Jihong Guan} and {Dong-qing Yang} and {Fabio Grandi} and {Eleni E. Mangina} and {Il-Yeol Song} and {Heinrich C. Mayr}},
  editora = {{David Hutchison} and {Takeo Kanade} and {Josef Kittler} and {Jon M. Kleinberg} and {Friedemann Mattern} and {John C. Mitchell} and {Moni Naor} and {Oscar Nierstrasz} and {C. Pandu Rangan} and {Bernhard Steffen} and {Madhu Sudan} and {Demetri Terzopoulos} and {Dough Tygar} and {Moshe Y. Vardi} and {Gerhard Weikum}},
  editoratype = {redactor},
  date = {2004},
  volume = {3289},
  pages = {246--257},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-30466-1_23},
  url = {http://link.springer.com/10.1007/978-3-540-30466-1_23},
  urldate = {2022-09-19},
  isbn = {978-3-540-23722-8 978-3-540-30466-1}
}

@inproceedings{claudiogutierrezKnowledgeGraphsTutorial2020,
  title = {Knowledge {{Graphs}}: {{A Tutorial}} on the {{History}} of {{Knowledge Graph}}'s {{Main Ideas}}},
  shorttitle = {Knowledge {{Graphs}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {{Claudio Gutierrez} and {Juan F. Sequeda}},
  year = {10 月 19, 2020},
  series = {{{CIKM}} '20},
  pages = {3509--3510},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3340531.3412176},
  url = {https://dl.acm.org/doi/10.1145/3340531.3412176},
  urldate = {2023-08-23},
  abstract = {Knowledge Graphs can be considered as fulfilling an early vision in Computer Science of creating intelligent systems that integrate knowledge and data at large scale. Stemming from scientific advancements in research areas of Semantic Web, Databases, Knowledge representation, NLP, Machine Learning, among others, Knowledge Graphs have rapidly gained popularity in academia and industry in the past years. The integration of such disparate disciplines and techniques give the richness to Knowledge Graphs, but also present the challenge to practitioners and theoreticians to know how current advances develop from early techniques in order, on one hand, take full advantage of them, and on the other, avoid reinventing the wheel. This tutorial will provide a historical context on the roots of Knowledge Graphs grounded in the advancements of Logic, Data and the combination thereof.},
  isbn = {978-1-4503-6859-9},
  langid = {english},
  keywords = {databases,knowledge,knowledge graph,logic,semantic web,回收,知識圖譜},
  annotation = {7 citations (Crossref) [2024-03-26]\\
titleTranslation: 知識圖：知識圖主要思想歷史的教程\\
abstractTranslation:  知識圖可以被認為實現了計算機科學的早期願景，即創建大規模集成知識和數據的智能係統。得益於語義網、數據庫、知識表示、自然語言處理、機器學習等研究領域的科學進步，知識圖譜近年來在學術界和工業界迅速受到歡迎。這些不同學科和技術的整合為知識圖譜帶來了豐富性，但也給實踐者和理論家帶來了挑戰，他們需要了解當前的進步如何從早期技術發展而來，一方面充分利用它們，另一方面，避免重新發明輪子。本教程將提供基於邏輯、數據及其組合進步的知識圖根源的歷史背景。},
  file = {C:\Users\BlackCat\Zotero\storage\JK4Q47GC\Gutierrez 與 Sequeda - 2020 - Knowledge Graphs A Tutorial on the History of Kno.pdf}
}

@inproceedings{claudiopinhanezUsingMetaKnowledgeMined2021,
  title = {Using {{Meta-Knowledge Mined}} from {{Identifiers}} to {{Improve Intent Recognition}} in {{Conversational Systems}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {{Claudio Pinhanez} and {Paulo Cavalin} and {Victor Henrique Alves Ribeiro} and {Ana Appel} and {Heloisa Candello} and {Julio Nogima} and {Mauro Pichiliani} and {Melina Guerra} and {Maira de Bayser} and {Gabriel Malfatti} and {Henrique Ferreira}},
  date = {2021-08},
  pages = {7014--7027},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.acl-long.545},
  url = {https://aclanthology.org/2021.acl-long.545},
  urldate = {2023-09-19},
  abstract = {In this paper we explore the improvement of intent recognition in conversational systems by the use of meta-knowledge embedded in intent identifiers. Developers often include such knowledge, structure as taxonomies, in the documentation of chatbots. By using neuro-symbolic algorithms to incorporate those taxonomies into embeddings of the output space, we were able to improve accuracy in intent recognition. In datasets with intents and example utterances from 200 professional chatbots, we saw decreases in the equal error rate (EER) in more than 40\% of the chatbots in comparison to the baseline of the same algorithm without the meta-knowledge. The meta-knowledge proved also to be effective in detecting out-of-scope utterances, improving the false acceptance rate (FAR) in two thirds of the chatbots, with decreases of 0.05 or more in FAR in almost 40\% of the chatbots. When considering only the well-developed workspaces with a high level use of taxonomies, FAR decreased more than 0.05 in 77\% of them, and more than 0.1 in 39\% of the chatbots.},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  langid = {english},
  keywords = {未整理,重要},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用從標識符中挖掘的元知識來改進會話系統中的意圖識別\\
abstractTranslation:  在本文中，我們探索透過使用嵌入意圖標識符中的元知識來改進會話系統中的意圖識別。開發人員通常在聊天機器人的文件中包含諸如分類法之類的知識、結構。透過使用神經符號演算法將這些分類法合併到輸出空間的嵌入中，我們能夠提高意圖辨識的準確性。在包含 200 個專業聊天機器人的意圖和範例話語的資料集中，我們發現與沒有元知識的相同演算法的基線相比，超過 40\% 的聊天機器人的等錯誤率 (EER) 有所下降。事實證明，元知識在檢測超出範圍的話語方面也很有效，提高了三分之二聊天機器人的錯誤接受率 (FAR)，近 40\% 的聊天機器人的 FAR 降低了 0.05 或更多。當僅考慮高度使用分類法的開發良好的工作空間時，其中 77\% 的 FAR 下降超過 0.05，39\% 的聊天機器人下降超過 0.1。},
  file = {C:\Users\BlackCat\Zotero\storage\J4S7CBH6\Pinhanez 等。 - 2021 - Using Meta-Knowledge Mined from Identifiers to Imp.pdf}
}

@online{CongXuQiuDaoWeiFuWuYiLingYuQuDongSheJiYuJiQiXueXiWeiFangFa__TaiWanBoShuoShiLunWenZhiShiJiaZhiXiTong,
  title = {從需求到微服務:以領域驅動設計與機器學習為方法\_\_臺灣博碩士論文知識加值系統},
  url = {https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=key1YF/record?r1=8&h1=4#XXX},
  urldate = {2024-04-28},
  langid = {chinese},
  keywords = {微服務,未整理,碩博士論文},
  file = {D\:\\Paper\\From Requirements to Microservice A Domain Driven Approach with Machine Learning.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\6WQRB5WC\\record.html}
}

@inproceedings{cookeRecommendationsElectronicLaboratory2017,
  title = {Recommendations for Electronic Laboratory Notebooks in Undergraduate Engineering Faculty: {{A}} Student-Led Case Study},
  shorttitle = {Recommendations for Electronic Laboratory Notebooks in Undergraduate Engineering Faculty},
  author = {Cooke, N.J. and Robbins, P.T. and Lodge, J.M. and Shannon, I. and Hawwash, K.I.M. and Lodge, J.M.},
  date = {2017},
  pages = {551--558},
  abstract = {Industry and academic research labs increasingly use Electronic Laboratory Notebooks (ELN). In engineering education, the ELN affords new laboratory technologies and pedagogies requiring embedding rich data and supporting virtual labs. We present a student-led design of a prototype ELN based on Microsoft OneNote software for our undergraduate engineering programmes. The solution addresses the requirements for lab redesign aspirations around new laboratory pedagogies requiring entirely digital workflows. To enhance the solution's ecological validity, students themselves developed it while undertaking their existing undergraduate labs. The result is an ELN based on the digital notetaking software Microsoft OneNote. We make available a requirements and design tool that includes a set of 21 use cases, 102 requirements, and 32 failure modes. Staff and student feedback is reported. The work will assist engineering and science faculty looking to introduce an ELN solution into their curricula.},
  eventtitle = {Proceedings of the 45th {{SEFI Annual Conference}} 2017 - {{Education Excellence}} for {{Sustainability}}, {{SEFI}} 2017},
  isbn = {978-989-98875-7-2},
  langid = {english},
  keywords = {Educational technologies,Electronic laboratory notebook,ELN,Microsoft OneNote,User acceptance,待讀,重要},
  annotation = {titleTranslation: 本科工程學院電子實驗室筆記本的建議：學生主導的案例研究\\
abstractTranslation:  產業和學術研究實驗室越來越多地使用電子實驗室筆記本 (ELN)。在工程教育中，ELN 提供了新的實驗室技術和教學法，需要嵌入豐富的數據並支援虛擬實驗室。我們為大學部工程課程展示了由學生主導的基於 Microsoft OneNote 軟體的 ELN 原型設計。該解決方案滿足了圍繞需要完全數位化工作流程的新實驗室教學法的實驗室重新設計願望的要求。為了增強該解決方案的生態有效性，學生們在開展現有的本科實驗室的同時自行開發了該解決方案。結果是基於數位筆記軟體 Microsoft OneNote 的 ELN。我們提供了一個需求和設計工具，其中包括一組 21 個用例、102 個需求和 32 種故障模式。報告教職員工和學生的回饋。這項工作將幫助工程和科學教師將 ELN 解決方案引入他們的課程。},
  file = {C:\Users\BlackCat\Zotero\storage\FGIGPPXR\display.html}
}

@article{CuiYuanShuJuWaJueZaiZhongWenBingLiFenLeiZhongDeYingYong2011,
  title = {数据挖掘在中文病历分类中的应用},
  author = {{崔园}},
  date = {2011},
  journaltitle = {计算机与数字工程},
  shortjournal = {COMPUTER AND DIGITAL ENGINEERING},
  volume = {39},
  number = {3},
  pages = {160--163},
  doi = {10.3969/j.issn.1672-9722.2011.03.044},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhFqc2p5c3pnYzIwMTEwMzA0NBoIbWkxejE3dXU%3D},
  urldate = {2022-08-14},
  abstract = {文中展示了一种新的基于短语匹配的中文电子病历分类算法.这种方法能够充分地运用信息量更大的短语匹配而不是独立的字词匹配,并且它能够保留文档结构信息.通过用向量空间表示匹配的短语,每个病历记录被表示为一个向量,向量中每个元素表示一个短语在记录中出现的次数.所有的向量组成了病例数据集,并把它作为自组织神经网络的输人数据.实验表示,将这种方法应用在不同病种的数据集分类上具有较高的正确率,分类正确率平均值为95.417\%.分类结果能够有效地辅助医务工作者诊断疾病,帮助他们总结出不同病种间以前从未发现的重要表现特征.},
  langid = {zh\_CN},
  keywords = {COMPUTER AND DIGITAL ENGINEERING,DIG算法,文本分类,电子病历,计算机与数字工程},
  annotation = {成都医学院人文信息管理学院计算机教研室,成都,610083\\
2011-06-28 （万方平台首次上网日期，不代表论文的发表时间）\\
abstractTranslation:  文中展示了一種新的基於詞彙匹配的中文電子病歷分類算法。這種方法能夠充分利用信息量較大的詞彙匹配而不是獨立的詞詞匹配，並且能夠保留文檔結構信息。空間表示每個輸出的主板，每個病歷記錄表示一個存儲，存儲中元素表示一個存儲在記錄中出現的次數。所有存儲組成了病例數據集，並將其作為自組織神經網絡的人數據實驗表示，將這種方法應用在不同病種的數據集分類上具有較高的正確率，分類正確率為95.417\%。分類結果能夠有效地輔助醫護人員診斷疾病，幫助他們總結出不同病種間以前從未發現的重要表現特徵。\\
titleTranslation: 數據挖掘在中文病歷分類中的應用},
  file = {C:\Users\BlackCat\Zotero\storage\HZZ3HH7I\崔 - 2011 - 数据挖掘在中文病历分类中的应用.pdf}
}

@article{cukimlongNovelFuzzyKnowledge2022,
  title = {A Novel Fuzzy Knowledge Graph Pairs Approach in Decision Making},
  author = {{Cu Kim Long} and {Pham Van Hai} and {Tran Manh Tuan} and {Luong Thi Hong Lan} and {Pham Minh Chuan} and {Le Hoang Son}},
  date = {2022-07-01},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {81},
  number = {18},
  pages = {26505--26534},
  issn = {1573-7721},
  doi = {10.1007/s11042-022-13067-9},
  url = {https://doi.org/10.1007/s11042-022-13067-9},
  urldate = {2023-09-12},
  abstract = {Fuzzy Knowledge Graph (FKG) has recently been emerging as one of the key techniques for supporting classification and decision-making problems. FKG is a novel concept that was firstly introduced in 2020 by integrating approximate reasoning with inference mechanism to find labels of new records, which are impossible for inference by the rule base. However, one of the key limitations of FKG is the use of a single pair to find new records’ label that leads to low performance in approximation. This paper presents a novel approach of using FKG pairs instead of a single pair as in the classical model. A novel FKG-Pairs model including a new representing method and an approximation algorithm is presented. Theoretical analysis of the FKG-Pairs model such as identification of a threshold for the best value (k∗) pairs is also investigated. Finally, to validate the proposed model, a numerical example and the experiments on the UCI datasets are presented. In addition, a two-way ANOVA method is also conducted to validate the model statistically. The novel concept about the FKG-Pairs given in this paper exposes new ideas in the effort to realize the much-anticipated decision-making and classification problems in fuzzy systems},
  langid = {english},
  keywords = {Approximate reasoning,Decision making,FKG-pairs,Fuzzy knowledge graph,Knowledge graph,M-CFIS-FKG,無法取得},
  annotation = {9 citations (Crossref) [2024-03-26]\\
titleTranslation: 一種新穎的模糊知識圖對決策方法\\
abstractTranslation:  模糊知識圖（FKG）最近已成為支持分類和決策問題的關鍵技術之一。 FKG是一個新穎的概念，於2020年首次提出，通過將近似推理與推理機制相結合，找到規則庫無法推理的新記錄的標籤。然而，FKG 的一個關鍵限制是使用單個對來查找新記錄的標籤，這會導致近似性能較低。本文提出了一種使用 FKG 對而不是經典模型中的單個對的新穎方法。提出了一種新穎的 FKG-Pairs 模型，包括新的表示方法和近似算法。還研究了 FKG-Pairs 模型的理論分析，例如識別最佳值 (k*) 對的閾值。最後，為了驗證所提出的模型，給出了一個數值示例和 UCI 數據集上的實驗。此外，還採用雙向方差分析方法對模型進行統計驗證。本文給出的 FKG-Pairs 的新穎概念為實現模糊系統中備受期待的決策和分類問題提供了新的思路}
}

@article{d.e.avisonInformationSystemsAction2018,
  title = {Information Systems Action Research: {{Debunking}} Myths and Overcoming Barriers},
  shorttitle = {Information Systems Action Research},
  author = {{D. E. Avison} and {R. M. Davison} and {J. Malaurent}},
  date = {2018-03-01},
  journaltitle = {Information \& Management},
  shortjournal = {Information \& Management},
  volume = {55},
  number = {2},
  pages = {177--187},
  issn = {0378-7206},
  doi = {10.1016/j.im.2017.05.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0378720617300605},
  urldate = {2023-10-23},
  abstract = {The relevance of action research as a research method in the information systems (IS) discipline is not disputed. Nevertheless, the extent to which action research is published in good journals is infrequent enough to indicate a serious problem. In this article, we explore the reasons underlying this situation and make recommendations aiming to increase both the practice and the publication of action research. To identify both the barriers to undertaking action research and potential ways of overcoming those barriers, we survey 218 authors of 120 articles demonstrating empirical action research published in 12 of our good journals during the period 1982–2016. We received 70 usable responses. We also surveyed 52 editors of selected IS journals and received 25 usable responses. Our findings are revealing as they indicate both genuine barriers associated with action research and some apparent barriers that are in reality misperceptions or myths. In reflecting on these, we emphasize the special qualities of action research. We also reflect on the critical role that action research plays in the IS field as a whole and its potential for further contributions to research and practice, given the strong and close connections with organizational problem contexts that action research requires. Finally, we make a number of recommendations that are designed to increase the incidence of action research in the IS discipline},
  keywords = {Action research,Doctoral studies,Engaged research,Journals,Publishing,Relevance,Rigor,研究流程,行動研究},
  annotation = {41 citations (Crossref) [2024-03-26]},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\8G2ZHUKL\\Avison et al. - 2018 - Information systems action research Debunking myt.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\HV3PIVQB\\S0378720617300605.html}
}

@inproceedings{d.koznovApproachDesigningSoftware2017,
  title = {An {{Approach}} to {{Designing Software Engineering Thesis Papers}}:},
  shorttitle = {An {{Approach}} to {{Designing Software Engineering Thesis Papers}}},
  booktitle = {Proceedings of the 9th {{International Joint Conference}} on {{Knowledge Discovery}}, {{Knowledge Engineering}} and {{Knowledge Management}}},
  author = {{D. Koznov} and {M. Nemeshev}},
  date = {2017},
  pages = {139--145},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  location = {Funchal, Madeira, Portugal},
  doi = {10.5220/0006497301390145},
  url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006497301390145},
  urldate = {2023-10-23},
  eventtitle = {9th {{International Conference}} on {{Knowledge Management}} and {{Information Sharing}}},
  isbn = {978-989-758-273-8},
  langid = {english},
  keywords = {使用者研究,研究流程},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 設計軟體工程論文的方法：},
  file = {C:\Users\BlackCat\Zotero\storage\DAY4KMPE\Koznov and Nemeshev - 2017 - An Approach to Designing Software Engineering Thes.pdf}
}

@thesis{DaiHaoZhuZhiYuanHunHeZiLiaoXingTaiDeXianZhiShiCeShiAnLiChanSheng2021,
  title = {支援混合資料型態的限制式測試案例產生},
  author = {{戴浩竹}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2021},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/97ryf5},
  abstract = {軟體測試為確保軟體品質的主要技術。本研究團隊已開發的限制式測試案例產生器，目前只支援整數資料型態為主的測試案例產生，並無支援其他基本資料型態的測試案例產生。限制式測試案例產生需要透過限制邏輯程式，來求解測試資料。因此，支援混合資料型態的測試案例產生，也需要在限制邏輯程式中，實作物件限制語言所支援的資料型態的相關函式。本論文改善原測試案例產生器兩個部分。第一為擴增測試模型產生器，實作抽象語法樹的型態推導系統。第二為擴增限制邏輯程式，實作物件限制語言所支援的型態轉換函式。基於這兩個改善，使得新測試案例產生系統可支援混合資料型態的測試案例產生。本論文也針對新系統做了初步的效能評估。測試案例品質方面，使用決策覆蓋標準及決策條件覆蓋標準，新測試案例產生系統產生的測試案例集可達100\%的程式覆蓋率及97\%的突變覆蓋率。測試案例產生時間方面，新測試案例產生系統的測試案例產生時間平均增加19\%。},
  pagetotal = {230},
  keywords = {實驗室}
}

@online{daiPromptagatorFewshotDense2022,
  title = {Promptagator: {{Few-shot Dense Retrieval From}} 8 {{Examples}}},
  shorttitle = {Promptagator},
  author = {Dai, Zhuyun and Zhao, Vincent Y. and Ma, Ji and Luan, Yi and Ni, Jianmo and Lu, Jing and Bakalov, Anton and Guu, Kelvin and Hall, Keith B. and Chang, Ming-Wei},
  date = {2022-09-23},
  eprint = {2209.11755},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.11755},
  url = {http://arxiv.org/abs/2209.11755},
  urldate = {2024-03-14},
  abstract = {Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by LLM's generalization ability, Promptagator makes it possible to create task-specific end-to-end retrievers solely based on a few examples \{without\} using Natural Questions or MS MARCO to train \%question generators or dual encoders. Surprisingly, LLM prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on MS MARCO like ColBERT v2 by more than 1.2 nDCG on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point nDCG improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.},
  langid = {english},
  pubstate = {preprint},
  keywords = {ChatGPT,Computer Science - Computation and Language,Computer Science - Information Retrieval,RAG,問答系統,已整理,微讀,文獻,資料集,重要},
  annotation = {titleTranslation: Promptagator：從 8 個範例中進行少樣本密集檢索\\
abstractTranslation:  最近關於資訊檢索的許多研究都集中在如何從一項任務（通常具有豐富的監督數據）轉移到監督有限的各種其他任務，並隱含假設可以從一項任務推廣到所有其他任務。然而，這忽略了這樣一個事實：存在許多多樣化且獨特的檢索任務，每個任務都針對不同的搜尋意圖、查詢和搜尋領域。在本文中，我們建議進行少樣本密集檢索，在這種設定中，每個任務都帶有簡短的描述和一些範例。為了增強一些範例的功能，我們提出了基於提示的檢索器查詢產生器（Promptagator），它利用大型語言模型（LLM）作為幾個查詢產生器，並根據產生的資料建立特定於任務的檢索器。在 LLM 泛化能力的支援下，Promptagator 可以僅基於幾個範例建立特定於任務的端對端檢索器，而無需使用 Natural Questions 或 MS MARCO 來訓練 \%question 產生器或雙編碼器。令人驚訝的是，LLM 提示不超過 8 個範例允許雙編碼器在 11 個檢索集上比在 MS MARCO（如 ColBERT v2）上訓練的精心設計的模型平均表現超過 1.2 nDCG。使用相同的生成資料進一步訓練標準大小的重新排序器會產生另一個 5.0 點的 nDCG 改進。我們的研究確定，查詢產生比之前觀察到的要有效得多，特別是在給出少量特定於任務的知識時。},
  note = {在BEIR中的18個語料庫中各隨機取100萬個檔案，並使用prompt模板及FLAN 137B LLM生成8個查詢
\par
一致性過濾: 通過比對(q, d)對，檢查q是否能在top-1中找到來源d，判斷此自動生成的query是否有效，無效則不保留。
\par
使用生成的查詢做訓練，基於T5-110M架構微調出PROMPTAGATOR及PROMPTAGATOR++
\par
對於RAG問題有蠻不錯的數學定義，在相關研究及銷融研究也描述了許多有用的訊息。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\9SQW7ZZQ\\Dai 等。 - 2022 - Promptagator Few-shot Dense Retrieval From 8 Exam.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\WQX28PAI\\Dai 等。 - 2022 - Promptagator Few-shot Dense Retrieval From 8 Exam.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\MQYNEHXI\\2209.html}
}

@inproceedings{damiengrauxLOVESGuidingOntology2022,
  title = {{{LOV-ES}}: {{Guiding}} the {{Ontology Selection}} to {{Structure Textual Data}} Using {{Topic Modeling}}},
  shorttitle = {{{LOV-ES}}},
  booktitle = {International {{Semantic Web Conference}}},
  author = {{Damien Graux} and {Anaïs Ollagnier}},
  date = {2022-10},
  location = {Hangzou, China},
  url = {https://hal.science/hal-03765857},
  urldate = {2023-09-26},
  abstract = {On-line availability of text corpora nowadays allow data practitioners to build complex knowledge combining various sources. One common shared challenge lays in the modelisation of intermediate knowledge structures able to gather at once the various topics present in the texts. Practically, practitioners often go through the creation of vocabularies. In order to help these domain experts, we design LOVES: a solution able to help them in this creative process, guiding them in the selection and the combination of already existing vocabularies available online. Technically, our solution relies on LDA to detect topics and on the LOV to then propose candidate vocabularies.},
  keywords = {LDA,LOV,Textual source,Topic Modeling,Vocabulary Recommender,待讀,本體建立,知識本體,重要},
  file = {C:\Users\BlackCat\Zotero\storage\3BT7442A\Graux 與 Ollagnier - 2022 - LOV-ES Guiding the Ontology Selection to Structur.pdf}
}

@inproceedings{danwuApproachConceptualOntology2012,
  title = {An Approach to Conceptual Ontology Integration with an Ontology Repository and a Rule Base},
  booktitle = {Proceedings of the 11th {{WSEAS}} International Conference on {{Artificial Intelligence}}, {{Knowledge Engineering}} and {{Data Bases}}},
  author = {{Dan Wu} and {Anne Håkansson}},
  year = {2 月 22, 2012},
  series = {{{AIKED}}'12},
  pages = {163--168},
  publisher = {{World Scientific and Engineering Academy and Society (WSEAS)}},
  location = {Stevens Point, Wisconsin, USA},
  abstract = {There exist a lot of ontologies that together can enrich knowledge within a domain or cross several related domains; and can provide advanced services based on these integrated ontologies. However, the concepts are often described differently in various ontologies, which create problems for the ontology integration. In this paper, an approach of an ontology repository and a rule base is proposed to handle the various concepts descriptions. A new mapping process is proposed. The results of the mappings are ontology joints, which are ontologies composed of sets of assertions about the concepts validated in the original ontologies. Metadata are used for the description of ontology and ontology joints. A rule base of how to interpret the concepts by applying the metadata is created after the mappings. Ontologies and the ontology joints in the repository can be reused for further ontology integration. Human interaction is possible in the approach.},
  isbn = {978-1-61804-068-8},
  keywords = {data integration,knowledge base,knowledge representation,ontology reasoning,reasoning,未整理}
}

@article{danwuUndergraduateInformationBehaviors2016,
  title = {Undergraduate Information Behaviors in Thesis Writing: {{A}} Study Using the {{Information Search Process}} Model},
  shorttitle = {Undergraduate Information Behaviors in Thesis Writing},
  author = {{Dan Wu} and {Wanyu Dang} and {Daqing He} and {Renmin Bi}},
  date = {2016-06-22},
  journaltitle = {Journal of Librarianship and Information Science},
  shortjournal = {Journal of Librarianship and Information Science},
  volume = {49},
  doi = {10.1177/0961000616654960},
  abstract = {The study investigates whether information-seeking behavior models and theories obtained in previous research are applicable to more complex tasks. It also aims to gather students’ opinions on the importance and helpfulness of various traditional and online information sources in their thesis-writing process. This study would help to develop a better understanding of the roles and impacts of these information sources in the current networked academic infrastructure. Inspired by the Information Search Process model, we divided the process into six stages and conducted three separate surveys that covered students’ feelings, thoughts and actions, as well as other important factors that might affect their behaviors in each of the stages. Our study shows that both the feelings and thoughts of students changed during the different stages of the process, and that they were generally consistent with the descriptions in the Information Search Process model. The study indicates that it is beneficial to use the Information Search Process model as the starting point for studying the student thesis-writing processes. As the outcome of the study, we ultimately proposed a multi-stage model for Chinese undergraduate students’ thesis-writing process.},
  annotation = {7 citations (Crossref) [2024-03-26]}
}

@inproceedings{darrishuppSmartBookmarksAutomatic2007,
  title = {Smart Bookmarks: Automatic Retroactive Macro Recording on the Web},
  shorttitle = {Smart Bookmarks},
  booktitle = {Proceedings of the 20th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},
  author = {{Darris Hupp} and {Robert C. Miller}},
  year = {10 月 7, 2007},
  series = {{{UIST}} '07},
  pages = {81--90},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1294211.1294226},
  url = {https://doi.org/10.1145/1294211.1294226},
  urldate = {2023-09-06},
  abstract = {We present a new web automation system that allows users to create a smart bookmark, consisting of a starting URL plus a script of commands that returns to a particular web page or state of a web application. A smart bookmark can be requested for any page, and the necessary commands are automatically extracted from the user's interaction history. Unlike other web macro recorders, which require the user to start recording before navigating to the desired page, smart bookmarks are generated retroactively, after the user has already reached a page, and the starting point of the macro is found automatically. Smart bookmarks have a rich graphical visualization that combines textual commands, web page screenshots, and animations to explain what the bookmark does. A bookmark's script consists of keyword commands, interpreted without strict reliance on syntax, allowing bookmarks to be easily edited and shared.},
  isbn = {978-1-59593-679-0},
  langid = {english},
  keywords = {browsers,macro recording,web automation},
  annotation = {36 citations (Crossref) [2024-03-26]\\
titleTranslation: 智能書籤：在網絡上自動追溯宏錄製\\
abstractTranslation:  我們提出了一種新的 Web 自動化系統，允許用戶創建智能書籤，其中包含起始 URL 以及返回特定網頁或 Web 應用程序狀態的命令腳本。可以為任何頁面請求智能書籤，並且會自動從用戶的交互歷史記錄中提取必要的命令。與其他網絡宏錄製器不同，其他網絡宏錄製器要求用戶在導航到所需頁面之前開始錄製，而智能書籤是在用戶到達頁面後追溯生成的，並自動找到宏的起點。智能書籤具有豐富的圖形可視化，結合了文本命令、網頁屏幕截圖和動畫來解釋書籤的功能。書籤的腳本由關鍵字命令組成，在不嚴格依賴語法的情況下進行解釋，從而允許輕鬆編輯和共享書籤。},
  file = {C:\Users\BlackCat\Zotero\storage\7EFYKXFN\Hupp 與 Miller - 2007 - Smart bookmarks automatic retroactive macro recor.pdf}
}

@book{davenportWorkingKnowledgeHow1998,
  title = {Working Knowledge: {{How}} Organizations Manage What They Know},
  shorttitle = {Working Knowledge},
  author = {Davenport, Thomas H. and Prusak, Laurence},
  date = {1998},
  publisher = {Harvard Business Press},
  url = {https://www.google.com/books?hl=zh-TW&lr=&id=-4-7vmCVG5cC&oi=fnd&pg=PR7&dq=Davenport+%26+Prusak+(Davenport+and+Prusak+1997+knowledge&ots=mBfeX0alI-&sig=5NsROHJB2SB_-K23sMaSoNpIM40},
  urldate = {2024-02-23},
  langid = {english},
  keywords = {已整理,文獻,經典,重要},
  annotation = {titleTranslation: 工作知識：組織如何管理他們所知道的知識},
  note = {定義知識
\par
知識是最有價值的訊息，因此也是最難管理的形式。它之所以有價值，正是因為有人賦予了資訊背景、意義和特定的解釋；有人反思了這些知識，添加了自己的智慧，並考慮了其更大的意義},
  file = {C:\Users\BlackCat\Zotero\storage\V88KVNFQ\Davenport 與 Prusak - 1998 - Working knowledge How organizations manage what t.pdf}
}

@inproceedings{davetoweyResearchingSupportingStudent2015,
  title = {Researching and Supporting Student Note-Taking: {{Building}} a Multimedia Note-Taking App},
  shorttitle = {Researching and Supporting Student Note-Taking},
  booktitle = {2015 {{IEEE International Conference}} on {{Teaching}}, {{Assessment}}, and {{Learning}} for {{Engineering}} ({{TALE}})},
  author = {{Dave Towey} and {David Foster} and {Filippo Gilardi} and {Paul Martin} and {Andy White} and {Cecilia Goria}},
  date = {2015-02},
  pages = {54--58},
  doi = {10.1109/TALE.2015.7386015},
  url = {https://ieeexplore.ieee.org/document/7386015/citations?tabFilter=papers#citations},
  urldate = {2023-10-12},
  abstract = {In late 2014 collaborative research was undertaken between colleagues in the departments of Computer Science, International Communications, the Centre for English language Education (CELE), and the Language Centre at a global higher education institution (HEI-A), located partly in mainland China. This research aims to explore student note-taking habits, especially in the context of more multimedia-intense content. As part of this, student note-taking habits have been observed and studied, and a software application is being developed which will both support student multimedia note-taking, and facilitate research into note-taking habits. An interesting aspect of this project has been the inclusion of HEI-A undergraduate computer science students as software developers. This paper outlines some findings of our research in terms of defining student note-taking habits, describes some of the issues we faced during the first stages of the project and gives a short description on how this application could be used for teaching and research purposes in the future.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Teaching}}, {{Assessment}}, and {{Learning}} for {{Engineering}} ({{TALE}})},
  annotation = {9 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\I6GTZ98W\citations.html}
}

@article{davidmartin-lammerdingOntologybasedSystemAvoid2023,
  title = {An Ontology-Based System to Avoid {{UAS}} Flight Conflicts and Collisions in Dense Traffic Scenarios},
  author = {{David Martín-Lammerding} and {José Javier Astrain} and {Alberto Córdoba} and {Jesús Villadangos}},
  year = {4 月 1, 2023},
  journaltitle = {Expert Systems with Applications: An International Journal},
  shortjournal = {Expert Syst. Appl.},
  volume = {215},
  number = {C},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2022.119027},
  url = {https://doi.org/10.1016/j.eswa.2022.119027},
  urldate = {2023-09-27},
  abstract = {New Unmanned Aerial Systems (UAS) applications will increase air traffic densities in metropolitan regions. Collision avoidance systems (CAS) are a key component in integrating a high number of UAS into the airspace in a safe way. This paper presents a distributed, autonomous, and knowledge-based CAS, called Dronetology System (DroS), for UASs. The CAS proposed here is managed using a novel ontology, called Dronetology-cas, which allows to make autonomous decisions according to the knowledge inferred from the data gathered by the UAS. DroS is deployed as part of the payload of the UAS. So, it is designed to run in an embedded platform with limited processing capacity and low battery consumption. DroS collects data from sensors and collaborative elements to make smart decisions using knowledge obtained from collaborative UASs, adapting the maneuvers of the aerial vehicles to their original flight plans, their kind of vehicle, and the collision scenario. DroS accountability involves recording its internal operation to assist with reconstructing the circumstances surrounding an autonomous maneuver or the details previous to a collision. DroS has been verified using the hardware in the loop (HIL) technique with a UAS traffic environment simulator. Results obtained show a significant improvement in terms of safety by avoiding collisions. • Distributed, autonomous and knowledge-based collision avoidance system for UASs. • Autonomous decisions according to the knowledge inferred from the data gathered. • Accountability of the circumstances surrounding a maneuver or collision. • Verified using HIL with an UAS traffic environment simulator. • Significant improvement in terms of safety by avoiding collisions.},
  keywords = {Autonomous,Collision avoidance systems,Knowledge,Ontology,Situational-awareness,UAS,未整理},
  annotation = {1 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\LBD65P6A\Martín-Lammerding 等。 - 2023 - An ontology-based system to avoid UAS flight confl.pdf}
}

@article{davidn.nicholsonConstructingKnowledgeGraphs2020,
  title = {Constructing Knowledge Graphs and Their Biomedical Applications},
  author = {{David N. Nicholson} and {Casey S. Greene}},
  date = {2020-01-01},
  journaltitle = {Computational and Structural Biotechnology Journal},
  shortjournal = {Computational and Structural Biotechnology Journal},
  volume = {18},
  pages = {1414--1428},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2020.05.017},
  url = {https://www.sciencedirect.com/science/article/pii/S2001037020302804},
  urldate = {2023-10-11},
  abstract = {Knowledge graphs can support many biomedical applications. These graphs represent biomedical concepts and relationships in the form of nodes and edges. In this review, we discuss how these graphs are constructed and applied with a particular focus on how machine learning approaches are changing these processes. Biomedical knowledge graphs have often been constructed by integrating databases that were populated by experts via manual curation, but we are now seeing a more robust use of automated systems. A number of techniques are used to represent knowledge graphs, but often machine learning methods are used to construct a low-dimensional representation that can support many different applications. This representation is designed to preserve a knowledge graph’s local and/or global structure. Additional machine learning methods can be applied to this representation to make predictions within genomic, pharmaceutical, and clinical domains. We frame our discussion first around knowledge graph construction and then around unifying representational learning techniques and unifying applications. Advances in machine learning for biomedicine are creating new opportunities across many domains, and we note potential avenues for future work with knowledge graphs that appear particularly promising.},
  keywords = {knowledge graphs,Lterature review,Machine learning,Natural language processing,Network embeddings,Text mining,已整理,已讀,本體建立,機器學習,知識圖譜,醫學},
  annotation = {128 citations (Crossref) [2024-03-26]},
  note = {挑重點的部分看，掠過機器學習的部分。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\47A7XTCC\\Nicholson and Greene - 2020 - Constructing knowledge graphs and their biomedical.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\2E3XIDMN\\S2001037020302804.html}
}

@inproceedings{davidsonProvenanceScientificWorkflows2008,
  title = {Provenance and Scientific Workflows: Challenges and Opportunities},
  shorttitle = {Provenance and Scientific Workflows},
  booktitle = {Proceedings of the 2008 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Davidson, Susan B. and Freire, Juliana},
  year = {6 月 9, 2008},
  series = {{{SIGMOD}} '08},
  pages = {1345--1350},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1376616.1376772},
  url = {https://doi.org/10.1145/1376616.1376772},
  urldate = {2024-03-15},
  abstract = {Provenance in the context of workflows, both for the data they derive and for their specification, is an essential component to allow for result reproducibility, sharing, and knowledge re-use in the scientific community. Several workshops have been held on the topic, and it has been the focus of many research projects and prototype systems. This tutorial provides an overview of research issues in provenance for scientific workflows, with a focus on recent literature and technology in this area. It is aimed at a general database research audience and at people who work with scientific data and workflows. We will (1) provide a general overview of scientific workflows, (2) describe research on provenance for scientific workflows and show in detail how provenance is supported in existing systems; (3) discuss emerging applications that are enabled by provenance; and (4) outline open problems and new directions for database-related research.},
  isbn = {978-1-60558-102-6},
  langid = {english},
  keywords = {provenance,scientific workflows},
  annotation = {311 citations (Crossref) [2024-03-26]\\
titleTranslation: 來源與科學工作流程：挑戰與機遇\\
abstractTranslation:  工作流程背景下的來源，無論是其衍生的資料或其規範，都是科學界實現結果再現、共享和知識重用的重要組成部分。已經圍繞該主題舉辦了多次研討會，並且它一直是許多研究項目和原型系統的焦點。本教程概述了科學工作流程起源的研究問題，重點關注該領域的最新文獻和技術。它面向一般資料庫研究受眾以及使用科學數據和工作流程的人員。我們將（1）提供科學工作流程的總體概述，（2）描述科學工作流程起源的研究，並詳細展示現有系統如何支持起源； (3) 討論由來源啟用的新興應用程式； (4)概述資料庫相關研究的開放問題和新方向。},
  file = {C:\Users\BlackCat\Zotero\storage\2TC9FPDS\Davidson 與 Freire - 2008 - Provenance and scientific workflows challenges an.pdf}
}

@article{debarshikumarsanyalReviewAuthorName2021,
  title = {A Review of Author Name Disambiguation Techniques for the {{PubMed}} Bibliographic Database},
  author = {{Debarshi Kumar Sanyal} and {Plaban Kumar Bhowmick} and {Partha Pratim Das}},
  date = {2021},
  journaltitle = {Journal of Information Science},
  volume = {47},
  number = {2},
  pages = {227--254},
  publisher = {SAGE Publications Sage UK: London, England},
  doi = {10.1177/0165551519888605},
  langid = {english},
  keywords = {已整理},
  annotation = {38 citations (Crossref) [2024-03-26]\\
titleTranslation: PubMed 書目資料庫作者姓名消歧技術綜述},
  note = {說明關於如何解決姓名重複的問題。也許可以應用在研究中。}
}

@inproceedings{deepakguptaCanTaxonomyHelp2018,
  title = {Can {{Taxonomy Help}}? {{Improving Semantic Question Matching}} Using {{Question Taxonomy}}},
  shorttitle = {Can {{Taxonomy Help}}?},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Computational Linguistics}}},
  author = {{Deepak Gupta} and {Rajkumar Pujari} and {Asif Ekbal} and {Pushpak Bhattacharyya} and {Anutosh Maitra} and {Tom Jain} and {Shubhashis Sengupta}},
  date = {2018-08},
  pages = {499--513},
  publisher = {Association for Computational Linguistics},
  location = {Santa Fe, New Mexico, USA},
  url = {https://aclanthology.org/C18-1042},
  urldate = {2023-09-19},
  abstract = {In this paper, we propose a hybrid technique for semantic question matching. It uses a proposed two-layered taxonomy for English questions by augmenting state-of-the-art deep learning models with question classes obtained from a deep learning based question classifier. Experiments performed on three open-domain datasets demonstrate the effectiveness of our proposed approach. We achieve state-of-the-art results on partial ordering question ranking (POQR) benchmark dataset. Our empirical analysis shows that coupling standard distributional features (provided by the question encoder) with knowledge from taxonomy is more effective than either deep learning or taxonomy-based knowledge alone.},
  eventtitle = {{{COLING}} 2018},
  langid = {english},
  keywords = {問答系統,未整理,知識分類},
  annotation = {titleTranslation: 分類法有幫助嗎？使用問題分類改進語意問題匹配\\
abstractTranslation:  在本文中，我們提出了一種用於語義問題匹配的混合技術。它透過使用從基於深度學習的問題分類器獲得的問題類別來增強最先進的深度學習模型，使用建議的英語問題兩層分類法。在三個開放域資料集上進行的實驗證明了我們提出的方法的有效性。我們在偏序問題排名（POQR）基準資料集上取得了最先進的結果。我們的實證分析表明，將標準分佈特徵（由問題編碼器提供）與分類學知識相結合比單獨深度學習或基於分類學的知識更有效。},
  file = {C:\Users\BlackCat\Zotero\storage\CTD8DQAK\Gupta 等。 - 2018 - Can Taxonomy Help Improving Semantic Question Mat.pdf}
}

@article{dieudonnetchuenteUserModelingProfiling2022,
  title = {User {{Modeling}} and {{Profiling}} in {{Information Systems}}: {{A Bibliometric Study}} and {{Future Research Directions}}},
  shorttitle = {User {{Modeling}} and {{Profiling}} in {{Information Systems}}},
  author = {{Dieudonne Tchuente}},
  date = {2022-07-14},
  journaltitle = {Journal of Global Information Management},
  volume = {30},
  number = {1},
  pages = {1--25},
  issn = {1062-7375, 1533-7995},
  doi = {10.4018/JGIM.307116},
  url = {https://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/JGIM.307116},
  urldate = {2023-10-23},
  abstract = {User modeling or user profiling is fundamental to manage information overload issues in many adaptive and personalized systems (e.g., recommender systems, personalized search engines, adaptive user interfaces). Although there are some literature review papers that provide an overview of existing studies in user modeling and their usage, there is currently a lack of bibliometric studies that can provide a systematic and quantitative overview of this research area. Therefore, this paper aims to complete the existing literature in this research area through a bibliometric study based on 52,027 relevant publications extracted from Scopus, a world-leading publisher-independent global citation database. The analyses enabled us to identify the most relevant publications, sources of publications, authors, institutions, countries, and their collaboration. We also identify and classify the twelve most important associated topics, along with their subtopics and their trends. Some identified weak signals in topic trend analysis also provide good ideas of potential future research directions.},
  langid = {english},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 信息系统中的用户建模和分析：文献计量学研究与未来研究方向\\
abstractTranslation:  在许多自适应和个性化系统（如推荐系统、个性化搜索引擎、自适应用户界面）中，用户建模或用户剖析是管理信息过载问题的基础。虽然有一些文献综述概述了现有的用户建模研究及其使用情况，但目前还缺乏能对这一研究领域进行系统和定量概述的文献计量学研究。因此，本文旨在通过基于 Scopus（一个独立于出版商的世界领先的全球引文数据库）中提取的 52,027 篇相关出版物的文献计量学研究，完善该研究领域的现有文献。通过分析，我们确定了最相关的出版物、出版物来源、作者、机构、国家及其合作关系。我们还确定了十二个最重要的相关主题及其子主题和趋势，并对其进行了分类。在主题趋势分析中发现的一些微弱信号也为未来潜在的研究方向提供了很好的思路。},
  note = {[TLDR] This paper aims to complete the existing literature in this research area through a bibliometric study based on 52,027 relevant publications extracted from Scopus, a world-leading publisher-independent global citation database.},
  file = {C:\Users\BlackCat\Zotero\storage\Y7EPZQ5Q\Tchuente - 2022 - User Modeling and Profiling in Information Systems.pdf}
}

@inproceedings{dinglerReadingbasedScreenshotSummaries2016,
  title = {Reading-Based {{Screenshot Summaries}} for {{Supporting Awareness}} of {{Desktop Activities}}},
  booktitle = {Proceedings of the 7th {{Augmented Human International Conference}} 2016},
  author = {Dingler, Tilman and Agroudy, Passant El and Matheis, Gerd and Schmidt, Albrecht},
  year = {2 月 25, 2016},
  series = {{{AH}} '16},
  pages = {1--5},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2875194.2875224},
  url = {https://dl.acm.org/doi/10.1145/2875194.2875224},
  urldate = {2024-02-23},
  abstract = {Lifelogging augments people's ability to keep track of their daily activities and helps them create rich archives and foster memory. Information workers perform a lot of their key activities throughout the day on their desktop computers. We argue that activity summaries can be informed by eye-tracking data. Therefore we investigate 3 heuristics to create such summaries based on screenshots to help reconstruct people's work day: a fixed time interval, people's focus of attention as indicated by their eye gaze, and a reading detection algorithm. In a field study with 12 participants who logged their desktop activities for 3 consecutive days we evaluated the usefulness of screenshot summaries based on these heuristics. Our results show the utility of eye tracking data, and more specifically of using reading detection to determine key activities throughout the day to inform the creation of activity summaries that are more relevant and require less time to review.},
  isbn = {978-1-4503-3680-2},
  langid = {english},
  keywords = {desktop activities,lifelogging,memory augmentation,productivity,recall,smart summaries,私人},
  annotation = {6 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於閱讀的螢幕截圖摘要，支援桌面活動意識},
  file = {C:\Users\BlackCat\Zotero\storage\MVIUJ8WI\Dingler 等。 - 2016 - Reading-based Screenshot Summaries for Supporting .pdf}
}

@article{DingYouWeiYiZhongMianXiangZhongYiDianZiBingLiDeShiTiChouQuSuanFa2021,
  title = {一种面向中医电子病历的实体抽取算法},
  author = {{丁有伟} and {郭坤} and {胡孔法} and {戴彩艳}},
  date = {2021},
  journaltitle = {软件导刊},
  shortjournal = {Software Guide},
  volume = {20},
  number = {12},
  pages = {99--104},
  doi = {10.11907/rjdk.212057},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5Eg1yamRrMjAyMTEyMDE3GghtbTJ3NDlkNw%3D%3D},
  urldate = {2022-08-14},
  abstract = {为提高中医电子病历的实体抽取精度,针对中医电子病历中语言半白话半文言的特征,基于双向长短时记忆网络、自注意力机制和条件随机场构建实体识别模型,基于双向长短时记忆网络与多层感知器构建实体依存关系挖掘模型,分两步抽取中医电子病历中的实体类别和关系,并采用真实的中医电子病历数据集进行算法性能测试.实验结果表明,与其他常用自然语言处理模型相比,实体识别模型对5类实体的识别召回率最高,均达到85％以上;依存关系挖掘模型在保证较高召回率、F值和准确率的情况下受空依存关系的影响最小.面向中医电子病历的实体抽取算法能很好地应对半白话半文言的语言特色,提高实体抽取精度.},
  langid = {zh\_CN},
  keywords = {Software Guide,中医电子病历,双向长短时记忆网络,实体抽取,条件随机场,软件导刊},
  annotation = {南京中医药大学人工智能与信息技术学院,江苏南京210023南京航空航天大学计算机科学与技术学院,江苏南京211106\\
江苏省高等学校自然科学研究面上项目 国家自然科学基金~国家重点研发计划\\
2022-01-12 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 一種面向中醫電子病歷的實體抽取算法\\
abstractTranslation:  為提高中醫電子病歷的實體識別精度，針對中醫電子病歷中語言半白話半文言的特徵，基於肢體長短時記憶網絡、自註意力機制和條件隨機場構建實體識別模型，基於肢體長短時記憶網絡與多層採集器構建依實體存挖掘模型，分兩步抽取中醫電子病歷中的實體類別和關係，並採用真實的中醫電子病歷數據集進行算法性能測試。實驗結果論證，與其他常用的自然語言處理模型相比之下，實體識別模型對5類實體的識別響應率，均達到85\%以上；最高依存關係挖掘模型在保證響應率、F值和準確率的情況下受空依存關係的影響最小。中醫電子病歷的實體抽取算法能夠很好地應對半白話半文言的語言特色，提高實體抽取精度。},
  file = {C:\Users\BlackCat\Zotero\storage\3VB6QUBL\丁有伟 等。 - 2021 - 一种面向中医电子病历的实体抽取算法.pdf}
}

@article{dirkahlersDocumentCorpusQuality2015,
  title = {Document and {{Corpus Quality Challenges}} for {{Knowledge Management}} in {{Engineering Enterprises}}},
  author = {{Dirk Ahlers} and {John Krogstie}},
  year = {10 月 19, 2015},
  journaltitle = {數據與信息質量雜誌},
  shortjournal = {J. Data and Information Quality},
  volume = {6},
  number = {4},
  pages = {14:1--14:3},
  issn = {1936-1955},
  doi = {10.1145/2818379},
  url = {https://dl.acm.org/doi/10.1145/2818379},
  urldate = {2023-08-24},
  langid = {english},
  keywords = {corpus quality,Document quality,linkability},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 工程企業知識管理的文檔和語料庫質量挑戰},
  file = {C:\Users\BlackCat\Zotero\storage\5DC27WSN\Ahlers 與 Krogstie - 2015 - Document and Corpus Quality Challenges for Knowled.pdf}
}

@inproceedings{dominicseylerKnowledgeQuestionsKnowledge2017,
  title = {Knowledge {{Questions}} from {{Knowledge Graphs}}},
  booktitle = {Proceedings of the {{ACM SIGIR International Conference}} on {{Theory}} of {{Information Retrieval}}},
  author = {{Dominic Seyler} and {Mohamed Yahya} and {Klaus Berberich}},
  year = {10 月 1, 2017},
  series = {{{ICTIR}} '17},
  pages = {11--18},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3121050.3121073},
  url = {https://dl.acm.org/doi/10.1145/3121050.3121073},
  urldate = {2023-09-11},
  abstract = {We address the problem of automatically generating quiz-style knowledge questions from a knowledge graph such as DBpedia. Questions of this kind have ample applications, for instance, to educate users about or to evaluate their knowledge in a specific domain. To solve the problem, we propose a novel end-to-end approach. The approach first selects a named entity from the knowledge graph as an answer. It then generates a structured triple-pattern query, which yields the answer as its sole result. If a multiple-choice question is desired, the approach selects alternative answer options as distractors. Finally, our approach uses a template-based method to verbalize the structured query and yield a natural language question. A key challenge is estimating how difficult the generated question is to human users. To do this, we make use of historical data from the Jeopardy! quiz show and a semantically annotated Web-scale document collection, engineer suitable features, and train a logistic regression classifier to predict question difficulty. Experiments demonstrate the viability of our overall approach.},
  isbn = {978-1-4503-4490-6},
  langid = {english},
  keywords = {knowledge graphs,query verbalization,question difficulty estimation,question generation},
  annotation = {27 citations (Crossref) [2024-03-26]\\
abstractTranslation:  我們解決從 DBpedia 等知識圖譜自動生成測驗式知識問題的問題。此類問題有很多應用，例如，教育用戶或評估他們在特定領域的知識。為了解決這個問題，我們提出了一種新穎的端到端方法。該方法首先從知識圖中選擇一個命名實體作為答案。然後，它生成一個結構化的三重模式查詢，該查詢產生答案作為其唯一結果。如果需要多項選擇題，該方法會選擇替代答案選項作為乾擾項。最後，我們的方法使用基於模板的方法來表達結構化查詢並產生自然語言問題。一個關鍵的挑戰是估計生成的問題對人類用戶來說有多困難。為此，我們利用了《危險邊緣》的歷史數據！測驗節目和語義註釋的網絡規模文檔集合，設計合適的特徵，並訓練邏輯回歸分類器來預測問題難度。實驗證明了我們整體方法的可行性。\\
titleTranslation: 知識圖的知識問題},
  file = {C:\Users\BlackCat\Zotero\storage\5H37UJQF\Seyler 等。 - 2017 - Knowledge Questions from Knowledge Graphs.pdf}
}

@article{dominikatkaczykCERMINEAutomaticExtraction2015,
  title = {{{CERMINE}}: Automatic Extraction of Structured Metadata from Scientific Literature},
  shorttitle = {{{CERMINE}}},
  author = {{Dominika Tkaczyk} and {Paweł Szostek} and {Mateusz Fedoryszak} and {Piotr Jan Dendek} and {Lukasz Bolikowski}},
  year = {12 月 1, 2015},
  journaltitle = {International Journal on Document Analysis and Recognition},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  volume = {18},
  number = {4},
  pages = {317--335},
  issn = {1433-2833},
  doi = {10.1007/s10032-015-0249-8},
  url = {https://doi.org/10.1007/s10032-015-0249-8},
  urldate = {2023-10-31},
  abstract = {CERMINE is a comprehensive open-source system for extracting structured metadata from scientific articles in a born-digital form. The system is based on a modular workflow, whose loosely coupled architecture allows for individual component evaluation and adjustment, enables effortless improvements and replacements of independent parts of the algorithm and facilitates future architecture expanding. The implementations of most steps are based on supervised and unsupervised machine learning techniques, which simplifies the procedure of adapting the system to new document layouts and styles. The evaluation of the extraction workflow carried out with the use of a large dataset showed good performance for most metadata types, with the average F score of 77.5 \%. CERMINE system is available under an open-source licence and can be accessed at http://cermine.ceon.pl. In this paper, we outline the overall workflow architecture and provide details about individual steps implementations. We also thoroughly compare CERMINE to similar solutions, describe evaluation methodology and finally report its results.},
  langid = {english},
  keywords = {Bibliography extraction,Content classification,Metadata extraction,Reference parsing,Scientific literature analysis,已整理,知識抽取,被引用,論文分析},
  annotation = {108 citations (Crossref) [2024-03-26]\\
titleTranslation: CERMINE：從科學文獻中自動擷取結構化元資料\\
abstractTranslation:  CERMINE 是一個全面的開源系統，用於以原生數位形式從科學文章中提取結構化元資料。該系統基於模組化工作流程，其鬆散耦合的架構允許單獨的組件評估和調整，可以輕鬆改進和替換演算法的獨立部分，並有利於未來的架構擴展。大多數步驟的實現都是基於監督和無監督的機器學習技術，這簡化了系統適應新文件佈局和樣式的過程。使用大型資料集進行的提取工作流程的評估顯示，大多數元資料類型都具有良好的效能，平均 F 得分為 77.5\%。 CERMINE 系統可在開源許可證下使用，可透過 http://cermine.ceon.pl 存取。在本文中，我們概述了整體工作流程架構，並提供了有關各個步驟實現的詳細資訊。我們還將 CERMINE 與類似解決方案進行了徹底比較，描述了評估方法並最終報告了其結果。},
  file = {C:\Users\BlackCat\Zotero\storage\3JZZ6429\Tkaczyk et al. - 2015 - CERMINE automatic extraction of structured metada.pdf}
}

@article{dongEnhancingKnowledgeSharing2016,
  title = {Enhancing Knowledge Sharing Intention through the Satisfactory Context of Continual Service of Knowledge Management Systems},
  author = {Dong, Tse-Ping and Hung, Chia-Liang and Cheng, Nai-Chang},
  date = {2016-01-01},
  journaltitle = {Information Technology \& People},
  volume = {29},
  number = {4},
  pages = {807--829},
  publisher = {Emerald Group Publishing Limited},
  issn = {0959-3845},
  doi = {10.1108/ITP-09-2014-0195},
  url = {https://doi.org/10.1108/ITP-09-2014-0195},
  urldate = {2024-03-13},
  abstract = {Purpose The purpose of this paper is to show how continual enhancement of knowledge management systems (KMSs) enhances knowledge sharing intention. Design/methodology/approach This study integrates information system (IS) success with social cognitive theory (SCT) to explain knowledge sharing intention. Based on a survey of 276 KMS users in Taiwan’s information technology industry, the structural equation model has been applied to examine the influence process from a user satisfactory context to personal cognitive beliefs, and thus knowledge sharing intention. Findings The results indicate that the user satisfactory context stimulated by continual KMS enhancement increases knowledge sharing intention through the mediation of personal cognition of self-efficacy and outcome expectancy. Practical implications The results have empirical implications for learning how to motivate developers’ patience and passion for follow-up improvements to meet user expectations empathically, which has been emphasized for service provision. Originality/value The originality of this research is its explanation of system adoption behavior, which combines the core of IS success with SCT, links user satisfaction to intention to use, and concerns behavior within a specific context.},
  langid = {english},
  keywords = {Cognitive theories,IT services,Knowledge management systems,LISREL,Personality,Service quality (SERVQUAL),User satisfaction,Virtual community,使用者研究,已整理,知識管理},
  annotation = {21 citations (Crossref) [2024-03-26]\\
titleTranslation: 透過知識管理系統持續服務的滿意環境增強知識共享意願},
  file = {C:\Users\BlackCat\Zotero\storage\44MJCQIV\Dong 等。 - 2016 - Enhancing knowledge sharing intention through the .pdf}
}

@report{doucetteDrowningResearchData2013,
  type = {Working Paper},
  title = {Drowning in {{Research Data}}: {{Addressing Data Management Literacy}} of {{Graduate Students}}},
  shorttitle = {Drowning in {{Research Data}}},
  author = {Doucette, Lise and Fyfe, Bruce},
  date = {2013},
  url = {https://alair.ala.org/handle/11213/18087},
  urldate = {2024-01-15},
  langid = {english},
  keywords = {RDM,使用者調查,已整理,文獻,研究流程},
  annotation = {Accepted: 2022-06-01T16:27:19Z\\
titleTranslation: 淹沒在研究數據中：解決研究生的資料管理素養},
  note = {屬於問卷研究，研究碩士生/博士生，社會院/理學院學生對於自身研究資料整理的能力與結果。可以做為論文的基本理論。
\par
“Research Data Management (RDM)” (Doucette 與 Fyfe, 2013, p. 165)
\par
根據問卷分析理學院及社會學院學生對RDM的理解與看法},
  file = {C:\Users\BlackCat\Zotero\storage\73UXG7YD\Doucette 與 Fyfe - 2013 - Drowning in Research Data Addressing Data Managem.pdf}
}

@article{dsouzaSentencePhraseTriple2021,
  title = {Sentence, {{Phrase}}, and {{Triple Annotations}} to {{Build}} a {{Knowledge Graph}} of {{Natural Language Processing Contributions}}—{{A Trial Dataset}}},
  author = {D’Souza, Jennifer and Auer, Sören},
  date = {2021-06-01},
  journaltitle = {Journal of Data and Information Science},
  volume = {6},
  number = {3},
  pages = {6--34},
  doi = {10.2478/jdis-2021-0023},
  url = {https://sciendo.com/article/10.2478/jdis-2021-0023},
  urldate = {2024-03-25},
  abstract = {Purpose This work aims to normalize the NlpContributions scheme (henceforward, NlpContributionGraph) to structure, directly from article sentences, the contributions information in Natural Language Processing (NLP) scholarly articles via a two-stage annotation methodology: 1) pilot stage—to define the scheme (described in prior work); and 2) adjudication stage—to normalize the graphing model (the focus of this paper). Design/methodology/approach We re-annotate, a second time, the contributions-pertinent information across 50 prior-annotated NLP scholarly articles in terms of a data pipeline comprising: contribution-centered sentences, phrases, and triple statements. To this end, specifically, care was taken in the adjudication annotation stage to reduce annotation noise while formulating the guidelines for our proposed novel NLP contributions structuring and graphing scheme. Findings The application of NlpContributionGraph on the 50 articles resulted finally in a dataset of 900 contribution-focused sentences, 4,702 contribution-information-centered phrases, and 2,980 surface-structured triples. The intra-annotation agreement between the first and second stages, in terms of F1-score, was 67.92\% for sentences, 41.82\% for phrases, and 22.31\% for triple statements indicating that with increased granularity of the information, the annotation decision variance is greater. Research limitations NlpContributionGraph has limited scope for structuring scholarly contributions compared with STEM (Science, Technology, Engineering, and Medicine) scholarly knowledge at large. Further, the annotation scheme in this work is designed by only an intra-annotator consensus—a single annotator first annotated the data to propose the initial scheme, following which, the same annotator reannotated the data to normalize the annotations in an adjudication stage. However, the expected goal of this work is to achieve a standardized retrospective model of capturing NLP contributions from scholarly articles. This would entail a larger initiative of enlisting multiple annotators to accommodate different worldviews into a “single” set of structures and relationships as the final scheme. Given that the initial scheme is first proposed and the complexity of the annotation task in the realistic timeframe, our intra-annotation procedure is well-suited. Nevertheless, the model proposed in this work is presently limited since it does not incorporate multiple annotator worldviews. This is planned as future work to produce a robust model. Practical implications We demonstrate NlpContributionGraph data integrated into the Open Research Knowledge Graph (ORKG), a next-generation KG-based digital library with intelligent computations enabled over structured scholarly knowledge, as a viable aid to assist researchers in their day-to-day tasks. Originality/value NlpContributionGraph is a novel scheme to annotate research contributions from NLP articles and integrate them in a knowledge graph, which to the best of our knowledge does not exist in the community. Furthermore, our quantitative evaluations over the two-stage annotation tasks offer insights into task difficulty.},
  langid = {english},
  keywords = {NLP,未整理,知識圖譜,資料集},
  annotation = {5 citations (Crossref) [2024-03-26]\\
titleTranslation: 用於建構自然語言處理貢獻知識圖的句子、片語和三重註釋——試驗資料集},
  file = {C:\Users\BlackCat\Zotero\storage\GXSE4ERQ\D’Souza 與 Auer - 2021 - Sentence, Phrase, and Triple Annotations to Build .pdf}
}

@online{dsouzaSTEMECRDatasetGrounding2020,
  title = {The {{STEM-ECR Dataset}}: {{Grounding Scientific Entity References}} in {{STEM Scholarly Content}} to {{Authoritative Encyclopedic}} and {{Lexicographic Sources}}},
  shorttitle = {The {{STEM-ECR Dataset}}},
  author = {D'Souza, Jennifer and Hoppe, Anett and Brack, Arthur and Jaradeh, Mohamad Yaser and Auer, Sören and Ewerth, Ralph},
  date = {2020-07-28},
  eprint = {2003.01006},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.01006},
  url = {http://arxiv.org/abs/2003.01006},
  urldate = {2024-04-25},
  abstract = {We introduce the STEM (Science, Technology, Engineering, and Medicine) Dataset for Scientific Entity Extraction, Classification, and Resolution, version 1.0 (STEM-ECR v1.0). The STEM-ECR v1.0 dataset has been developed to provide a benchmark for the evaluation of scientific entity extraction, classification, and resolution tasks in a domain-independent fashion. It comprises abstracts in 10 STEM disciplines that were found to be the most prolific ones on a major publishing platform. We describe the creation of such a multidisciplinary corpus and highlight the obtained findings in terms of the following features: 1) a generic conceptual formalism for scientific entities in a multidisciplinary scientific context; 2) the feasibility of the domain-independent human annotation of scientific entities under such a generic formalism; 3) a performance benchmark obtainable for automatic extraction of multidisciplinary scientific entities using BERT-based neural models; 4) a delineated 3-step entity resolution procedure for human annotation of the scientific entities via encyclopedic entity linking and lexicographic word sense disambiguation; and 5) human evaluations of Babelfy returned encyclopedic links and lexicographic senses for our entities. Our findings cumulatively indicate that human annotation and automatic learning of multidisciplinary scientific concepts as well as their semantic disambiguation in a wide-ranging setting as STEM is reasonable.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Digital Libraries,Computer Science - Information Retrieval,問答系統,未整理,資料集},
  note = {Comment: Published in LREC 2020. Publication URL https://www.aclweb.org/anthology/2020.lrec-1.268/; Dataset DOI https://doi.org/10.25835/0017546},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\DJMKAZTP\\D'Souza et al. - 2020 - The STEM-ECR Dataset Grounding Scientific Entity .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\6DZXQMSK\\2003.html}
}

@article{duApplicationEnglishEducation2023,
  title = {Application of {{English}} Education Big Data System Based on Multi-Source Information Fusion and Machine Learning},
  author = {Du, Kehan},
  date = {2023-05-22},
  journaltitle = {Soft Computing},
  shortjournal = {Soft Comput},
  issn = {1432-7643, 1433-7479},
  doi = {10.1007/s00500-023-08429-w},
  url = {https://link.springer.com/10.1007/s00500-023-08429-w},
  urldate = {2024-04-11},
  abstract = {For administrators in the field of education, the data generated in the actual education process is difficult to achieve effective management due to its large amount of information, complex structure and multiple types. However, in the actual construction of education informatization, it is impossible to explore the laws and development trends of education information only by manual analysis, so it is difficult to obtain effective concepts and relevant views. College education administrators cannot fully optimize the process of teaching diagnosis and reform through education informatization. Based on this background, this research introduces machine learning technology to achieve technical support for the integrated application of educational data. This paper focuses on the design and construction of an English teaching management data analysis system for college teaching, which combines machine learning technology with multi-source information technology. The system is mainly composed of hardware layer, data layer, service layer and application layer, which can assist teaching and realize learning career planning, faculty authority management, curriculum data modification, teaching resource allocation and scientific research review. The simulation results show that this big data system has high processing efficiency, and is superior to other algorithms in terms of mean value and FVDF extreme value, so it can effectively assist English teaching management. This paper completes the design of English education big data system by introducing machine learning and multi-source information fusion into the field of education.},
  langid = {english},
  keywords = {回收,多源資訊,未整理},
  annotation = {abstractTranslation:  對於教育領域的管理者來說，實際教育過程中產生的數據由於資訊量大、結構複雜、類型多，難以實現有效管理。然而，在教育資訊化的實際建設中，僅靠人工分析無法探討教育資訊化的規律和發展趨勢，因此很難獲得有效的理念和相關觀點。高校教育管理者無法透過教育資訊化充分優化教學診斷和改革流程。基於此背景，本研究引進機器學習技術，實現教育資料整合應用的技術支援。本文重點研究了機器學習技術與多源資訊科技結合的大學英語教學管理資料分析系統的設計與建構。系統主要由硬體層、資料層、服務層和應用層組成，可輔助教學，實現學習生涯規劃、師資權限管理、課程資料修改、教學資源配置和科學研究評審等。模擬結果表明，此大數據系統處理效率較高，在平均值和FVDF極值方面均優於其他演算法，可有效輔助英語教學管理。本文將機器學習與多源資訊融合引入教育領域，完成英語教育大數據系統的設計。\\
titleTranslation: 基於多源資訊整合與機器學習的英語教育大數據系統應用},
  note = {本研究只是把多源資訊當成聳動的標題，在研究中並沒有特別解釋。
\par
[TLDR] The design and construction of an English teaching management data analysis system for college teaching is completed, which combines machine learning technology with multi-source information technology and is superior to other algorithms in terms of mean value and FVDF extreme value.},
  file = {C:\Users\BlackCat\Zotero\storage\4WCMC9PR\Du - 2023 - Application of English education big data system b.pdf}
}

@inproceedings{ducdungnguyenKnowledgeVisualizationHepatitis2006,
  title = {Knowledge Visualization in Hepatitis Study},
  booktitle = {Proceedings of the 2006 {{Asia-Pacific Symposium}} on {{Information Visualisation}} - {{Volume}} 60},
  author = {{DucDung Nguyen} and {TuBao Ho} and {Saori Kawasaki}},
  year = {1 月 1, 2006},
  series = {{{APVis}} '06},
  pages = {59--62},
  publisher = {Australian Computer Society, Inc.},
  location = {AUS},
  abstract = {Evidence-based medicine (EBM) is a shift in which medicine from being based on individual experience of doctors to evidence with clear background. In this shift, data mining can play a significant role as its ability of uncovering medical evidence from large volumes of medical data. Recognizing the crucial role of visualization in discovering such evidences, this work presents some developed tools integrated in our data mining system D2MS for appropriately visualizing knowledge, and their usage in hepatitis study. We emphasize on our two rule visualizers, one for individual rule and the other for rule in its relations with the others.},
  isbn = {978-1-920682-41-5},
  langid = {english},
  keywords = {data and knowledge visualization,hepatitis study,model selection,資料挖掘},
  annotation = {titleTranslation: 肝炎研究中的知識可視化\\
abstractTranslation:  循證醫學（EBM）是醫學從基於醫生個人經驗向有明確背景的證據的轉變。在這種轉變中，數據挖掘可以發揮重要作用，因為它能夠從大量醫療數據中發現醫學證據。認識到可視化在發現此類證據中的關鍵作用，這項工作提出了一些集成在我們的數據挖掘系統 D2MS 中的開發工具，用於適當可視化知識及其在肝炎研究中的使用。我們強調兩個規則可視化工具，一個用於個人規則，另一個用於與其他規則的關係的規則。},
  file = {C:\Users\BlackCat\Zotero\storage\JYX4RS8B\Nguyen 等。 - 2006 - Knowledge visualization in hepatitis study.pdf}
}

@article{e.a.nismimolReviewKnowledgeExtraction2023,
  title = {Review on Knowledge Extraction from Text and Scope in Agriculture Domain},
  author = {{E. A. Nismi Mol} and {M. B. Santosh Kumar}},
  date = {2023-05-01},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {56},
  number = {5},
  pages = {4403--4445},
  issn = {1573-7462},
  doi = {10.1007/s10462-022-10239-9},
  url = {https://doi.org/10.1007/s10462-022-10239-9},
  urldate = {2023-10-25},
  abstract = {Knowledge extraction is meant by acquiring relevant information from the unstructured document in natural language and representing them in a structured form. Enormous information in various domains, including agriculture, is available in the natural language from several resources. The knowledge needs to be represented in a structured format to understand and process by a machine for automating various applications. This paper reviews different computational approaches like rule-based and learning-based methods and explores the various techniques, features, tools, datasets, and evaluation metrics adopted for knowledge extraction from the most relevant literature.},
  langid = {english},
  keywords = {Information extraction,Knowledge extraction,Natural language processing,Structured knowledge},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 農業領域文本與範圍知識擷取綜述}
}

@article{edsonm.lucasKnowledgeOrientedModelsBased2020,
  title = {Knowledge-{{Oriented Models Based}} on {{Developer-Artifact}} and {{Developer-Developer Interactions}}},
  author = {{Edson M. Lucas} and {Toacy C. Oliveira} and {Daniel Schneider} and {Paulo S. C. Alencar}},
  date = {2020},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {8},
  pages = {218702--218719},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3042429},
  url = {https://ieeexplore.ieee.org/document/9279201/},
  urldate = {2023-10-23},
  abstract = {INTRODUCTION: Software development is organized around developers working collaboratively promoting two types of interactions for knowledge sharing. Developer-Artifact interactions indicate developers define or access pieces of information within artifacts. Developer-Developer interactions indicate the exchange of information among developers using a collaboration platform to clarify an issue, promote an idea, or expose any thoughtful comment. PROBLEM: The number of such interactions grows over time and makes it difficult to capture and assess the evolution of the developers’ knowledge about specific software project artifacts and tasks. Further, this knowledge decreases over time due to the natural limitations of human cognition that restrict our capabilities to cope with information overload. Besides, who has more knowledge about specific project elements are important to promote collaboration. AIMS: The {$<$}inline-formula{$>$} {$<$}tex-math notation="LaTeX"{$>\$$}K\_\{a\}\$ {$<$}/tex-math{$><$}/inline-formula{$>$}, {$<$}inline-formula{$>$} {$<$}tex-math notation="LaTeX"{$>\$$}K\_\{s\}\$ {$<$}/tex-math{$><$}/inline-formula{$>$}, {$<$}inline-formula{$>$} {$<$}tex-math notation="LaTeX"{$>\$$}K\_\{c\}\$ {$<$}/tex-math{$><$}/inline-formula{$>$}, and {$<$}inline-formula{$>$} {$<$}tex-math notation="LaTeX"{$>\$$}K\_\{p\}\$ {$<$}/tex-math{$><$}/inline-formula{$>$} models capture the evolution of the developers’ knowledge about software project elements such as artifacts, tasks, similar tasks, and the whole software project. These models represent not only the knowledge developers have about these elements but also capture how this knowledge decreases over time based on forgetting and relearning functions. EVALUATION: An experimental study analyzed some developers’ interactions on artifacts for the purpose of predicting the evolution of developers’ knowledge in six software projects. The results show that the developers’ rankings by performed tasks and by our models have 72\% or more of similarity. CONCLUSION: Our models can capture and assess the evolution of the developers’ knowledge and help to identify which developers have more knowledge about specific elements of software projects.},
  langid = {english},
  annotation = {4 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於開發者-工件和開發者-開發者互動的知識導向的模型\\
abstractTranslation:  簡介：軟體開發是圍繞開發人員進行組織的，開發人員協作促進兩種類型的知識共享互動。開發人員與工件互動顯示開發人員定義或存取工件內的資訊片段。開發人員與開發人員之間的互動是指開發人員之間使用協作平台交換訊息，以澄清問題、推廣想法或公開任何深思熟慮的評論。問題：此類互動的數量隨著時間的推移而增加，使得捕捉和評估開發人員對特定軟體專案工件和任務的知識的演變變得困難。此外，由於人類認知的自然限制限制了我們應對資訊超載的能力，這些知識會隨著時間的推移而減少。此外，誰對特定專案要素有更多了解對於促進協作也很重要。目標： {$<$}inline-formula{$>$} {$<$}tex-math notation="LaTeX"{$>\$$}K\_\{a\}\$ {$<$}/tex-math{$><$}/inline-formula{$>$}、{$<$}inline-formula{$>$} {$<$}tex-math notation="LaTeX " {$>\$$}K\_\{s\}\$ {$<$}/tex-math{$><$}/inline-formula{$>$}, {$<$}inline-formula{$>$} {$<$}tex-math notation="LaTeX"{$>\$$}K\_\{c\}\$ {$<$}/tex-math{$><$}/ inline -formula{$>$} 和{$<$}inline-formula{$>$} {$<$}tex-math notation="LaTeX"{$>\$$}K\_\{p\}\$ {$<$}/tex-math{$><$}/inline-formula{$>$} 模型捕捉開發人員軟體知識的演進專案元素，例如工件、任務、類似任務和整個軟體專案。這些模型不僅代表了開發人員擁有的這些元素的知識，而且還捕捉了這些知識如何根據遺忘和重新學習功能而隨著時間的推移而減少。評估：一項實驗研究分析了一些開發人員在工件上的交互，目的是預測開發人員在六個軟體專案中知識的演變。結果顯示，開發人員按執行任務進行的排名與我們模型的排名有 72\% 或更多的相似度。結論：我們的模型可以捕捉和評估開發人員知識的演變，並幫助識別哪些開發人員對軟體專案的特定元素有更多的了解。},
  note = {[TLDR] A model that captures the evolution of the developers’ knowledge about software project elements such as artifacts, tasks, similar tasks, and the whole software project and captures how this knowledge decreases over time based on forgetting and relearning functions.}
}

@inproceedings{elagroudyDonKnowWhat2023,
  title = {I ({{Don}}’t) {{Know What You Did Last Summer}}: {{A Framework}} for {{Ubiquitous Research Preservation}}},
  shorttitle = {I ({{Don}}’t) {{Know What You Did Last Summer}}},
  booktitle = {Extended {{Abstracts}} of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Elagroudy, Passant and Knierim, Pascal and Schmidt, Albrecht and Woźniak, Paweł W. and Feger, Sebastian S.},
  year = {4 月 19, 2023},
  series = {{{CHI EA}} '23},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3544549.3585754},
  url = {https://dl.acm.org/doi/10.1145/3544549.3585754},
  urldate = {2024-01-10},
  abstract = {Research preservation is a pillar for knowledge transfer, science reproducibility and saving time by reusing existing resources. However, human compliance with efficient capturing strategies is a key barrier to creating complete scientific repositories. To circumvent this issue, we introduce the term: Ubiquitous Research Preservation (URP), describing automated knowledge capturing and retrieval in computational science. We also propose a framework composed of three models for designing URP systems (URPS) to 1) understand users’ interaction and data governance, 2) propose technical pipelines for data management, and 3) understand users’ sharing practices. Our work is a theoretical reflection on our past experiences in designing URPS. We plan future evaluation by using the framework to analyze existing URPS. We expect a positive impact from using URPS on researchers’ sense-making and ability to share findings and resources. Our framework is a checklist for design decisions needed to build successful URPS.},
  isbn = {978-1-4503-9422-2},
  langid = {english},
  keywords = {Computational Science,Reproducibility,Sensemaking,Training,Ubiquitous Research Preservation,實驗記錄,已整理,已讀,文獻},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 我（不）知道你去年夏天做了什麼：無處不在的研究保存框架\\
abstractTranslation:  研究保存是知識轉移、科學可重複性和透過重複使用現有資源節省時間的支柱。然而，人類對有效捕獲策略的遵守是創建完整科學知識庫的關鍵障礙。為了解決這個問題，我們引入了術語：無所不在的研究保存（URP），描述計算科學中的自動化知識擷取和檢索。我們還提出了一個由三個模型組成的框架，用於設計 URP 系統（URPS），以 1）了解使用者的互動和資料治理，2）提出資料管理的技術管道，3）以了解使用者的共享實踐。我們的工作是對我們過去設計 URPS 經驗的理論反思。我們透過使用該框架來分析現有的 URPS 來規劃未來的評估。我們期望使用 URPS 對研究人員的意義建構以及分享發現和資源的能力產生正面影響。我們的框架是建立成功的 URPS 所需的設計決策清單。},
  note = {提出術語: Ubiquitous Research Preservation，主要著重在研究人員如何保存資料並幫助傳承及重現。
\par
來源評價:\\
相當少的引用，沒聽過的學校，相當不集中的研究領域。},
  file = {D:\Paper\I (Don’t) Know What You Did Last Summer A Framework for Ubiquitous Research Preservation.pdf}
}

@inproceedings{elagroudyModelSelectingMedia2022,
  title = {A {{Model}} for {{Selecting Media Type}} of {{Memory Cues}} in {{Ubiquitous Prostheses}}},
  booktitle = {Proceedings of the {{Augmented Humans International Conference}} 2022},
  author = {Elagroudy, Passant and Feger, Sebastian and Schmidt, Albrecht},
  year = {4 月 18, 2022},
  series = {{{AHs}} '22},
  pages = {313--316},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3519391.3524169},
  url = {https://dl.acm.org/doi/10.1145/3519391.3524169},
  urldate = {2024-01-10},
  abstract = {記憶塑造了我們與世界的互動。因此，有一個新的趨勢研究方向是建構記憶假體，幫助我們增強認知能力並成為更好的自己。這種義肢透過增強或退化來改變我們的記憶。然而，設計此類系統的系統過程仍存在研究空白。在這項工作中，我們反思了設計幾種記憶假體的經驗，並提出了一個二維設計空間來背景化現有文獻。我們也提出了八個標準來選擇媒體類型來捕捉、儲存和呈現記憶線索。我們的工作是在確定建立有針對性的記憶假體的整體框架要素方面向前邁出的一步。},
  isbn = {978-1-4503-9632-5},
  langid = {english},
  keywords = {design space,media design,memory cues,memory prostheses},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 在無所不在的義肢中選擇記憶線索媒體類型的模型},
  file = {C:\Users\BlackCat\Zotero\storage\EKBZ4YAZ\Elagroudy et al. - 2022 - A Model for Selecting Media Type of Memory Cues in.pdf}
}

@article{f.richardyuInformationNetworkingIntelligence2021,
  title = {From {{Information Networking}} to {{Intelligence Networking}}: {{Motivations}}, {{Scenarios}}, and {{Challenges}}},
  shorttitle = {From {{Information Networking}} to {{Intelligence Networking}}},
  author = {{F. Richard Yu}},
  date = {2021-11},
  journaltitle = {IEEE Network},
  shortjournal = {IEEE Network},
  volume = {35},
  number = {6},
  pages = {209--216},
  issn = {0890-8044, 1558-156X},
  doi = {10.1109/MNET.011.2000788},
  url = {https://ieeexplore.ieee.org/document/9448012/},
  urldate = {2023-10-23},
  abstract = {By enabling information networking among people and machines, the Internet has become one of the major foundations for our socio-economic systems. After several decades of research and development of the Internet, it is relatively easy for humans/machines to obtain information. However, there are new challenges in the post-Internet era, including information overload, fake information and the design of trustworthy, cost-effective autonomous systems. In order to address these challenges, we need to think about networking in a larger timescale. Actually, in order to facilitate humans' cooperation, we have invented technologies enabling networking for matter (grid of transportation), for energy (grid of energy), and for information (the Internet). In this article, we argue that the next networking paradigm could be intelligence networking, where intelligence can be easily obtained, like matter, energy, and information. Specifically, we present the motivations, scenarios and challenges of intelligence networking. In addition, we present a novel collective reinforcement learning scheme enabled by intelligence networking. Some simulation results are presented to show the effectiveness of the proposed intelligence networking paradigm.},
  langid = {english},
  annotation = {26 citations (Crossref) [2024-03-26]\\
titleTranslation: 從資訊網路到情報網：動機、場景與挑戰},
  note = {[TLDR] This article argues that the next networking paradigm could be intelligence networking, where intelligence can be easily obtained, like matter, energy, and information, and presents a novel collective reinforcement learning scheme enabled by intelligence networking.}
}

@inproceedings{fabianm.suchanekYagoCoreSemantic2007,
  title = {Yago: A Core of Semantic Knowledge},
  shorttitle = {Yago},
  booktitle = {Proceedings of the 16th International Conference on {{World Wide Web}}},
  author = {{Fabian M. Suchanek} and {Gjergji Kasneci} and {Gerhard Weikum}},
  year = {5 月 8, 2007},
  series = {{{WWW}} '07},
  pages = {697--706},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1242572.1242667},
  url = {https://dl.acm.org/doi/10.1145/1242572.1242667},
  urldate = {2023-09-19},
  abstract = {We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95\%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.},
  isbn = {978-1-59593-654-7},
  langid = {english},
  keywords = {wikipedia,WordNet,基礎理論,已整理,知識本體,重要},
  annotation = {1971 citations (Crossref) [2024-03-26]\\
abstractTranslation:  我們推出 YAGO，一種輕量級、可擴展、覆蓋率高、品質高的本體。 YAGO 建立在實體和關係之上，目前包含超過 100 萬個實體和 500 萬個事實。這包括 Is-A 層次結構以及實體之間的非分類關係（例如 HASONEPRIZE）。使用本文中描述的基於規則和啟發式方法的精心設計組合，事實已從維基百科自動提取並與 WordNet 統一。由此產生的知識庫是超越 WordNet 的重要一步：在品質上，透過添加關於個人、組織、產品等的知識及其語義關係，在數量上，透過將事實數量增加一個數量級以上。我們對事實正確性的實證評估顯示準確度約為 95\%。 YAGO 是基於邏輯乾淨的模型，該模型是可判定的、可擴展的並且與 RDFS 相容。最後，我們展示瞭如何透過最先進的資訊擷取技術進一步擴展 YAGO。\\
titleTranslation: Yago：語意知識的核心},
  file = {C:\Users\BlackCat\Zotero\storage\ZM3KY74W\Suchanek 等。 - 2007 - Yago a core of semantic knowledge.pdf}
}

@article{fakharmaneshKnowledgeManagementFourth2021,
  title = {Knowledge {{Management}} in the {{Fourth Industrial Revolution}}: {{Mapping}} the {{Literature}} and {{Scoping Future Avenues}}},
  shorttitle = {Knowledge {{Management}} in the {{Fourth Industrial Revolution}}},
  author = {Fakhar Manesh, Mohammad and Pellegrini, Massimiliano Matteo and Marzi, Giacomo and Dabic, Marina},
  date = {2021-02},
  journaltitle = {IEEE Transactions on Engineering Management},
  volume = {68},
  number = {1},
  pages = {289--300},
  issn = {1558-0040},
  doi = {10.1109/TEM.2019.2963489},
  url = {https://ieeexplore.ieee.org/document/8964410/citations?tabFilter=papers#citations},
  urldate = {2024-01-18},
  abstract = {Due to increased competitive pressure, modern organizations tend to rely on knowledge and its exploitation to sustain a long-term advantage. This calls for a precise understanding of knowledge management (KM) processes and, specifically, how knowledge is created, shared/transferred, acquired, stored/retrieved, and applied throughout an organizational system. However, since the beginning of the new millennium, such KM processes have been deeply affected and molded by the advent of the fourth industrial revolution, also called Industry 4.0, which involves the interconnectedness of machines and their ability to learn and share data autonomously. For this reason, the present article investigates the intellectual structure and trends of KM in Industry 4.0. Bibliometric analysis and a systematic literature review are conducted on a total of 90 relevant articles. The results reveal six clusters of keywords, subsequently explored via a systematic literature review to identify potential stream of this emergent field and future research avenues capable of producing meaningful advances in managerial knowledge of Industry 4.0 and its consequences.},
  eventtitle = {{{IEEE Transactions}} on {{Engineering Management}}},
  langid = {english},
  keywords = {Review,大量引用,待整理,重要},
  annotation = {150 citations (Crossref) [2024-03-26]\\
abstractTranslation:  由於競爭壓力的增加，現代組織傾向於依靠知識及其利用來維持長期優勢。這需要準確地理解知識管理 (KM) 流程，特別是知識如何在整個組織系統中創建、共享/轉移、獲取、儲存/檢索和應用。然而，進入新千年以來，第四次工業革命（也稱為工業4.0）的出現深刻影響和塑造了這種知識管理流程，其中涉及機器的互聯性及其自主學習和共享數據的能力。為此，本文探討了工業4.0中知識管理的知識結構與趨勢。總共 90 篇相關文章進行了文獻計量分析和系統性文獻綜述。結果揭示了六組關鍵字，隨後透過系統性文獻綜述進行探索，以確定這一新興領域的潛在趨勢以及能夠在工業 4.0 管理知識及其後果方面產生有意義進展的未來研究途徑。\\
titleTranslation: 第四次工業革命中的知識管理：繪製文獻圖並確定未來發展方向},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\CYU4UMAI\\Fakhar Manesh 等。 - 2021 - Knowledge Management in the Fourth Industrial Revo.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\86S5KESA\\citations.html}
}

@article{fanInterdisciplinaryProjectBasedLearning2023,
  title = {Interdisciplinary {{Project-Based Learning}}: {{Experiences}} and {{Reflections From Teaching Electronic Engineering}} in {{China}}},
  shorttitle = {Interdisciplinary {{Project-Based Learning}}},
  author = {Fan, Hua and Xie, Huajiang and Feng, Quanyuan and Bonizzoni, Edoardo and Heidari, Hadi and McEwan, Michael P. and Ghannam, Rami},
  date = {2023-02},
  journaltitle = {IEEE Transactions on Education},
  volume = {66},
  number = {1},
  pages = {73--82},
  issn = {1557-9638},
  doi = {10.1109/TE.2022.3186184},
  url = {https://ieeexplore.ieee.org/abstract/document/9833275?casa_token=fObVBZu-0GcAAAAA:y2hysa1ckwKDiyhodFG2ApyX3PCctRgGb2Tif0vsrpE9kdWaQ_BY2JsyFRrzURVMbJlzGN_Iiqg},
  urldate = {2024-04-06},
  abstract = {Continuous developments in the electronics industry have led to constantly changing career roles and graduate skills requirements. Students in the University of Electronic Science and Technology of China (UESTC) have complained that numerous courses in electronic engineering are heavily focused on theoretical knowledge that is disconnected from the needs of the industry. Thus, in an effort toward delivering student-centered educational programmes that meet the needs of the industry, this article introduces an innovative course that was developed using the project-based learning (PBL) method and situated in the electronic engineering undergraduate programme at UESTC. Since real-world engineering projects require teams to collaborate on ill-defined problems, we focused this innovative course on developing professional and technical skills, drawing from a range of more typical electronic engineering courses. We provide full details of two projects that were created for this PBL approach and evaluate them as practice examples that demonstrate the impact of this practical pedagogic innovation in UESTC. According to our evaluation, completed by all 40 of our enrolled students, our innovative course based on interdisciplinary PBL exercises demonstrated a significant improvement in student satisfaction and 65\% of students preferred the interdisciplinary PBL course in comparison to traditional lecture-based courses.},
  eventtitle = {{{IEEE Transactions}} on {{Education}}},
  langid = {english},
  keywords = {Consumer electronics,Course-related projects,curriculum design,Education,Fans,Industries,innovative training programme,project-based learning (PBL),Teamwork,Technological innovation,Training,教育,未整理},
  annotation = {titleTranslation: 跨學科專案式學習：我國電子工程教學的經驗與思考\\
abstractTranslation:  電子產業的不斷發展導致職業角色和畢業生技能要求不斷變化。電子科技大學（UESTC）的學生抱怨說，許多電子工程課程過於注重理論知識，與產業需求脫節。因此，為了提供以學生為中心、滿足產業需求的教育項目，本文介紹了電子科技大學電子工程學士課程採用基於專案的學習（PBL）方法開發的創新課程。由於現實世界的工程項目需要團隊合作解決不明確的問題，因此我們借鑒了一系列更典型的電子工程課程，將這項創新課程的重點放在培養專業和技術技能上。我們提供了為這種 PBL 方法創建的兩個項目的完整細節，並將它們作為實踐示例進行評估，以展示這種實用教學創新對電子科技大學的影響。根據我們對所有40 名註冊學生完成的評估，我們基於跨學科PBL 練習的創新課程顯示出學生滿意度的顯著提高，與傳統的基於講座的課程相比，65\% 的學生更喜歡跨學科PBL課程。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\EHG8VVBJ\\Fan 等。 - 2023 - Interdisciplinary Project-Based Learning Experien.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\GLFX4JGP\\9833275.html}
}

@article{farahamirahbintisuhaimiInfluenceInformationOverload2017,
  title = {The {{Influence}} of {{Information Overload}} on {{Students}}’ {{Academic Performance}}},
  author = {{Farah Amirah Binti Suhaimi} and {Norhayati Binti Hussin}},
  date = {2017-10-01},
  journaltitle = {International Journal of Academic Research in Business and Social Sciences},
  shortjournal = {IJARBSS},
  volume = {7},
  number = {8},
  pages = {Pages 760-766},
  issn = {2222-6990},
  doi = {10.6007/IJARBSS/v7-i8/3292},
  url = {http://hrmars.com/index.php/journals/papers/IJARBSS/v7-i8/3292},
  urldate = {2023-10-23},
  abstract = {There is no generally agreed with the definition of information overload, yet many of us had experiencing it. Information overload, usually characterized by an overabundance of information, it is a main cause of concern for general information users, researchers and information managers. In this article it is shown how the relationships of the influence of information overload on student’s academic performance. For students, academic performance is important in displays their successful of their academic. It also outlines people, task, technology and information as the influence of information overload. Aim of this paper is to propose a framework regarding the influence of information overload on the student’s academic performance.},
  langid = {english},
  keywords = {資訊超載},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 信息超载对学生学习成绩的影响\\
abstractTranslation:  对于信息超载的定义还没有达成普遍共识，但我们中的许多人都经历过信息超载。信息超载通常以信息过量为特征，是普通信息用户、研究人员和信息管理人员关注的主要问题。本文阐述了信息超载对学生学习成绩的影响关系。对于学生来说，学业成绩是他们学业成功的重要标志。本文还概述了人、任务、技术和信息对信息超载的影响。本文旨在就信息超载对学生学业成绩的影响提出一个框架。},
  note = {[TLDR] The aim of this paper is to propose a framework regarding the influence of information overload on the student’s academic performance and outlines people, task, technology and information as the influence.},
  file = {C:\Users\BlackCat\Zotero\storage\2SFCNELM\Suhaimi and Hussin - 2017 - The Influence of Information Overload on Students’.pdf}
}

@online{fatehkiaTRAGLessonsLLM2024,
  title = {T-{{RAG}}: {{Lessons}} from the {{LLM Trenches}}},
  shorttitle = {T-{{RAG}}},
  author = {Fatehkia, Masoomali and Lucas, Ji Kim and Chawla, Sanjay},
  date = {2024-02-12},
  eprint = {2402.07483},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.07483},
  url = {http://arxiv.org/abs/2402.07483},
  urldate = {2024-03-13},
  abstract = {Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure to represent entity hierarchies within the organization. This is used to generate a textual description to augment the context when responding to user queries pertaining to entities within the organization's hierarchy. Our evaluations show that this combination performs better than a simple RAG or finetuning implementation. Finally, we share some lessons learned based on our experiences building an LLM application for real-world use.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,LLM,RAG,問答系統,已整理,重要},
  annotation = {titleTranslation: T-RAG：法學碩士的經驗教訓\\
abstractTranslation:  大型語言模型 (LLM) 已顯示出卓越的語言功能，推動了將其整合到廣泛領域的應用程式中的嘗試。一個重要的應用領域是針對私人企業文件的問答，其中主要考慮因素是資料安全，這需要可以在本地部署的應用程式、有限的運算資源以及需要正確回應查詢的強大應用程式。檢索增強生成（RAG）已成為建立基於 LLM 的應用程式的最著名的框架。雖然建立 RAG 相對簡單，但要使其成為強大且可靠的應用程序，需要廣泛的客製化和對應用程式領域相對深入的了解。我們分享建構和部署法學碩士應用程式以透過私人組織文件進行問答的經驗。我們的應用程式將 RAG 的使用與經過微調的開源 LLM 結合起來。此外，我們的系統稱為 Tree-RAG (T-RAG)，使用樹狀結構來表示組織內的實體層次結構。這用於產生文字描述，以在回應與組織層次結構內的實體相關的使用者查詢時增強上下文。我們的評估表明，這種組合比簡單的 RAG 或微調實現表現更好。最後，我們根據建立實際使用的 LLM 應用程式的經驗分享一些經驗教訓。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\NFE6VKLX\\Fatehkia et al. - 2024 - T-RAG Lessons from the LLM Trenches.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\HC5R24S5\\2402.html}
}

@article{fegerUbiquitousResearchPreservation2020,
  title = {Ubiquitous {{Research Preservation}}: {{Transforming Knowledge Preservation}} in {{Computational Science}}},
  shorttitle = {Ubiquitous {{Research Preservation}}},
  author = {Feger, Sebastian S. and Munich, L. M. U. and Dallmeier-Tiessen, Sünje and Knierim, Pascal and Agroudy, Passant El and El, Passant and Woźniak, Pawe\textbackslash l W. and Schmidt, Albrecht},
  date = {2020},
  journaltitle = {MetaArXiv, March},
  url = {https://osf.io/qmkc9/download},
  urldate = {2024-02-23},
  langid = {english},
  keywords = {⛔ No DOI found,待讀},
  annotation = {titleTranslation: 無所不在的研究保存：改變計算科學中的知識保存},
  file = {C:\Users\BlackCat\Zotero\storage\WXKSPCZK\Feger 等。 - 2020 - Ubiquitous Research Preservation Transforming Kno.pdf}
}

@inproceedings{fengRobustNLtoCypherTranslation2023,
  title = {Robust {{NL-to-Cypher Translation}} for~{{KBQA}}: {{Harnessing Large Language Model}} with~{{Chain}} of~{{Prompts}}},
  shorttitle = {Robust {{NL-to-Cypher Translation}} for~{{KBQA}}},
  booktitle = {Knowledge {{Graph}} and {{Semantic Computing}}: {{Knowledge Graph Empowers Artificial General Intelligence}}},
  author = {Feng, Guandong and Zhu, Guoliang and Shi, Shengze and Sun, Yue and Fan, Zhongyi and Gao, Sulin and Hu, Jun},
  editor = {Wang, Haofen and Han, Xianpei and Liu, Ming and Cheng, Gong and Liu, Yongbin and Zhang, Ningyu},
  date = {2023},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {317--326},
  publisher = {Springer Nature},
  location = {Singapore},
  doi = {10.1007/978-981-99-7224-1_25},
  abstract = {Knowledge Base Question Answering (KBQA) is a significant task in natural language processing, aiming to retrieve answers from structured knowledge bases in response to natural language questions. NL2Cypher is crucial for accurately querying answers from knowledge bases, but there is limited research in this area or the results are unsatisfactory. Our work explores the convergence of advanced natural language processing techniques with knowledge base question answering (KBQA), focusing on the automated generation of Cypher queries from natural language queries. By leveraging the capabilities of large language model (LLM), our approach bridges the gap between textual questions and structured knowledge representations. The proposed methodology showcases promising results in accurately formulating Cypher queries. We achieved substantial performance in the CCKS2023 Foreign Military Unmanned Systems Knowledge Graph Reasoning Question-Answering Evaluation Task. Our method achieved an F1 score of 0.94269 on the final testing dataset.},
  isbn = {978-981-9972-24-1},
  langid = {english},
  keywords = {ChatGPT,Cypher,KBQA,LLM,問答系統,已整理,待讀,機器學習,知識圖譜,重要},
  annotation = {0 citations (Crossref) [2024-04-27]\\
titleTranslation: 用於 KBQA 的魯棒 NL 到密碼翻譯：利用帶有提示鏈的大型語言模型\\
abstractTranslation:  知識庫問答（KBQA）是自然語言處理中的重要任務，旨在從結構化知識庫中檢索答案以回應自然語言問題。 NL2Cypher 對於從知識庫中準確查詢答案至關重要，但該領域的研究有限或結果不理想。我們的工作探索先進的自然語言處理技術與知識庫問答 (KBQA) 的融合，重點是從自然語言查詢自動產生 Cypher 查詢。透過利用大語言模型（LLM）的功能，我們的方法彌合了文本問題和結構化知識表示之間的差距。所提出的方法展示了準確制定 Cypher 查詢的有希望的結果。我們在CCKS2023國外軍事無人系統知識圖推理問答評測任務中取得了實質的成績。我們的方法在最終測試資料集上取得了 0.94269 的 F1 分數。},
  note = {結合NER模組生成的實體數據，結合ChatGPT API及prompt，大幅提高其生成的Cypher品質。
\par
但沒有甚麼實際的作法也沒有公開程式碼},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\Y6H2L34V\\Feng 等。 - 2023 - Robust NL-to-Cypher Translation for KBQA Harnessi.pdf;D\:\\Paper\\Robust NL-to-Cypher Translation for KBQA： Harnessing Large Language Model with Chain of Prompts.pdf}
}

@article{ferdinandjilandawieSystematicLiteratureReview2022,
  title = {Systematic {{Literature Review}}: {{Information}} Overload of Online Distance Learners},
  shorttitle = {Systematic {{Literature Review}}},
  author = {{Ferdinand Jilan Dawie} and {Mohamad Noorman Masrek} and {Mohammad Fazli Baharuddin}},
  date = {2022-11-30},
  journaltitle = {Environment-Behaviour Proceedings Journal},
  shortjournal = {E-BPJ},
  volume = {7},
  pages = {153--159},
  issn = {2398-4287},
  doi = {10.21834/ebpj.v7iSI10.4107},
  url = {https://ebpj.e-iph.co.uk/index.php/EBProceedings/article/view/4107},
  urldate = {2023-10-23},
  abstract = {This paper aims to summarize the developments of previous studies done in Information Overload fields in the past five years and gives a prospect to future research in this field using the systematic literature review method. The results show very limitedly and low publication activity has been done in the area of information overload with Online Distance Learners. It is anticipated that this paper will trigger further studies that could focus on the impact of information overload on education fields. Keywords: Information Overload; Distance Learners; Online Learning; Systematic Literature Review. eISSN: 2398-4287 © 2022. The Authors. Published for AMER ABRA cE-Bs by E-International Publishing House, Ltd., UK. This is an open-access article under the CC ~BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Peer-review under the responsibility of AMER (Association of Malaysian Environment-Behavior Researchers), ABRA (Association of Behavioral Researchers on Asians), and cE-Bs (Centre for Environment-Behavior Studies), Faculty of Architecture, Planning \& Surveying, Universiti Teknologi MARA, Malaysia.},
  issue = {SI10},
  langid = {english},
  keywords = {Review,已整理,已讀,資訊超載},
  annotation = {0 citations (Crossref) [2024-03-26]\\
abstractTranslation:  本文旨在透過系統性文獻綜述的方法，總結近五年來資訊過載領域的研究進展，並對該領域的未來研究進行展望。結果顯示，線上遠距學習者在資訊過載領域的出版活動非常有限且較低。預計本文將引發進一步的研究，重點在於資訊過載對教育領域的影響。關鍵字：資訊過載；遠距學習者；線上學習;系統性文獻綜述。 eISSN：2398-4287 © 2022。作者。由英國 E-International Publishing House, Ltd. 為 AMER ABRA cE-B 出版。這是一篇遵循 CC BY-NC-ND 授權 (http://creativecommons.org/licenses/by-nc-nd/4.0/) 的開放取用文章。同行評審由 AMER（馬來西亞環境行為研究人員協會）、ABRA（亞洲人行為研究人員協會）和 cE-Bs（環境行為研究中心）、大學建築、規劃與測量學院負責馬來西亞瑪拉技術公司。\\
titleTranslation: 系統性文獻綜述：線上遠距學習者資訊的過載},
  note = {通過文獻分析調查資訊過載與遠距學習者的相關研究。
\par
除了證明資訊過載的相關研究還很好之外，沒什麼用
\par
[TLDR] The developments of previous studies done in Information Overload fields in the past five years are summarized and a prospect to future research in this field is given using the systematic literature review method.},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\5BEAF4GG\\Dawie et al. - 2022 - Systematic Literature Review Information overload.pdf;D\:\\Paper\\Systematic Literature Review- Information overload of online distance learners.pdf}
}

@article{FinetuneEntireRAG2021,
  title = {Fine-Tune the {{Entire RAG Architecture}} (Including {{DPR}} Retriever) for {{Question-Answering}}.},
  date = {2021-06-22},
  journaltitle = {arXiv: Information Retrieval},
  url = {https://typeset.io/papers/fine-tune-the-entire-rag-architecture-including-dpr-18wuy27vm5},
  urldate = {2024-04-16},
  abstract = {In this paper, we illustrate how to fine-tune the entire Retrieval Augment Generation (RAG) architecture in an end-to-end manner. We highlighted the main engineering challenges that needed to be addressed to achieve this objective. We also compare how end-to-end RAG architecture outperforms the original RAG architecture for the task of question answering. We have open-sourced our implementation in the HuggingFace Transformers library.},
  langid = {english},
  annotation = {titleTranslation: 微調整個 RAG 架構（包括 DPR 檢索器）以進行問答。\\
abstractTranslation:  在本文中，我們說明如何以端到端的方式微調整個檢索增強生成（RAG）架構。我們強調了實現這一目標需要解決的主要工程挑戰。我們也比較了端對端 RAG 架構在問答任務方面如何優於原始 RAG 架構。我們已經開源了 HuggingFace Transformers 庫中的實作。},
  file = {C:\Users\BlackCat\Zotero\storage\QWPREZXQ\2021 - Fine-tune the Entire RAG Architecture (including D.pdf}
}

@inproceedings{florianleiserChatGPTFactGPTParticipatory2023,
  title = {From {{ChatGPT}} to {{FactGPT}}: {{A Participatory Design Study}} to {{Mitigate}} the {{Effects}} of {{Large Language Model Hallucinations}} on {{Users}}},
  shorttitle = {From {{ChatGPT}} to {{FactGPT}}},
  booktitle = {Mensch Und {{Computer}} 2023},
  author = {{Florian Leiser} and {Sven Eckhardt} and {Merlin Knaeble} and {Alexander Maedche} and {Gerhard Schwabe} and {Ali Sunyaev}},
  year = {9 月 3, 2023},
  series = {{{MuC}} '23},
  pages = {81--90},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3603555.3603565},
  url = {https://doi.org/10.1145/3603555.3603565},
  urldate = {2023-08-23},
  abstract = {Large language models (LLMs) like ChatGPT recently gained interest across all walks of life with their human-like quality in textual responses. Despite their success in research, healthcare, or education, LLMs frequently include incorrect information, called hallucinations, in their responses. These hallucinations could influence users to trust fake news or change their general beliefs. Therefore, we investigate mitigation strategies desired by users to enable identification of LLM hallucinations. To achieve this goal, we conduct a participatory design study where everyday users design interface features which are then assessed for their feasibility by machine learning (ML) experts. We find that many of the desired features are well-perceived by ML experts but are also considered as difficult to implement. Finally, we provide a list of desired features that should serve as a basis for mitigating the effect of LLM hallucinations on users.},
  isbn = {9798400707711},
  langid = {english},
  keywords = {Artificial Hallucinations,ChatGPT,Disney Method,Large Language Models,LLM,Participatory Design,使用者研究,可解釋性,回收,機器學習},
  annotation = {2 citations (Crossref) [2024-04-27]\\
titleTranslation: 從 ChatGPT 到 FactGPT：一項減輕大型語言模型幻覺對用戶影響的參與式設計研究\\
abstractTranslation:  像 ChatGPT 這樣的大型語言模型 (LLM) 最近因其在文本響應方面的類人品質而引起了各行各業的興趣。儘管法學碩士在研究、醫療保健或教育方面取得了成功，但法學碩士經常在他們的回答中包含不正確的信息，稱為幻覺。這些幻覺可能會影響用戶相信假新聞或改變他們的總體信念。因此，我們研究了用戶期望的緩解策略，以識別法學碩士幻覺。為了實現這一目標，我們進行了一項參與式設計研究，其中日常用戶設計界面功能，然後由機器學習 (ML) 專家評估其可行性。我們發現許多所需的功能得到了機器學習專家的認可，但也被認為難以實現。最後，我們提供了所需功能的列表，這些功能應作為減輕 LLM 幻覺對用戶影響的基礎。},
  note = {結合使用者研究與迪士尼法，思考可信的LLM應該要有怎樣的功能，並將功能轉成設計後，由評審判斷是否可行(?)。總之這是一個探討設計方法的論文，與研究主題較無相關。},
  file = {C:\Users\BlackCat\Zotero\storage\ELFX4C5G\Leiser 等。 - 2023 - From ChatGPT to FactGPT A Participatory Design St.pdf}
}

@inproceedings{florianmatthesMultifacetedContextdependentKnowledge2012,
  title = {Multi-Faceted Context-Dependent Knowledge Organisation with {{TACKO}}},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Knowledge Management}} and {{Knowledge Technologies}}},
  author = {{Florian Matthes} and {Christian Neubert} and {Alexander Steinhoff}},
  year = {9 月 5, 2012},
  series = {I-{{KNOW}} '12},
  pages = {1--8},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2362456.2362468},
  url = {https://doi.org/10.1145/2362456.2362468},
  urldate = {2023-09-11},
  abstract = {Purely hierarchical classification schemes such as taxonomies have been the primary means for knowledge organization for millennia and are still predominant in current information systems, for example in the form of hierarchical menus or folder hierarchies. However, due to the growing amount of digital information as well as improved technical possibilities, other organization schemes such as faceted classification schemes or so-called folksonomies, the result of social tagging activity, recently gained popularity. While these approaches address particular shortcomings of purely hierarchical organization of information, they exhibit certain characteristics limiting their general applicability. In this paper, we first give an overview of the particular strengths and weaknesses of different approaches to knowledge organization. Subsequently, we present a formal model of TACKO, a novel organization scheme that is inspired by the flexibility of tagging systems, i.e., the assignment of keywords to information resources, and incorporates elements of hierarchical and faceted organization schemes, so it can be considered a synthesis of the aforementioned schemes. We illustrate TACKO using a small example scenario. Additionally, we present a user interface for our approach, that we implemented in a modern web-based collaboration software.},
  isbn = {978-1-4503-1242-4},
  langid = {english},
  keywords = {faceted classification,knowledge organization systems},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用 TACKO 進行多方面的上下文相關知識組織\\
abstractTranslation:  純粹的分層分類方案（例如分類法）幾千年來一直是知識組織的主要手段，並且在當前的信息系統中仍然占主導地位，例如以分層菜單或文件夾層次結構的形式。然而，由於數字信息量的不斷增長以及技術可能性的提高，其他組織方案，例如多面分類方案或所謂的大眾分類法（社會標籤活動的結果）最近變得流行。雖然這些方法解決了純粹的信息分層組織的特定缺點，但它們表現出限制其普遍適用性的某些特徵。在本文中，我們首先概述了不同知識組織方法的特定優點和缺點。隨後，我們提出了TACKO 的形式模型，這是一種新穎的組織方案，其靈感來自於標籤系統的靈活性，即將關鍵字分配給信息資源，並結合了分層和分面組織方案的元素，因此它可以被認為是上述方案的綜合。我們使用一個小示例場景來說明 TACKO。此外，我們還為我們的方法提供了一個用戶界面，我們在基於網絡的現代協作軟件中實現了該方法。},
  file = {C:\Users\BlackCat\Zotero\storage\T97WHP2E\Matthes 等。 - 2012 - Multi-faceted context-dependent knowledge organisa.pdf}
}

@article{fouriePersonalInformationManagement2011,
  title = {Personal Information Management ({{PIM}}), Reference Management and Mind Maps: The Way to Creative Librarians?},
  shorttitle = {Personal Information Management ({{PIM}}), Reference Management and Mind Maps},
  author = {Fourie, Ina},
  date = {2011-01-01},
  journaltitle = {Library Hi Tech},
  volume = {29},
  number = {4},
  pages = {764--771},
  publisher = {Emerald Group Publishing Limited},
  issn = {0737-8831},
  doi = {10.1108/07378831111189822},
  url = {https://doi.org/10.1108/07378831111189822},
  urldate = {2024-03-13},
  abstract = {Purpose – This column aims to explore the potential of personal information management (PIM) and reference management. It focuses on combining the use of PIM and reference management software with mind maps to stimulate the creative and innovative use of information collected Design/methodology/approach – Following a brief review of the literature on the topic per se, awareness is raised of appropriate software and the potential of mind maps. The column is written against the background of research from information behaviour, PIM, mind maps, creativity, innovation and the reflective and evidence‐focused librarian. Findings – There is growing emphasis on more than information literacy skills and the responsible use of information. Creative and innovative use of information is stressed. More intensive use and exploitation of information is necessary to justify the time and effort spent in using PIM and reference management software. Mind maps and collaboration might support creative and innovative use of information, and need to be further exploited. Although software is strong in supporting collaboration, there is very limited built‐in support for the combination of features for information and reference management with features for mind maps. Originality/value – Although much has been published on developments in PIM and reference management, there is limited coverage of combining PIM and reference management with the use of mind maps and creativity, reflection and noting evidence.},
  langid = {english},
  keywords = {Computer software,Creative thinking,Creativity,Information,Information management,Innovation,Librarians,Mind maps,知識管理},
  annotation = {12 citations (Crossref) [2024-03-26]\\
titleTranslation: 個人資訊管理 (PIM)、參考文獻管理與心智圖：創意圖書館員之路？},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\AIBNQXUF\\Fourie - 2011 - Personal information management (PIM), reference m.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\7M4JPIW8\\html.html}
}

@inproceedings{francoisremyAutomaticGlossaryClinical2023,
  title = {Automatic {{Glossary}} of {{Clinical Terminology}}: A {{Large-Scale Dictionary}} of {{Biomedical Definitions Generated}} from {{Ontological Knowledge}}},
  shorttitle = {Automatic {{Glossary}} of {{Clinical Terminology}}},
  booktitle = {The 22nd {{Workshop}} on {{Biomedical Natural Language Processing}} and {{BioNLP Shared Tasks}}},
  author = {{François Remy} and {Kris Demuynck} and {Thomas Demeester}},
  date = {2023-07},
  pages = {265--272},
  publisher = {Association for Computational Linguistics},
  location = {Toronto, Canada},
  doi = {10.18653/v1/2023.bionlp-1.23},
  url = {https://aclanthology.org/2023.bionlp-1.23},
  urldate = {2023-09-12},
  abstract = {Background: More than 400.000 biomedical concepts and some of their relationships are contained in SnomedCT, a comprehensive biomedical ontology. However, their concept names are not always readily interpretable by non-experts, or patients looking at their own electronic health records (EHR). Clear definitions or descriptions in understandable language or often not available. Therefore, generating human-readable definitions for biomedical concepts might help make the information they encode more accessible and understandable to a wider public.Objective: In this article, we introduce the Automatic Glossary of Clinical Terminology (AGCT), a large-scale biomedical dictionary of clinical concepts generated using high-quality information extracted from the biomedical knowledge contained in SnomedCT.Methods: We generate a novel definition for every SnomedCT concept, after prompting the OpenAI Turbo model, a variant of GPT 3.5, using a high-quality verbalization of the SnomedCT relationships of the to-be-defined concept. A significant subset of the generated definitions was subsequently evaluated by NLP researchers with biomedical expertise on 5-point scales along the following three axes: factuality, insight, and fluency.Results: AGCT contains 422,070 computer-generated definitions for SnomedCT concepts, covering various domains such as diseases, procedures, drugs, and anatomy. The average length of the definitions is 49 words. The definitions were assigned average scores of over 4.5 out of 5 on all three axes, indicating a majority of factual, insightful, and fluent definitions.Conclusion: AGCT is a novel and valuable resource for biomedical tasks that require human-readable definitions for SnomedCT concepts. It can also serve as a base for developing robust biomedical retrieval models or other applications that leverage natural language understanding of biomedical knowledge.},
  eventtitle = {{{BioNLP}} 2023},
  langid = {english},
  keywords = {ChatGPT,已整理,知識本體,醫學},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 臨床術語自動詞彙表：根據本體知識生成的大型生物醫學定義詞典\\
abstractTranslation:  背景：SnomedCT 是一個綜合性生物醫學本體，包含超過 400,000 個生物醫學概念及其一些關係。然而，它們的概念名稱並不總是容易被非專家或查看自己的電子健康記錄 (EHR) 的患者所理解。使用易於理解的語言進行清晰的定義或描述，或者通常不可用。因此，為生物醫學概念生成人類可讀的定義可能有助於使它們編碼的信息更容易被更廣泛的公眾訪問和理解。 目標：在本文中，我們介紹臨床術語自動詞彙表（AGCT），這是一個大型生物醫學詞典使用從SnomedCT 中包含的生物醫學知識中提取的高質量信息生成臨床概念。方法：在提示OpenAI Turbo 模型（GPT 3.5 的變體）後，我們使用高質量的語言表達為每個SnomedCT 概念生成一個新穎的定義。待定義概念的 SnomedCT 關係。隨後，具有生物醫學專業知識的NLP 研究人員對生成的定義的一個重要子集進行了評估，沿著以下三個軸進行5 點量表：事實性、洞察力和流暢性。結果：AGCT 包含422,070個計算機生成的SnomedCT 概念定義，涵蓋各個領域例如疾病、手術、藥物和解剖學。定義的平均長度為 49 個單詞。這些定義在所有三個軸上的平均得分超過4.5 分（滿分5 分），表明大多數定義是事實性的、有洞察力的和流暢的。結論：AGCT 對於需要人類可讀的SnomedCT 概念定義的生物醫學任務來說是一種新穎且有價值的資源。它還可以作為開發強大的生物醫學檢索模型或其他利用生物醫學知識的自然語言理解的應用程序的基礎。},
  note = {使用ChatGPT將SnomedCT中的實體重新定義解釋，並由醫生做確認，最終得到4.5/5的高分。並稱為AGCT。},
  file = {C:\Users\BlackCat\Zotero\storage\PHC7E8NT\Remy 等。 - 2023 - Automatic Glossary of Clinical Terminology a Larg.pdf}
}

@article{franks.c.tsengEnrichingClassDiagram2008,
  title = {Enriching the Class Diagram Concepts to Capture Natural Language Semantics for Database Access},
  author = {{Frank S. C. Tseng} and {Chun-Ling Chen}},
  year = {10 月 1, 2008},
  journaltitle = {Data \& Knowledge Engineering},
  shortjournal = {Data Knowl. Eng.},
  volume = {67},
  number = {1},
  pages = {1--29},
  issn = {0169-023X},
  doi = {10.1016/j.datak.2008.05.006},
  url = {https://doi.org/10.1016/j.datak.2008.05.006},
  urldate = {2023-09-11},
  abstract = {Research on accessing databases using natural languages usually utilizes an intermediate logical form for the mapping process from natural languages to database query languages. However, there is still much that needs to be accomplished to bridge the gap between natural language constructs and database schemas. In this paper, we present a translation scheme for transforming natural language queries into relational algebra through the class diagram representations. Based on a logical form developed by extending the UML class diagram notations, a transformation model is presented to support the automatic transformation of natural language queries into relational algebra by employing appropriate natural language processing techniques and object-oriented analysis methods. The proposed logical form has the advantage that it can be mapped from natural language constructs by referring to the conceptual schema modeled by class diagrams, and can be efficiently transformed into relational algebra for query execution. We believe the whole process could offer a clear and natural framework for processing natural language queries to retrieve data from database systems.},
  langid = {english},
  keywords = {Class diagram,Conceptual schema,Natural language queries,Object-oriented modeling (OO modeling)},
  annotation = {5 citations (Crossref) [2024-03-26]\\
titleTranslation: 豐富類圖概念以捕獲數據庫訪問的自然語言語義\\
abstractTranslation:  使用自然語言訪問數據庫的研究通常利用中間邏輯形式來進行從自然語言到數據庫查詢語言的映射過程。然而，要彌合自然語言結構和數據庫模式之間的差距，仍有許多工作要做。在本文中，我們提出了一種通過類圖表示將自然語言查詢轉換為關係代數的翻譯方案。基於通過擴展UML類圖符號開發的邏輯形式，提出了一種轉換模型，通過採用適當的自然語言處理技術和麵向對象的分析方法，支持自然語言查詢到關係代數的自動轉換。所提出的邏輯形式的優點是，它可以通過參考類圖建模的概念模式從自然語言結構映射，並且可以有效地轉換為關係代數以進行查詢執行。我們相信整個過程可以提供一個清晰自然的框架來處理自然語言查詢以從數據庫系統檢索數據。},
  file = {C:\Users\BlackCat\Zotero\storage\ZUKKK6A6\Tseng 與 Chen - 2008 - Enriching the class diagram concepts to capture na.pdf}
}

@article{franksTextClassificationRecords2022,
  title = {Text {{Classification}} for {{Records Management}}},
  author = {Franks, Jason},
  year = {9 月 16, 2022},
  journaltitle = {Journal on Computing and Cultural Heritage},
  shortjournal = {J. Comput. Cult. Herit.},
  volume = {15},
  number = {3},
  pages = {42:1--42:19},
  issn = {1556-4673},
  doi = {10.1145/3485846},
  url = {https://dl.acm.org/doi/10.1145/3485846},
  urldate = {2023-12-01},
  abstract = {Automatic classification of electronic records is necessary to address the brewing crisis in the recordkeeping discipline, caused by escalating data volumes and digital rights legislation. Current solutions usually employ expert systems that classify records based on their metadata, but this approach is becoming unfeasible due to the increased variety of records and a growing lack of metadata. Text classification is a promising alternative now that the records themselves are machine readable. In this study, the performance of traditional text classification techniques was compared to newer natural language processing technologies in a series of experiments using authentic records data. While the latest Transformer language models showed superior classification skill, traditional methods still perform well. These results were discussed by a focus group of record managers, who believe that text classification can help them manage risk and meet compliance obligations. This is a first step toward aspirations of being able to synthesize narrative from a corpus of records.},
  langid = {english},
  keywords = {language models,neural networks,records management,text classification,Tf-idf,待整理,文字分類,機器學習},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 記錄管理的文字分類},
  file = {C:\Users\BlackCat\Zotero\storage\M9YSV6IY\Franks - 2022 - Text Classification for Records Management.pdf}
}

@article{fredtorres-cruzUnsupervisedLearningAlgorithms2022,
  title = {Unsupervised {{Learning Algorithms}} for {{Keyword Extraction}} in an {{Undergraduate Thesis}}},
  author = {{Fred Torres-Cruz} and {Edelfre Flores} and {William E. Arcaya} and {Irenio L. Chagua} and {Marga I. Ingaluque}},
  date = {2022},
  publisher = {arXiv},
  doi = {10.48550/arxiv.2206.12016},
  url = {https://arxiv.org/abs/2206.12016},
  urldate = {2022-09-19},
  abstract = {The amount of data managed in many academic institutions has increased in recent years, particularly in all the research work done by undergraduate students, who simply use empirical techniques for keyword selection, forgetting existing technical methods to assist their students in this process. Information and communication technologies, such as the platform for integrated research and academic work with responsibility (PILAR), which records information about research projects, such as titles, summaries, and keywords in their various modalities, have gained relevance and importance in the management of these. We proved algorithms with these records of research projects that have been analysed in this study, and predictions were made for each of the nine (09) models of unsupervised machine learning algorithms that were implemented for each of the 7430 records from the dataset. The most efficient way of extracting keywords for this dataset was the TF-IDF method, obtaining 72\% accuracy and [0.4786, SD 0.0501] in average extraction time for each thesis file processed by this model.},
  version = {1},
  keywords = {FOS: Computer and information sciences,Information Retrieval (cs.IR)},
  annotation = {0 citations (Semantic Scholar/arXiv) [2022-10-26]}
}

@thesis{GaoMingYuanZhongXiYiHeBingDaChangAiZhiLiaoDeCunHuoFenXi2018,
  title = {中西醫合併大腸癌治療的存活分析},
  author = {{高明源}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2018},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/45ze4u},
  abstract = {本論文分析中西醫合併治療的大腸癌病患和非中西醫治療的大腸癌病患的存活時間是否有顯著差異。研究首先從台北與大林慈濟醫院取得2003年至2016年的癌症登記資料庫與中醫門診紀錄資料庫，篩選出欲分析的病患資料共2064人，其中包括非中西醫組1777人、中西醫組287人。本論文先透過卡方檢定得知在許多研究變項上治療方式分布有顯著差異，因此無法直接運用Kaplan-Meier存活分析。本論文接著使用Cox比例風險模型來分析多個研究變項對存活率的影響，並且考慮研究變項之間是否存在交互作用。分析結果顯示中西醫相對於非中西醫的治療方式減少約54.6\%的死亡風險，且達到顯著差異。此外，本論文也發現2個單味藥、6個方劑、1個方劑組合在治療大腸癌上有顯著的效果。本論文顯示使用中醫改善病患免疫力及減緩西醫治療副作用可以顯著延長大腸癌病患的存活時間。},
  pagetotal = {55},
  keywords = {實驗室}
}

@online{gaoPreciseZeroShotDense2022,
  title = {Precise {{Zero-Shot Dense Retrieval}} without {{Relevance Labels}}},
  author = {Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
  date = {2022-12-20},
  eprint = {2212.10496},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.10496},
  url = {http://arxiv.org/abs/2212.10496},
  urldate = {2024-03-28},
  abstract = {While dense retrieval has been shown effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings\textasciitilde (HyDE). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is unreal and may contain false details. Then, an unsupervised contrastively learned encoder\textasciitilde (e.g. Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder's dense bottleneck filtering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search, QA, fact verification) and languages\textasciitilde (e.g. sw, ko, ja).},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,問答系統,嵌入,已整理,待讀,機器學習,重要},
  annotation = {titleTranslation: 無需相關標籤的精確零樣本密集檢索\\
abstractTranslation:  雖然密集檢索已被證明跨任務和語言有效且高效，但在沒有相關標籤可用時，仍很難創建有效的完全零樣本密集檢索系統。在本文中，我們認識到零樣本學習和編碼相關性的困難。相反，我們建議以假設文檔嵌入（HyDE）為中心。給定一個查詢，HyDE 首先零樣本指示指令追蹤語言模型（例如 InstructGPT）產生假設文件。該文件捕獲了相關模式，但並不真實，並且可能包含虛假細節。然後，無監督對比學習編碼器（例如 Contriever）將文件編碼為嵌入向量。此向量標識語料庫嵌入空間中的鄰域，其中基於向量相似性檢索相似的真實文件。第二步將產生的文件與實際語料庫結合起來，利用編碼器的密集瓶頸過濾掉不正確的細節。我們的實驗表明，HyDE 顯著優於最先進的無監督密集檢索者 Contriever，並且在各種任務（例如網路搜尋、QA、事實驗證）和語言〜（例如 sw、科，雅）。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\MUF5Q38A\\Gao et al. - 2022 - Precise Zero-Shot Dense Retrieval without Relevanc.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\4BDXMPP4\\2212.html}
}

@online{gaoRetrievalAugmentedGenerationLarge2024,
  title = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}},
  author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
  date = {2024-03-27},
  eprint = {2312.10997},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.10997},
  url = {http://arxiv.org/abs/2312.10997},
  urldate = {2024-04-30},
  abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,LLM,RAG,Survey},
  annotation = {titleTranslation: 大型語言模型的檢索增強生成：一項調查\\
abstractTranslation:  大型語言模型（LLM）展示了令人印象深刻的能力，但遇到了諸如幻覺、過時的知識以及不透明、不可追踪的推理過程等挑戰。透過整合外部資料庫的知識，檢索增強生成（RAG）已成為一種有前景的解決方案。這提高了生成的準確性和可信度，特別是對於知識密集型任務，並允許持續的知識更新和特定領域資訊的整合。 RAG 將法學碩士的內在知識與外部資料庫的龐大動態儲存庫協同整合。這篇綜合綜述論文詳細研究了 RAG 範式的進展，包括樸素 RAG、高級 RAG 和模組化 RAG。它仔細研究了 RAG 框架的三方基礎，包括檢索、產生和增強技術。這篇論文重點介紹了每個關鍵組件中嵌入的最先進技術，讓人們對 RAG 系統的進步有了深刻的了解。此外，本文也介紹了最新的評估架構和基準。最後，本文概述了當前面臨的挑戰，並指出了研究和開發的未來途徑。},
  note = {Comment: Ongoing Work},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\39K24UKQ\\Gao 等。 - 2024 - Retrieval-Augmented Generation for Large Language .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\5YZZDC2Q\\2312.html}
}

@book{geoffreyc.bowkerSortingThingsOut2000,
  title = {Sorting {{Things Out}}: {{Classification}} and {{Its Consequences}}},
  shorttitle = {Sorting {{Things Out}}},
  author = {{Geoffrey C. Bowker} and {Susan Leigh Star}},
  date = {2000},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/6352.001.0001},
  url = {https://direct.mit.edu/books/book/4738/Sorting-Things-OutClassification-and-Its},
  urldate = {2023-09-13},
  isbn = {978-0-262-26907-0},
  langid = {english},
  keywords = {無法取得},
  annotation = {titleTranslation: 整理事物：分類及其後果}
}

@inproceedings{gerarddemeloUWNLargeMultilingual2012,
  title = {{{UWN}}: {{A Large Multilingual Lexical Knowledge Base}}},
  shorttitle = {{{UWN}}},
  booktitle = {Proceedings of the {{ACL}} 2012 {{System Demonstrations}}},
  author = {{Gerard de Melo} and {Gerhard Weikum}},
  date = {2012-07},
  pages = {151--156},
  publisher = {Association for Computational Linguistics},
  location = {Jeju Island, Korea},
  url = {https://aclanthology.org/P12-3026},
  urldate = {2023-09-19},
  langid = {english},
  keywords = {未整理},
  annotation = {titleTranslation: UWN：大型多語言詞彙知識庫},
  file = {C:\Users\BlackCat\Zotero\storage\YQ9Q8EIM\de Melo 與 Weikum - 2012 - UWN A Large Multilingual Lexical Knowledge Base.pdf}
}

@article{ghannamYouCallThat2020,
  title = {Do {{You Call That}} a {{Lab Notebook}}?},
  author = {Ghannam, Rami},
  date = {2020-09},
  journaltitle = {IEEE Potentials},
  volume = {39},
  number = {5},
  pages = {21--24},
  issn = {1558-1772},
  doi = {10.1109/MPOT.2020.2968798},
  url = {https://ieeexplore.ieee.org/document/9199323},
  urldate = {2024-01-11},
  abstract = {"Electronic laboratory notebooks (ELNs)? What are those?" At the start of every academic year, I prepare myself for a battle with my third-year engineering students about laboratory notebooks. More often than not, they either hand me a neatly written diary or loose bits of paper that are half folded and randomly glued to the back cover of a jotter. At first, my students dismiss the idea of keeping a properly documented lab notebook, but I notice a change in attitude when they are introduced to electronic notebooks.},
  eventtitle = {{{IEEE Potentials}}},
  langid = {english},
  keywords = {使用者研究,實驗筆記,略讀},
  annotation = {7 citations (Crossref) [2024-03-26]\\
titleTranslation: 你稱之為實驗筆記本嗎？\\
abstractTranslation:  “電子實驗室筆記本（ELN）？那些是什麼？”在每個學年開始時，我都會準備與三年級工科學生就實驗室筆記本進行一場戰鬥。通常，他們要么遞給我一本寫得很工整的日記，要么遞給我一些半折的鬆散的紙片，隨意地粘在筆記本的封底上。起初，我的學生拒絕保留正確記錄的實驗室筆記本的想法，但我注意到當他們接觸電子筆記本時態度發生了變化。},
  note = {本研究調查學生對於不同ELN的選擇及使用心得，認為只有不到5\%的實驗室使用ELN，但90\%的學生認為ELN很有幫助。
\par
好像特別針對工科。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\AQFJI6MM\\Ghannam - 2020 - Do You Call That a Lab Notebook.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\L7LT9Q2X\\9199323.html}
}

@inproceedings{ghislainaugusteatemezingQA4LOVNaturalLanguage2016,
  title = {{{QA4LOV}}: {{A Natural Language Interface}} to {{Linked Open Vocabulary}}.},
  shorttitle = {{{QA4LOV}}},
  booktitle = {{{ISWC}} ({{Posters}} \& {{Demos}})},
  author = {{Ghislain Auguste Atemezing} and {Pierre-Yves Vandenbussche}},
  date = {2016},
  publisher = {Citeseer},
  url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=39f8375d1741cfcf8dccc8ee9f3b054ea65028c2},
  urldate = {2023-09-28},
  keywords = {No DOI found},
  file = {C:\Users\BlackCat\Zotero\storage\VP7ZT4TC\Atemezing and Vandenbussche - 2016 - QA4LOV A Natural Language Interface to Linked Ope.pdf}
}

@online{giancarloguizzardiSemanticsOntologyExplanation2023,
  title = {Semantics, {{Ontology}} and {{Explanation}}},
  author = {{Giancarlo Guizzardi} and {Nicola Guarino}},
  date = {2023-04-21},
  eprint = {2304.11124},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.11124},
  url = {http://arxiv.org/abs/2304.11124},
  urldate = {2023-07-18},
  abstract = {The terms 'semantics' and 'ontology' are increasingly appearing together with 'explanation', not only in the scientific literature, but also in organizational communication. However, all of these terms are also being significantly overloaded. In this paper, we discuss their strong relation under particular interpretations. Specifically, we discuss a notion of explanation termed ontological unpacking, which aims at explaining symbolic domain descriptions (conceptual models, knowledge graphs, logical specifications) by revealing their ontological commitment in terms of their assumed truthmakers, i.e., the entities in one's ontology that make the propositions in those descriptions true. To illustrate this idea, we employ an ontological theory of relations to explain (by revealing the hidden semantics of) a very simple symbolic model encoded in the standard modeling language UML. We also discuss the essential role played by ontology-driven conceptual models (resulting from this form of explanation processes) in properly supporting semantic interoperability tasks. Finally, we discuss the relation between ontological unpacking and other forms of explanation in philosophy and science, as well as in the area of Artificial Intelligence.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,可解釋性,基礎理論,已整理,知識本體,語意分析,預印本},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-07-20]\\
0 citations (Semantic Scholar/DOI) [2023-07-20]\\
titleTranslation: 語義、本體和解釋\\
abstractTranslation:  “語義”和“本體論”這兩個術語越來越多地與“解釋”一起出現，不僅出現在科學文獻中，而且出現在組織傳播中。然而，所有這些術語也都被嚴重超載。在本文中，我們在特定的解釋下討論了它們之間的緊密關係。具體來說，我們討論了一種稱為本體論解包的解釋概念，其目的是通過揭示其假定的真理製造者的本體論承諾來解釋符號域描述（概念模型、知識圖譜、邏輯規範），即一個人本體中的實體，這些實體使這些描述中的命題是正確的。為了說明這個想法，我們採用關係本體論來解釋（通過揭示其隱藏語義）以標準建模語言 UML 編碼的非常簡單的符號模型。我們還討論了本體驅動的概念模型（由這種形式的解釋過程產生）在正確支持語義互操作性任務方面所發揮的重要作用。最後，我們討論本體論解包與哲學和科學以及人工智能領域的其他形式的解釋之間的關係。},
  file = {C:\Users\BlackCat\Zotero\storage\CJUX2UWT\Guizzardi and Guarino - 2023 - Semantics, Ontology and Explanation.pdf}
}

@article{gillesaudemardExplanatoryPowerBoolean2022,
  title = {On the Explanatory Power of {{Boolean}} Decision Trees},
  author = {{Gilles Audemard} and {Steve Bellart} and {Louenas Bounia} and {Frédéric Koriche} and {Jean-Marie Lagniez} and {Pierre Marquis}},
  date = {2022-11-01},
  journaltitle = {Data \& Knowledge Engineering},
  shortjournal = {Data \& Knowledge Engineering},
  volume = {142},
  pages = {102088},
  issn = {0169-023X},
  doi = {10.1016/j.datak.2022.102088},
  url = {https://www.sciencedirect.com/science/article/pii/S0169023X22000799},
  urldate = {2023-09-13},
  abstract = {Decision trees have long been recognized as models of choice in sensitive applications where interpretability is of paramount importance. In this paper, we examine the computational ability of Boolean decision trees for the explanation purpose. We focus on both abductive explanations (suited to explaining why a given instance has been classified as such by the decision tree at hand) and on contrastive explanations (suited to explaining why a given instance has not been classified by the decision tree as it was expected). More precisely, we are interested in deriving, minimizing, and counting abductive explanations and contrastive explanations. We prove that the set of all irredundant abductive explanations (also known as PI-explanations or sufficient reasons) for an instance given a decision tree can be exponentially larger than the size of the input (the instance and the decision tree). Therefore, generating the full set of sufficient reasons for an instance can be out of reach. In addition, deriving a single sufficient reason, though computationally easy when dealing with decision trees, does not prove enough in general; indeed, two sufficient reasons for the same instance may differ on many features. To deal with this issue and generate synthetic views of the set of all sufficient reasons, we define notions of relevant features and of necessary features that characterize the (possibly negated) features appearing in at least one or in every sufficient reason for an instance, and we show that they can be computed in polynomial time. We also introduce the notion of explanatory importance, that indicates how frequent each (possibly negated) feature is in the set of all sufficient reasons. We show how the explanatory importance of a (possibly negated) feature and the number of sufficient reasons for an instance can be obtained via a model counting operation, which turns out to be practical in many cases. We also explain how to enumerate minimum-size sufficient reasons. We finally show that, unlike sufficient reasons, the set of all contrastive explanations for an instance given a decision tree can be derived, minimized and counted in polynomial time.},
  langid = {english},
  keywords = {可解釋性,已整理,機器學習,決策樹},
  annotation = {10 citations (Crossref) [2024-03-26]\\
titleTranslation: 論布爾決策樹的解釋力\\
abstractTranslation:  決策樹長期以來被認為是可解釋性至關重要的敏感應用中的首選模型。在本文中，我們出於解釋目的檢查了布林決策樹的計算能力。我們關注溯因解釋（適合於解釋為什麼給定的實例已被當前的決策樹分類）和對比解釋（適合於解釋為什麼給定的實例沒有按預期被決策樹分類） ）。更準確地說，我們感興趣的是推導、最小化和計算溯因解釋和對比解釋。我們證明，對於給定決策樹的實例，所有不冗餘的溯因解釋（也稱為 PI 解釋或充分理由）的集合可以以指數方式大於輸入（實例和決策樹）的大小。因此，為一個實例產生全套充分理由可能是遙不可及的。此外，雖然在處理決策樹時計算起來很容易，但推導出一個充分的理由在一般情況下還不夠。事實上，同一實例的兩個充分理由可能在許多特徵上有所不同。為了處理這個問題並產生所有充分理由集的綜合視圖，我們定義了相關特徵和必要特徵的概念，這些特徵描述了在實例的至少一個或每個充分理由中出現的（可能否定的）特徵，並且我們證明它們可以在多項式時間內計算出來。我們也引入了解釋重要性的概念，它顯示每個（可能被否定的）特徵在所有充分理由的集合中出現的頻率。我們展示瞭如何透過模型計數操作來獲得（可能被否定的）特徵的解釋重要性以及實例的充分理由的數量，這在許多情況下都是實用的。我們也解釋瞭如何列舉最小尺寸的充分理由。我們最終表明，與充分理由不同，給定決策樹的實例的所有對比解釋的集合可以在多項式時間內導出、最小化和計數。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\WP8A3GGV\\Audemard 等。 - 2022 - On the explanatory power of Boolean decision trees.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\SMQIMVQF\\S0169023X22000799.html}
}

@inproceedings{giraldoSMARTProtocolsSemantic2014,
  title = {{{SMART}} Protocols: Semantic Representation for Experimental Protocols},
  shorttitle = {{{SMART}} Protocols},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Linked Science}} - {{Volume}} 1282},
  author = {Giraldo, Olga and Garcia, Alexander and Corcho, Oscar},
  year = {10 月 19, 2014},
  series = {{{LISC}}'14},
  pages = {36--47},
  publisher = {CEUR-WS.org},
  location = {Aachen, DEU},
  abstract = {Two important characteristics of science are the "reproducibility" and "clarity". By rigorous practices, scientists explore aspects of the world that they can reproduce under carefully controlled experimental conditions. The clarity, complementing reproducibility, provides unambiguous descriptions of results in a mechanical or mathematical form. Both pillars depend on well-structured and accurate descriptions of scientific practices, which are normally recorded in experimental protocols, scientific workflows, etc. Here we present SMART Protocols (SP), our ontology-based approach for representing experimental protocols and our contribution to clarity and reproducibility. SP delivers an unambiguous description of processes by means of which data is produced; by doing so, we argue, it facilitates reproducibility. Moreover, SP is thought to be part of e-science infrastructures. SP results from the analysis of 175 protocols; from this dataset, we extracted common elements. From our analysis, we identified document, workflow and domain-specific aspects in the representation of experimental protocols. The ontology is available at http://purl.org/net/SMARTprotocol},
  langid = {english},
  keywords = {⛔ No DOI found,experimental protocol,in vitro workflow,ontology,reproducibility},
  annotation = {titleTranslation: SMART 協議：實驗協議的語義表示\\
abstractTranslation:  科學的兩個重要特徵是「可重複性」和「清晰性」。透過嚴格的實踐，科學家們探索了他們可以在嚴格控制的實驗條件下重現的世界的各個方面。其清晰度補充了可重複性，以機械或數學形式提供了結果的明確描述。這兩個支柱都依賴對科學實踐的結構良好且準確的描述，這些描述通常記錄在實驗協議、科學工作流程等。在這裡，我們介紹SMART 協議(SP)，這是我們用於表示實驗協議的基於本體的方法，以及我們對清晰度的貢獻和再現性。 SP 對產生資料的過程提供了明確的描述；我們認為，這樣做可以促進可重複性。此外，SP 被認為是電子科學基礎設施的一部分。 SP結果來自於175個協議的分析；從這個資料集中，我們提取了共同元素。根據我們的分析，我們確定了實驗協議表示中的文件、工作流程和特定領域的方面。該本體可在 http://purl.org/net/SMARTprotocol 取得},
  file = {D:\Paper\SMART protocols semantic representation for experimental protocols.pdf}
}

@article{girayPromptEngineeringChatGPT2023,
  title = {Prompt {{Engineering}} with {{ChatGPT}}: {{A Guide}} for {{Academic Writers}}},
  shorttitle = {Prompt {{Engineering}} with {{ChatGPT}}},
  author = {Giray, Louie},
  date = {2023-12-01},
  journaltitle = {Annals of Biomedical Engineering},
  shortjournal = {Ann Biomed Eng},
  volume = {51},
  number = {12},
  pages = {2629--2633},
  issn = {1573-9686},
  doi = {10.1007/s10439-023-03272-4},
  url = {https://doi.org/10.1007/s10439-023-03272-4},
  urldate = {2024-05-06},
  abstract = {Prompt engineering is a relatively new discipline that refers to the practice of developing and optimizing prompts to effectively utilize large language models, particularly in natural language processing tasks. However, not many writers and researchers are familiar about this discipline. Hence, in this paper, I aim to highlight the significance of prompt engineering for academic writers and researchers, particularly the fledgling, in the rapidly evolving world of artificial intelligence. I also discuss the concepts of prompt engineering, large language models, and the techniques and pitfalls of writing prompts. Here, I contend that by acquiring prompt engineering skills, academic writers can navigate the changing landscape and leverage large language models to enhance their writing process. As artificial intelligence continues to advance and penetrate the arena of academic writing, prompt engineering equips writers and researchers with the essential skills to effectively harness the power of language models. This enables them to confidently explore new opportunities, enhance their writing endeavors, and remain at the forefront of utilizing cutting-edge technologies in their academic pursuits.},
  langid = {english},
  keywords = {Academic writing,ChatGPT,Large language models,LLM,Natural language processing,Prompt,Prompt engineering,Prompts,未整理,無法取得}
}

@inproceedings{glorial.zunigaOntologyItsTransformation2001,
  title = {Ontology: Its Transformation from Philosophy to Information Systems},
  shorttitle = {Ontology},
  booktitle = {Proceedings of the International Conference on {{Formal Ontology}} in {{Information Systems}} - {{Volume}} 2001},
  author = {{Gloria L. Zúñiga}},
  year = {10 月 17, 2001},
  series = {{{FOIS}} '01},
  pages = {187--197},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/505168.505187},
  url = {https://dl.acm.org/doi/10.1145/505168.505187},
  urldate = {2023-09-16},
  abstract = {It is no secret that the multidisciplinary sphere of information systems has borrowed the term 'ontology' from philosophy, and reinterpreted it to be more suitable for information systems. However, there is some disagreement about what this reinterpretation should be. This paper examines two prominent and distinct views on what information systems ontology is, and attempts to advance a unified definition that can be understood interdisciplinarily. But the goal of this paper is to show the specific points of variance between information systems ontology and philosophical ontology in order to shed light on the transformation of the term 'ontology' in its adoption by the information systems community. The relatively new information systems ontology is facing great challenges that may be better confronted with the insights that can be discovered through philosophical ontology.},
  isbn = {978-1-58113-377-6},
  langid = {english},
  keywords = {基礎理論,已整理,待讀,知識本體},
  annotation = {41 citations (Crossref) [2024-03-26]\\
abstractTranslation:  眾所周知，資訊系統的多學科領域借用了哲學中的「本體論」一詞，並將其重新解釋為更適合資訊系統。然而，對於這種重新解釋應該是什麼存在一些分歧。本文探討了資訊系統本體是什麼的兩種突出且不同的觀點，並試圖提出一個可以跨學科理解的統一定義。但本文的目的是展示資訊系統本體論和哲學本體論之間的具體差異點，以闡明資訊系統界採用「本體論」一詞的轉變。相對較新的資訊系統本體論正面臨著巨大的挑戰，透過哲學本體論可以發現的見解可能會更好地應對這些挑戰。\\
titleTranslation: 本體論：從哲學到資訊系統的轉變},
  file = {C:\Users\BlackCat\Zotero\storage\D6X8C87J\Zúñiga - 2001 - Ontology its transformation from philosophy to in.pdf}
}

@article{goharzamanKnowledgeMappingResearch2019,
  title = {Knowledge {{Mapping}} for {{Research Papers}}},
  author = {{Gohar Zaman} and {Anas Alghamdi} and {Nawaf Abdulrhman Alowain}},
  date = {2019},
  journaltitle = {IJCSNS},
  volume = {19},
  number = {10},
  pages = {158},
  url = {https://www.researchgate.net/profile/Atta-Rahman/publication/337341208_Knowledge_Mapping_for_Research_Papers/links/5dd3de5f299bf1b74b4e69d9/Knowledge-Mapping-for-Research-Papers.pdf},
  urldate = {2023-10-25},
  langid = {english},
  keywords = {No DOI found,待讀,重要},
  annotation = {titleTranslation: 研究論文的知識圖譜},
  file = {C:\Users\BlackCat\Zotero\storage\CZH2HHJZ\Zaman et al. - 2019 - Knowledge Mapping for Research Papers.pdf}
}

@article{goharzamanOntologicalFrameworkInformation2021,
  title = {An {{Ontological Framework}} for {{Information Extraction From Diverse Scientific Sources}}},
  author = {{Gohar Zaman} and {Hairulnizam Mahdin} and {Khalid Hussain} and {Atta-Ur-Rahman} and {Jemal Abawajy} and {Salama A. Mostafa}},
  date = {2021},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {9},
  pages = {42111--42124},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3063181},
  url = {https://ieeexplore.ieee.org/document/9366868/},
  urldate = {2023-10-23},
  abstract = {Automatic information extraction from online published scientific documents is useful in various applications such as tagging, web indexing and search engine optimization. As a result, automatic information extraction has become among the hottest areas of research in text mining. Although various information extraction techniques have been proposed in the literature, their efficiency demands domain specific documents with static and well-defined format. Furthermore, their accuracy is challenged with a slight modification in the format. To overcome these issues, a novel ontological framework for information extraction (OFIE) using fuzzy rule-base (FRB) and word sense disambiguation (WSD) is proposed. The proposed approach is validated with a significantly wider document domains sourced from well-known publishing services such as IEEE, ACM, Elsevier, and Springer. We have also compared the proposed information extraction approach against state-of-the-art techniques. The results of the experiment show that the proposed approach is less sensitive to changes in the document format and has a significantly better average accuracy of 89.14\% and F-score as 89\%.},
  langid = {english},
  keywords = {已整理,概讀,模糊理論,知識抽取,知識本體,論文分析},
  annotation = {18 citations (Crossref) [2024-03-26]\\
titleTranslation: 從不同科學來源提取資訊的本體論框架\\
abstractTranslation:  從線上發布的科學文件中自動提取資訊可用於各種應用，例如標記、網路索引和搜尋引擎優化。因此，自動資訊擷取已成為文本探勘中最熱門的研究領域之一。儘管文獻中已經提出了各種資訊擷取技術，但它們的效率需要具有靜態和明確定義格式的特定領域文件。此外，其準確性因格式的輕微修改而受到挑戰。為了克服這些問題，提出了一種使用模糊規則庫（FRB）和詞義消歧（WSD）的新型資訊來提取本體框架（OFIE）。所提出的方法透過來自 IEEE、ACM、Elsevier 和 Springer 等知名出版服務的更廣泛的文檔域進行了驗證。我們也將所提出的資訊擷取方法與最先進的技術進行了比較。實驗結果表明，所提出的方法對文檔格式的變化不太敏感，並且具有明顯更好的平均準確率（89.14％）和F-score（89％）。},
  note = {結合fuzzy rule-base (FRB) 和 word sense disambiguation (WSD) 等技術，建立基於ontology的文獻抽取系統。但此論文撰寫於ChatGPT之前。可能可以作為輕量化的解決方案?
\par
建立OFIE系統，基於模糊正則表達式(FRE)的FRBS及基於word2vec的詞標準化(WSD)元件，並結合過去已有的PDF2XML、PDF2docx技術。和CERMINE相比，提供更多功能並提升在多個出版商(IEEE、ACM、Elsevier、Springer、Hindawi、MDPI、BMC、PubMed、PLOS)的準確度。但是程式碼未公開。
\par
關於FRE可能要參考(Atta-ur-Rahman et al., 2019)
\par
[TLDR] A novel ontological framework for information extraction (OFIE) using fuzzy rule-base (FRB) and word sense disambiguation (WSD) is proposed and shown to be less sensitive to changes in the document format and has a significantly better average accuracy.},
  file = {C:\Users\BlackCat\Zotero\storage\3J26WS9Z\Zaman et al. - 2021 - An Ontological Framework for Information Extractio.pdf}
}

@article{GongFanWangMengJieRuanTongWangHaoFenLuHaoDianZiBingLiWenBenZhengZhuangZiDongShiBieFangFa2016,
  title = {电子病历文本症状自动识别方法},
  author = {{龚凡 王梦婕 阮彤 王昊奋 陆灏}},
  date = {2016},
  journaltitle = {医学信息学杂志},
  volume = {37},
  number = {7},
  pages = {7--14},
  publisher = {上海中医药大学附属曙光医院 上海201203\%华东理工大学 上海200237\%上海中医院大学附属曙光医院 上海201203},
  issn = {1673-6036},
  doi = {10.3969/j.issn.1673-6036.2016.07.002},
  abstract = {基于症状体系识别的难点,提出一种创新的基于症状构成模式的非监督学习方法来实现电子病历症状实体的自动抽取,介绍其总体过程并与基于CRF序列标注的监督学习方法进行比较,试验证明本文所提出的方法具有良好的识别效果和可扩展性。},
  langid = {chi},
  keywords = {医疗实体抽取,症状构成模式,结构化电子病历},
  annotation = {abstractTranslation:  基於體系識別的難點，提出一種創新的基於症狀構成模式的非監督學習方法來實現電子病歷症狀實體的自動抽取，介紹其總體流程並與基於CRF序列標籤的監督學習方法進行比較、試驗證明本文提出的方法具有良好的識別效果和可擴展性。\\
titleTranslation: 電子病歷文本症狀自動識別方法}
}

@article{grahamattwellPersonalLearningEnvironments2007,
  title = {Personal {{Learning Environments}} - the Future of {{eLearning}}?},
  author = {{Graham Attwell}},
  date = {2007},
  volume = {2},
  abstract = {This paper explores some of the ideas behind the Personal Learning Environment and considers why PLEs might be useful or indeed central to learning in the future. This is not so much a technical question as an educational one, although changing technologies are key drivers in educational change.},
  langid = {english},
  keywords = {PLE,已整理,已概覽},
  annotation = {abstractTranslation:  本文探討了個人學習環境背後的一些想法，並考慮了為什麼 PLE 可能對未來的學習有用或確實是核心。儘管不斷變化的技術是教育變革的關鍵驅動力，但這與其說是技術問題，不如說是教育問題。\\
titleTranslation: 個人學習環境－電子學習的未來？},
  note = {說明PLE在新科技(社群媒體)中的應用。較舊的論文，且認為PLE是教育或哲學問題而非技術問題。對於終身學習、PLE的實用性可能能參考但價值不大。},
  file = {C:\Users\BlackCat\Zotero\storage\7GFQ7U9R\Attwell - 2007 - Personal Learning Environments - the future of eLe.pdf}
}

@online{guangyuwangClinicalGPTLargeLanguage2023,
  title = {{{ClinicalGPT}}: {{Large Language Models Finetuned}} with {{Diverse Medical Data}} and {{Comprehensive Evaluation}}},
  shorttitle = {{{ClinicalGPT}}},
  author = {{Guangyu Wang} and {Guoxing Yang} and {Zongxin Du} and {Longjun Fan} and {Xiaohu Li}},
  date = {2023-06-16},
  eprint = {2306.09968},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.09968},
  url = {http://arxiv.org/abs/2306.09968},
  urldate = {2023-06-27},
  abstract = {Large language models have exhibited exceptional performance on various Natural Language Processing (NLP) tasks, leveraging techniques such as the pre-training, and instruction fine-tuning. Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience. In this study, we present ClinicalGPT, a language model explicitly designed and optimized for clinical scenarios. By incorporating extensive and diverse real-world data, such as medical records, domain-specific knowledge, and multi-round dialogue consultations in the training process, ClinicalGPT is better prepared to handle multiple clinical task. Furthermore, we introduce a comprehensive evaluation framework that includes medical knowledge question-answering, medical exams, patient consultations, and diagnostic analysis of medical records. Our results demonstrate that ClinicalGPT significantly outperforms other models in these tasks, highlighting the effectiveness of our approach in adapting large language models to the critical domain of healthcare.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,LLM,中文醫學,待讀,機器學習},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-06-28]\\
0 citations (Semantic Scholar/DOI) [2023-06-28]\\
titleTranslation: ClinicalGPT：根據多樣化醫學數據和綜合評估進行微調的大型語言模型\\
abstractTranslation:  大型語言模型利用預訓練和指令微調等技術，在各種自然語言處理 (NLP) 任務上表現出了卓越的性能。儘管取得了這些進步，但由於事實不准確、推理能力和缺乏現實經驗基礎等挑戰，它們在醫學應用中的有效性仍然有限。在這項研究中，我們提出了 ClinicalGPT，這是一種針對臨床場景明確設計和優化的語言模型。通過在訓練過程中融入廣泛且多樣化的現實世界數據，例如病歷、特定領域知識和多輪對話諮詢，ClinicalGPT可以更好地處理多種臨床任務。此外，我們還引入了綜合評估框架，包括醫學知識問答、體檢、患者諮詢和病歷診斷分析。我們的結果表明，ClinicalGPT 在這些任務中顯著優於其他模型，突顯了我們的方法在使大型語言模型適應醫療保健關鍵領域方面的有效性。},
  file = {C:\Users\BlackCat\Zotero\storage\U8BEW242\Wang et al. - 2023 - ClinicalGPT Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation.pdf}
}

@online{guantingdongBridgingKBTextGap2023,
  title = {Bridging the {{KB-Text Gap}}: {{Leveraging Structured Knowledge-aware Pre-training}} for {{KBQA}}},
  shorttitle = {Bridging the {{KB-Text Gap}}},
  author = {{Guanting Dong} and {Rumei Li} and {Sirui Wang} and {Yupeng Zhang} and {Yunsen Xian} and {Weiran Xu}},
  date = {2023-08-28},
  eprint = {2308.14436},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.14436},
  url = {http://arxiv.org/abs/2308.14436},
  urldate = {2023-08-30},
  abstract = {Knowledge Base Question Answering (KBQA) aims to answer natural language questions with factual information such as entities and relations in KBs. However, traditional Pre-trained Language Models (PLMs) are directly pre-trained on large-scale natural language corpus, which poses challenges for them in understanding and representing complex subgraphs in structured KBs. To bridge the gap between texts and structured KBs, we propose a Structured Knowledge-aware Pre-training method (SKP). In the pre-training stage, we introduce two novel structured knowledge-aware tasks, guiding the model to effectively learn the implicit relationship and better representations of complex subgraphs. In downstream KBQA task, we further design an efficient linearization strategy and an interval attention mechanism, which assist the model to better encode complex subgraphs and shield the interference of irrelevant subgraphs during reasoning respectively. Detailed experiments and analyses on WebQSP verify the effectiveness of SKP, especially the significant improvement in subgraph retrieval (+4.08\% H@10).},
  langid = {english},
  pubstate = {preprint},
  keywords = {BERT,Computer Science - Computation and Language,Computer Science - Information Retrieval,問答系統,完整公開,已整理,知識系統,預訓練},
  annotation = {titleTranslation: 彌合 KB 與文本之間的差距：利用 KBQA 的結構化知識感知預訓練\\
abstractTranslation:  知識庫問答（KBQA）旨在利用知識庫中的實體和關係等事實信息回答自然語言問題。然而，傳統的預訓練語言模型（PLM）是直接在大規模自然語言語料庫上進行預訓練的，這給它們在理解和表示結構化知識庫中的複雜子圖方面帶來了挑戰。為了彌合文本和結構化知識庫之間的差距，我們提出了一種結構化知識感知預訓練方法（SKP）。在預訓練階段，我們引入了兩個新穎的結構化知識感知任務，指導模型有效地學習複雜子圖的隱式關係和更好的表示。在下游的KBQA任務中，我們進一步設計了高效的線性化策略和區間注意力機制，分別幫助模型更好地編碼複雜子圖並屏蔽推理過程中不相關子圖的干擾。 WebQSP 上的詳細實驗和分析驗證了 SKP 的有效性，特別是在子圖檢索方面的顯著改進（+4.08\% H@10）。},
  note = {通過設計新的預訓練任務來減少LLM在文本上預訓練及在結構化資料上學習之間的差距，提升LLM對於結構化資料的讀取能力，來提升最終問答系統的準確度。
\par
Accepted as a short paper at CIKM 2023
\par
10月將發表\href{https://uobevents.eventsair.com/cikm2023/}{https://uobevents.eventsair.com/cikm2023/}},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\ZCHDLDN7\\Dong 等。 - 2023 - Bridging the KB-Text Gap Leveraging Structured Kn.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\9K65N9K8\\2308.html}
}

@inproceedings{guanTraditionalChineseMedicine2021,
  title = {A {{Traditional Chinese Medicine Terminology Recognition Model Based}} on {{Deep Learning}}: {{A TCM Terminology Recognition Model}}},
  shorttitle = {A {{Traditional Chinese Medicine Terminology Recognition Model Based}} on {{Deep Learning}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Big Data}} and {{Computing}}},
  author = {Guan, Yu and Li, Huan and Xu, Wenjing},
  year = {10 月 6, 2021},
  series = {{{ICBDC}} '21},
  pages = {15--20},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3469968.3469971},
  url = {https://dl.acm.org/doi/10.1145/3469968.3469971},
  urldate = {2024-01-10},
  abstract = {Although the classics of traditional Chinese medicine are extensive and profound, the application and research of the deep learning model in traditional Chinese medicine terminology recognition stay scarce. In order to make use of the knowledge in these classics, a traditional Chinese medicine terminology recognition model named BERT-BiLSTM-CRF is presented and achieve superior performance as shown in the Results section. The design process of the BERT-BiLSTM-CRF model combines the transfer learning strategy, the pre-training language model, and the classical machine learning model. Specifically, the semantic features of TCM sample sequences is first extracted by transferring a BERT model that pre-trained on other large-scale Chinese corpora. Then go to the BiLSTM module to abstract the semantic features of sequence context. And last, introduce CRF to learn the transfer features between context tags. In the experiment, the BERT-BiLSTM-CRF model is compared with a variety of benchmark models and outperforms others.},
  isbn = {978-1-4503-8980-8},
  langid = {english},
  keywords = {Deep learning,Named entity recognition,Natural language processing,NLP,TCM terminology recognition,Traditional Chinese medicine,中醫,已整理,機器學習},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於深度學習的中醫術語辨識模型：中醫術語辨識模型},
  note = {以機器學習做中醫辨證，未公開資料集，但有3000\textasciitilde 10000筆。},
  file = {C:\Users\BlackCat\Zotero\storage\FV7ECKQW\Guan et al. - 2021 - A Traditional Chinese Medicine Terminology Recogni.pdf}
}

@inproceedings{guanwangHybridPatternKnowledge2022,
  title = {A {{Hybrid Pattern Knowledge Graph-Based API Recommendation Approach}}},
  booktitle = {Artificial {{Intelligence}}},
  author = {{Guan Wang} and {Weidong Wang} and {Dian Li}},
  editor = {{Lu Fang} and {Daniel Povey} and {Guangtao Zhai} and {Tao Mei} and {Ruiping Wang}},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {465--476},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-20503-3_37},
  abstract = {There are a large number of application program interfaces (APIs) on the Internet. Due to frequently updating APIs, programmers are prompted to frequently consult API documents in the process of software development. However, the traditional query approaches of the documents have certain limitations. For example, the programmers need to know the API name as a prerequisite and are often unable to obtain the expected search results because of the difference in understanding between the description of the problem and the description of the documents. Only these “known-unknown” information can be queried, and it is difficult to query the “unknown-unknown” information. To address the limitations, we establish the knowledge graph of software source code combined with knowledge which is derived from the documents, Github project code warehouse, Stack overflow platform, and local project code warehouse. Moreover, we propose a hybrid pattern knowledge graph-based API recommendation approach for programmers to complete the query task of unknown-unknown information. Finally, we constructed large-scale real experiments. Evaluation results prove that the proposed approach significantly outperforms the state-of-the-art approach.},
  isbn = {978-3-031-20503-3},
  langid = {english},
  keywords = {API,Knowledge graph,Machine learning,人機互動,問答系統,已整理,知識圖譜},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 一種基於混合模式知識圖譜的API推薦方法},
  file = {C:\Users\BlackCat\Zotero\storage\ZP34P96C\Wang et al. - 2022 - A Hybrid Pattern Knowledge Graph-Based API Recomme.pdf}
}

@article{GuanYiWangRunQiLiXueLiHuangYuLiHeBinZhongWenDianZiBingLiMingMingShiTiShiBieDeZhuDongXueXiFangFaYanJiu2017,
  title = {中文电子病历命名实体识别的主动学习方法研究},
  author = {{关毅王润奇 李雪莉 黄玉丽 何彬}},
  date = {2017},
  journaltitle = {中国数字医学},
  volume = {12},
  number = {10},
  pages = {51--53},
  issn = {1673-7571},
  keywords = {No DOI found,知網}
}

@inproceedings{gui-minjieDesignApplicationMultidimensional2020,
  title = {Design and {{Application}} of {{Multidimensional Analysis System}} for {{Medical Literature Retrieval}}:},
  shorttitle = {Design and {{Application}} of {{Multidimensional Analysis System}} for {{Medical Literature Retrieval}}},
  booktitle = {Proceedings of the 2020 9th {{International Conference}} on {{Applied Science}}, {{Engineering}} and {{Technology}} ({{ICASET}} 2020)},
  author = {{Gui-Min Jie} and {Zhen-Guo Wang} and {Zhen-Lei Yu}},
  date = {2020},
  publisher = {Atlantis Press},
  location = {Qingdao, China},
  doi = {10.2991/aer.k.201203.020},
  url = {https://www.atlantis-press.com/article/125947516},
  urldate = {2022-09-19},
  eventtitle = {2020 9th {{International Conference}} on {{Applied Science}}, {{Engineering}} and {{Technology}} ({{ICASET}} 2020)},
  isbn = {978-94-6239-287-8},
  langid = {english},
  annotation = {0 citations (Crossref) [2024-03-26]\\
0 citations (Semantic Scholar/DOI) [2022-10-26]\\
titleTranslation: 醫學文獻檢索多維分析系統的設計與應用：},
  note = {臨床病歷之多維分析，不確定是不是中醫相關},
  file = {C:\Users\BlackCat\Zotero\storage\E8DQ8LPX\Jie 等。 - 2020 - Design and Application of Multidimensional Analysi.pdf}
}

@article{guilhermeavelinoWhoCanMaintain2019,
  title = {Who {{Can Maintain This Code}}?: {{Assessing}} the {{Effectiveness}} of {{Repository-Mining Techniques}} for {{Identifying Software Maintainers}}},
  shorttitle = {Who {{Can Maintain This Code}}?},
  author = {{Guilherme Avelino} and {Leonardo Passos} and {Fabio Petrillo} and {Marco Tulio Valente}},
  date = {2019-01},
  journaltitle = {IEEE Software},
  volume = {36},
  number = {6},
  pages = {34--42},
  issn = {1937-4194},
  doi = {10.1109/MS.2018.185140155},
  abstract = {In large and complex systems, identifying developers capable of maintaining a piece of code is an essential task. Repository-mining techniques can help by providing some level of automation; however, whether such techniques effectively identify skilled software maintainers is still unclear.},
  eventtitle = {{{IEEE Software}}},
  langid = {english},
  keywords = {Data mining,distribution maintenance and enhancement,History,Linear regression,maintenance management,management,Object recognition,Open source software,programming teams,Software,software engineering,Task analysis},
  annotation = {9 citations (Crossref) [2024-03-26]\\
titleTranslation: 誰可以維護此代碼？：評估用於識別軟件維護者的存儲庫挖掘技術的有效性\\
abstractTranslation:  在大型複雜的系統中，識別有能力維護一段代碼的開發人員是一項重要任務。存儲庫挖掘技術可以通過提供一定程度的自動化來提供幫助；然而，此類技術是否能有效識別熟練的軟件維護人員仍不清楚。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\BGDHAWV6\\Avelino 等。 - 2019 - Who Can Maintain This Code Assessing the Effecti.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\E8YDM9RB\\8328969.html}
}

@thesis{GuoHuiXiangZhongYiZhengZhuangCiKuDeYanZhi2013,
  title = {中醫症狀詞庫的研製},
  author = {{郭慧翔}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2013},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/b4xna2},
  abstract = {中醫的診斷治療是以「辨證論治」的方式進行。中醫師會先根據望、聞、問、切四診收集病患的症狀及體徵資料，再根據收集到的症狀及體徵資料進行詳盡地分析，最後歸納出疾病的病因、病位、及病機，此即為「辨證」。中醫師最後根據辨證結果對病人進行適當的治療及用藥，此即為「論治」。我們希望研製一套中醫自動辨證系統，做為中醫的研究平台。當我們開始研製系統時，首先遇到了中醫症狀未標準化的困難，因此在研製中醫自動辨證系統前，應該先處理症狀標準化的問題。 本研究建立了一套中醫症狀詞庫系統，用來解決中醫症狀標準化的問題。我們除了使用「症狀字詞比對」的方式進行症狀標準化，還增加了使用「關鍵字詞比對」的方式進行症狀標準化。如果原始症狀已經過標準化，存在資料庫中，系統可透過症狀字詞比對正確地查詢到對應的標準症狀。如果原始症狀尚未經過標準化，不在資料庫中，系統會先將原始症狀拆成一串關鍵字詞，再透過關鍵字詞比對的方式，查詢這些關鍵字詞的語義。最後系統依據這些關鍵字詞的語義，查詢語義最接近的標準症狀。本研究顯示，運用語義資訊，可以明顯地提昇系統查詢標準症狀的正確率。},
  pagetotal = {53},
  keywords = {中醫,實驗室,標準化,病歷分析},
  file = {C:\Users\BlackCat\Zotero\storage\CWPJ5X38\郭慧翔 - 2013 - 中醫症狀詞庫的研製.pdf}
}

@thesis{GuoJunYiZhiYuanZiChuanJiQunJiZiLiaoXingTaiDeXianZhiShiCeShiAnLiChanSheng2016,
  title = {支援字串及群集資料型態的限制式測試案例產生},
  author = {{郭俊毅}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2016},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/5nbx8g},
  abstract = {軟體測試是確保軟體品質的主要方法。測試案例的自動產生目前仍是一個非常具挑戰性的問題。限制式測試案例產生是一個非常具潛力的技術。限制式測試案例產生技術將測試案例產生問題定義為限制滿足問題，透過解限制滿足問題產生測試案例。我們曾經利用限制邏輯程式系統ECLiPSe開發一個黑箱函式層級單元測試的限制式測試案例產生器。這個測試案例產生器可以完全支援整數資料型態的測試案例產生，但沒有完全支援其他資料型態的測試案例產生。雖然ECLiPSe有支援非整數型態的限制敘述，但ECLiPSe並沒有支援自動設定非整數型態值域的機制，與自動將值域裡的值賦予非整數型態變數的機制。本論文實作了自動設定字串型態與群集型態值域的機制，及自動將值域裡的值賦予字串型態與群集型態變數的機制。這些機制的提供同時也改善了字串型態及群集型態的限制敘述的求解效率。},
  pagetotal = {95}
}

@article{GuoKunJiYuYiCunGuanXiHeDaoPaiSuoYinDeZhongYiDianZiBingLiJianSuoFangFa2020,
  title = {基于依存关系和倒排索引的中医电子病历检索方法},
  author = {{郭坤} and {丁有伟}},
  date = {2020},
  journaltitle = {计算机时代},
  number = {12},
  pages = {4},
  doi = {10.16644/j.cnki.cn33-1094/tp.2020.12.015},
  abstract = {医疗信息化背景下,依托大数据的智慧医疗成为研究热点,而电子病历检索作为中医数据处理的基础操作,其性能直接影响到上层统计分析和挖掘应用的性能.目前中医电子病历采集和存储尚未形成统一标准,存在大量非结构化数据,基于传统关系型数据库的检索方式已不再适用.文章提出一种基于依存关系和倒排索引的中医电子病历高效检索方法,通过挖掘关键词之间的依存关系,使用二元组表示,并为二元组建立倒排索引以提高检索效率.该方法根据关键词之间的依存关系重构原文,创建倒排索引提高检索性能,保证海量数据的高效访问.},
  keywords = {中醫,依存關係,倒排索引,結果重構,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\LQS3ZU2M\郭坤 與 丁有伟 - 2020 - 基于依存关系和倒排索引的中医电子病历检索方法.pdf}
}

@article{GuoRongChuanJiYuBPShenJingWangLuoDeZhongYiBianZhengMoXingGouJianFangFa2022,
  title = {基于BP神经网络的中医辩证模型构建方法},
  author = {{郭荣传} and {曾青霞} and {胡鑫才}},
  date = {2022},
  journaltitle = {现代商贸工业},
  shortjournal = {Modern Business Trade Industry},
  number = {15},
  pages = {239--240},
  doi = {10.19311/j.cnki.1672-3198.2022.15.095},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5Eg94ZHNtZ3kyMDIyMTUwOTUaCGgyMXpraGJx},
  urldate = {2022-08-14},
  abstract = {[目的]以太阴风湿表证辨证模型为例,探讨中医辨证模型构建方法.[方法]以江西中医药大学中医门诊规范化培训改革试点基地的"岐黄中医门诊规培系统"中2600例中医电子病历为样本数据,在中医理论指导下创建医案症状关键字词典,训练词向量模型,将其作为BP神经网络的输入,将2080例医案作为训练数据,剩余520份病例作为测试数据.[结果]该辨证模型的准确率为88.29％.[结论]本文利用BP神经网络技术,构建了太阴风湿表证的系统中医辨证模型,准确率较高,为名老中医智能辨证提供了一条新的途径,值得推广.},
  langid = {zh\_CN},
  keywords = {BP神经网络,Modern Business Trade Industry,太阴风湿表证,機器學習,现代商贸工业,辨證},
  annotation = {江西中医药大学岐黄国医书院,江西 南昌330025\\
江西省中医药管理局科技计划一般项目 江西省卫生健康委科技计划项目\\
2022-06-27 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 基於BP神經網絡的中醫辯證模型構建方法\\
abstractTranslation:  []以太陰風濕表證辨證模型，探討中醫辨證模型構建方法。[方法]以江西中醫藥大學中醫規範化培訓改革主題基地的《岐中醫黃中醫規范培系統》中2600例中醫電子病歷為樣本數據，在中醫理論指導下創建醫案症狀關鍵字搜索，訓練詞提供模型，將其作為BP神經網絡的輸入，將2080例醫案作為訓練數據，剩餘520份病例作為測試數據。[結果]該辨證模型的準確率為88.29\%。[結論]本文利用BP神經網絡技術，構建了太陰風濕表證的系統中醫辨證模型，準確率較高，為名老中醫智能辨證提供了一條新的途徑,值得推廣。},
  file = {C:\Users\BlackCat\Zotero\storage\SAQRTZBK\郭 等。 - 2022 - 基于BP神经网络的中医辩证模型构建方法.pdf}
}

@article{GuoRongChuanZhongYiDianZiBingLiXiTongDeSheJiYuYingYong2018,
  title = {中医电子病历系统的设计与应用},
  author = {{郭荣传} and {张光荣}},
  date = {2018},
  journaltitle = {江西中医药大学学报},
  volume = {30},
  number = {5},
  pages = {4},
  keywords = {中醫,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\DSB69AXF\郭荣传 與 张光荣 - 2018 - 中医电子病历系统的设计与应用.pdf}
}

@article{h.alperencetinAnalyzingDeveloperContributions2022,
  title = {Analyzing Developer Contributions Using Artifact Traceability Graphs},
  author = {{H. Alperen Çetin} and {Eray Tüzün}},
  date = {2022-03-28},
  journaltitle = {Empirical Software Engineering},
  shortjournal = {Empir Software Eng},
  volume = {27},
  number = {3},
  pages = {77},
  issn = {1573-7616},
  doi = {10.1007/s10664-022-10129-2},
  url = {https://doi.org/10.1007/s10664-022-10129-2},
  urldate = {2023-08-24},
  abstract = {In a software project, properly analyzing the contributions of developers could provide valuable insights for decision-makers. The contributions of a developer could be in many different forms such as committing and reviewing code, opening and resolving issues. Previous approaches mainly consider the commit-based contributions which provide an incomplete picture of developer contributions.},
  langid = {english},
  keywords = {Artifact traceability graphs,Developer replacement,Developer turnover,Key developers,Knowledge distribution,Social networks,應用,知識圖譜,知識本體,程式碼分析},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用工件可追溯性圖分析開發人員的貢獻\\
abstractTranslation:  在軟件項目中，正確分析開發人員的貢獻可以為決策者提供有價值的見解。開發人員的貢獻可以有多種不同的形式，例如提交和審查代碼、提出和解決問題。以前的方法主要考慮基於提交的貢獻，這提供了開發人員貢獻的不完整情況。},
  file = {C:\Users\BlackCat\Zotero\storage\HIF48X6Y\Çetin 與 Tüzün - 2022 - Analyzing developer contributions using artifact t.pdf}
}

@inproceedings{hadzhievImplementationDataInformation2022,
  title = {Implementation of a {{Data}} and {{Information Management System Based}} on a {{Hybrid Model}} for {{Structuring}}, {{Storing}}, and {{Processing Distributed Data}} on the {{Internet}}},
  booktitle = {2022 13th {{International Conference}} on {{Computing Communication}} and {{Networking Technologies}} ({{ICCCNT}})},
  author = {Hadzhiev, Velin and Rashidov, Aldeniz},
  date = {2022-10},
  pages = {1--6},
  doi = {10.1109/ICCCNT54827.2022.9984384},
  url = {https://ieeexplore.ieee.org/abstract/document/9984384},
  urldate = {2024-04-06},
  abstract = {This article presents a simple way for data management by means of a Web-based information system, which provides an opportunity to perform operations on structuring, storing and processing data from users on the Internet, and the management of these data in various processes (scientific, researching, etc.). The system is based on a hybrid model for structuring, storing, and processing data on the Internet. The description of the data structure is carried out in a way that is understandable for every user of the global network. This allows database design and construction to be carried out by any user in the global network, even if this user is not a specialist in this field and significantly shortens the database design time. In addition, each user can share his data with other users, which increases the amount of structured data on the Internet. That provides users to work together on various projects, perform analysis of the shared data and extract valuable information.},
  eventtitle = {2022 13th {{International Conference}} on {{Computing Communication}} and {{Networking Technologies}} ({{ICCCNT}})},
  langid = {english},
  keywords = {Centralized systems,Costs,Data management,Data processing,Data store,Database,Distributed databases,Information System,Internet,Maintenance engineering,NoSQL databases,Relational databases,Scalability,Solid modeling,Workflows,未整理},
  annotation = {titleTranslation: 基於混合模型的數據和資訊管理系統的實現，用於建立、儲存和處理互聯網上的分散式數據\\
abstractTranslation:  本文提出了一種透過基於Web 的資訊系統進行資料管理的簡單方法，該系統提供了對互聯網上用戶的資料進行結構化、儲存和處理的操作，以及在各種流程中管理這些資料的機會（科學的資料管理）。 、研究等）。該系統基於用於在互聯網上建立、儲存和處理資料的混合模型。資料結構的描述以全球網路的每個使用者都可以理解的方式進行。這使得全球網路中的任何用戶都可以進行資料庫設計和構建，即使該用戶不是該領域的專家，並且顯著縮短了資料庫設計時間。此外，每個使用者都可以與其他使用者分享他的數據，這增加了網路上結構化資料的數量。這使用戶可以在各種項目上一起工作，對共享數據進行分析並提取有價值的資訊。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\BRFMFPUF\\Hadzhiev 與 Rashidov - 2022 - Implementation of a Data and Information Managemen.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\I6GWIEL4\\9984384.html}
}

@article{haiv.phamFuzzyKnowledgeGraph2023,
  title = {A {{Fuzzy Knowledge Graph Pairs-Based Application}} for {{Classification}} in {{Decision Making}}: {{Case Study}} of {{Preeclampsia Signs}}},
  author = {{Hai V. Pham} and {Cu K. Long} and {Phan H. Khanh} and {Ha Q. Trung}},
  date = {2023},
  journaltitle = {Information},
  volume = {14},
  number = {2},
  issn = {2078-2489},
  doi = {10.3390/info14020104},
  abstract = {Problems of preeclampsia sign diagnosis are mostly based on symptom data with the characteristics of data collected periodically in uncertain, ambiguous, and obstetrician opinions. To reduce the effects of preeclampsia, many studies have investigated the disease, prevention, and complication. Conventional fuzzy inference techniques can solve several diagnosis problems in health such as fuzzy inference systems (FIS), and Mamdani complex fuzzy inference systems with rule reduction (M-CFIS-R), however, the computation time is quite high. Recently, the research direction of approximate inference based on fuzzy knowledge graph (FKG) has been proposed in the M-CFIS-FKG model with the combination of regimens in traditional medicine and subclinical data gathered from medical records. The paper has presented a proposed model of FKG-Pairs3 to support patients\&rsquo; disease diagnosis, together with doctors\&rsquo; preferences in decision-making. The proposed model has been implemented in real-world applications for disease diagnosis in traditional medicine based on input data sets with vague information, quantified by doctor\&rsquo;s preferences. To validate the proposed model, it has been tested in a real-world case study of preeclampsia signs in a hospital for disease diagnosis with the traditional medicine approach. Experimental results show that the proposed model has demonstrated the model\&rsquo;s effectiveness in the decision-making of preeclampsia signs.},
  langid = {english},
  keywords = {decision making,disease diagnosis,FKG-Pairs,fuzzy knowledge graph,preeclampsia},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於模糊知識圖對的決策分類應用：子癇前症體徵的案例研究\\
abstractTranslation:  子癇前症的徵兆診斷問題大多基於症狀數據，其特徵是定期收集的數據具有不確定性、模糊性和產科醫生意見。為了減少子癇前症的影響，許多研究調查了疾病、預防和併發症。傳統的模糊推理技術可以解決健康中的多種診斷問題，例如模糊推理系統（FIS）和規則縮減的Mamdani複雜模糊推理系統（M-CFIS-R），但計算時間相當長。最近，結合傳統醫學的治療方案和從病歷中收集的亞臨床數據，在M-CFIS-FKG模型中提出了基於模糊知識圖（FKG）的近似推理的研究方向。該論文提出了一個 FKG-Pairs3 模型來支持患者的疾病診斷以及醫生的決策偏好。所提出的模型已在傳統醫學疾病診斷的現實應用中實現，該應用基於具有模糊資訊的輸入資料集，並根據醫生的偏好進行量化。為了驗證所提出的模型，它已經在醫院的子癇前症症狀的真實案例研究中進行了測試，以使用傳統醫學方法進行疾病診斷。實驗結果表明，所提出的模型證明了該模型在子癇前症決策方面的有效性。},
  file = {C:\Users\BlackCat\Zotero\storage\GGCH9QMQ\Pham 等。 - 2023 - A Fuzzy Knowledge Graph Pairs-Based Application fo.pdf}
}

@article{hamidrezarostamiSemanticallyRichLearning2021,
  title = {The {{Semantically Rich Learning Environments}}: {{A Systematic Literature Review}}},
  shorttitle = {The {{Semantically Rich Learning Environments}}},
  author = {{Hamidreza Rostami} and {Shaban Elahi} and {Ali Moeini} and {Alireza Hassanzadeh}},
  year = {1 月 2021},
  journaltitle = {International Journal of Digital Content Management},
  shortjournal = {International Journal of Digital Content Management},
  volume = {2},
  number = {2},
  doi = {10.22054/dcm.2021.13067},
  url = {https://doi.org/10.22054/dcm.2021.13067},
  urldate = {2023-10-13},
  langid = {english},
  keywords = {Review,待讀},
  annotation = {titleTranslation: 語意豐富的學習環境：系統性文獻綜述},
  file = {D:\Paper\Rostami et al. - 2021 - The Semantically Rich Learning Environments A Sys.pdf}
}

@article{HanHongQiDaGuiMoZhuTiCiZiDongBiaoYinFangFa2022,
  title = {大规模主题词自动标引方法},
  author = {{韩红旗} and {桂婕} and {张运良} and {翁梦娟} and {薛陕} and {悦林东}},
  date = {2022},
  journaltitle = {Qing bao xue bao},
  volume = {41},
  number = {5},
  pages = {475--485},
  publisher = {中国科学技术信息研究所,北京 100038},
  issn = {1000-0135},
  doi = {10.3772/j.issn.1000-0135.2022.05.004},
  url = {http://www.wanfangdata.com.cn/details/detail.do?_type=perio&id=qbxb202205004},
  urldate = {2023-09-13},
  abstract = {现有的主题标引方法一般只能抽取文本中出现的词汇,无法从几万或数十万主题词中选择语义关联强且未出现的词汇;基于机器学习的多标签分类算法则需要每一个标签下有训练数据,限制了它们在主题标引上的应用.面向大规模主题词在海量文献上的标引需求,提出一个基于分布式词向量的混合型自动标引方法,利用大规模语料训练的词向量生成同维度的主题词表示向量和文本表示向量,实现主题词与文本语义相似度的计算.基于大规模语料构建主题词与普通词的映射表,使文本向量只和少量的语义强相关主题词向量比较,大大减少了计算量,提高了标引效率.开发的自动标引工具对近亿篇文献进行了主题标引,达到了较高的速度.与结巴关键词的实验对比结果显示,本文方法抽取的主题词与作者关键词重合度较低,且在去除结巴关键词中的非主题词后,取得了比结巴关键词更高的标引准确率;与人工标引的实验对比结果显示,随着人工标引词数量的增加,本文方法的效果、结果与人工标引结果的一致性在不断增加.},
  langid = {chinese},
  annotation = {titleTranslation: 大規模主題詞自動引用方法\\
abstractTranslation:  現有的主題標引方法一般只能抽取文本中出現的詞彙，無法從幾萬或剩餘萬個主題詞中選擇語義關聯強且未出現的詞彙；基於機器學習的多標籤分類算法則需要每一個標籤下有訓練數據，限制了它們在主題標引上的應用。 針對大規模主題詞在海量文獻上的標引需求，提出一種基於全球化詞服務的混合型自動標引方法，利用大規模語料訓練的詞支持生成同維度的主題詞表示支持和文本表示支持，實現主題詞與文本語義相似度的計算。基於大規模語料構建主題詞與普通詞的映射表，使文本支持只和少量的語義強相關主題詞處理比較，大大減少了計算量，提高了標引效率。開發的自動標引工具對近億篇文獻進行了主題標引，達到了以上的速度。與結巴關鍵詞的實驗對比結果作者顯示，論文方法抽取的主題詞與重合度較低，且在達到結巴關鍵詞中的非主題詞後，取得了比結巴關鍵詞更高的標引準確率；與人工標引的實驗對比結果顯示，隨著人工標引詞數量的增加，論文方法的效果、結果與人工標引結果的一致性在不斷增加。},
  file = {C:\Users\BlackCat\Zotero\storage\N4VNP4WJ\韩红旗 等。 - 2022 - 大规模主题词自动标引方法.pdf}
}

@article{hanqingzhangSurveyControllableText2023,
  title = {A {{Survey}} of {{Controllable Text Generation}} Using {{Transformer-based Pre-trained Language Models}}},
  author = {{Hanqing Zhang} and {HaoLin Song} and {Shaoyu Li} and {Ming Zhou} and {Dawei Song}},
  year = {8 月 30, 2023},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  issn = {0360-0300},
  doi = {10.1145/3617680},
  url = {https://dl.acm.org/doi/10.1145/3617680},
  urldate = {2023-09-09},
  abstract = {Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks that require different types of controlled constraints. In this paper, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey paper to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.},
  langid = {english},
  keywords = {controllability,controllable text generation,CTG,pre-trained language models,Survey,systematic review,transformer,已整理,機器學習,預訓練},
  annotation = {22 citations (Crossref) [2024-03-26]\\
Just Accepted\\
titleTranslation: 使用基於 Transformer 的預訓練語言模型進行可控文本生成的調查\\
abstractTranslation:  可控文本生成（CTG）是自然語言生成（NLG）領域的新興領域。它被認為對於開發更好地滿足實際應用中的特定約束的先進文本生成技術至關重要。近年來，使用大規模預訓練語言模型（PLM）的方法，特別是廣泛使用的基於 Transformer 的 PLM，已成為 NLG 的新範式，可以生成更加多樣化和流暢的文本。然而，由於深度神經網絡的可解釋性水平有限，這些方法的可控性需要保證。為此，使用基於 Transformer 的 PLM 進行可控文本生成已成為快速增長但具有挑戰性的新研究熱點。近 3-4 年來出現了多種方法，針對需要不同類型受控約束的不同 CTG 任務。在本文中，我們對該領域的常見任務、主要方法和評估方法進行了系統的批判性回顧。最後，我們討論了該領域面臨的挑戰，並提出了各種有前景的未來方向。據我們所知，這是第一篇從基於 Transformer 的 PLM 角度總結最先進 CTG 技術的調查論文。我們希望它能夠幫助相關領域的研究人員和實踐者快速追踪學術和技術前沿，為他們提供該領域的概況和未來研究的路線圖。},
  note = {以預訓練語言模型PLM的角度總結可控文本生成CTG技術。相關性較低。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\D4WE4A88\\Zhang 等。 - 2023 - A Survey of Controllable Text Generation using Tra.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\SG2AE4ZV\\A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models.pdf}
}

@article{haoliuOntologybasedCategorizationClinical2022,
  title = {Ontology-Based Categorization of Clinical Studies by Their Conditions},
  author = {{Hao Liu} and {Simona Carini} and {Zhehuan Chen} and {Spencer Phillips Hey} and {Ida Sim} and {Chunhua Weng}},
  year = {11 月 1, 2022},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {J. of Biomedical Informatics},
  volume = {135},
  number = {C},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2022.104235},
  url = {https://doi.org/10.1016/j.jbi.2022.104235},
  urldate = {2023-09-27},
  abstract = {Display Omitted The free-text Condition data field in the ClinicalTrials.gov is not amenable to computational processes for retrieving, aggregating and visualizing clinical studies by condition categories. This paper contributes a method for automated ontology-based categorization of clinical studies by their conditions. Our method first maps text entries in ClinicalTrials.gov’s Condition field to standard condition concepts in the OMOP Common Data Model by using SNOMED CT as a reference ontology and using Usagi for concept normalization, followed by hierarchical traversal of the SNOMED ontology for concept expansion, ontology-driven condition categorization, and visualization. We compared the accuracy of this method to that of the MeSH-based method. We reviewed the 4,506 studies on Vivli.org categorized by our method. Condition terms of 4,501 (99.89\%) studies were successfully mapped to SNOMED CT concepts, and with a minimum concept mapping score threshold, 4,428 (98.27\%) studies were categorized into 31 predefined categories. When validating with manual categorization results on a random sample of 300 studies, our method achieved an estimated categorization accuracy of 95.7\%, while the MeSH-based method had an accuracy of 85.0\%. We showed that categorizing clinical studies using their Condition terms with referencing to SNOMED CT achieved a better accuracy and coverage than using MeSH terms. The proposed ontology-driven condition categorization was useful to create accurate clinical study categorization that enables clinical researchers to aggregate evidence from a large number of clinical studies.},
  keywords = {Categorization,Clinical Study,Data Visualization,Ontology,SNOMED CT,未整理},
  annotation = {4 citations (Crossref) [2024-03-26]}
}

@inproceedings{haominzhaoKnowledgeGraphBased2022,
  title = {Knowledge {{Graph}} Based {{Question Pair Matching}} for {{Domain-Oriented FAQ System}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {{Haomin Zhao} and {Yu Liu} and {Along Hou} and {Jinguang Gu}},
  date = {2022-10},
  pages = {2103--2108},
  issn = {2577-1655},
  doi = {10.1109/SMC53654.2022.9945243},
  url = {https://ieeexplore.ieee.org/document/9945243},
  urldate = {2023-11-23},
  abstract = {The matching methods of question pair in the most FAQ system are less optimized for the specific domains, where proper nouns and irregular expressions always exist in the questions. To address the above problems, we propose a knowledge filtering method and the FK-BERT (FAQ-oriented knowledge-enabled BERT) model, which make full use of the domain knowledge graph. In the knowledge filtering stage, the semantic relationships between candidate entities and question sentences are thoroughly evaluated to choose the applicable entities. Furthermore, FK-BERT model considers both the relationships between entities within a single question, as well as the relationships of entities between two questions. The experimental results show that the joint use of knowledge filtering and FK-BERT model can improve the performance of FAQ systems for the domain of operating system.},
  eventtitle = {2022 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  langid = {english},
  keywords = {問答系統,已整理,機器學習,語意匹配},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於知識圖譜的面向領域常見問題解答系統的問題對匹配},
  note = {這篇研究參考更多資訊來將使用者提出的自然語言問題與開發者設計的問題庫來做匹配。與研究主題稍微相關。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\HUWGQQSR\\Zhao et al. - 2022 - Knowledge Graph based Question Pair Matching for D.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\CK362Z8M\\9945243.html}
}

@article{harshadashrivastavInformationOverloadOrganization2021,
  title = {Information {{Overload}} in {{Organization}}: {{Impact}} on {{Decision Making}} and {{Influencing Strategies}}},
  shorttitle = {Information {{Overload}} in {{Organization}}},
  author = {{Harshada Shrivastav} and {Elif Kongar}},
  date = {2021-05-17},
  journaltitle = {2021 IEEE Technology \& Engineering Management Conference - Europe (TEMSCON-EUR)},
  pages = {1--5},
  publisher = {IEEE},
  location = {Dubrovnik, Croatia},
  doi = {10.1109/TEMSCON-EUR52034.2021.9488649},
  url = {https://ieeexplore.ieee.org/document/9488649/},
  urldate = {2023-10-23},
  abstract = {In the last decade, we have experienced rapid advancement in technology that is impacting every area of our lives. The world has grown to a level where people are able to quickly learn and adapt to newly emerging technologies. The technological changes in the organizational sector have changed the way of communication and content management systems. Several modes of communication are available in today’s organizational environment; instant messaging, email communication, data flowing from social networks at increasing velocity and volume. These new technologies together are generating large amounts of data, making its utilization for effective and efficient decision making has become an important challenge for many businesses. Is this rapidly increasing data causing information overload and hindering decision-making processes? This study investigates the problem of information overload via quantitative analysis.},
  eventtitle = {2021 {{IEEE Technology}} \& {{Engineering Management Conference}} - {{Europe}} ({{TEMSCON-EUR}})},
  isbn = {9781665440912},
  langid = {english},
  keywords = {資訊超載},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 組織中資訊的過載：對決策和策略的影響\\
abstractTranslation:  在過去的十年中，我們經歷了科技的快速進步，這些進步正在影響我們生活的各個領域。世界已經發展到人們能夠快速學習和適應新興技術的水平。組織領域的技術變革改變了溝通方式和內容管理系統。在現今的組織環境中可以使用多種溝通模式；即時訊息、電子郵件通訊、來自社交網路的數據以越來越快的速度和數量流動。這些新技術共同產生大量數據，利用其進行有效且有效率的決策已成為許多企業面臨的重要挑戰。這種快速增長的數據是否會導致資訊過載並阻礙決策過程？本研究透過量化分析研究資訊超載問題。},
  note = {[TLDR] This study investigates the problem of information overload via quantitative analysis by investigating the way the technological changes in the organizational sector have changed the way of communication and content management systems.}
}

@online{hashemiDenseRetrievalAdaptation2023,
  title = {Dense {{Retrieval Adaptation}} Using {{Target Domain Description}}},
  author = {Hashemi, Helia and Zhuang, Yong and Kothur, Sachith Sri Ram and Prasad, Srivas and Meij, Edgar and Croft, W. Bruce},
  date = {2023-07-05},
  eprint = {2307.02740},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.02740},
  url = {http://arxiv.org/abs/2307.02740},
  urldate = {2024-05-02},
  abstract = {In information retrieval (IR), domain adaptation is the process of adapting a retrieval model to a new domain whose data distribution is different from the source domain. Existing methods in this area focus on unsupervised domain adaptation where they have access to the target document collection or supervised (often few-shot) domain adaptation where they additionally have access to (limited) labeled data in the target domain. There also exists research on improving zero-shot performance of retrieval models with no adaptation. This paper introduces a new category of domain adaptation in IR that is as-yet unexplored. Here, similar to the zero-shot setting, we assume the retrieval model does not have access to the target document collection. In contrast, it does have access to a brief textual description that explains the target domain. We define a taxonomy of domain attributes in retrieval tasks to understand different properties of a source domain that can be adapted to a target domain. We introduce a novel automatic data construction pipeline that produces a synthetic document collection, query set, and pseudo relevance labels, given a textual domain description. Extensive experiments on five diverse target domains show that adapting dense retrieval models using the constructed synthetic data leads to effective retrieval performance on the target domain.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,RAG,未整理},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\EH8JJ58F\\Hashemi 等。 - 2023 - Dense Retrieval Adaptation using Target Domain Des.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\TMS9965E\\2307.html}
}

@inproceedings{heConversationRecommendationKnowledgeEnhanced2021,
  title = {Conversation and {{Recommendation}}: {{Knowledge-Enhanced Personalized Dialog System}}},
  shorttitle = {Conversation and {{Recommendation}}},
  booktitle = {Web {{Engineering}}},
  author = {He, Ming and Shen, Tong and Dong, Ruihai},
  editor = {Brambilla, Marco and Chbeir, Richard and Frasincar, Flavius and Manolescu, Ioana},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {209--224},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-74296-6_17},
  abstract = {Traditional recommender systems are usually single-shot systems, lacking real-time dialog with customers. Using dialog as an interactive method can more accurately capture user preferences and enhance system transparency. However, building such a goal-oriented dialog system suffered many challenges as the system itself needs to collaborate with various sub-tasks, such as collecting user needs through interaction, recommending appropriate products to users. Most existing work of dialog systems does not comprehensively consider this scenario and the challenges caused. In this paper, we propose a novel memory network framework for conversational recommendation, which harness dialog historical information to endows our model with adaptability in different dialog scenarios, and leverage the knowledge base and user profiles to reweight candidates, to reduce the ambiguity during interactions and improve the quality of conversational recommender systems. Through the experiments on the personalized bAbI dialog dataset and restaurant recommendation application, we demonstrate that the proposed method can achieve state-of-the-art performance in a few classical tasks, such as options display and information provision, etc.},
  isbn = {978-3-030-74296-6},
  langid = {english},
  keywords = {Dialog systems,Knowledge base,Memory network,Recommender systems,未整理},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 對話與推薦：知識增強的個人化對話系統},
  file = {C:\Users\BlackCat\Zotero\storage\MPSER9YZ\He 等。 - 2021 - Conversation and Recommendation Knowledge-Enhance.pdf}
}

@article{heConversationRecommendationKnowledgeenhanced2023,
  title = {Conversation and Recommendation: Knowledge-Enhanced Personalized Dialog System},
  shorttitle = {Conversation and Recommendation},
  author = {He, Ming and Wang, Jiwen and Ding, Tianyu and Shen, Tong},
  date = {2023-01-01},
  journaltitle = {Knowledge and Information Systems},
  shortjournal = {Knowl Inf Syst},
  volume = {65},
  number = {1},
  pages = {261--279},
  issn = {0219-3116},
  doi = {10.1007/s10115-022-01766-6},
  url = {https://doi.org/10.1007/s10115-022-01766-6},
  urldate = {2024-01-15},
  abstract = {Traditional recommender systems are usually single-shot systems, lacking real-time dialog with customers. Using dialog as an interactive method can help capture user preferences more accurately and enhance system transparency. However, developing such a goal-oriented dialog system has suffered many challenges as the system must collaborate with other subtasks, such as collecting user demands through interaction and recommending appropriate products to users. Additionally, most previous studies on dialog systems do not consider this situation and its challenges. This paper proposes a novel memory network framework for conversational recommendation, which harnesses dialog historical information to endow our model with adaptability in various dialog scenarios. Additionally, it leverages the knowledge base and user profiles to reweight candidates, reducing the ambiguity during interactions and improving the quality of conversational recommender systems. We demonstrate that the proposed method can achieve state-of-the-art performance in a few traditional tasks, such as options display and information provision, through experiments on the personalized bAbI dialog dataset and restaurant recommendation application.},
  langid = {english},
  keywords = {Dialog systems,Knowledge base,Memory network,Recommender systems,未整理},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 對話與推薦：知識增強的個人化對話系統}
}

@article{hectoryagoONSMMILEOntologyNetworkbased2018,
  title = {{{ON-SMMILE}}: {{Ontology Network-based Student Model}} for {{MultIple Learning Environments}}},
  shorttitle = {{{ON-SMMILE}}},
  author = {{Hector Yago} and {Julia Clemente} and {Daniel Rodriguez} and {Pedro Fernandez-de-Cordoba}},
  date = {2018-05-01},
  journaltitle = {Data \& Knowledge Engineering},
  shortjournal = {Data \& Knowledge Engineering},
  volume = {115},
  pages = {48--67},
  issn = {0169-023X},
  doi = {10.1016/j.datak.2018.02.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0169023X17301945},
  urldate = {2023-10-18},
  abstract = {Currently, many educational researchers focus on the extraction of information about the learning progress to properly assist students. We present ON-SMMILE, a student-centered and flexible student model which is represented as an ontology network combining information related to (i) students and their knowledge state, (ii) assessments that rely on rubrics and different types of objectives, (iii) units of learning and (iv) information resources previously employed as support for the student model in intelligent virtual environment for training/instruction and here extended. The aim of this work is to design and build methodologically, throughout ontological engineering, the ON-SMMILE model to be used as support of future works closely linked to supervision of student's learning as competence-based recommender system. For this purpose, our model is designed as a set of ontological resources that have been extended, standardized, interrelated and adapted to be used in multiple learning environments. In this paper, we also analyze the available approaches based on instructional design which can be added to ontology network to build the proposed model. As a case study, a chemical experiment in a virtual environment and its instantiation are described in terms of ON-SMMILE.},
  keywords = {Learning supervision,Ontological engineering,Ontology network,Semantic web,Student modeling,已整理,教育本體,本體建立,知識本體},
  annotation = {17 citations (Crossref) [2024-03-26]},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\IAUFS84L\\Yago et al. - 2018 - ON-SMMILE Ontology Network-based Student Model fo.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\5X4UGSWW\\S0169023X17301945.html}
}

@online{heDeepOntoPythonPackage2023,
  title = {{{DeepOnto}}: {{A Python Package}} for {{Ontology Engineering}} with {{Deep Learning}}},
  shorttitle = {{{DeepOnto}}},
  author = {He, Yuan and Chen, Jiaoyan and Dong, Hang and Horrocks, Ian and Allocca, Carlo and Kim, Taehun and Sapkota, Brahmananda},
  date = {2023-07-06},
  eprint = {2307.03067},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.03067},
  url = {http://arxiv.org/abs/2307.03067},
  urldate = {2024-01-30},
  abstract = {Applying deep learning techniques, particularly language models (LMs), in ontology engineering has raised widespread attention. However, deep learning frameworks like PyTorch and Tensorflow are predominantly developed for Python programming, while widely-used ontology APIs, such as the OWL API and Jena, are primarily Java-based. To facilitate seamless integration of these frameworks and APIs, we present Deeponto, a Python package designed for ontology engineering. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more "Pythonic" manner and extending its capabilities to include other essential components including reasoning, verbalisation, normalisation, projection, and more. Building on this module, Deeponto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning methodologies, primarily pre-trained LMs. In this paper, we also demonstrate the practical utility of Deeponto through two use-cases: the Digital Health Coaching in Samsung Research UK and the Bio-ML track of the Ontology Alignment Evaluation Initiative (OAEI).},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Logic in Computer Science,Computer Science - Machine Learning,LLM,已整理,機器學習,知識本體,重要},
  annotation = {titleTranslation: DeepOnto：用於深度學習本體工程的 Python 套件\\
abstractTranslation:  在本體工程中應用深度學習技術，特別是語言模型（LM）引起了廣泛的關注。然而，PyTorch 和 Tensorflow 等深度學習框架主要是為 Python 程式設計而開發的，而廣泛使用的本體 API（如 OWL API 和 Jena）主要基於 Java。為了促進這些框架和 API 的無縫集成，我們推出了 Deeponto，一個專為本體工程設計的 Python 套件。該軟體包包含一個基於廣泛認可且可靠的OWL API 的核心本體處理模組，以更「Pythonic」的方式封裝其基本功能，並擴展其功能以包含其他基本組件，包括推理、語言化、規範化、投影等。在此模組的基礎上，Deeponto 提供了一套工具、資源和演算法，透過利用深度學習方法（主要是預先訓練的 LM）來支援各種本體工程任務，例如本體對齊和完成。在本文中，我們還透過兩個用例展示了 Deeponto 的實用性：英國三星研究院的數位健康輔導和本體對齊評估計劃 (OAEI) 的生物機器學習軌道。},
  note = {用python風格的方式包裝知識本體，使其可以應用於LLM等基於python的工具
\par
Comment: under review at Semantic Web Journal},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\73KUPR9A\\He et al. - 2023 - DeepOnto A Python Package for Ontology Engineerin.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\F645KCS5\\2307.html}
}

@article{heImprovedHumanComputerInteraction2023,
  title = {An {{Improved Human-Computer Interaction Content Recommendation Method Based}} on {{Knowledge Graph}}},
  author = {He, Zhu},
  date = {2023},
  journaltitle = {International Journal of Human–Computer Interaction},
  volume = {0},
  number = {0},
  pages = {1--12},
  publisher = {Taylor \& Francis},
  issn = {1044-7318},
  doi = {10.1080/10447318.2023.2295734},
  url = {https://doi.org/10.1080/10447318.2023.2295734},
  urldate = {2024-01-15},
  abstract = {Since human-to-human communication is contentful and coherent, during human-computer interaction (HCI), people naturally hope that the computer’s reply will be contentful and coherent. How to enable computers to recognize, understand and generate quality content and coherent responses like humanshas drawn a great deal of interest from bothindustry and academia.In order to address the current problems of lack of background knowledge of robots and low consistency of responses in open-domain HCI systems, we present a content recommendation method on the basis of knowledge graph ripple network. In order to realize a human-computer interaction system with better content and stronger coherence, and this model simulates the real process of human-human communication. At first, the emotional friendliness of HCI is gotten through calculating the emotional confidence level together with emotional evaluation value of HCI. After that, the external knowledge graph is introduced as the robot’s background knowledge, and the dialog entities are embedded in the ripple network of knowledge graphsto access the content of entities that may be of interest to participants. Lastly, the robot reply is given by comprehensively considering the content and emotional friendliness. The results of the experiments indicate a comparison with comparative modeling approaches like MECs models, robots with emotional measures and background knowledge can effectively enhance their emotional consistency and friendliness when performing HCI.},
  langid = {english},
  keywords = {deep learning,Human-computer interaction,knowledge graph,ripple network},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 一種改進的基於知識圖譜的人機互動內容推薦方法}
}

@article{heMultisourceInformationResources2012,
  title = {Multi-Source Information Resources Management in Cloud Computin g Environment},
  author = {He, Luo},
  date = {2012},
  journaltitle = {Computer Integrated Manufacturing Systems},
  url = {https://www.semanticscholar.org/paper/Multi-source-information-resources-management-in-g-He/2919ef258b0f85255d563cd5af382002eda3949f},
  urldate = {2024-04-16},
  abstract = {To realize the effective management for multi-source information resources under dynamic cloud computing environment,and to ensure efficient system operation,high quality resource sharing and real-time service providing of cloud computing system,the key problems and challenges were proposed on the basis of summarizing the research results in multi-source information resource cataloguing format and description language,discovery and matching mechanism,dynamic organization and allocation methods as well as real-time monitoring.The research prospect of multi-source information resource management in cloud computing was given,and a multi-source information management framework in cloud computing was constructed.Its application in manufacturing was also discussed.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {titleTranslation: 雲端運算環境下多源資訊資源管理\\
abstractTranslation:  為實現動態雲端運算環境下多源資訊資源的有效管理，保證雲端運算系統高效運作、高品質資源共享和即時服務提供，在此基礎上提出了關鍵問題和挑戰。與描述語言、發現與匹配機制、動態組織與分配方法以及實時監控等方面的研究成果，並對雲端運算下多源資訊資源管理的研究進行了展望：建構了雲端運算下的多源資訊管理框架，並探討了其在製造業的應用。},
  note = {[TLDR] The key problems and challenges were proposed on the basis of summarizing the research results in multi-source information resource cataloguing format and description language, discovery and matching mechanism, dynamic organization and allocation methods as well as real-time monitoring.}
}

@incollection{hengwengFrameworkAutomatedKnowledge2017,
  title = {A {{Framework}} for {{Automated Knowledge Graph Construction Towards Traditional Chinese Medicine}}},
  booktitle = {Health {{Information Science}}},
  author = {{Heng Weng} and {Ziqing Liu} and {Shixing Yan} and {Meiyu Fan} and {Aihua Ou} and {Dacan Chen} and {Tianyong Hao}},
  editor = {{Siuly Siuly} and {Zhisheng Huang} and {Uwe Aickelin} and {Rui Zhou} and {Hua Wang} and {Yanchun Zhang} and {Stanislav Klimenko}},
  date = {2017},
  volume = {10594},
  pages = {170--181},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-69182-4_18},
  url = {http://link.springer.com/10.1007/978-3-319-69182-4_18},
  urldate = {2022-09-19},
  isbn = {978-3-319-69181-7 978-3-319-69182-4},
  keywords = {中醫知識圖譜},
  file = {C:\Users\BlackCat\Zotero\storage\8RIBFBQS\Weng 等。 - 2017 - A Framework for Automated Knowledge Graph Construc.pdf}
}

@inproceedings{henrikhanssonHowProduceQuality2014,
  title = {How to Produce Quality Theses at Universities in a Large Scale: {{SciPro IT}} System — {{Supporting}} the {{Scientific Process}}},
  shorttitle = {How to Produce Quality Theses at Universities in a Large Scale},
  booktitle = {2014 {{IEEE Frontiers}} in {{Education Conference}} ({{FIE}}) {{Proceedings}}},
  author = {{Henrik Hansson}},
  date = {2014-10},
  pages = {1--1},
  issn = {2377-634X},
  doi = {10.1109/FIE.2014.7044383},
  url = {https://ieeexplore.ieee.org/document/7044383/metrics#metrics},
  urldate = {2023-10-13},
  abstract = {Problem: To manage and supervise a large number of theses at bachelor, master and PhD level, with increased quality of both processes and product. Based on 995 publications, Jones (2013) identified four main problems in PhD thesis production; attrition, supervisor relationship, supervisor quality, social isolation. These issues also apply to theses at bachelor and master level In theses applied research methods are used in order to solve a problem. Theses end an education level and constitute a bridge into work or further studies and even an academic career. This core activity at universities requires a lot of resources and is very time consuming. The thesis quality varies and topic relevance to societal needs is often weak. Another problem is the high dropout and inefficient administrative procedures. Furthermore the digital resources for autonomous learning of research methods aren't optimally provided The European Union's approximately 4000 Higher Education Institutions and about 20 million students need better support in this area The demand for better mass scale support systems in higher education is also a global need with 1200 Higher Education Institutions worldwide (listed by ARWU). The number of higher education students is about 178 million (2010) and are forecasted to increase to 262 million by 2025. The online IT-support system, SciPro (Supporting the Scientific Process), has been developed during five years at the Department of Computer and Systems Sciences, Stockholm University. During the period 2012-2013 706 bachelor and master theses was managed through the SciPro system In 2014 91 PhD students are included as well This study summaries the experience and results during five years, including and going beyond our previous studies (17 publications). The focus of the SciPro project is technical implementation of pedagogical processes leading to quality learning and quality theses. The SciPro online system is modular and unique, consisting of: 1) Idea bank; with students, supervisors and business organisation's thesis topic ideas, 2) Matching; students paired with supervisors based on ideas and research area, 3) Objective (Milestones) and subjective (Project state) progress indicators, 4) Meta-supervision; instructions and cues for all students and supervisors, 5) Realtime statistics; for monitoring, quality assurance and evaluation, 6) Communication module; forum, notifications, shared files, checklists, 7) Peer Portal; peer review process and peer interaction 8) Multimedia resources for autonomous learning of research methods, and 9) Anti-plagiarism control; integrated and automatic The SciPro system works on all devices connected to Internet. It significantly reduces administration of all thesis related tasks for all involved SciPro can manage theses at a whole University fulfilling administrators', supervisors' and student's needs. With minor adaptations and development SciPro can be useful at most universities. The presentation will include live demonstrations and evaluations from all stakeholders' points of view.},
  eventtitle = {2014 {{IEEE Frontiers}} in {{Education Conference}} ({{FIE}}) {{Proceedings}}},
  annotation = {4 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\KNM5SNNI\Hansson - 2014 - How to produce quality theses at universities in a.pdf}
}

@thesis{HePeiZhenYiGeKongZhiPingXingChengShiYuYanDeZhiXingShiQiXiTong2003,
  title = {一個控制平行程式語言的執行時期系統},
  author = {{何佩真}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2003},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/b5z89u},
  abstract = {平行計算的模式可分成二大類：資料平行與控制平行。控制平行通常有訊息傳遞與共享變數二種溝通機制。CCC平行程式語言同時支援資料平行與控制平行，而控制平行提供了訊息傳遞與共享變數二種溝通機制。對稱式多處理器與對稱式多處理器群組已經變成普遍的平行及分散計算機器。本篇論文主要是探討如何設計一個CCC平行程式語言的執行時期系統，來提昇CCC在單一台對稱式多處理器及對稱式多處理器群組上的跨平台性。 我們使用虛擬機器的概念來提昇CCC平行程式語言的跨平台性。我們設計了一個具有虛擬共享記憶體機器特性的共同介面。並且分別利用Pthread函式庫及Millipede函式庫來實作此共同介面，以分別支援單一對稱式多處理器和對稱式多處理器群組。同時為了簡化CCC編譯器的編譯工作，我們也設計及實作了一個CCC執行時期函式庫。在論文中，我們利用CMU大學所提出的控制平行程式組來評估我們編譯器的效能。實驗結果顯示CCC編譯器產生的程式和手寫的程式之平均增加效能差為0.4。},
  pagetotal = {245}
}

@thesis{HeShuoYanPingXingChengShiYuYanCCCDeXingTaiJianChaQi2002,
  title = {{{平行程式語言CCC的型態檢查器}}},
  author = {{何碩晏}},
  namea = {{林迺衛} and {Lin Nai-Wei}},
  nameatype = {collaborator},
  date = {2002},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/at9y56},
  abstract = {高階平行程式語言CCC是傳統的C程式語言的一個簡單的擴充，它同時支援資料平行以及控制平行。資料平行會在SIMD的模型架構下被執行，控制平行則在MIMD的模型架構下被執行，控制平行的工作間可以透過訊息傳遞或者共享變數的溝通方式來進行溝通。 CCC的型態檢查器靜態地檢查CCC程式以幫助程式設計師提早在編譯期發現潛在的錯誤，主要分成型態檢查和溝通方式的靜態分析兩部分：型態檢查部分檢查型態的使用的靜態的語意，包括型態的宣告以及型態的檢查。溝通方式的靜態分析檢查訊息傳遞的溝通方式，包括一階通道和二階通道的訊息溝通。},
  pagetotal = {119}
}

@article{HeTingJiYuEHRDeYiLiaoZhiShiTuPuYanJiuYuYingYongZongShu2018,
  title = {{{基于EHR的医疗知识图谱研究与应用综述}}},
  author = {{何霆} and {吴雅婷} and {王华珍} and {熊英杰} and {孙偲} and {徐汉川}},
  date = {2018},
  journaltitle = {哈尔滨工业大学学报},
  volume = {50},
  number = {11},
  pages = {137--144},
  url = {http://www.cqvip.com/qk/90629x/201811/7000858771.html},
  urldate = {2022-08-05},
  abstract = {电子健康记录(EHR)作为一种医疗信息化手段,在数十年的使用过程中储存和积累了越来越多的医疗过程和结果大数据.知识图谱作为一种从海量数据中抽取结构化知识的手段,近年来在多个行业展示了广阔的应用前景.知识图谱的优...},
  keywords = {No DOI found}
}

@article{higginsConsiderationsImplementingElectronic2022,
  title = {Considerations for Implementing Electronic Laboratory Notebooks in an Academic Research Environment},
  author = {Higgins, Stuart G. and Nogiwa-Valdez, Akemi A. and Stevens, Molly M.},
  date = {2022-02},
  journaltitle = {Nature Protocols},
  shortjournal = {Nat Protoc},
  volume = {17},
  number = {2},
  pages = {179--189},
  publisher = {Nature Publishing Group},
  issn = {1750-2799},
  doi = {10.1038/s41596-021-00645-8},
  url = {https://www.nature.com/articles/s41596-021-00645-8},
  urldate = {2024-01-11},
  abstract = {As research becomes predominantly digitalized, scientists have the option of using electronic laboratory notebooks to record and access entries. These systems can more readily meet volume, complexity, accessibility and preservation requirements than paper notebooks. Although the technology can yield many benefits, these can be realized only by choosing a system that properly fulfills the requirements of a given context. This review explores the factors that should be considered when introducing electronic laboratory notebooks to an academically focused research group. We cite pertinent studies and discuss our own experience implementing a system within a multidisciplinary research environment. We also consider how the required financial and time investment is shared between individuals and institutions. Finally, we discuss how electronic laboratory notebooks fit into the broader context of research data management. This article is not a product review; it provides a framework for both the initial consideration of an electronic laboratory notebook and the evaluation of specific software packages.},
  issue = {2},
  langid = {english},
  keywords = {Software,使用者研究,實驗筆記,已整理,略讀},
  annotation = {24 citations (Crossref) [2024-03-26]\\
titleTranslation: 在學術研究環境中實施電子實驗室筆記本的注意事項\\
abstractTranslation:  隨著研究主要數位化，科學家可以選擇使用電子實驗室筆記本來記錄和存取條目。與紙本筆記本相比，這些系統更容易滿足容量、複雜性、可訪問性和保存要求。儘管該技術可以帶來許多好處，但只有選擇正確滿足給定環境要求的系統才能實現這些好處。本綜述探討了在向學術研究小組引入電子實驗室筆記本時應考慮的因素。我們引用相關研究並討論我們自己在多學科研究環境中實施系統的經驗。我們也考慮如何在個人和機構之間分配所需的財務和時間投資。最後，我們討論電子實驗室筆記本如何適應更廣泛的研究資料管理背景。本文不是產品評論；它為電子實驗室筆記本的初步考慮和特定軟體包的評估提供了一個框架。},
  note = {本研究探討ELN的特性與選擇的方向。但比較注重共享等應用方式而非收錄，且沒有提到任何資工方面的應用。},
  file = {C:\Users\BlackCat\Zotero\storage\UD8BDSE3\Higgins et al. - 2022 - Considerations for implementing electronic laborat.pdf}
}

@inproceedings{hongqingyuHealthCausalProbability2021,
  title = {Health {{Causal Probability Knowledge Graph}}: {{Another Intelligent Health Knowledge Discovery Approach}}},
  shorttitle = {Health {{Causal Probability Knowledge Graph}}},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Bioinformatics Research}} and {{Applications}}},
  author = {{Hongqing Yu}},
  year = {3 月 2, 2021},
  series = {{{ICBRA}} '20},
  pages = {49--58},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3440067.3440077},
  url = {https://dl.acm.org/doi/10.1145/3440067.3440077},
  urldate = {2023-08-23},
  abstract = {Currently, most of the health-data research concentrates on applying Deep Learning technologies for prediction and reasoning. Deep Learning processes build the prediction model purely based on fitting weights on the raw data inside multiple neural layers, which is difficult to explain the prediction outputs. However, telling ‘WHY’ is crucial for healthcare research. The major difficulty to explain in Deep Learning models is a lack of knowledge-based analysis environment that not only can model the knowledge in a machine-understandable way but also can create causal probability relations inside the knowledge. In our research, we propose a Causal Probability Description Logic (CPDL) framework that extended the current Description Logic (DL). The key extension is to have a two-layer DL representation. One layer represents causality knowledge. The other layer takes observation inputs e.g. symptoms for generating a runtime probability knowledge graph based on the previous layer's knowledge. The CPDL framework can support probability-based causal reasoning tasks in a transparent and human-understandable way. CPDL can be easily implemented using existing programming standards such as OWL, RDF, SPARQL and probability network programming libraries. The experimental evaluations extract 383 common disease conditions from the UK NHS (National Healthcare Service) and enable automatically linked 418 condition terms from the DBpedia dataset. The CPDL-based knowledge graph can support disease prediction with traceable pieces of evidence behind the ranking results.},
  isbn = {978-1-4503-8813-9},
  langid = {english},
  keywords = {Bioinformatics,Healthcare,Information retrieval,Knowledge graph,Machine learning,Probability and causal analysis,Web of data,可解釋性,待讀,知識圖譜,知識本體,重要},
  annotation = {4 citations (Crossref) [2024-03-26]\\
titleTranslation: 健康因果概率知識圖：另一種智能健康知識發現方法\\
abstractTranslation:  目前，大多數健康數據研究都集中在應用深度學習技術進行預測和推理。深度學習過程純粹基於多個神經層內原始數據的擬合權重來構建預測模型，這很難解釋預測輸出。然而，告訴“為什麼”對於醫療保健研究至關重要。深度學習模型解釋的主要困難是缺乏基於知識的分析環境，該環境不僅可以以機器可理解的方式對知識進行建模，而且可以在知識內部創建因果概率關係。在我們的研究中，我們提出了一種因果概率描述邏輯（CPDL）框架，該框架擴展了當前的描述邏輯（DL）。關鍵的擴展是擁有兩層深度學習表示。一層代表因果關係知識。另一層接受觀察輸入，例如根據前一層的知識生成運行時概率知識圖的症狀。 CPDL 框架可以以透明且人類可理解的方式支持基於概率的因果推理任務。 CPDL 可以使用現有的編程標準（例如 OWL、RDF、SPARQL 和概率網絡編程庫）輕鬆實現。實驗評估從英國 NHS（國家醫療保健服務）中提取了 383 種常見疾病狀況，並從 DBpedia 數據集中自動鏈接了 418 種狀況術語。基於CPDL的知識圖譜可以通過排名結果背後的可追溯證據來支持疾病預測。},
  file = {C:\Users\BlackCat\Zotero\storage\767GGNBR\Yu - 2021 - Health Causal Probability Knowledge Graph Another.pdf}
}

@thesis{HongXinJieZaiChaoChangZhiLingChuLiQiShangShiYongMoShuShiPaiChengDeCBianYiQi2003,
  title = {{{在超長指令處理器上使用模數式排程的C編譯器}}},
  author = {{洪新傑}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2003},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/6rq4nd},
  abstract = {超長指令處理器必須藉由編譯器找出程式中可平行執行的指令，進而將它們排程為有效率的超長指令程式碼。資料相依性分折可有效的找出程式中可平行的指令。模數式排程是一個針對內迴圈做軟體管線的演算法，可有效的支援超長指令程式碼的排程。 LCC是一個針對一般用途的可重標的C程式語言編譯器。LCC具有精簡及模組化的特性，但是尚缺乏支援超長指令處理器旳功能。本篇論文以維持LCC的精簡及模組化的方式來修改LCC，以支援超長指令處理器。我們將分別加入一個資料相依性分析模組來找出程式中可平行執行的指令，以及一個模數式排程模組來產生有效率的超長指令目的碼。我們研發的平台為德州儀器TMS320C6711處理器。我們也評比了我們所研發的編譯器與德州儀器所研發之編譯器。},
  pagetotal = {61}
}

@thesis{HongZhiZongJianDuJieGouPingXingChengShiDeXiaoNengYuCe2005,
  title = {監督結構平行程式的效能預測},
  author = {{洪志宗}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2005},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/n65x9a},
  abstract = {平行程式的開發是為了提高大型應用程式的效能。因此平行程式的效能預測就變得非常重要。CCC是一個高階的平行程式語言，它可以支援基於監督結構的平行程式。基於監督結構的平行程式，大多數具有非確定性的執行行為，因此它們的效能預測非常困難。 這篇論文設計和實作一個基於監督結構的平行程式的效能預測工具。它所提出的方法是一個混合式的方法。它使用靜態程式分析來預測循序程式的效能，以及同時使用測量和模擬的方法來預測平行程式的效能。這篇論文也針對這個效能預測工具提出一個初步的評估。},
  pagetotal = {81}
}

@article{HowWriteGood,
  title = {How to Write a Good State of the Art: Should It Be the First Step of Your Thesis?},
  langid = {english},
  annotation = {titleTranslation: 如何写好艺术现状：它应该是论文的第一步吗？},
  file = {C:\Users\BlackCat\Zotero\storage\AW5QTSD6\How to write a good state of the art should it be.pdf}
}

@inproceedings{hsiang-shengtsaiSUPERBSGEnhancedSpeech2022,
  title = {{{SUPERB-SG}}: {{Enhanced Speech}} Processing {{Universal PERformance Benchmark}} for {{Semantic}} and {{Generative Capabilities}}},
  shorttitle = {{{SUPERB-SG}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {{Hsiang-Sheng Tsai} and {Heng-Jui Chang} and {Wen-Chin Huang} and {Zili Huang} and {Kushal Lakhotia} and {Shu-wen Yang} and {Shuyan Dong} and {Andy Liu} and {Cheng-I Lai} and {Jiatong Shi} and {Xuankai Chang} and {Phil Hall} and {Hsuan-Jui Chen} and {Shang-Wen Li} and {Shinji Watanabe} and {Abdelrahman Mohamed} and {Hung-yi Lee}},
  date = {2022-05},
  pages = {8479--8492},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.580},
  url = {https://aclanthology.org/2022.acl-long.580},
  urldate = {2023-09-17},
  abstract = {Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years. In speech, a model pre-trained by self-supervised learning transfers remarkably well on multiple tasks. However, the lack of a consistent evaluation methodology is limiting towards a holistic understanding of the efficacy of such models. SUPERB was a step towards introducing a common benchmark to evaluate pre-trained models across various speech tasks. In this paper, we introduce SUPERB-SG, a new benchmark focusing on evaluating the semantic and generative capabilities of pre-trained models by increasing task diversity and difficulty over SUPERB. We use a lightweight methodology to test the robustness of representations learned by pre-trained models under shifts in data domain and quality across different types of tasks. It entails freezing pre-trained model parameters, only using simple task-specific trainable heads. The goal is to be inclusive of all researchers, and encourage efficient use of computational resources. We also show that the task diversity of SUPERB-SG coupled with limited task supervision is an effective recipe for evaluating the generalizability of model representation.},
  eventtitle = {{{ACL}} 2022},
  langid = {english},
  keywords = {已整理,機器學習,語音識別},
  annotation = {18 citations (Crossref) [2024-03-26]\\
titleTranslation: SUPERB-SG：增強型語音處理通用語意與生成能力效能基準\\
abstractTranslation:  近年來，遷移學習已被證明對於推進語音和自然語言處理研究至關重要。在語音方面，透過自監督學習預先訓練的模型在多項任務上的遷移效果非常好。然而，缺乏一致的評估方法限制了對此類模型功效的整體理解。 SUPERB 是朝著引入通用基準來評估跨各種語音任務的預訓練模型邁出的一步。在本文中，我們介紹了 SUPERB-SG，這是一個新的基準，專注於透過增加 SUPERB 的任務多樣性和難度來評估預訓練模型的語義和生成能力。我們使用輕量級方法來測試預訓練模型在不同類型任務的資料領域和品質變化下學習的表示的穩健性。它需要凍結預先訓練的模型參數，僅使用簡單的特定任務的可訓練頭。目標是包容所有研究人員，並鼓勵有效利用計算資源。我們還表明，SUPERB-SG 的任務多樣性與有限的任務監督相結合是評估模型表示的泛化性的有效方法。},
  file = {C:\Users\BlackCat\Zotero\storage\FWFNT67U\Tsai 等。 - 2022 - SUPERB-SG Enhanced Speech processing Universal PE.pdf}
}

@online{HttpsNdltdNcl2020,
  title = {{{https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/login?o=dnclcdr\&s=id=\%22108CCU00392037\%22.\&searchmode=basic}}},
  date = {2020},
  url = {https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/login?o=dnclcdr&s=id=%22108CCU00392037%22.&searchmode=basic},
  urldate = {2024-03-22},
  abstract = {本論文設計與實作一個書籤知識筆記平台，透過網頁主文偵測與機器學習的方法實現收藏書籤過程的內容提取與自動分類，同時提供對書籤文本的編輯與筆記註解來達成內容個人化的目的。並且在日後瀏覽網頁時將當前瀏覽的網頁文本與使用者保存的書籤或歷史瀏覽紀錄進行相似度比對，主動推送相關聯的書籤連結，目的是改變使用者查看書籤的「後閱讀」行為，讓原本被動查看的次級需求改為更積極主動的通知推送。最後實做一個管理書籤筆記的平台，方便使用者之後進行書籤的全文搜尋以及離線的閱讀與編輯。},
  langid = {english},
  keywords = {重要},
  annotation = {titleTranslation: https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/login?o=dnclcdr\&s=id=\%22108CCU00392037\%22.\&searchmode=basic},
  file = {C:\Users\BlackCat\Zotero\storage\MS48U2KZ\login.html}
}

@article{huajunchenOWLReasoningFramework2014,
  title = {{{OWL}} Reasoning Framework over Big Biological Knowledge Network},
  author = {{Huajun Chen} and {Xi Chen} and {Peiqin Gu} and {Zhaohui Wu} and {Tong Yu}},
  date = {2014},
  journaltitle = {BioMed Research International},
  shortjournal = {Biomed Res Int},
  volume = {2014},
  eprint = {24877076},
  eprinttype = {pmid},
  pages = {272915},
  issn = {2314-6141},
  doi = {10.1155/2014/272915},
  abstract = {Recently, huge amounts of data are generated in the domain of biology. Embedded with domain knowledge from different disciplines, the isolated biological resources are implicitly connected. Thus it has shaped a big network of versatile biological knowledge. Faced with such massive, disparate, and interlinked biological data, providing an efficient way to model, integrate, and analyze the big biological network becomes a challenge. In this paper, we present a general OWL (web ontology language) reasoning framework to study the implicit relationships among biological entities. A comprehensive biological ontology across traditional Chinese medicine (TCM) and western medicine (WM) is used to create a conceptual model for the biological network. Then corresponding biological data is integrated into a biological knowledge network as the data model. Based on the conceptual model and data model, a scalable OWL reasoning method is utilized to infer the potential associations between biological entities from the biological network. In our experiment, we focus on the association discovery between TCM and WM. The derived associations are quite useful for biologists to promote the development of novel drugs and TCM modernization. The experimental results show that the system achieves high efficiency, accuracy, scalability, and effectivity.},
  langid = {english},
  pmcid = {PMC4022201},
  keywords = {Animals,Databases Factual,Humans,Information Storage and Retrieval,Knowledge,Models Theoretical},
  annotation = {2 citations (Crossref) [2024-03-26]\\
4 citations (Semantic Scholar/DOI) [2023-07-24]\\
titleTranslation: 生物大知識網絡上的OWL推理框架\\
abstractTranslation:  最近，生物學領域產生了大量數據。嵌入不同學科的領域知識，孤立的生物資源之間存在隱式聯繫。因此，它形成了一個多功能生物學知識的龐大網絡。面對如此海量、分散且相互關聯的生物數據，提供一種有效的方法來建模、集成和分析大生物網絡成為一個挑戰。在本文中，我們提出了一個通用的 OWL（網絡本體語言）推理框架來研究生物實體之間的隱式關係。跨中醫（TCM）和西醫（WM）的綜合生物本體論被用來創建生物網絡的概念模型。然後將相應的生物數據整合成生物知識網絡作為數據模型。基於概念模型和數據模型，利用可擴展的OWL推理方法從生物網絡中推斷生物實體之間的潛在關聯。在我們的實驗中，我們重點關注中醫和西醫之間的關聯發現。所得出的關聯對於生物學家促進新藥開發和中藥現代化非常有用。實驗結果表明，該系統具有較高的效率、準確性、可擴展性和有效性。},
  file = {C:\Users\BlackCat\Zotero\storage\D3WS4A5R\Chen 等。 - 2014 - OWL reasoning framework over big biological knowle.pdf}
}

@inproceedings{huajunchenSemanticWebRelational2006,
  title = {Towards a {{Semantic Web}} of {{Relational Databases}}: {{A Practical Semantic Toolkit}} and an {{In-Use Case}} from {{Traditional Chinese Medicine}}},
  shorttitle = {Towards a {{Semantic Web}} of {{Relational Databases}}},
  booktitle = {The {{Semantic Web}} - {{ISWC}} 2006},
  author = {{Huajun Chen} and {Yimin Wang} and {Heng Wang} and {Yuxin Mao} and {Jinmin Tang} and {Cunyin Zhou} and {Ainin Yin} and {Zhaohui Wu}},
  editor = {{Isabel Cruz} and {Stefan Decker} and {Dean Allemang} and {Chris Preist} and {Daniel Schwabe} and {Peter Mika} and {Mike Uschold} and {Lora M. Aroyo}},
  date = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {750--763},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11926078_54},
  abstract = {Integrating relational databases is recently acknowledged as an important vision of the Semantic Web research, however there are not many well-implemented tools and not many applications that are in large-scale real use either. This paper introduces the Dartgrid which is an application development framework together with a set of semantic tools to facilitate the integration of heterogenous relational databases using semantic web technologies. For examples, DartMapping is a visualized mapping tool to help DBA in defining semantic mappings from heterogeneous relational schemas to ontologies. DartQuery is an ontology-based query interface helping user to construct semantic queries, and capable of rewriting SPARQL semantic queries to a set of SQL queries. DartSearch is an ontology-based search engine enabling user to make full-text search over all databases and to navigate across the search results semantically. It is also enriched with a concept ranking mechanism to enable user to find more accurate and reliable results. This toolkit has been used to develop an currently in-use application for China Academy of Traditional Chinese Medicine (CATCM). In this application, over 70 legacy relational databases are semantically interconnected by an ontology with over 70 classes and 800 properties, providing integrated semantic-enriched query, search and navigation services to TCM communities.},
  isbn = {978-3-540-49055-5},
  langid = {english},
  keywords = {Query Interface,Relational Database,Semantic Mapping,Semantic View,SPARQL Query,中醫,人機互動,問答系統,已整理,知識本體,語意分析},
  annotation = {35 citations (Crossref) [2024-03-26]\\
abstractTranslation:  整合關係資料庫最近被認為是語意網路研究的重要願景，然而，實現良好的工具並不多，大規模實際使用的應用程式也不多。本文介紹了 Dartgrid，它是一個應用程式開發框架和一組語義工具，旨在利用語義網路技術促進異質關係資料庫的整合。例如，DartMapping 是一個視覺化映射工具，可協助 DBA 定義從異質關係模式到本體的語意映射。 DartQuery 是一個基於本體的查詢接口，幫助使用者建立語義查詢，並且能夠將 SPARQL 語義查詢重寫為一組 SQL 查詢。 DartSearch 是一個基於本體的搜尋引擎，使用戶能夠對所有資料庫進行全文搜尋並在搜尋結果中進行語義導航。它還豐富了概念排名機制，使用戶能夠找到更準確、更可靠的結果。該工具包已用於開發中國中醫研究院 (CATCM) 先前正在使用的應用程式。在此應用程式中，70 多個遺留關係資料庫透過具有 70 多個類別和 800 個屬性的本體在語義上互連，為中醫社群提供整合的語義豐富的查詢、搜尋和導航服務。\\
titleTranslation: 邁向關聯式資料庫語意網路：實用語意工具包和中醫使用案例},
  note = {介紹Dartgrid及其在中醫知識本體上的應用},
  file = {C:\Users\BlackCat\Zotero\storage\CCIB5ANE\Chen 等。 - 2006 - Towards a Semantic Web of Relational Databases A .pdf}
}

@incollection{huajunchenTCMGridWeavingMedical2003,
  title = {{{TCM-Grid}}: {{Weaving}} a {{Medical Grid}} for {{Traditional Chinese Medicine}}},
  shorttitle = {{{TCM-Grid}}},
  booktitle = {Computational {{Science}} — {{ICCS}} 2003},
  author = {{Huajun Chen} and {Zhaohui Wu} and {Chang Huang} and {Jiefeng Xu}},
  editor = {{Peter M. A. Sloot} and {David Abramson} and {Alexander V. Bogdanov} and {Yuriy E. Gorbachev} and {Jack J. Dongarra} and {Albert Y. Zomaya}},
  editora = {{G. Goos} and {J. Hartmanis} and {J. van Leeuwen}},
  editoratype = {redactor},
  date = {2003},
  volume = {2659},
  pages = {1143--1152},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-44863-2_113},
  url = {http://link.springer.com/10.1007/3-540-44863-2_113},
  urldate = {2022-09-19},
  isbn = {978-3-540-40196-4 978-3-540-44863-1},
  keywords = {中醫知識圖譜},
  note = {類似知識本體?},
  file = {C:\Users\BlackCat\Zotero\storage\KPLFTVY6\Chen 等。 - 2003 - TCM-Grid Weaving a Medical Grid for Traditional C.pdf}
}

@online{huangCluesAnswersGenerationEnhanced2022,
  title = {Clues {{Before Answers}}: {{Generation-Enhanced Multiple-Choice QA}}},
  shorttitle = {Clues {{Before Answers}}},
  author = {Huang, Zixian and Wu, Ao and Zhou, Jiaying and Gu, Yu and Zhao, Yue and Cheng, Gong},
  date = {2022-04-30},
  eprint = {2205.00274},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.00274},
  url = {http://arxiv.org/abs/2205.00274},
  urldate = {2024-03-22},
  abstract = {A trending paradigm for multiple-choice question answering (MCQA) is using a text-to-text framework. By unifying data in different tasks into a single text-to-text format, it trains a generative encoder-decoder model which is both powerful and universal. However, a side effect of twisting a generation target to fit the classification nature of MCQA is the under-utilization of the decoder and the knowledge that can be decoded. To exploit the generation capability and underlying knowledge of a pre-trained encoder-decoder model, in this paper, we propose a generation-enhanced MCQA model named GenMC. It generates a clue from the question and then leverages the clue to enhance a reader for MCQA. It outperforms text-to-text models on multiple MCQA datasets.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,問答系統,未整理},
  annotation = {titleTranslation: 答案之前的線索：產生增強型多項選擇 QA},
  note = {Comment: 12 pages, accepted to NAACL 2022},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\LWVAIEQ7\\Huang 等。 - 2022 - Clues Before Answers Generation-Enhanced Multiple.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\N592ZSYS\\2205.html}
}

@inproceedings{huangDesignImplementationJava2022,
  title = {Design and {{Implementation}} of {{Java Knowledge Graph Based}} on {{Neo4j}}},
  booktitle = {2022 {{International Conference}} on {{Knowledge Engineering}} and {{Communication Systems}} ({{ICKES}})},
  author = {Huang, Bin and Deng, Wenlong},
  date = {2022-02},
  pages = {1--6},
  doi = {10.1109/ICKECS56523.2022.10059598},
  url = {https://ieeexplore.ieee.org/document/10059598},
  urldate = {2023-12-03},
  abstract = {Under the background of the information explosion, there are numerous knowledge stars in all walks of life on the Internet. Taking java learning as the main case, you can easily find a lot of learning knowledge by searching “Java” in station B; On the one hand, while enjoying the dividends brought by the times, learners also feel a lot of inconvenience on the other hand, such as selecting high-quality learning materials, storing the source of learning materials, and building other people's knowledge into their own knowledge system. In view of the above problems, this paper proposes to develop and design a java knowledge map system with the advantages of mind mapping, which can effectively record the source of high-quality learning materials and the idea of high-quality personal system. The system adopts the development mode of front-end and back-end separation. The back-end and front-end adopt Spring Boot and Vue architecture respectively, integrate the ORM framework MyBatis, and the database adopts Neo4j diagram database and MySQL relational database for storage. The system shows the system of knowledge in the form of map, effectively records learning materials and transmits learning ideas.},
  eventtitle = {2022 {{International Conference}} on {{Knowledge Engineering}} and {{Communication Systems}} ({{ICKES}})},
  langid = {english},
  keywords = {已整理,待讀,微讀,知識圖譜,知識管理,重要},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於Neo4j的Java知識圖譜設計與實現\\
abstractTranslation:  在資訊爆炸的背景下，網路上湧現出無數各行各業的知識明星。以java學習為主案例，在B站搜尋“Java”，可以輕鬆找到大量學習知識；一方面，學習者在享受時代帶來的紅利的同時，另一方面也感受到了許多不便，例如選擇優質的學習材料、儲存學習材料的來源、將別人的知識建構成自己的知識等。知識體系。針對上述問題，本文提出發展設計一款發揮心智圖優勢的java知識圖譜系統，能夠有效記錄優質學習資料的來源及優質個人系統的想法。系統採用前後端分離的開發模式。後端和前端分別採用Spring Boot和Vue架構，整合ORM框架MyBatis，資料庫採用Neo4j圖資料庫和MySQL關聯式資料庫進行儲存。系統以地圖的形式展現知識體系，有效記錄學習資料，傳遞學習思想。},
  note = {實做方面很詳細，但理論基礎很少。
\par
在撰寫論文方面可能有點幫助。},
  file = {/home/domaj/下載/Design_and_Implementation_of_Java_Knowledge_Graph_Based_on_Neo4j.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\GLQNNP6Y\\Huang and Deng - 2022 - Design and Implementation of Java Knowledge Graph .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\JNWRJ8QZ\\10059598.html}
}

@article{HuangMengXingJiYuDianZiBingLiDeShiTiShiBieHeZhiShiTuPuGouJianDeYanJiu2019,
  title = {基于电子病历的实体识别和知识图谱构建的研究},
  author = {{黄梦醒} and {李梦龙} and {韩惠蕊}},
  date = {2019},
  journaltitle = {计算机应用研究},
  shortjournal = {Application Research of Computers},
  volume = {36},
  number = {12},
  pages = {3735--3739},
  doi = {10.19734/j.issn.1001-3695.2018.07.0414},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhBqc2p5eXlqMjAxOTEyMDQ1Ggg4djlvYmx6Mg%3D%3D},
  urldate = {2022-08-14},
  abstract = {针对中文电子病历中命名实体识别和实体关系抽取研究方法中存在的问题,提出了一种基于双向长短时记忆网络(bidirectional long short-term memory)与CRF(conditional random field)结合的实体识别和实体关系抽取方法.该方法首先使用词嵌入技术将文本转换为数值向量,作为神经网络BiLSTM的输入,再结合CRF链式结构进行序列标注,输出最大概率序列,并对识别结果知识图谱化.实验证明,该方法对中文电子病历进行实体识别和实体关系抽取时的准确率、召回率、F值有明显的提升.实验结果满足临床中系统应用需求,对帮助研究构建临床决策支持系统、个性化医疗推荐服务有},
  langid = {zh\_CN},
  keywords = {Application Research of Computers,实体关系,实体识别,知识图谱,计算机应用研究,长短时记忆网络,黄梦醒},
  annotation = {海南大学南海海洋资源利用国家重点实验室,海口570228;海南大学信息科学技术学院,海口570228海南大学南海海洋资源利用国家重点实验室,海口570228;海南大学信息科学技术学院,海口570228海南大学南海海洋资源利用国家重点实验室,海口570228;海南大学信息科学技术学院,海口570228\\
国家科技支撑计划资助项目 海南省重大科技计划资助项目~国家自然科学基金资助项目~海南省产学研一体化专项资助项目~海南省自然科学基金资助项目\\
2020-05-11 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 基於電子病歷的識別實體和知識圖譜構建的研究\\
abstractTranslation:  針對中文電子病歷中命名實體識別和實體關係抽取研究方法中存在的問題，提出了一種基於實體長短時記憶網絡（雙向長短時記憶）與CRF（條件隨機場）結合的實體識別和實體該方法首先採用詞嵌入技術將文本轉換為數值處理，神經網絡BiLSTM的關係輸入，再結合CRF鏈實驗式結構進行序列標註，輸出最大概率序列，文本識別結果知識圖譜化。該方法對中文電子病歷實體進行識別和關係實體抽取時的準確率、召回率、F值有明顯提升。實驗結果滿足臨床中系統應用需求，對幫助研究構建臨床中系統、個性化醫療決策支持系統推薦服務有},
  file = {C:\Users\BlackCat\Zotero\storage\PL8EBPML\黄 等。 - 2019 - 基于电子病历的实体识别和知识图谱构建的研究.pdf}
}

@thesis{HuangQiaoYiDianJiYuXianZhiLuoJiTuZhiCeShiAnLiChanShengQi2015,
  title = {奠基於限制邏輯圖之測試案例產生器},
  author = {{黃喬苡}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2015},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/xvn69z},
  abstract = {限制式測試案例產生技術是一種主要的測試案例產生技術。限制式測試案例產生技術將測試案例產生問題制定為限制滿足問題。在限制式測試案例產生技術中，從軟體行為規格的敘述、軟體等價行為的分割、軟體測試覆蓋標準滿足的管理、到軟體等價行為所對應的限制滿足問題的敘述，都會牽涉到限制邏輯運算式。本論文提出使用限制邏輯圖做為限制邏輯運算式的精簡圖形化的表示式。 本論文描述一個基於限制邏輯圖的黑箱函式層級測試案例產生器的設計與實作。限制邏輯運算式通常都表示成分離性標準式來，一個限制邏輯圖可以看成是分離性標準式的精簡圖形化的表示式。限制邏輯圖的每一條完整路徑對應分離性標準式中的一個連結性子句，亦即一個測試案例。因此，限制邏輯圖適合於從事軟體等價行為的分割。限制邏輯圖也適合於從事圖形式測試覆蓋標準滿足的管理。這個測試案例產生器被實作成一個Eclipse開發平台的外掛套件。本論文也提供這個測試案例產生器的初步效能評估。},
  pagetotal = {169}
}

@thesis{HuangQinWeiGuiGeQuDongKaiFaHuanJingDeZengLiangCeShiAnLiChanShengYuBanBenKongZhi2021,
  title = {規格驅動開發環境的增量測試案例產生與版本控制},
  author = {{黃欽偉}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2021},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/pwycx8},
  abstract = {規格驅動開發為一種軟體開發的方法，先依據軟體規格，自動產生測試案例，再以通過測試案例，來驅動程式碼的實作。在軟體開發的過程中，會以增量的方式逐步完成。本研究團隊已結合Eclipse開發環境及JUnit測試框架，開發測試案例自動產生技術，及規格驅動開發整合環境。為支援規格驅動開發的增量驅動特性，本論文開發一個增量測試案例自動產生技術，依據增量軟體規格，有效率地自動產生增量測試案例。增量規格驅動開發過程中，會產生大量版本的增量軟體規格、增量測試案例、及增量程式碼文件。支援完整的增量規格驅動開發，必須妥適維護及追蹤這些大量版本的增量文件。本論文也開發一個版本控制技術來維護及追蹤大量版本的增量文件。本論文也初步評估了增量測試案例自動產生技術對規格驅動開發整合環境的效率與品質影響。在測試案例產生的效率上，使用DC測試覆蓋標準時，增量測試案例產生比全量測試案例產生可以減少平均約64\%的時間。使用DCC測試覆蓋標準時，則可以減少平均約70\%的時間。在測試案例產生的品質上，增量測試案例產生所產生的測試案例集和全量測試案例產生所產生的完全一樣。使用DC測試覆蓋標準時，可以達到平均100\%的語句覆蓋率及平均85\%的突變覆蓋率。使用DCC測試覆蓋標準時，則可以達到平均100\%的語句覆蓋率及平均99.5\%的突變覆蓋率。},
  pagetotal = {184},
  keywords = {實驗室}
}

@thesis{HuangShunGengYingYongBieMingZiXunDeChengXuJianChangShuChuanDi1996,
  title = {應用別名資訊的程序間常數傳遞},
  author = {{黃舜耕}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {1996},
  journaltitle = {資訊工程學系},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/27a75q},
  abstract = {在編譯器的最佳化過程中，如果能夠知道程式中那些變數恒為常數，並以 該常數值取代該 變數，則可以簡化計算過程，產生較精簡的程式，增加 程式的執行效率。然而程序間的叫 用會造成分析上的困難，使得常數傳 遞的能力降低。所以，在本篇論文中，我們說明一個 如何跨越程序與程 序間的邊界而對整個程式進行常數傳遞的演算法，以改進常數傳遞對程 式最佳化的幫助。另外，我們討論別名關係對於常數傳遞的影響，利用程 序間別名關係分 析的結果來增加常數傳遞的能力。 In the optimizing process of a compiling system，the compiler P better code to enhance the performance of the program if it knows which        variables will have constant values at run time，and replace them with those   constant values .  Procedure invocations make constant propagation difficult   and will reduce the capability of constant propagation without careful considerations. This paper presents an algorithm for interprocedural constant  propagation to deal with that difficulty. In additional，we discuess the       influences of aliases in constant propagation and use the results of interprocedural alias analysis to increase the capability of constant          propagation .},
  pagetotal = {78}
}

@thesis{HuangZhiWenYingYongJingLuoNengLiangDeRuAiFenXi2008,
  title = {應用經絡能量的乳癌分析},
  author = {{黃治文}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2008},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/29rp69},
  abstract = {乳癌是婦女常見的癌症之一，目前仍然沒有有效的醫學預防方法，只能期望及早發現、及早治療。而中醫歷史悠久，有一套完善的治療理論，適合診斷病患整體狀況，發現潛藏疾病。近代更有良導絡診療學與能量醫學理論的出現，促使中醫的科學化。 本論文利用經絡能量分析儀檢測乳癌病患與正常人的經絡能量，並使用「支援向量機」與「最近K個鄰居」兩種分類技術分析經絡能量與乳癌的關係。本研究希望尋找一個能比一般乳癌偵測技術更早發現乳癌的方法。 實驗結果顯示，使用右十二經脈原穴配合支援向量機的線性核心是所有實驗中準確率最高者，達到78.33％，其次是使用右膀胱經原穴京骨穴配合最近K個鄰居分析技術，達到77.31\%。實驗結果並且顯示右京骨穴數值在13.5以下者，有88％是乳癌患者，在49.3以上者，有83％是正常人。 在目前中醫文獻中，尚未出現京骨穴與乳癌有關的記載，本論文實驗顯示京骨穴與乳癌有顯著的相關性。本論文雖然尚無法提出有力的證據來驗證它們之間的關係，但是相信這個發現未來非常值得做進一步的深入探討。},
  pagetotal = {46}
}

@inproceedings{hubenxiangGuanYuZhongYiYaoYanJiuFangFaDeSiKao2015,
  title = {关于中医药研究方法的思考},
  author = {{Hu Benxiang} and {胡本祥} and {Sun Lijun} and {孙理军} and {Lin Jie} and {林洁}},
  date = {2015},
  url = {https://d.wanfangdata.com.cn/conference/8794112},
  urldate = {2022-10-23},
  abstract = {近二十多年来,由于相关的政策导向,中医药自身困难和存在问题,中医药研究在一定程度上偏离了中医药学自身的发展轨迹,陷入了悖离甚至否定中医药学的泥潭和误区中.因此,中医药工作者必须认真思考自身存在的问题和面临的挑战,加强自身的队伍建设,理性面对和理解在东西方不同文化背景下形成的中西医两个医学体系之间的异同,清醒地认识中药治病的物质基础并非完全是化学成分,中药治病的机理不同于西药的作用靶点,客观评价其理论特点,研究方法,研究成果,临床疗效等,相互借鉴,取长补短,探寻中医药研究的新方法,创新和发展中医药理论.},
  keywords = {No DOI found},
  file = {C:\Users\BlackCat\Zotero\storage\TWFMDZDD\Benxiang 等。 - 2015 - 关于中医药研究方法的思考.pdf}
}

@inproceedings{hubertdariuszzajacGroundTruthDare2023,
  title = {Ground {{Truth Or Dare}}: {{Factors Affecting The Creation Of Medical Datasets For Training AI}}},
  shorttitle = {Ground {{Truth Or Dare}}},
  booktitle = {Proceedings of the 2023 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {{Hubert Dariusz Zając} and {Natalia Rozalia Avlona} and {Finn Kensing} and {Tariq Osman Andersen} and {Irina Shklovski}},
  year = {8 月 29, 2023},
  series = {{{AIES}} '23},
  pages = {351--362},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3600211.3604766},
  url = {https://dl.acm.org/doi/10.1145/3600211.3604766},
  urldate = {2023-09-13},
  abstract = {One of the core goals of responsible AI development is ensuring high-quality training datasets. Many researchers have pointed to the importance of the annotation step in the creation of high-quality data, but less attention has been paid to the work that enables data annotation. We define this work as the design of ground truth schema and explore the challenges involved in the creation of datasets in the medical domain even before any annotations are made. Based on extensive work in three health-tech organisations, we describe five external and internal factors that condition medical dataset creation processes. Three external factors include regulatory constraints, the context of creation and use, and commercial and operational pressures. These factors condition medical data collection and shape the ground truth schema design. Two internal factors include epistemic differences and limits of labelling. These directly shape the design of the ground truth schema. Discussions of what constitutes high-quality data need to pay attention to the factors that shape and constrain what is possible to be created, to ensure responsible AI design.},
  isbn = {9798400702310},
  keywords = {Data Creation,Medical Datasets,Responsible Artificial Intelligence and Machine Learning,已整理,機器學習,監督式學習,醫學},
  annotation = {1 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\33D9QH6Y\Zając 等。 - 2023 - Ground Truth Or Dare Factors Affecting The Creati.pdf}
}

@article{HuDingXingMianXiangZhongYiDianZiBingLiDeZhengZhuangShiTiJiShuXingChouQu2022,
  title = {面向中医电子病历的症状实体及属性抽取},
  author = {{胡定兴} and {杜建强} and {石强} and {罗计根} and {刘勇}},
  date = {2022},
  journaltitle = {现代信息科技},
  shortjournal = {Modern Informationn Technology},
  volume = {6},
  number = {3},
  pages = {70--75},
  doi = {10.19850/j.cnki.2096-4706.2022.03.019},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5Eg94ZHh4a2oyMDIyMDMwMjAaCG1tMnc0OWQ3},
  urldate = {2022-08-14},
  abstract = {文章针对中医临床症状实体及属性抽取存在医疗短文本语义信息欠缺,常用的流水线方法易导致多任务之间产生错误累积的问题,提出一种基于深度学习的症状实体及属性抽取方法.首先通过基于BLSTM-CRF的序列标注模型完成"实体/修饰属性"识别;其次根据扩展步长的就近匹配原则生成高覆盖率、低冗余度的"实体—属性值"候选对;最后基于ERNIE-BGRU-MP完成关系分类,利用ERNIE丰富文本上下文信息,联合BGRU提取文本全局特征信息,采用最大池化法过滤冗余和噪声信息,提高模型的泛化性和鲁棒性.},
  langid = {zh\_CN},
  keywords = {BGRU,ERNIE,Modern Informationn Technology,中医药信息学,最大池化,实体及属性抽取,现代信息科技,石强},
  annotation = {江西中医药大学 计算机学院,江西 南昌 330004江西中医药大学 岐黄国医书院,江西 南昌 330004\\
江西省自然科学基金 江西省教育厅科技项目~江西省一流学科建设科研启动基金专项项目~国家重点研发计划~国家自然科学基金\\
2022-06-15 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 面向中醫電子病歷的症狀實體及屬性抽取\\
abstractTranslation:  文章針對中醫臨床症狀實體及屬性抽取存在醫療短文本語義信息不足、常用的模擬方法易導致多任務之間產生錯誤累積的問題，提出一種基於深度學習的症狀實體及屬性抽取方法。 BLSTM-CRF的序列核心模型完成“實體/修飾屬性”識別；其次根據擴展步長的就近匹配原則生成高覆蓋率、低動畫度的“實體—屬性值”候選對；最後基於ERNIE-BGRU- MP完成分類，利用ERNIE豐富的文本上下文信息，聯合BGRU提取文本全局特徵信息，採用最大池關係化法過濾噪聲和噪聲信息，提高模型的泛化性和魯棒性。},
  file = {C:\Users\BlackCat\Zotero\storage\K8Q2DYRI\胡 等。 - 2022 - 面向中医电子病历的症状实体及属性抽取.pdf}
}

@online{huibuAISHELL1OpenSourceMandarin2017,
  title = {{{AISHELL-1}}: {{An Open-Source Mandarin Speech Corpus}} and {{A Speech Recognition Baseline}}},
  shorttitle = {{{AISHELL-1}}},
  author = {{Hui Bu} and {Jiayu Du} and {Xingyu Na} and {Bengu Wu} and {Hao Zheng}},
  date = {2017-09-16},
  eprint = {1709.05522},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1709.05522},
  url = {http://arxiv.org/abs/1709.05522},
  urldate = {2023-09-25},
  abstract = {An open-source Mandarin speech corpus called AISHELL-1 is released. It is by far the largest corpus which is suitable for conducting the speech recognition research and building speech recognition systems for Mandarin. The recording procedure, including audio capturing devices and environments are presented in details. The preparation of the related resources, including transcriptions and lexicon are described. The corpus is released with a Kaldi recipe. Experimental results implies that the quality of audio recordings and transcriptions are promising.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,中文,語音辨識,資料集},
  annotation = {titleTranslation: AISHELL-1：開源普通話語音語料庫與語音辨識基線\\
abstractTranslation:  名為 AISHELL-1 的開源普通話語音語料庫發布。它是迄今為止最大的適合進行普通話語音辨識研究和建構語音辨識系統的語料庫。詳細介紹了錄音過程，包括音訊擷取設備和環境。描述了相關資源的準備，包括轉錄和字典。該語料庫隨 Kaldi 配方一起發布。實驗結果顯示錄音和轉錄的品質是有希望的。},
  note = {Comment: Oriental COCOSDA 2017},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\74NJE2KR\\Bu 等。 - 2017 - AISHELL-1 An Open-Source Mandarin Speech Corpus a.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\P27D5758\\1709.html}
}

@inproceedings{huiyangLearningDistanceMetric2008,
  title = {Learning the Distance Metric in a Personal Ontology},
  booktitle = {Proceedings of the 2nd International Workshop on {{Ontologies}} and Information Systems for the Semantic Web},
  author = {{Hui Yang} and {Jamie Callan}},
  year = {10 月 30, 2008},
  series = {{{ONISW}} '08},
  pages = {17--24},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1458484.1458488},
  url = {https://dl.acm.org/doi/10.1145/1458484.1458488},
  urldate = {2023-09-16},
  abstract = {Personal ontology construction is the task of sorting through relevant materials, identifying the main topics and concepts, and organizing them to suit personal needs. Automatic construction of personal ontologies is difficult in part because measuring the semantic distance between two concepts is difficult. Knowledge-based approaches use either knowledge bases, such as WordNet, or lexico-syntactic patterns to induce the differences between concepts. However, these techniques are only applicable for a subset of concepts and leave the majority unmeasurable. On the other hand, statistical approaches are able to induce the differences between any concept pair but lack of human knowledge involvement and hence suffer from low precision. In the context of personal ontology construction, semantic distances between concepts need to reflect personal preferences. Based on that, this paper presents a supervised hierarchical clustering framework to incorporate personal preferences for distance metric learning in personal ontology construction. In this framework, periodic manual guidance provides training data for learning a distance metric and the learned metric is used during automatic activities to further construct the ontology. A detailed user study demonstrates that the approach is effective and accelerates the construction of personal ontologies.},
  isbn = {978-1-60558-255-9},
  langid = {english},
  keywords = {/unread,supervised hierarchical clustering,人機互動,可解釋性,嵌入,已整理,機器學習,知識本體},
  annotation = {5 citations (Crossref) [2024-03-26]\\
abstractTranslation:  個人本體建構是對相關資料進行整理，確定主要主題和概念，並根據個人需求進行組織的任務。自動建立個人本體很困難，部分原因是測量兩個概念之間的語意距離很困難。基於知識的方法使用知識庫（例如 WordNet）或詞彙句法模式來歸納概念之間的差異。然而，這些技術僅適用於一部分概念，而大多數概念無法測量。另一方面，統計方法能夠歸納任何概念對之間的差異，但缺乏人類知識的參與，因此精度較低。在個人本體建構的脈絡下，概念之間的語意距離需要反映個人偏好。在此基礎上，本文提出了一個有監督的層次聚類框架，將個人對距離度量學習的偏好納入個人本體建構中。在此框架中，定期手動指導提供用於學習距離測量的訓練數據，並且在自動活動期間使用學習到的測量來進一步建立本體。詳細的使用者研究表明該方法是有效的，並加速了個人本體的建構。\\
titleTranslation: 學習個人本體中的距離測量},
  note = {建議使用者定期使用此模型來分析詞向量。},
  file = {C:\Users\BlackCat\Zotero\storage\2VXUZG9S\Yang 與 Callan - 2008 - Learning the distance metric in a personal ontolog.pdf}
}

@inproceedings{huiyangMetricbasedFrameworkAutomatic2009,
  title = {A {{Metric-based Framework}} for {{Automatic Taxonomy Induction}}},
  booktitle = {Proceedings of the {{Joint Conference}} of the 47th {{Annual Meeting}} of the {{ACL}} and the 4th {{International Joint Conference}} on {{Natural Language Processing}} of the {{AFNLP}}},
  author = {{Hui Yang} and {Jamie Callan}},
  date = {2009-08},
  pages = {271--279},
  publisher = {Association for Computational Linguistics},
  location = {Suntec, Singapore},
  url = {https://aclanthology.org/P09-1031},
  urldate = {2023-09-19},
  eventtitle = {{{ACL-IJCNLP}} 2009},
  langid = {english},
  keywords = {未整理},
  annotation = {titleTranslation: 基於度量的自動分類歸納框架},
  file = {C:\Users\BlackCat\Zotero\storage\2BP25V5S\Yang 與 Callan - 2009 - A Metric-based Framework for Automatic Taxonomy In.pdf}
}

@article{HuJiaHuiJiYuZhuDongXueXiDeZhongWenDianZiBingLiMingMingShiTiShiBieYanJiu2020,
  title = {基于主动学习的中文电子病历命名实体识别研究},
  author = {{胡佳慧} and {赵琬清} and {方安} and {范云满}},
  date = {2020},
  journaltitle = {中国数字医学},
  shortjournal = {China Digital Medicine},
  volume = {15},
  number = {11},
  pages = {6--9},
  doi = {10.3969/j.issn.1673-7571.2020.11.002},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5Eg96Z3N6eXgyMDIwMTEwMDIaCGE5d3V5bmo2},
  urldate = {2022-08-14},
  abstract = {目的:开展基于主动学习的中文电子病历命名实体识别研究,旨在优化机器学习模型,并降低标注成本.方法:分析比较信息熵、语料长度以及随机查询3种不同算法,采用CCKS 2018 CNER评测语料,识别病历文本中的解剖部位、手术、药物、独立症状、症状描述5类实体.结果:在字数成本节约方面,基于熵的方法相比基于长度的方法节省了41％的字数;在语料成本节约方面,基于熵的算法相比随机抽样节省了46％的语料.结论:基于信息熵的主动学习方法通过选择待选标注集中最不确定的语料进行标注,可显著减少对标注语料的依赖,降低人工标注成本.},
  langid = {zh\_CN},
  keywords = {中文電子病歷CEMR,主動學習,信息熵,命名實體識別,語料標注},
  annotation = {中国医学科学院医学信息研究所,100020,北京市朝阳区雅宝路3号中国医学科学院医学信息研究所,100020,北京市朝阳区雅宝路3号中国医学科学院医学信息研究所,100020,北京市朝阳区雅宝路3号中国医学科学院医学信息研究所,100020,北京市朝阳区雅宝路3号\\
创新工程项目 中央级公益性科研院所基本科研业务费专项\\
2021-01-12 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 基於學習主動性的中文電子病歷命名實體識別研究\\
abstractTranslation:  目的：開展基於主動學習的中文電子病歷命名實體識別研究，旨在優化機器學習模型，並降低標籤成本。方法：分析比較信息熵、語料長度以及隨機查詢3種不同算法，採用CCKS 2018 CNER得分語料，識別病歷文本中的解剖部位、手術、藥物、獨立症狀、症狀描述5 類實體。結果：在字數成本節省方面，基於熵的方法與基於長度的方法節省了41\% 的字數；用語料成本節約方面，基於熵的算法比隨機浪費浪費了46\%的語料。結論：基於信息熵的主動學習方法通\hspace{0pt}\hspace{0pt}過選擇待選標籤集中最不確定的語料進行標籤，可顯著減少對標籤語料的依賴，降低人工標註成本。},
  file = {C:\Users\BlackCat\Zotero\storage\PJXGRAFD\胡 等。 - 2020 - 基于主动学习的中文电子病历命名实体识别研究.pdf}
}

@article{HuJiaHuiMianXiangZhiShiFaXianDeZhongWenDianZiBingLiBiaoZhuFangFaYanJiu2019,
  title = {面向知识发现的中文电子病历标注方法研究},
  author = {{胡佳慧} and {方安} and {赵琬清} and {杨晨柳} and {任慧玲}},
  date = {2019},
  journaltitle = {数据分析与知识发现},
  shortjournal = {Data Analysis and Knowledge Discovery},
  volume = {3},
  number = {7},
  pages = {123--132},
  doi = {10.11925/infotech.2096-3467.2018.1454},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhF4ZHRzcWJqczIwMTkwNzAxMxoIOHY5b2JsejI%3D},
  urldate = {2022-08-14},
  abstract = {[目的]研究基于中文电子病历的标注方法,提升临床文本分析与处理能力,促进临床知识发现.[方法]提出中文电子病历标注思路,并构建可视化交互平台,基于电子病历文本的字与词特征,综合利用自然语言处理和机器学习方法开展临床命名实体识别实证研究.[结果]获得700份标注病历语料,基于Pipeline的标注方法总体F值达0.8772,较基于原始标注病历数据集的命名实体识别效果提升32.9％.[局限]由于电子病历包含与隐私相关的敏感信息,本研究基于开放评测数据开展实验研究,语料库大小受限.[结论]本研究所提出的中文电子病历标注方法和所构建的标注平台适用于临床文本处理,能够促进医学临床文本资源的知识关联化.},
  langid = {zh\_CN},
  keywords = {Data Analysis and Knowledge Discovery,中文电子病历,数据分析与知识发现,文本标注,机器学习,知识发现,自然语言处理,赵琬清},
  annotation = {中国医学科学院医学信息研究所 北京100020中国医学科学院医学信息研究所 北京100020中国医学科学院医学信息研究所 北京100020中国医学科学院医学信息研究所 北京100020中国医学科学院医学信息研究所 北京100020\\
中国医学科学院中央级公益性科研院所基本科研业务费项目“面向知识发现的中文电子病历语义标注方法研究” 中国医学科学院医学与健康科技创新工程协同创新团队项目“中文临床医学术语系统构建研究”\\
2019-10-15 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 面向知識發現的中文電子病歷標註方法研究\\
abstractTranslation:  [目的]研究基於中文電子病歷的標註方法，提升臨床文本分析與處理能力，促進臨床知識發現。[方法]提出中文電子病歷的標註思路，並構建可視化交互平台，基於電子病歷文本的文字與關鍵詞特徵綜合利用自然語言處理和機器學習方法開展臨床命名實體識別特徵研究。[結果]獲得700份標註病歷語料，基於Pipeline的標註方法總體F值達到0.8772，基於較原始的標註病歷數據集的命名實體識別效果提升32.9\%。[十一]由於電子病歷包含與隱私的敏感信息，本研究基於開放體育數據開展實驗研究，語料庫大小確定。[結論]本研究所提出了中文電子病歷標註相關方法和所構建標註平台適用於臨床文本處理，能夠促進醫學臨床文本資源的知識關聯化。},
  file = {C:\Users\BlackCat\Zotero\storage\2G58JYP8\胡 等。 - 2019 - 面向知识发现的中文电子病历标注方法研究.pdf}
}

@online{huXTREMEMassivelyMultilingual2020,
  title = {{{XTREME}}: {{A Massively Multilingual Multi-task Benchmark}} for {{Evaluating Cross-lingual Generalization}}},
  shorttitle = {{{XTREME}}},
  author = {Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  date = {2020-09-04},
  eprint = {2003.11080},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.11080},
  url = {http://arxiv.org/abs/2003.11080},
  urldate = {2024-03-14},
  abstract = {Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,已整理,待讀,資料集,資訊檢索},
  annotation = {titleTranslation: XTREME：用於評估跨語言泛化的大規模多語言多任務基準\\
abstractTranslation:  機器學習模型在 NLP 中的應用的最新進展都是由跨各種任務評估模型的基準驅動的。然而，這些廣泛涵蓋的基準大多僅限於英語，儘管人們對多語言模型的興趣日益濃厚，但仍缺乏能夠在多種語言和任務上綜合評估此類方法的基準。為此，我們引入了多語言編碼器 XTREME 基準的跨語言遷移評估，這是一個多任務基準，用於評估跨 40 種語言和 9 項任務的多語言表示的跨語言泛化能力。我們證明，雖然在英語上測試的模型在許多任務上達到了人類的表現，但跨語言遷移模型的表現仍然存在相當大的差距，特別是在句法和句子檢索任務上。跨語言的結果也有廣泛的分佈。我們發布該基準是為了鼓勵跨語言學習方法的研究，這些方法可以跨多種代表性的語言和任務傳遞語言知識。},
  note = {Comment: In Proceedings of the 37th International Conference on Machine Learning (ICML). July 2020},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\C5RT6UJY\\Hu 等。 - 2020 - XTREME A Massively Multilingual Multi-task Benchm.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\RYDE9TY2\\2003.html}
}

@thesis{HuYiLiangDianJiYuQuYuSouXunDeZuiJiaHuaXuanXiangZiDongXuanQu2008,
  title = {奠基於區域搜尋的最佳化選項自動選取},
  author = {{胡逸良}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2008},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/y5gz7f},
  abstract = {傳統編譯器最佳化選項自動選取工具，大都是奠基於像基因演算法這類的全域搜尋演算法。這類搜尋演算法為了找到全域最佳解，通常都會在龐大的搜尋空間花上大量的搜尋時間。由於現存的最佳化層級，對程式效能已有不錯的提升效果。本論文利用爬山法這類的區域搜尋演算法快速地去找出比原先結果還要好的區域最佳解。我們並且針對弁鈱},
  pagetotal = {49}
}

@thesis{HuYiTingDianJiYuWuJianXianZhiYuYanYuXianZhiLuoJiChengShiDeHanShiCengJiDanYuanCeShiGongJu2010,
  title = {奠基於物件限制語言與限制邏輯程式的函式層級單元測試工具},
  author = {{胡伊婷}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2010},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/734mgq},
  abstract = {為了確保軟體品質，軟體測試在軟體開發中佔有很重要的階段。本論文提出一個黑箱測試採用合約設計的軟體開發方法，並使用物件限制語言描述Java函式的先決條件及後置條件，便能依據先決條件及後置條件自動產生測試案例。本論文開發一個自動測試工具架構建立在JUnit 平台上，能產生針對Java類別的測試類別。搭配限制邏輯語言能自動產生測試輸入及預期輸出，而限制邏輯語言擁有強大的能力可以找出限制式的解。如此一來便能同時產生測試輸入及預期輸出。},
  pagetotal = {58}
}

@thesis{HuZiFanJavaLeiBieCengJiDanYuanCeShiGongJu2011,
  title = {Java類別層級單元測試工具},
  author = {{胡子凡}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2011},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/t9xne4},
  abstract = {軟體測試是確保軟體品質最主要的方法。本論文開發一個針對Java程式的類別層級黑箱測試案例產生器。本論文使用類別圖與物件限制語言來描述函式層級的規格，定義函式之內的行為；使用狀態圖來描述類別層級的規格，定義函式之間的行為。軟體測試最困難的地方是測試案例的產生，測試案例包含測試輸入與預期輸出。奠基於限制邏輯程式，本論文可以使用一致的方法同時產生測試輸入與預期輸出，這個一致的方法成因於限制邏輯程式所擁有的強大限制式求解能力。這個方法分成三個步驟。第一個步驟系統化地條列出狀態圖上的測試路徑。第二個步驟將每一條測試路徑上的限制試轉換成一個限制邏輯程式的敘述式，這個敘述式的每一個解答就是一組測試輸入與預期輸出。第三個步驟將這些測試輸入與預期輸出轉換成Java測試函式。本論文使用JUnit測試架構自動執行產生的測試案例。},
  pagetotal = {89}
}

@inproceedings{hyunnamgoongOntologyBasedControlledNatural2023,
  title = {Ontology-{{Based Controlled Natural Language Editor Using CFG}} with {{Lexical Dependency}}},
  booktitle = {The {{Semantic Web}}: 6th {{International Semantic Web Conference}}, 2nd {{Asian Semantic Web Conference}}, {{ISWC}} 2007 + {{ASWC}} 2007, {{Busan}}, {{Korea}}, {{November}} 11-15, 2007. {{Proceedings}}},
  author = {{Hyun Namgoong} and {Hong-Gee Kim}},
  year = {3 月 15, 2023},
  pages = {353--366},
  publisher = {Springer-Verlag},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-76298-0_26},
  url = {https://doi.org/10.1007/978-3-540-76298-0_26},
  urldate = {2023-09-26},
  abstract = {In recent years, CNL (Controlled Natural Language) has received much attention with regard to ontology-based knowledge acquisition systems. CNLs, as subsets of natural languages, can be useful for both humans and computers by eliminating ambiguity of natural languages. Our previous work, OntoPath [10], proposed to edit natural language-like narratives that are structured in RDF (Resource Description Framework) triples, using a domain-specific ontology as their language constituents. However, our previous work and other systems employing CFG for grammar definition have difficulties in enlarging the expression capacity. A newly developed editor, which we propose in this paper, permits grammar definitions through CFG-LD (Context-Free Grammar with Lexical Dependency) that includes sequential and semantic structures of the grammars. With CFG describing the sequential structure of grammar, lexical dependencies between sentence elements can be designated in the definition system. Through the defined grammars, the implemented editor guides users’ narratives in more familiar expressions with a domain-specific ontology and translates the content into RDF triples.},
  isbn = {978-3-540-76297-3},
  keywords = {CNL,Context-Free Grammar,Controlled Natural Language,Lexical Dependency,Look-Ahead Editor,NLP,Ontology,OntoPath,已整理,知識本體},
  annotation = {7 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\HYHZ46Y6\Namgoong 與 Kim - 2023 - Ontology-Based Controlled Natural Language Editor .pdf}
}

@article{ingridhuntIndustryRequirementsThesiswriting2022,
  title = {Industry Requirements, Thesis-Writing and the Emergence of Flexible Educational Programmes: {{Reflections}} on the University Learner Experience},
  shorttitle = {Industry Requirements, Thesis-Writing and the Emergence of Flexible Educational Programmes},
  author = {{Ingrid Hunt} and {Alan Ryan} and {Mícheál Ó hAodha} and {Morteza Rezaei-Zadeh}},
  date = {2022-06},
  journaltitle = {Industry and Higher Education},
  shortjournal = {Industry and Higher Education},
  volume = {36},
  number = {3},
  pages = {319--333},
  issn = {0950-4222, 2043-6858},
  doi = {10.1177/09504222211032908},
  url = {http://journals.sagepub.com/doi/10.1177/09504222211032908},
  urldate = {2023-10-23},
  abstract = {The transformation of new paradigms for online learning delivery has evolved rapidly over the past few years. For postgraduate programmes, the thesis module is regularly the capstone and significant in terms of academic credits. In reality, this module can be just an ‘add on’ set of resources with no dedicated online learning space. Industry students undertaking postgraduate programmes online traditionally feel overwhelmed while embarking on a thesis. Notably, too, they face the challenge of not being on campus and having the same learning opportunities as their on-campus counterparts. This paper highlights the importance of supporting and assisting online industry learners in participating fully with their thesis. The authors identify the challenges that face these learners at postgraduate level, recognising that a new way to help and prepare them to carry out and write a good thesis is essential. By focusing on a dedicated online module for ‘all things thesis’, the paper presents the positive experiences learners can have when participating in this module. The findings emphasise the need for educational providers to offer as part of their programmes a high-quality thesis module designed to support the postgraduate online industry learner.},
  langid = {english},
  keywords = {研究流程},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 行业要求、论文写作和灵活教育课程的出现：对大学学习者经历的思考\\
abstractTranslation:  在过去几年里，在线学习交付新模式的变革发展迅速。对于研究生课程而言，毕业论文模块通常是顶点模块，在学分方面意义重大。实际上，这一模块可能只是一套 "附加 "资源，没有专门的在线学习空间。通过网络学习研究生课程的行业学生在撰写论文时通常会感到不知所措。值得注意的是，他们还面临着不在校内，无法获得与校内学生同等学习机会的挑战。本文强调了支持和帮助在线行业学习者全面参与毕业论文的重要性。作者指出了这些学习者在研究生阶段所面临的挑战，认识到必须采用一种新的方法来帮助他们完成和撰写一篇优秀的毕业论文。通过关注 "毕业论文 "的专门在线模块，论文介绍了学习者在参与该模块时可能获得的积极体验。研究结果强调，教育机构有必要提供高质量的论文模块，作为其课程的一部分，以支持研究生在线行业学习者。},
  note = {[TLDR] The need for educational providers to offer as part of their programmes a high-quality thesis module designed to support the postgraduate online industry learner is emphasised.}
}

@inproceedings{iqramuhammadQueryResolutionLiterature2022,
  title = {Query {{Resolution}} of~{{Literature Knowledge Graphs Using Hybrid Document Embeddings}}},
  booktitle = {Artificial {{Intelligence XXXIX}}},
  author = {{Iqra Muhammad} and {Frans Coenen} and {Carol Gamble} and {Anna Kearney} and {Paula Williamson}},
  editor = {{Max Bramer} and {Frederic Stahl}},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {98--111},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-21441-7_7},
  abstract = {Literature Knowledge Graphs play a critical role in helping domain experts carry out query resolution for finding relevant articles in published literature. Such knowledge graphs are usually in the form of Curated Document Databases (CDDs). Domain Experts and researchers typically query such literature knowledge graphs using some form of query-resolution mechanism. Machine learning techniques can be used to automate query-resolution. This paper presents a document query-resolution mechanism, given a query and set of documents in a knowledge graph, based on a hybrid word embedding that combines knowledge graph embeddings with “traditional” embeddings. A query-document data set extracted from a clinical trials CDD (the ORRCA CDD) was used. Three “traditional” word embeddings were considered: CBOW, BERT and SciBERT. The evaluation demonstrated that hybrid embeddings produced better results than when the embedding models were used in isolation. A best Mean Average Precision of 0.486 was obtained when using a CBOW and random walk knowledge graph hybrid embedding.},
  isbn = {978-3-031-21441-7},
  langid = {english},
  keywords = {Document ranking,Query resolution,Word embedding},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用混合文檔嵌入的文獻知識圖查詢解析\\
abstractTranslation:  文獻知識圖在幫助領域專家進行查詢解析以在已發表文獻中查找相關文章方面發揮著關鍵作用。此類知識圖通常採用策劃文檔數據庫（CDD）的形式。領域專家和研究人員通常使用某種形式的查詢解析機制來查詢此類文獻知識圖。機器學習技術可用於自動化查詢解析。本文提出了一種文檔查詢解析機制，給定知識圖中的查詢和文檔集，基於將知識圖嵌入與“傳統”嵌入相結合的混合詞嵌入。使用從臨床試驗 CDD（ORRCA CDD）中提取的查詢文檔數據集。考慮了三種“傳統”詞嵌入：CBOW、BERT 和 SciBERT。評估表明，混合嵌入比單獨使用嵌入模型產生更好的結果。使用 CBOW 和隨機遊走知識圖混合嵌入時，獲得了 0.486 的最佳平均精度。},
  file = {C:\Users\BlackCat\Zotero\storage\JRRG2542\Muhammad 等。 - 2022 - Query Resolution of Literature Knowledge Graphs Us.pdf}
}

@article{iskandarSolutionThresholdInformation2018,
  title = {The Solution Threshold of Information Overload: {{A}} Systematic Literature Review},
  shorttitle = {The Solution Threshold of Information Overload},
  author = {Iskandar, Karto and Prabowo, Harjanto and Kosala, Raymond and Trisetyarso, Agung},
  date = {2018},
  journaltitle = {ICIC Express Letters},
  volume = {12},
  number = {12},
  pages = {1223--1233},
  url = {http://www.icicel.org/ell/contents/2018/12/el-12-12-05.pdf},
  urldate = {2024-04-30},
  langid = {english},
  keywords = {⛔ No DOI found,Review,已整理,資訊超載},
  file = {C:\Users\BlackCat\Zotero\storage\QMUGRVHL\Iskandar 等。 - 2018 - The solution threshold of information overload A .pdf}
}

@article{ismailbabajideadewumiSyntacticGenerationResearch2023,
  title = {Syntactic {{Generation}} of {{Research Thesis Sketches Across Disciplines Using Formal Grammars}}},
  author = {{Ismail Babajide Adewumi} and {Abejide Ade-Ibijola}},
  date = {2023-05-26},
  journaltitle = {Journal of Information Systems and Informatics},
  shortjournal = {J. Inf. Syst. Informatics},
  volume = {5},
  number = {2},
  pages = {696--718},
  issn = {2656-4882, 2656-5935},
  doi = {10.51519/journalisi.v5i2.470},
  url = {https://journal-isi.org/index.php/isi/article/view/470},
  urldate = {2023-10-23},
  abstract = {A part of the prerequisites for granting a degree in higher education institutions, students at postgraduate levels normally carry out research, which they do report in the form of theses or dissertations. Study has shown that students tend to go through difficulties in writing research thesis across all disciplines because they do not fully comprehend what constitutes a research thesis. This project proposes the syntactic generation of research thesis sketches across disciplines using formal grammars. Sketching is a synthesis technique which enables users to deliver high-level intuitions into a synthesis snag while leaving low-level details to synthesis tools. This work extends sketching to document generation for research thesis documents. Context-free grammar rules were designed and implemented for this task. A link to 10,000 generated thesis sketches was presented.},
  langid = {english},
  keywords = {待讀,研究流程,重要},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用形式語法跨學科研究論文草圖的句法生成\\
abstractTranslation:  作為高等教育機構授予學位的先決條件之一，研究生水平的學生通常會進行研究，並以論文或論文的形式進行報告。研究表明，學生在撰寫跨學科的研究論文時往往會遇到困難，因為他們不完全理解研究論文的組成。該計畫提出使用形式語法跨學科研究論文草圖的句法生成。草圖繪製是一種綜合技術，使用戶能夠將高級直覺傳遞到綜合障礙中，同時將低階細節留給綜合工具。這項工作將草圖擴展到研究論文文檔的文檔生成。為此任務設計並實作了上下文無關語法規則。提供了 10,000 個生成的論文草圖的連結。},
  note = {[TLDR] This project proposes the syntactic generation of research thesis sketches across disciplines using formal grammars using Context-free grammar rules, which were designed and implemented for this task.},
  file = {C:\Users\BlackCat\Zotero\storage\TIP2LTEB\Adewumi and Ade-Ibijola - 2023 - Syntactic Generation of Research Thesis Sketches A.pdf}
}

@article{ismailmendilFormalDomaindrivenSystem2023,
  title = {Formal Domain-Driven System Development in {{Event-B}}: {{Application}} to Interactive Critical Systems},
  shorttitle = {Formal Domain-Driven System Development in {{Event-B}}},
  author = {{Ismail Mendil} and {Yamine Aït-Ameur} and {Neeraj Kumar Singh} and {Guillaume Dupont} and {Dominique Méry} and {Philippe Palanque}},
  year = {2 月 1, 2023},
  journaltitle = {Journal of Systems Architecture: the EUROMICRO Journal},
  shortjournal = {J. Syst. Archit.},
  volume = {135},
  number = {C},
  issn = {1383-7621},
  doi = {10.1016/j.sysarc.2022.102798},
  url = {https://doi.org/10.1016/j.sysarc.2022.102798},
  urldate = {2023-09-27},
  abstract = {The design of complex and/or critical systems requires handling the environment constraints in which these systems evolve. Formal methods allow system developers to design models of such systems. They provide constructs for modelling components and views of these systems. However, these formal methods do not include built-in constructs for modelling the environment, and more broadly, domain knowledge associated with system models. Although ontologies have demonstrated their efficiency in modelling domain-specific features, they are not available as built-in constructs in formal methods. This paper shows how formal ontologies can be used to model domain-specific knowledge, as well as how system models may refer to these ontologies through annotation. We rely on the Event-B refinement and proof state-based method, and the associated theories, to define a framework in which domain-specific knowledge ontologies are formalised as Event-B theories defining data types used to type Event-B system design models. Finally, this framework is deployed for the specific case of interactive critical systems. To illustrate the proposed approach, a case study of the Traffic Collision Avoidance System (TCAS) is developed.},
  keywords = {Domain knowledge,Event-B,Formal methods,Interactive system,Ontology,Refinement and proofs,TCAS,未整理},
  annotation = {0 citations (Crossref) [2024-03-26]}
}

@article{j.a.morente-molineraCreatingKnowledgeDatabases2016,
  title = {Creating Knowledge Databases for Storing and Sharing People Knowledge Automatically Using Group Decision Making and Fuzzy Ontologies},
  author = {{J. A. Morente-Molinera} and {I. J. Pérez} and {M. R. Ureña} and {E. Herrera-Viedma}},
  date = {2016-01-20},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {328},
  pages = {418--434},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2015.08.051},
  url = {https://www.sciencedirect.com/science/article/pii/S0020025515006507},
  urldate = {2023-09-16},
  abstract = {Over the last decade, the Internet has undergone a profound change. Thanks to Web 2.0 technologies, the Internet has become a platform where everybody can participate and provide their own personal information and experiences. Ontologies were designed in an effort to sort and categorize all sorts of information. In this paper, an automatized method for retrieving the subjective Internet users information and creating ontologies is described. Thanks to this method, it is possible to automatically create knowledge databases using the common knowledge of a large amount of people. Using these databases, anybody can consult and benefit from the retrieved information. Group decision making methods are used to extract users information and fuzzy ontologies are employed to store the collected knowledge.},
  langid = {english},
  keywords = {/unread,Computing with words,Fuzzy linguistic modeling,Fuzzy ontology,Group decision making,Multi-granular linguistic information,回收,知識分享,知識本體},
  annotation = {33 citations (Crossref) [2024-03-26]\\
abstractTranslation:  在過去的十年裡，網路發生了深刻的變化。由於 Web 2.0 技術，互聯網已成為每個人都可以參與並提供自己的個人資訊和體驗的平台。本體的設計是為了對各種資訊進行排序和分類。本文描述了一種檢索主觀網路使用者資訊並創建本體的自動化方法。透過這種方法，可以利用大量人員的共同知識自動建立知識資料庫。使用這些資料庫，任何人都可以查閱檢索到的資訊並從中受益。使用群體決策方法來提取使用者訊息，並使用模糊本體來儲存收集到的知識。\\
titleTranslation: 使用群體決策和模糊本體自動建立知識資料庫來儲存和分享人們的知識},
  note = {群體決策group decision making
\par
知識術語(?)linguistic term},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\BIZ2DGUD\\Morente-Molinera 等。 - 2016 - Creating knowledge databases for storing and shari.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\TK5GVIWM\\S0020025515006507.html}
}

@article{j.agoguenLfuzzySets1967,
  title = {L-Fuzzy Sets},
  author = {{J. A Goguen}},
  date = {1967-04-01},
  journaltitle = {Journal of Mathematical Analysis and Applications},
  shortjournal = {Journal of Mathematical Analysis and Applications},
  volume = {18},
  number = {1},
  pages = {145--174},
  issn = {0022-247X},
  doi = {10.1016/0022-247x(67)90189-8},
  url = {https://www.sciencedirect.com/science/article/pii/0022247X67901898},
  urldate = {2023-05-24},
  langid = {english},
  annotation = {1745 citations (Crossref) [2024-03-26]\\
2455 citations (Semantic Scholar/DOI) [2023-05-24]\\
titleTranslation: L-模糊集},
  file = {C:\Users\BlackCat\Zotero\storage\2EQR4NRV\Goguen - 1967 - L-fuzzy sets.pdf}
}

@article{j.d.myersReintegratingResearchRecord2003,
  title = {Re-Integrating the Research Record},
  author = {{J.D. Myers} and {A. Chappell} and {M. Elder} and {A. Geist} and {L. Schwidder}},
  date = {2003-05},
  journaltitle = {Computing in Science \& Engineering},
  volume = {5},
  number = {3},
  pages = {44--50},
  issn = {1558-366X},
  doi = {10.1109/mcise.2003.1196306},
  abstract = {Describes the Scientific Annotation Middleware, a set of components and services that support the creation and use of annotation metadata describing data objects and the semantic relationships among them. It captures aspects of data processing history and the research process and federates the results into a coherent human- and machine-interpretable research record.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  langid = {english},
  keywords = {Collaboration,Collaborative software,Costs,Databases,Information analysis,Laboratories,Logic,Middleware,Protocols,Scientific Databases,Standardization,未分類},
  annotation = {58 citations (Crossref) [2024-03-26]\\
titleTranslation: 重新整合研究記錄\\
abstractTranslation:  描述科學註釋中間件，這是一組組件和服務，支持創建和使用描述數據對象及其語義關係的註釋元數據。它捕獲數據處理歷史和研究過程的各個方面，並將結果聯合成連貫的人類和機器可解釋的研究記錄。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\9K7FCRU7\\Myers 等。 - 2003 - Re-integrating the research record.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\37BIPACM\\keywords.html}
}

@article{jamshaidashrafOntologyUsageAnalysis2015,
  title = {Ontology Usage Analysis in the Ontology Lifecycle},
  author = {{Jamshaid Ashraf} and {Elizabeth Chang} and {Omar Khadeer Hussain} and {Farookh Khadeer Hussain}},
  year = {5 月 1, 2015},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Know.-Based Syst.},
  volume = {80},
  number = {C},
  pages = {34--47},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2015.02.026},
  url = {https://doi.org/10.1016/j.knosys.2015.02.026},
  urldate = {2023-09-27},
  abstract = {The Semantic Web envisions a Web where information is accessible and processable by computers as well as humans. Ontologies are the cornerstones for realizing this vision of the Semantic Web by capturing domain knowledge through the defined terms and the relationship between them to provide a formal representation of the domain with machine-understandable semantics. Given the importance of ontologies, a significant amount of work in the literature has been done on knowledge representation on the Web which includes ontology development (ontology engineering), ontology evaluation, ontology population and ontology evolution. As a result, numerous domain ontologies have been developed however, not much attention has been given to the area of ontology usage analysis that shows how the developed ontologies are being used. Such a study is very important as we explain in this paper by using some motivational scenarios of a Semantic Web user in different roles. The discussion is followed by a summary of the state-of-the-art of the existing literature on ontology usage to highlight gaps in this area. We define Ontology Usage Analysis and discuss our proposed Ontology Usage Analysis Framework (OUSAF) to measure the usage of an ontology on the web from different perspectives. On a real-world collected dataset, the results obtained from OUSAF demonstrate the practical utilization of OUSAF in measuring ontology usage analysis.},
  keywords = {Ontology,Ontology lifecycle,Ontology usage analysis framework,Semantic web,Usage analysis,未整理},
  annotation = {19 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\W5HKTKQZ\Ashraf 等。 - 2015 - Ontology usage analysis in the ontology lifecycle.pdf}
}

@article{jandemooijComputationalFrameworkOrganizing2022,
  title = {A {{Computational Framework}} for {{Organizing}} and {{Querying Cultural Heritage Archives}}},
  author = {{Jan de Mooij} and {Can Kurtan} and {Jurian Baas} and {Mehdi Dastani}},
  year = {9 月 16, 2022},
  journaltitle = {Journal on Computing and Cultural Heritage},
  shortjournal = {J. Comput. Cult. Herit.},
  volume = {15},
  number = {3},
  pages = {45:1--45:25},
  issn = {1556-4673},
  doi = {10.1145/3485843},
  url = {https://doi.org/10.1145/3485843},
  urldate = {2023-09-09},
  abstract = {Now that within the humanities more and more data sources have been created, a new opportunity is within reach: the searching of patterns spanning across data sources from archives, museums, and other cultural heritage institutes. These institutes adopt various digitization strategies based on differences in selection procedures. This results in heterogeneous data sources with a huge impact on the accessibility and interoperability of data within and between these distributed collections. We identify three interrelated challenges that researchers may encounter when querying such distributed data sources, namely query formulation, source selection, and alignment of data sources. We present a multiagent architecture to overcome these challenges and discuss a prototype implementation of the architecture by developing and integrating various technologies. To measure and validate the performance of integrated technologies that meet these three interrelated challenges, we propose a methodology for setting up and conducting experiments. We take an existing data source for which we can establish a baseline query result, against which we measure the precision and recall performance, and create various sets of data sources with realistic characteristics. We report on the results of a number of experiments that show the performance of the developed and integrated technologies.},
  langid = {english},
  keywords = {alignment,entity disambiguation,Multi-agent systems,validation by simulation,回收},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 用於組織和查詢文化遺產檔案的計算框架\\
abstractTranslation:  現在，在人文學科中已經創建了越來越多的數據源，一個新的機會已經觸手可及：搜索檔案、博物館和其他文化遺產機構的跨數據源的模式。這些機構根據選拔程序的差異采取不同的數字化策略。這導致異構數據源對這些分佈式集合內部和之間的數據的可訪問性和互操作性產生巨大影響。我們確定了研究人員在查詢此類分佈式數據源時可能遇到的三個相互關聯的挑戰，即查詢制定、源選擇和數據源對齊。我們提出了一種多代理架構來克服這些挑戰，並通過開發和集成各種技術來討論該架構的原型實現。為了測量和驗證滿足這三個相互關聯的挑戰的集成技術的性能，我們提出了一種用於設置和進行實驗的方法。我們採用現有的數據源，為其建立基線查詢結果，根據該結果衡量精確度和召回率性能，並創建具有實際特徵的各種數據源集。我們報告了許多實驗的結果，這些實驗顯示了所開發和集成技術的性能。},
  note = {注重文化遺產與資料庫搜尋。技術層面為多代理查詢多個資料庫。和研究較無相關。},
  file = {C:\Users\BlackCat\Zotero\storage\C888XVRX\de Mooij 等。 - 2022 - A Computational Framework for Organizing and Query.pdf}
}

@inproceedings{janm.pawlowskiPositiveKnowledgeManagement2016,
  title = {Positive {{Knowledge Management}}: {{Changing Perceptions}} towards {{Knowledge Processes}} in {{Organizations}}},
  shorttitle = {Positive {{Knowledge Management}}},
  booktitle = {Proceedings of the {{The}} 11th {{International Knowledge Management}} in {{Organizations Conference}} on {{The}} Changing Face of {{Knowledge Management Impacting Society}}},
  author = {{Jan M. Pawlowski}},
  year = {7 月 25, 2016},
  series = {{{KMO}} '16},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2925995.2926002},
  url = {https://dl.acm.org/doi/10.1145/2925995.2926002},
  urldate = {2023-08-23},
  abstract = {Why do so many Knowledge Management projects fail, why are knowledge management activities still perceived as cumbersome and time consuming? In this paper, the solution to overcome the barriers towards knowledge management. By using principles from Positive Psychology and Positive Computing, the concept of Positive Knowledge Management is elaborated. It shows how strategies, processes, measurements and technologies need to be changed. The key issue is to work towards employees' happiness and well-being. Knowledge-related activities need to be perceived positively to change peoples' attitude and emotions. The paper outlines what is necessary to create Positive Knowledge Management in organizations.},
  isbn = {978-1-4503-4064-9},
  langid = {english},
  keywords = {Affective Computing,Knowledge Management,Positive Computing,Positive Knowledge Management},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 積極的知識管理：改變組織中對知識流程的看法\\
abstractTranslation:  如此多的知識管理項目失敗了，為什麼知識管理活動仍然被認為是繁瑣而瑣碎的？本文提出了克服知識管理障礙的解決方案。通過運用積極心理學和積極計算的原理，闡述了積極知識管理的概念。它展示了需要如何改變的戰略、流程、措施和技術。關鍵問題是努力實現員工的幸福感和福祉。需要積極地感知與知識相關的活動，以改變情緒和情緒。論文中人們概述了組織中創造積極的知識管理的必要條件。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\2K3KU5DS\\Pawlowski - 2016 - Positive Knowledge Management Changing Perception.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\CGRMP57E\\Pawlowski - 2016 - Positive Knowledge Management Changing Perception.pdf}
}

@inproceedings{jaradehQuestionAnsweringScholarly2020,
  title = {Question {{Answering}} on {{Scholarly Knowledge Graphs}}},
  booktitle = {Digital {{Libraries}} for {{Open Knowledge}}},
  author = {Jaradeh, Mohamad Yaser and Stocker, Markus and Auer, Sören},
  editor = {Hall, Mark and Merčun, Tanja and Risse, Thomas and Duchateau, Fabien},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {19--32},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-54956-5_2},
  abstract = {Answering questions on scholarly knowledge comprising text and other artifacts is a vital part of any research life cycle. Querying scholarly knowledge and retrieving suitable answers is currently hardly possible due to the following primary reason: machine inactionable, ambiguous and unstructured content in publications. We present JarvisQA, a BERT based system to answer questions on tabular views of scholarly knowledge graphs. Such tables can be found in a variety of shapes in the scholarly literature (e.g., surveys, comparisons or results). Our system can retrieve direct answers to a variety of different questions asked on tabular data in articles. Furthermore, we present a preliminary dataset of related tables and a corresponding set of natural language questions. This dataset is used as a benchmark for our system and can be reused by others. Additionally, JarvisQA is evaluated on two datasets against other baselines and shows an improvement of two to three folds in performance compared to related methods.},
  isbn = {978-3-030-54956-5},
  langid = {english},
  keywords = {BERT,Digital Libraries,Information retrieval,Question Answering,Scholarly knowledge,Semantic search,Semantic web,問答系統,學術知識圖譜,已整理,資料集},
  annotation = {6 citations (Crossref) [2024-03-26]\\
titleTranslation: 學術知識圖問答},
  note = {針對學術文獻的表格設計資料集。和研究無直接相關，但研究問題的描述可以參考:
\par
問答（QA）系統，例如蘋果的 Siri、亞馬遜的 Alexa 或 Google Now，透過從非結構化文字語料庫或開放域知識圖（KG）中挖掘答案來回答問題[ 14~\href{https://link.springer.com/chapter/10.1007/978-3-030-54956-5_2\#ref-CR14}{]}。這些方法對學術知識等專業領域的直接適用性值得懷疑。一方面，不存在可用於問答系統的廣泛的學術知識知識圖。另一方面，學術知識主要表現為文章（會議記錄或期刊）中的非結構化原始文本[~\href{https://link.springer.com/chapter/10.1007/978-3-030-54956-5_2\#ref-CR3}{3}~]。在非結構化工件中，知識不是機器可操作的、難以處理的、不明確的[~\href{https://link.springer.com/chapter/10.1007/978-3-030-54956-5_2\#ref-CR4}{4}~]，尤其是不公平的[~\href{https://link.springer.com/chapter/10.1007/978-3-030-54956-5_2\#ref-CR32}{32}~]。},
  file = {C:\Users\BlackCat\Zotero\storage\XPNL9YJS\Jaradeh et al. - 2020 - Question Answering on Scholarly Knowledge Graphs.pdf}
}

@inproceedings{jatiputraDataManagementSystem2020,
  title = {Data {{Management System}} for {{Thesis Monitoring}} at {{STMIK IBBI Using B-Model}}},
  booktitle = {2020 3rd {{International Conference}} on {{Mechanical}}, {{Electronics}}, {{Computer}}, and {{Industrial Technology}} ({{MECnIT}})},
  author = {{Jati Putra} and {Yuliana} and {Sukiman} and {Waisen} and {Awan} and {Benny}},
  date = {2020-06},
  pages = {365--369},
  doi = {10.1109/MECnIT48290.2020.9166671},
  url = {https://ieeexplore.ieee.org/document/9166671},
  urldate = {2023-10-13},
  abstract = {Actually thesis is the final task of a student, but a good supervisor must be able to adjust and spur the completion of the thesis in accordance with the characteristics and abilities of the students. For this reason, this Thesis Monitoring Information System is needed to assist in monitoring and enabling interaction between the supervisors and students if there are obstacles in the face-to-face guidance process. Every university has many students who have gotten jobs before completing their lectures needs to implement this thesis monitoring information system so as to optimize the thesis guidance process. This study used a B-Model which was an enhancement of the Waterfall method by emphasizing the maintenance phase. The results showed that the thesis monitoring information system can help lecturers and students in the guidance process so that it can be implemented well. System Effectiveness Testing is done using the Wilcoxon Signed-Rank Test.},
  eventtitle = {2020 3rd {{International Conference}} on {{Mechanical}}, {{Electronics}}, {{Computer}}, and {{Industrial Technology}} ({{MECnIT}})},
  annotation = {1 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\6NBT5NXH\Putra et al. - 2020 - Data Management System for Thesis Monitoring at ST.pdf}
}

@article{jia-hueilinSPARQLGenerationNMTBased2022,
  title = {{{SPARQL Generation}} with an {{NMT-Based Approach}}},
  author = {{Jia-Huei Lin} and {Eric Jui-Lin Lu}},
  date = {2022-07},
  journaltitle = {Journal of Web Engineering},
  volume = {21},
  number = {5},
  pages = {1471--1490},
  issn = {1544-5976},
  doi = {10.13052/jwe1540-9589.2155},
  url = {https://ieeexplore.ieee.org/document/10246950},
  urldate = {2023-11-23},
  abstract = {SPARQL is a powerful query language which has been widely used in various natural language question answering (QA) systems. As the advances of deep neural networks, Neural Machine Translation (NMT) models are employed to directly translate natural language questions to SPARQL queries in recent years. In this paper, we propose an NMT-based approach with Transformer model to generate SPARQL queries. Transformer model is chosen due to its relatively high efficiency and effectiveness. We design a format to encode a SPARQL query into a simple sequence with only RDF triples reserved. The main purpose of this step is to shorten the sequences and reduce the complexity of the target language. Moreover, we employ entity type tags to further resolve mistranslated problems. The proposed approach is evaluated against three open-domain question answering datasets (QALD-7, QALD-8, and LC-QuAD) on BLEU score and accuracy, and obtains outstanding results (83.49\%, 90.13\%, and 76.32\% on BLEU score, respectively) which considerably outperform all known studies.},
  eventtitle = {Journal of {{Web Engineering}}},
  langid = {english},
  keywords = {SPARQL,問答系統,問題分析,已整理,待讀,機器學習,重要},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用基於 NMT 的方法產生 SPARQL},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\7TD5TZKH\\Lin and Lu - 2022 - SPARQL Generation with an NMT-Based Approach.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\XH6T9DRT\\10246950.html}
}

@inproceedings{jialieshenMultimediaTaggingPresent2011,
  title = {Multimedia Tagging: Past, Present and Future},
  shorttitle = {Multimedia Tagging},
  booktitle = {Proceedings of the 19th {{ACM}} International Conference on {{Multimedia}}},
  author = {{Jialie Shen} and {Meng Wang} and {Shuicheng Yan} and {Xian-Sheng Hua}},
  year = {11 月 28, 2011},
  series = {{{MM}} '11},
  pages = {639--640},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2072298.2072405},
  url = {https://doi.org/10.1145/2072298.2072405},
  urldate = {2023-09-05},
  abstract = {The tags have proved to be a very crucial mechanism to facilitate the effective sharing and organization of large scale of multimedia information. As a result, technical developments on intelligent multimedia tagging have attracted a substantial amount of efforts involving experts from information retrieval, multimedia computing and artificial intelligence (particularly computer vision). The truly interdisciplinary research has resulted in many algorithmic and methodological developments. Meanwhile, many commercial web systems (e.g., Youtube, Last.fm and Flickr) have successfully introduced a variety of toolkits to assist different users in discovering and exploring media content using tags. This tutorial aims to provide a comprehensive coverage on the evolution of research for developing multimedia tagging technologies and identify a range of major challenges for the further scholarly study in the coming years.},
  isbn = {978-1-4503-0616-4},
  langid = {english},
  keywords = {分類標籤},
  annotation = {11 citations (Crossref) [2024-03-26]\\
titleTranslation: 多媒體標籤：過去、現在和未來\\
abstractTranslation:  標籤已被證明是促進大規模多媒體信息的有效共享和組織的非常關鍵的機制。因此，智能多媒體標籤技術的發展吸引了來自信息檢索、多媒體計算和人工智能（特別是計算機視覺）專家的大量努力。真正的跨學科研究帶來了許多算法和方法論的發展。同時，許多商業網絡系統（例如Youtube、Last.fm和Flickr）已經成功引入了各種工具包來幫助不同的用戶使用標籤發現和探索媒體內容。本教程旨在全面介紹多媒體標籤技術開發研究的進展，並確定未來幾年進一步學術研究的一系列主要挑戰。},
  file = {C:\Users\BlackCat\Zotero\storage\SUURJJ4Q\Shen 等。 - 2011 - Multimedia tagging past, present and future.pdf}
}

@book{jiamingshenAutomatedTaxonomyDiscovery2022,
  title = {Automated {{Taxonomy Discovery}} and {{Exploration}}},
  author = {{Jiaming Shen} and {Jiawei Han}},
  date = {2022},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-11405-2},
  url = {https://link.springer.com/10.1007/978-3-031-11405-2},
  urldate = {2023-09-13},
  isbn = {978-3-031-11404-5 978-3-031-11405-2},
  langid = {english},
  keywords = {Knowledge Discovery from Text,Label-efficient Machine Learning,Taxonomy Construction,Taxonomy Discovery,Taxonomy Enrichment,Text Mining,Weakly-supervised Learning,無法取得},
  annotation = {titleTranslation: 自動分類發現和探索}
}

@article{jianfenggaoChineseWordSegmentation2005,
  title = {Chinese {{Word Segmentation}} and {{Named Entity Recognition}}: {{A Pragmatic Approach}}},
  shorttitle = {Chinese {{Word Segmentation}} and {{Named Entity Recognition}}},
  author = {{Jianfeng Gao} and {Mu Li} and {Chang-Ning Huang} and {Andi Wu}},
  date = {2005-12},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  volume = {31},
  number = {4},
  pages = {531--574},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/089120105775299177},
  url = {https://direct.mit.edu/coli/article/31/4/531-574/1887},
  urldate = {2022-09-19},
  abstract = {This article presents a pragmatic approach to Chinese word segmentation. It differs from most previous approaches mainly in three respects. First, while theoretical linguists have defined Chinese words using various linguistic criteria, Chinese words in this study are defined pragmatically as segmentation units whose definition depends on how they are used and processed in realistic computer applications. Second, we propose a pragmatic mathematical framework in which segmenting known words and detecting unknown words of different types (i.e., morphologically derived words, factoids, named entities, and other unlisted words) can be performed simultaneously in a unified way. These tasks are usually conducted separately in other systems. Finally, we do not assume the existence of a universal word segmentation standard that is application-independent. Instead, we argue for the necessity of multiple segmentation standards due to the pragmatic fact that different natural language processing applications might require different granularities of Chinese words.             These pragmatic approaches have been implemented in an adaptive Chinese word segmenter, called MSRSeg, which will be described in detail. It consists of two components: (1) a generic segmenter that is based on the framework of linear mixture models and provides a unified approach to the five fundamental features of word-level Chinese language processing: lexicon word processing, morphological analysis, factoid detection, named entity recognition, and new word identification; and (2) a set of output adaptors for adapting the output of (1) to different application-specific standards. Evaluation on five test sets with different standards shows that the adaptive system achieves state-of-the-art performance on all the test sets.},
  langid = {english},
  annotation = {120 citations (Crossref) [2024-03-26]\\
220 citations (Semantic Scholar/DOI) [2022-10-26]\\
abstractTranslation:  本文提出了一種實用的中文分詞方法。它與大多數以前的方法主要在三個方面有所不同。首先，雖然理論語言學家使用各種語言標准定義了中文單詞，但本研究中的中文單詞實際上被定義為分段單元，其定義取決於它們在實際計算機應用中的使用和處理方式。其次，我們提出了一個實用的數學框架，在該框架中，可以以統一的方式同時執行已知單詞的分割和不同類型的未知單詞（即形態派生單詞、事實、命名實體和其他未列出單詞）的檢測。這些任務通常在其他系統中單獨執行。最後，我們不假設存在獨立於應用程序的通用分詞標準。相反，我們認為多種分割標準的必要性，因為不同的自然語言處理應用程序可能需要不同的中文單詞粒度。這些實用方法已在稱為 MSRSeg 的自適應中文分詞器中實現，我們將對其進行詳細描述。它由兩個組件組成：（1）一個基於線性混合模型框架的通用分段器，為詞級中文語言處理的五個基本特徵提供統一的方法：詞典詞處理、形態分析、事實檢測、命名實體識別、新詞識別； (2)一組輸出適配器，用於使(1)的輸出適應不同的特定應用標準。對五個不同標準的測試集的評估表明，自適應系統在所有測試集上都實現了最先進的性能。\\
titleTranslation: 中文分詞和命名實體識別：一種實用的方法},
  file = {C:\Users\BlackCat\Zotero\storage\8IQWTDN6\Gao 等。 - 2005 - Chinese Word Segmentation and Named Entity Recogni.pdf}
}

@thesis{JiangJiaLianYiGeZhiYuanDuiChengShiDuoChuLiQiQunZuDeYanZhi1999,
  title = {一個支援對稱式多處理器群組的研製},
  author = {{江佳鍊}},
  namea = {{吳真貞} and {林迺衛} and {Jan-Jan Wu} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {1999},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/rhafy9},
  abstract = {這篇論文報告在對稱式多處理器群組上（SMP clusters）針對資料平行程式設計一個通訊程式庫，我們設計的演算法是利用SMP群組特有的混合式記憶體架構（在一個SMP中，有共享式記憶體多處理器的特性，然而在一個SMP群組中，有分散式記憶體多處理器的特性），設計一個混合式的方法，經由結合兩種程式設計方法 -- 在一個SMP中使用引線（共享式記憶體程式設計）及在SMP與SMP之間使用訊息傳遞（分散式記憶體程式設計），直接對應到SMP群組上的混合式記憶體系統。我們實做了一些在資料平行程式中較常見的群集運算。程式庫的原形是建在引線（POSIX）及訊息傳遞（MPI）的標準介面上。我們在四台雙CPU的Sun UltraSparc-II工作站上，測試我們的群集通訊函式及一些範例程式，包含規則運算及不規則運算，結果證實混合式方法確實比只用訊息傳遞有較好的效率。},
  pagetotal = {153}
}

@patent{JiangYingJiYuBenTiYiZhiXingYanZhengTuiLiDeZhongWenYuYiXiaoDuiFangFa2014,
  type = {patent},
  title = {基于本体一致性验证推理的中文语义校对方法},
  author = {{姜赢} and {曾杰} and {荆铭} and {廖文生} and {郭颖珊} and {林启红} and {高巾}},
  date = {2014-02-19},
  number = {CN103593335A},
  location = {CN},
  url = {https://patents.google.com/patent/CN103593335A/zh},
  urldate = {2023-07-28},
  abstract = {一种基于本体一致性验证推理的中文语义校对方法，包括语义提取，利用本体学习技术，从非结构化的中文自然语言中提取语义内容，进而提取的语义内容转换成结构化的本体形式；领域本体库的确立，根据不同的领域使用相应领域的本体库；建立模型，将上述形成的中文语义校对关键技术以插件形式整合到语法校验工具中，或者以其它的实现形式，如独立地开发成一个中文语义校正软件；推理验证，在语法校验工具中，利用本体推理语言本身包含的基于描述逻辑的一致性推理验证机制，把提取得到的语义内容按照预定顺序和正确的领域本体库一起输入到推理机中逐次进行逻辑一致性验证推理，将推理结果中逻辑不一致的中文语义内容标示出中文语义错误标志。本发明具有既可以实现字词级和语法级中文校对，也可以实现能够检测特定领域语义错误的中文语义校对的优点。},
  langid = {chinese},
  keywords = {chinese,ontology,proofreading,reasoning,semantic,未整理},
  annotation = {titleTranslation: 基於本體一致性驗證推理的中文推理校對方法\\
abstractTranslation:  一種基於本體一致性驗證推理的漢語語義學對方法，包括語義提取，利用本體學習技術，從非\hspace{0pt}\hspace{0pt}結構化的中文自然語言中提取語義內容，進而提取語義內容轉換成本體形式；領域本體庫的確立，根據不同的領域使用相應領域的本體庫；建立模型，將上述形成的中文語義校對關鍵技術以插件形式整合到語法校驗工具中，或者以其他的實現形式，如獨立地開發生成一個中文語義校正軟件；推理驗證，在語法校驗工具中，利用本體推理語言本身包含基於描述邏輯的一致性推理機制驗證，把得到的語義內容按照預定順序和正確的領域本體庫一起提供輸入到推理機中逐次進行邏輯一致性驗證推理，將推理結果中邏輯一致性的中文語義內容標稱中文語義錯誤標誌。本發明既可以實現字詞級和語法級中文校對，也可以實現能夠檢測特定領域語義錯誤的中文語義校對的優點。},
  file = {C:\Users\BlackCat\Zotero\storage\KKHEYNMQ\姜赢 等。 - 2014 - 基于本体一致性验证推理的中文语义校对方法.pdf}
}

@article{JiangZhiPengMianXiangZhongWenDianZiBingLiDeCiFaYuLiaoBiaoZhuYanJiu2014,
  title = {面向中文电子病历的词法语料标注研究},
  author = {{蒋志鹏} and {赵芳芳} and {关毅} and {杨锦锋}},
  date = {2014},
  journaltitle = {高技术通讯},
  shortjournal = {Chinese High Technology Letters},
  volume = {24},
  number = {6},
  pages = {609--615},
  doi = {10.3772/j.issn.1002-0470.2014.06.009},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhBnanN0eDk4MjAxNDA2MDA5GghxdGU1cGQ4Yg%3D%3D},
  urldate = {2022-08-14},
  abstract = {针对中文电子病历(CEMR)标注语料匮乏,目前面向中文电子病历的分词和词性标注研究仍处于空白阶段的实际情况,从中文电子病历语料的构建出发,提出了从数据预处理到语料标注的整体方案,获得了较高的标注一致性,为进行更大规模更高质量的病历语料标注工作提供了指导.通过实验量化中文电子病历与开放领域语料、英文电子病历语料的词法统计差异,系统地分析了通用标注模型在中文电子病历中的错误分布,为进行适用于中文电子病历分析的自然语言处理(NLP)技术研究奠定了基础.},
  langid = {zh\_CN},
  keywords = {annotation consistency,Chinese electronic medical record (CEMR),Chinese High Technology Letters,error analysis,part-of-speech tagging,statistical lexical differences,中文電子病歷CEMR,標註一致性(IAA),詞性標註,语料差异,錯誤分析},
  annotation = {哈尔滨工业大学计算机科学与技术学院 哈尔滨150001哈尔滨工业大学计算机科学与技术学院 哈尔滨150001哈尔滨工业大学计算机科学与技术学院 哈尔滨150001哈尔滨工业大学计算机科学与技术学院 哈尔滨150001\\
国家自然科学基金\\
2014-09-11 （万方平台首次上网日期，不代表论文的发表时间）\\
abstractTranslation:  針對中文電子病歷（CEMR）標註語料匱乏，目前針對中文電子病歷的分詞和詞性標註研究仍處於空白階段的實際情況，從中文電子病歷料的構建出發，提出了從數據缺失到語料標註的整體通過實驗量化中文電子病歷與開放領域語料、中文電子病歷料的詞法統計差異、系統地分析了通用標註模型在中文電子病歷中的錯誤分佈，為進行適用於中文電子病歷分析的語言自然處理（NLP）技術研究奠定了基礎。\\
titleTranslation: 針對中文電子病歷的關鍵詞標註標籤研究},
  file = {C:\Users\BlackCat\Zotero\storage\IDQCQLUG\蒋 等。 - 2014 - 面向中文电子病历的词法语料标注研究.pdf}
}

@thesis{JiangZhiPengZhongWenDianZiBingLiDeCiFaHeJuFaFenXiYanJiu2017,
  type = {博士},
  title = {中文电子病历的词法和句法分析研究},
  author = {{蒋志鹏}},
  namea = {{关毅}},
  nameatype = {collaborator},
  date = {2017},
  institution = {哈尔滨工业大学},
  doi = {10.7666/d.D01589172},
  url = {http://d.wanfangdata.com.cn/thesis/ChJUaGVzaXNOZXdTMjAyMjA1MjYSCUQwMTU4OTE3MhoIcXRlNXBkOGI%3D},
  urldate = {2022-08-14},
  abstract = {随着医疗大数据时代的来临，电子病历的知识挖掘和利用受到越来越多的关注。电子病历本身是一种半结构化的数据，其结构化的内容为计算机自动抽取和分析提供了便利，同时，非结构化数据的规模远大于结构化数据，并且蕴藏着丰富的医疗知识和患者的健康信息，但计算机处理起来也更加困难，成为电子病历知识获取的主要障碍。电子病历的知识获取过程一般分为语言分析和信息抽取两个阶段进行，词法分析和句法分析是主要的语言分析手段，为信息抽取提供必要的条件。　　本文主要研究针对中文电子病历子语言特征的特殊词法和句法分析模型。具体研究任务包括词性标注、组块分析和句法分析，在这三个任务中，词性标注是自然语言处理的基础研究，后两个任务的},
  langid = {zh\_CN},
  keywords = {句法分析,层次句法,电子病历,自然语言处理,蒋志鹏,词性标注},
  annotation = {2019-01-18 （万方平台首次上网日期，不代表论文的发表时间）\\
abstractTranslation:  隨著醫療大數據時代的到來，電子病歷的知識挖掘和利用受到越來越多的關注。電子病曆本身是一種半成型的數據，其成型的內容為計算機自動抽取和分析提供了便利同時，非結構化數據的規模遠大於結構化數據，並且蘊藏著豐富的醫療知識和患者的健康信息，但計算機處理起來也更加困難，成為電子病歷知識獲取的主要障礙。過程分為語言分析和信息抽取兩個階段進行，詞法分析和句法分析是主要的語言分析手段，為信息抽取提供必要的條件。本文主要研究中文電子病歷子語言特徵的特殊詞法和句法分析具體研究任務包括詞性標籤、組塊分析和句法分析，在這三個任務中，詞性標籤是自然語言處理的基礎研究，後面兩個任務的\\
titleTranslation: 中文電子病歷的詞法和句法分析研究},
  file = {C:\Users\BlackCat\Zotero\storage\2J7BFJVW\蒋 - 2017 - 中文电子病历的词法和句法分析研究.pdf}
}

@inproceedings{JianYiPingZiRanYuYanChuLiZaiDianZiBingLiWenBenWaJueLingYuYanJiuJinZhan2018,
  title = {自然语言处理在电子病历文本挖掘领域研究进展},
  booktitle = {中华医学会第二十四次全国医学信息学术会议},
  author = {{蹇奕苹}},
  date = {2018-10-30},
  file = {C:\Users\BlackCat\Zotero\storage\HJSWV8CB\蹇奕苹 - 2018 - 自然语言处理在电子病历文本挖掘领域研究进展.pdf}
}

@online{jiashuosunThinkonGraphDeepResponsible2023,
  title = {Think-on-{{Graph}}: {{Deep}} and {{Responsible Reasoning}} of {{Large Language Model}} with {{Knowledge Graph}}},
  shorttitle = {Think-on-{{Graph}}},
  author = {{Jiashuo Sun} and {Chengjin Xu} and {Lumingyuan Tang} and {Saizhuo Wang} and {Chen Lin} and {Yeyun Gong} and {Heung-Yeung Shum} and {Jian Guo}},
  date = {2023-09-19},
  eprint = {2307.07697},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.07697},
  url = {http://arxiv.org/abs/2307.07697},
  urldate = {2023-09-23},
  abstract = {Large language models (LLMs) have made significant strides in various tasks, yet they often struggle with complex reasoning and exhibit poor performance in scenarios where knowledge traceability, timeliness, and accuracy are crucial. To address these limitations, we present Think-on-Graph (ToG), a novel framework that leverages knowledge graphs to enhance LLMs' ability for deep and responsible reasoning. By employing ToG, we can identify entities relevant to a given question and conduct exploration and reasoning to retrieve related triples from an external knowledge database. This iterative procedure generates multiple reasoning pathways consisting of sequentially connected triplets until sufficient information is gathered to answer the question or the maximum depth is reached. Through experiments on complex multi-hop reasoning question-answering tasks, we demonstrate that ToG outperforms existing methods, effectively addressing the aforementioned limitations of LLMs without incurring additional training costs.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,LLM,未公開研究結果,知識圖譜,知識推理,重要,預印本},
  annotation = {titleTranslation: Think-on-Graph：利用知識圖譜對大型語言模型進行深度、負責任的推理\\
abstractTranslation:  大型語言模型（LLM）在各種任務中取得了顯著的進步，但它們經常難以進行複雜的推理，並且在知識可追溯性、及時性和準確性至關重要的場景中表現不佳。為了解決這些限制，我們提出了 Think-on-Graph (ToG)，這是一個利用知識圖來增強法學碩士深度和負責任推理能力的新穎框架。透過使用 ToG，我們可以識別與給定問題相關的實體，並進行探索和推理以從外部知識資料庫中檢索相關的三元組。這個迭代過程產生由順序連接的三元組組成的多個推理路徑，直到收集到足夠的資訊來回答問題或達到最大深度。透過對複雜多跳推理問答任務的實驗，我們證明了 ToG 優於現有方法，有效解決了法學碩士的上述局限性，且不會產生額外的培訓成本。},
  note = {Comment: 20 pages, 8 figures, 9 tables},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\VCP6GQVP\\Sun 等。 - 2023 - Think-on-Graph Deep and Responsible Reasoning of .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\B239N6LD\\2307.html}
}

@inproceedings{jiecaoDoesTakingCollaborative2023,
  title = {Does {{Taking Collaborative Notes Really Help}}? {{A Comparative Analysis}} of {{Synchronous}} and {{Asynchronous Online Learning}}},
  shorttitle = {Does {{Taking Collaborative Notes Really Help}}?},
  booktitle = {2023 5th {{International Conference}} on {{Computer Science}} and {{Technologies}} in {{Education}} ({{CSTE}})},
  author = {{Jie Cao} and {Zhou Li} and {Yuanchen Zheng} and {Ning Ma}},
  date = {2023-04},
  pages = {159--164},
  doi = {10.1109/CSTE59648.2023.00034},
  url = {https://ieeexplore.ieee.org/document/10261556},
  urldate = {2023-10-12},
  abstract = {Collaborative note-taking has become more popular for its benefits. In addition, with online learning development, more and more teachers and students tend to engage in the online synchronous or asynchronous learning mode. However, the effect of collaborative notes in different online learning modes has yet to be empirically investigated. This study compared the difference in notes quality and students' learning performance when taking collaborative notes between the two online learning modes. Results showed that (1) compared to the synchronous mode, students' collaborative notes quality is better in the asynchronous learning mode, and students could achieve higher learning gains; (2) students with low prior knowledge levels are more productive in producing high-quality notes and receive more significant learning gain from them. These findings suggested that teachers can adapt collaborative notes in online asynchronous learning of introductory courses to enhance students' learning outcomes.},
  eventtitle = {2023 5th {{International Conference}} on {{Computer Science}} and {{Technologies}} in {{Education}} ({{CSTE}})},
  annotation = {0 citations (Crossref) [2024-03-26]}
}

@online{jiexuMedGPTEvalDatasetBenchmark2023,
  title = {{{MedGPTEval}}: {{A Dataset}} and {{Benchmark}} to {{Evaluate Responses}} of {{Large Language Models}} in {{Medicine}}},
  shorttitle = {{{MedGPTEval}}},
  author = {{Jie Xu} and {Lu Lu} and {Sen Yang} and {Bilin Liang} and {Xinwei Peng} and {Jiali Pang} and {Jinru Ding} and {Xiaoming Shi} and {Lingrui Yang} and {Huan Song} and {Kang Li} and {Xin Sun} and {Shaoting Zhang}},
  date = {2023-05-12},
  eprint = {2305.07340},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.07340},
  url = {http://arxiv.org/abs/2305.07340},
  urldate = {2023-06-24},
  abstract = {METHODS: First, a set of evaluation criteria is designed based on a comprehensive literature review. Second, existing candidate criteria are optimized for using a Delphi method by five experts in medicine and engineering. Third, three clinical experts design a set of medical datasets to interact with LLMs. Finally, benchmarking experiments are conducted on the datasets. The responses generated by chatbots based on LLMs are recorded for blind evaluations by five licensed medical experts. RESULTS: The obtained evaluation criteria cover medical professional capabilities, social comprehensive capabilities, contextual capabilities, and computational robustness, with sixteen detailed indicators. The medical datasets include twenty-seven medical dialogues and seven case reports in Chinese. Three chatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor PuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental results show that Dr. PJ outperforms ChatGPT and ERNIE Bot in both multiple-turn medical dialogue and case report scenarios.},
  langid = {english},
  pubstate = {preprint},
  keywords = {LLM,問答系統,回收,未發表},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-06-24]\\
0 citations (Semantic Scholar/DOI) [2023-06-24]\\
titleTranslation: MedGPTEval：評估醫學中大型語言模型響應的數據集和基準\\
abstractTranslation:  方法：首先，在綜合文獻綜述的基礎上設計一套評價標準。其次，由五位醫學和工程專家使用德爾菲法對現有候選標准進行了優化。第三，三位臨床專家設計了一組與法學碩士互動的醫學數據集。最後，對數據集進行基準測試實驗。基於法學碩士的聊天機器人生成的響應被記錄下來，供五位持證醫學專家進行盲評估。結果：獲得的評價標準涵蓋醫學專業能力、社會綜合能力、情境能力、計算穩健性等16個詳細指標。醫學數據集包括二十七個醫學對話和七個中文病例報告。評估了三個聊天機器人：OpenAI 的 ChatGPT、百度公司的 ERNIE Bot 以及上海人工智能實驗室的 PuJiang 博士（Dr. PJ）。實驗結果表明，PJ 博士在多輪醫療對話和病例報告場景中均優於 ChatGPT 和 ERNIE Bot。},
  note = {比較三個LLM在問答方面的準確度等。未發表且結果無用，實驗設計可能有值得參考的地方。論文結論超出研究範圍。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\DANAPPK4\\Xu 等。 - 2023 - MedGPTEval A Dataset and Benchmark to Evaluate Re.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\NHVEQ2ZX\\2305.html}
}

@article{jieyangSurveyExtractionCausal2022,
  title = {A Survey on Extraction of Causal Relations from Natural Language Text},
  author = {{Jie Yang} and {Soyeon Caren Han} and {Josiah Poon}},
  date = {2022-05-01},
  journaltitle = {Knowledge and Information Systems},
  shortjournal = {Knowl Inf Syst},
  volume = {64},
  number = {5},
  pages = {1161--1186},
  issn = {0219-3116},
  doi = {10.1007/s10115-022-01665-w},
  url = {https://doi.org/10.1007/s10115-022-01665-w},
  urldate = {2023-09-12},
  abstract = {As an essential component of human cognition, cause–effect relations appear frequently in text, and curating cause–effect relations from text helps in building causal networks for predictive tasks. Existing causality extraction techniques include knowledge-based, statistical machine learning (ML)-based, and deep learning-based approaches. Each method has its advantages and weaknesses. For example, knowledge-based methods are understandable but require extensive manual domain knowledge and have poor cross-domain applicability. Statistical machine learning methods are more automated because of natural language processing (NLP) toolkits. However, feature engineering is labor-intensive, and toolkits may lead to error propagation. In the past few years, deep learning techniques attract substantial attention from NLP researchers because of its powerful representation learning ability and the rapid increase in computational resources. Their limitations include high computational costs and a lack of adequate annotated training data. In this paper, we conduct a comprehensive survey of causality extraction. We initially introduce primary forms existing in the causality extraction: explicit intra-sentential causality, implicit causality, and inter-sentential causality. Next, we list benchmark datasets and modeling assessment methods for causal relation extraction. Then, we present a structured overview of the three techniques with their representative systems. Lastly, we highlight existing open challenges with their potential directions.},
  langid = {english},
  keywords = {Causality extraction,Deep learning,Explicit intra-sentential causality,Implicit causality,Inter-sentential causality,review,文字處理,機器學習,資料挖掘},
  annotation = {15 citations (Crossref) [2024-03-26]\\
abstractTranslation:  作為人類認知的重要組成部分，因果關係經常出現在文本中，從文本中整理因果關係有助於為預測任務構建因果網絡。現有的因果關係提取技術包括基於知識、基於統計機器學習（ML）和基於深度學習的方法。每種方法都有其優點和缺點。例如，基於知識的方法雖然可以理解，但需要大量的手動領域知識，跨領域適用性較差。由於自然語言處理（NLP）工具包的存在，統計機器學習方法更加自動化。然而，特徵工程是勞動密集型的，工具包可能會導致錯誤傳播。在過去的幾年裡，深度學習技術因其強大的表示學習能力和計算資源的快速增長而引起了自然語言處理研究人員的廣泛關注。它們的局限性包括計算成本高和缺乏足夠的帶註釋的訓練數據。在本文中，我們對因果關係提取進行了全面的調查。我們首先介紹因果關係提取中存在的主要形式：顯式句內因果關係、隱式因果關係和句間因果關係。接下來，我們列出用於因果關係提取的基準數據集和建模評估方法。然後，我們對這三種技術及其代表性系統進行了結構化概述。最後，我們強調現有的開放挑戰及其潛在方向。\\
titleTranslation: 自然語言文本中因果關係提取的綜述},
  file = {C:\Users\BlackCat\Zotero\storage\HHERUAYM\Yang 等。 - 2022 - A survey on extraction of causal relations from na.pdf}
}

@article{jinghuipengWhatMultiModalKnowledge2023,
  title = {What {{Is}} a {{Multi-Modal Knowledge Graph}}: {{A Survey}}},
  shorttitle = {What {{Is}} a {{Multi-Modal Knowledge Graph}}},
  author = {{Jinghui Peng} and {Xinyu Hu} and {Wenbo Huang} and {Jian Yang}},
  year = {5 月 24, 2023},
  journaltitle = {Big Data Research},
  shortjournal = {Big Data Res.},
  volume = {32},
  number = {C},
  issn = {2214-5796},
  doi = {10.1016/j.bdr.2023.100380},
  url = {https://doi.org/10.1016/j.bdr.2023.100380},
  urldate = {2023-09-27},
  abstract = {With the explosive growth of multi-modal information on the Internet, the multi-modal knowledge graph (MMKG) has become an important research topic in knowledge graphs to meet the needs of data management and application. Most research on MMKG has taken image-text data as the research object and used the multi-modal deep learning approach to process multi-modal data. In comparison, the structure of the MMKG is no uniform statement. This paper focuses on MMKG, introduces the related theories of multi-modal knowledge, and analyzes several common ideas about its construction. The survey also explains the structural evolution, proposes mirror node alignment to represent cross-modal knowledge for MMKG, lists some tasks' difficulties, and ultimately gives a sample MMKG for the news scene.},
  keywords = {Knowledge representation,MMKG,Multi-modal,Ontology,Structure,未整理},
  annotation = {3 citations (Crossref) [2024-03-26]}
}

@online{jingjinliuSequentialConditionEvolved2023,
  title = {Sequential {{Condition Evolved Interaction Knowledge Graph}} for {{Traditional Chinese Medicine Recommendation}}},
  author = {{Jingjin Liu} and {Hankz Hankui Zhuo} and {Kebing Jin} and {Jiamin Yuan} and {Zhimin Yang} and {Zhengan Yao}},
  date = {2023-05-28},
  eprint = {2305.17866},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.17866},
  url = {http://arxiv.org/abs/2305.17866},
  urldate = {2023-06-24},
  abstract = {Traditional Chinese Medicine (TCM) has a rich history of utilizing natural herbs to treat a diversity of illnesses. In practice, TCM diagnosis and treatment are highly personalized and organically holistic, requiring comprehensive consideration of the patient's state and symptoms over time. However, existing TCM recommendation approaches overlook the changes in patient status and only explore potential patterns between symptoms and prescriptions. In this paper, we propose a novel Sequential Condition Evolved Interaction Knowledge Graph (SCEIKG), a framework that treats the model as a sequential prescription-making problem by considering the dynamics of the patient's condition across multiple visits. In addition, we incorporate an interaction knowledge graph to enhance the accuracy of recommendations by considering the interactions between different herbs and the patient's condition. Experimental results on a real-world dataset demonstrate that our approach outperforms existing TCM recommendation methods, achieving state-of-the-art performance.},
  langid = {english},
  pubstate = {preprint},
  keywords = {ChatGPT,Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,中醫,人機互動,可視化,已整理,機器學習,知識圖},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-06-24]\\
0 citations (Semantic Scholar/DOI) [2023-06-24]\\
titleTranslation: 用於中藥推薦的序列條件進化交互知識圖\\
abstractTranslation:  傳統中醫 (TCM) 在利用天然草藥治療多種疾病方面有著豐富的歷史。在實踐中，中醫診斷和治療具有高度個性化和有機整體性，需要綜合考慮患者隨時間的狀態和症狀。然而，現有的中醫推薦方法忽視了患者狀態的變化，僅探索症狀和處方之間的潛在模式。在本文中，我們提出了一種新穎的順序條件演化交互知識圖（SCEIKG），該框架通過考慮多次就診中患者病情的動態，將模型視為順序處方制定問題。此外，我們還結合了交互知識圖，通過考慮不同草藥和患者病情之間的相互作用來提高建議的準確性。真實數據集上的實驗結果表明，我們的方法優於現有的中醫推薦方法，實現了最先進的性能。},
  note = {結合過去的病患狀況來訓練藥方選擇},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\AP7IGFNZ\\Liu 等。 - 2023 - Sequential Condition Evolved Interaction Knowledge.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\IBIVIP3B\\2305.html}
}

@article{JingShenQiMianXiangZhongWenDianZiBingLiWenShuDeYiXueMingMingShiTiShiBieYanJiuYiZhongJiYuBanJianDuShenDuXueXiDeFangFa2021,
  title = {面向中文电子病历文书的医学命名实体识别研究——一种基于半监督深度学习的方法},
  author = {{景慎旗} and {赵又霖}},
  date = {2021},
  journaltitle = {信息资源管理学报},
  shortjournal = {Journal of Information Resources Management},
  volume = {11},
  number = {6},
  pages = {105--115},
  doi = {10.13365/j.jirm.2021.06.105},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhF4eHp5Z2x4YjIwMjEwNjAxMRoIYTl3dXluajY%3D},
  urldate = {2022-08-14},
  abstract = {电子病历文书详细记录患者诊疗全过程,蕴藏的医学知识是电子病历中最丰富的,因此挖掘电子病历文书潜在的知识结构具有十分重要的价值.面向非结构化电子病历知识挖掘的首要工作是命名实体识别,现有的医学领域命名实体识别方法面临标注数据质量偏低、标注数据不足的问题,同时现有方法中均只考虑文本的序列特性,忽略文本中词间、字间的依赖关系,限制了命名实体识别效果.本文提出一种基于半监督深度学习的医学命名实体识别方法,即结合具有专家权威的中文百科半自动化实体标注法及BERT-GCN-CRF框架,对电子病历文本进行医学命名实体识别抽取.以真实电子病历文本作为实验对象,该模型获取的准确率、召回率、F1值均有明显提高,其},
  langid = {zh\_CN},
  keywords = {BERT,半監督深度學習,命名實體,知識挖掘,電子病歷},
  annotation = {南京大学信息管理学院,南京,210023;南京医科大学生物医学工程与信息学院,南京,211166;南京医科大学第一附属医院(江苏省人民医院)数据应用管理中心,南京,210096南京大学信息管理学院,南京,210023\\
江苏省重点研发计划 国家重点研发计划\\
2022-03-16 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 面向中文電子病曆書籍的醫學命名實體識別研究——一種基於半監督深度學習的方法\\
abstractTranslation:  電子病歷手冊詳細記錄了患者診療全過程，蘊藏的醫學知識是電子病歷中最豐富的，因此挖掘電子病歷手冊潛在的具有重要價值的知識結構。實體識別，現有的醫學領域命名實體識別方法面臨標誌數據質量偏低、標誌數據不足的問題，同時現有方法中均只考慮文本的序列特性，忽略文本中詞間、字間的關係，本文提出了一種基於半監督深度學習的醫學命名實體識別方法，即結合專家權威的中文百科半自動化實體標記法及BERT-GCN-CRF框架，對電子病歷文本進行醫學以真實電子病歷文本作為實驗對象，該模型獲取的準確率、認知率、F1值明顯提高，其},
  file = {C:\Users\BlackCat\Zotero\storage\M4JRR2NN\景 與 赵 - 2021 - 面向中文电子病历文书的医学命名实体识别研究——一种基于半监督深度学习的方法.pdf}
}

@inproceedings{jingxieApplicationInformationRetrieval2017,
  title = {Application of {{Information Retrieval Course}} in {{College Students}}' {{Graduation Thesis Writing}}:},
  shorttitle = {Application of {{Information Retrieval Course}} in {{College Students}}' {{Graduation Thesis Writing}}},
  author = {{Jing Xie}},
  date = {2017},
  location = {Shenyang, China},
  doi = {10.2991/emcm-16.2017.47},
  url = {https://www.atlantis-press.com/article/25870427},
  urldate = {2023-10-20},
  abstract = {The purpose of this paper is to instruct the important role of the Information Retrieval Course in college students' graduation thesis. First, the significance and function of information retrieval course are illuminated. Then, one by one, the key points of the Information Retrieval are explained in the process of writing. Finally, the author described how to carry out the information retrieval, including selecting of the search tool, the search way, the retrieval method and so on for the college students' graduation thesis writing. It is the effective support for college student thesis, through the information retrieval to ensure the accuracy of the selected topic, the comprehensive of the collection of materials. Information retrieval course has become an important way to carry out information quality education in colleges and universities. In recent years, the information retrieval course has become a public basic course in many colleges and universities. Whether it is for the school or the ratio of starting courses, it has greatly enhanced students' information awareness and capability. [1] With the rapid development of modern information resources, learners change the habit of reading, more and more information resources are used by college students. Information resources are rich in content, not only the storage of information is large, but also has the advantages of high timeliness and high resource sharing, which is convenient for college students to carry out comprehensive, rapid and accurate literature retrieval. At present, most colleges and universities set the course as undergraduate elective courses, some colleges and universities set up undergraduate compulsory courses. Especially, it is very important to get information resources effectively, analysis and utilization to the graduate students for project research, thesis. Clarify the status of the course and integrate it into the reform of general education curriculum system in colleges and universities, and as the core curriculum of general education, set up information retrieval compulsory courses for undergraduates and postgraduates so as to enhance the readers' information accomplishment to the maximum extent. [2] It is worth studying on an important learning task for how to use the information retrieval, how to play a key role to information resources in college students thesis writing. At present, under the background of the information gathering, processing and transmission technology represented by computer, multimedia and network, the information network brings the opportunity and challenge to the traditional thesis writing. The paper writing itself is the process of expressing, exchanging and transmitting information. The ability to collect, process, use and create content directly influences the formation and efficiency of the writing process. Therefore, only to meet the requirements of the information society, have high information literacy, the students can competent and meet the needs of graduation thesis writing. [3] The Meaning and Role of Information Retrieval Courses Document retrieval is the process of collecting, organization, store of knowledge and information, and to search information for users. [4] We can get the documents needed in a large number of disordered literatures efficiently and conveniently through the literature search. The purpose of information retrieval course is to cultivate college students who have strong information awareness and information literacy, improve their self-learning ability, so that they can independently learn richer knowledge besides textbooks. [5] The final purpose of information retrieval course is that the students can learn and master the methods and skills of literature retrieval through the course, 7th International Conference on Education, Management, Computer and Medicine (EMCM 2016) Copyright © 2017, the Authors. Published by Atlantis Press. This is an open access article under the CC BY-NC license (http://creativecommons.org/licenses/by-nc/4.0/). Advances in Computer Science Research, volume 59},
  eventtitle = {2016 7th {{International Conference}} on {{Education}}, {{Management}}, {{Computer}} and {{Medicine}} ({{EMCM}} 2016)},
  langid = {english},
  keywords = {待讀,研究流程,資訊超載,重要},
  annotation = {0 citations (Crossref) [2024-03-26]\\
abstractTranslation:  本文旨在指導資訊檢索課程在大學生畢業論文中的重要角色。首先闡述了資訊檢索課程的意義與角色。然後在寫作過程中一一闡述了資訊檢索的要點。最後闡述了大學生畢業論文寫作如何進行資訊檢索，包括檢索工具的選擇、檢索方式、檢索方法等。它是大學生論文的有效支撐，透過資訊檢索保證選題的準確性、收集資料的全面性。資訊檢索課程已成為大學進行資訊素質教育的重要途徑。近年來，資訊檢索課程已成為許多大學的公共基礎課程。無論是對於學校或開課比例，都大大提升了學生的資訊意識和能力。 [1]隨著現代資訊資源的快速發展，學習者閱讀習慣的改變，越來越多的資訊資源被大學生使用。資訊資源內容豐富，不僅資訊儲存量大，而且具有時效性高、資源共享性高等優點，便於大學生進行全面、快速、準確的文獻檢索。目前，大部分大學將該課程設置為本科選修課，部分大學設置了本科必修課。尤其是有效地獲取資訊資源、分析和利用資訊資源對於研究生進行專案研究、論文尤其重要。明確課程的地位，納入高校通識教育課程體系改革，作為通識教育的核心課程，開設本科生和研究生的信息檢索必修課，提升讀者的信息素養最大限度地。 [2]如何利用資訊檢索、如何發揮資訊資源在大學生論文寫作中的關鍵作用，是值得研究的重要學習課題。目前，在以電腦、多媒體、網路為代表的資訊收集、處理和傳輸技術的背景下，資訊網路為傳統的論文寫作帶來了機會和挑戰。論文寫作本身就是表達、交換、傳遞訊息的過程。收集、加工、使用和創造內容的能力直接影響寫作過程的形成和效率。因此，只有適應資訊社會的要求，具備較高的資訊素養，學生才能勝任並滿足畢業論文寫作的需要。 [3] 資訊檢索課程的意義與作用 文獻檢索是知識與資訊的收集、組織、存儲，並為使用者檢索資訊的過程。 [4]透過文獻檢索，我們可以在大量雜亂的文獻中有效率且方便地找到所需的文獻。資訊檢索課程的目的是培養大學生具有較強的資訊意識和資訊素養，提高自學能力，使他們能夠獨立學習課本以外的更豐富的知識。 [5] 資訊檢索課程的最終目的是讓學生透過課程學習和掌握文獻檢索的方法和技能，第七屆教育、管理、電腦和醫學國際會議(EMCM 2016) 版權所有© 2017，作者。由亞特蘭提斯出版社出版。這是一篇遵循 CC BY-NC 授權 (http://creativecommons.org/licenses/by-nc/4.0/) 的開放取用文章。電腦科學研究進展，第 59 卷},
  note = {[TLDR] The author described how to carry out the information retrieval, including selecting of the search tool, the search way, the retrieval method and so on for the college students' graduation thesis writing.},
  file = {C:\Users\BlackCat\Zotero\storage\LHS3M3QT\Xie - 2017 - Application of Information Retrieval Course in Col.pdf}
}

@article{jingzhangNeuralSymbolicNeuralsymbolic2021,
  title = {Neural, Symbolic and Neural-Symbolic Reasoning on Knowledge Graphs},
  author = {{Jing Zhang} and {Bo Chen} and {Lingxi Zhang} and {Xirui Ke} and {Haipeng Ding}},
  date = {2021-01-01},
  journaltitle = {AI Open},
  shortjournal = {AI Open},
  volume = {2},
  pages = {14--35},
  issn = {2666-6510},
  doi = {10.1016/j.aiopen.2021.03.001},
  url = {https://www.sciencedirect.com/science/article/pii/S2666651021000061},
  urldate = {2023-08-01},
  abstract = {Knowledge graph reasoning is the fundamental component to support machine learning applications such as information extraction, information retrieval, and recommendation. Since knowledge graphs can be viewed as the discrete symbolic representations of knowledge, reasoning on knowledge graphs can naturally leverage the symbolic techniques. However, symbolic reasoning is intolerant of the ambiguous and noisy data. On the contrary, the recent advances of deep learning have promoted neural reasoning on knowledge graphs, which is robust to the ambiguous and noisy data, but lacks interpretability compared to symbolic reasoning. Considering the advantages and disadvantages of both methodologies, recent efforts have been made on combining the two reasoning methods. In this survey, we take a thorough look at the development of the symbolic, neural and hybrid reasoning on knowledge graphs. We survey two specific reasoning tasks — knowledge graph completion and question answering on knowledge graphs, and explain them in a unified reasoning framework. We also briefly discuss the future directions for knowledge graph reasoning.},
  langid = {english},
  keywords = {Symbolic reasoning,問答系統,機器學習,知識圖譜,知識推理,邏輯},
  annotation = {32 citations (Crossref) [2024-03-26]\\
42 citations (Semantic Scholar/DOI) [2023-08-11]\\
titleTranslation: 知識圖譜上的神經、符號和神經符號推理\\
abstractTranslation:  知識圖推理是支持信息提取、信息檢索和推薦等機器學習應用的基本組成部分。由於知識圖譜可以被視為知識的離散符號表示，因此知識圖譜的推理自然可以利用符號技術。然而，符號推理不能容忍模糊和嘈雜的數據。相反，深度學習的最新進展促進了知識圖譜的神經推理，它對模糊和噪聲數據具有魯棒性，但與符號推理相比缺乏可解釋性。考慮到這兩種方法的優點和缺點，最近人們致力於將兩種推理方法結合起來。在本次調查中，我們深入研究了知識圖上的符號推理、神經推理和混合推理的發展。我們調查了兩個具體的推理任務——知識圖補全和知識圖問答，並在統一的推理框架中解釋它們。我們還簡要討論了知識圖推理的未來方向。}
}

@article{jinhyukleeBioBERTPretrainedBiomedical2020,
  title = {{{BioBERT}}: A Pre-Trained Biomedical Language Representation Model for Biomedical Text Mining},
  shorttitle = {{{BioBERT}}},
  author = {{Jinhyuk Lee} and {Wonjin Yoon} and {Sungdong Kim} and {Donghyeon Kim} and {Sunkyu Kim} and {Chan Ho So} and {Jaewoo Kang}},
  date = {2020-02-15},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {36},
  number = {4},
  pages = {1234--1240},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btz682},
  url = {https://doi.org/10.1093/bioinformatics/btz682},
  urldate = {2023-06-24},
  abstract = {Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\% F1 score improvement), biomedical relation extraction (2.80\% F1 score improvement) and biomedical question answering (12.24\% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.},
  langid = {english},
  annotation = {2280 citations (Crossref) [2024-03-26]\\
3064 citations (Semantic Scholar/DOI) [2023-06-24]\\
titleTranslation: BioBERT：用於生物醫學文本挖掘的預訓練生物醫學語言表示模型\\
abstractTranslation:  隨著生物醫學文檔數量的快速增長，生物醫學文本挖掘變得越來越重要。隨著自然語言處理（NLP）的進步，從生物醫學文獻中提取有價值的信息受到研究人員的歡迎，深度學習推動了有效的生物醫學文本挖掘模型的發展。然而，由於詞分佈從一般領域語料庫到生物醫學語料庫的轉變，直接將 NLP 的進步應用於生物醫學文本挖掘往往會產生不令人滿意的結果。在本文中，我們研究了最近推出的預訓練語言模型BERT 如何適用於生物醫學語料庫。我們介紹BioBERT（用於生物醫學文本挖掘的Transformers 的雙向編碼器表示），它是預訓練的特定領域語言表示模型大規模生物醫學語料庫。由於跨任務的架構幾乎相同，在生物醫學語料庫上進行預訓練時，BioBERT 在各種生物醫學文本挖掘任務中的性能很大程度上優於 BERT 和之前最先進的模型。雖然BERT 獲得了與之前最先進模型相當的性能，但BioBERT 在以下三個代表性生物醫學文本挖掘任務上顯著優於它們：生物醫學命名實體識別（F1 分數提高0.62\%）、生物醫學關係提取（2.80\%） F1 分數提高）和生物醫學問答（MRR 提高 12.24\%）。我們的分析結果表明，在生物醫學語料庫上預訓練BERT 有助於其理解複雜的生物醫學文本。\hspace{0pt}\hspace{0pt}我們在https://github.com/naver/biobert-pretrained 上免費提供BioBERT 的預訓練權重，以及源代碼如需微調 BioBERT，請訪問 https://github.com/dmis-lab/biobert。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\N86Y6VE9\\Lee 等。 - 2020 - BioBERT a pre-trained biomedical language represe.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\7MCWCKJQ\\5566506.html}
}

@article{jinjiaolinDomainKnowledgeGraphbased2021,
  title = {Domain Knowledge Graph-Based Research Progress of Knowledge Representation},
  author = {{Jinjiao Lin} and {Yanze Zhao} and {Weiyuan Huang} and {Chunfang Liu} and {Haitao Pu}},
  year = {1 月 1, 2021},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput. Appl.},
  volume = {33},
  number = {2},
  pages = {681--690},
  issn = {0941-0643},
  doi = {10.1007/s00521-020-05057-5},
  url = {https://doi.org/10.1007/s00521-020-05057-5},
  urldate = {2023-09-18},
  abstract = {Domain knowledge graph has become a research topic in the era of artificial intelligence. Knowledge representation is the key step to construct domain knowledge graph. There have been quite a few well-established general knowledge graphs. However, there are still gaps on the domain knowledge graph construction. The research introduces the related concepts of the knowledge representation and analyzes knowledge representation of knowledge graphs by category, which includes some classical general knowledge graphs and several typical domain knowledge graphs. The paper also discusses the development of knowledge representation in accordance with the difference of entities, relationships and properties. It also presents the unsolved problems and future research trends in the knowledge representation of domain knowledge graph study.},
  langid = {english},
  keywords = {Domain knowledge graph,Entity,Knowledge representation,Property,Relationship,已整理,知識圖譜},
  annotation = {58 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於領域知識圖譜的知識表示研究進展\\
abstractTranslation:  領域知識圖譜已成為人工智慧時代的研究主題。知識表示是建構領域知識圖譜的關鍵步驟。已經有相當多成熟的常識圖譜了。然而，領域知識圖譜建構仍存在差距。研究介紹了知識表示的相關概念，並對知識圖譜的知識表示進行了分類分析，其中包括一些經典的通用知識圖譜和幾種典型的領域知識圖譜。論文也根據實體、關係和屬性的差異討論了知識表示的發展。也提出了領域知識圖譜研究的知識表示中尚未解決的問題和未來的研究趨勢。}
}

@article{JinMuLiYouZhongYiDianZiBingLiBiaoZhunFuHeXingCeShiPingTaiJianSheTanSuo2020,
  title = {中医电子病历标准符合性测试平台建设探索},
  author = {{金木李由} and {罗迦腾} and {何黎} and {杨光莹} and {范靖} and {罗悦}},
  date = {2020},
  journaltitle = {亚太传统医药},
  volume = {16},
  number = {8},
  pages = {4},
  abstract = {通过前期的需求调研,结合中医数字化标准的建立及遵循标准,分析出测试中医电子病历标准的详细过程,建立起一个完整的中医电子病历标准符合性测试平台,为医学工作者们提供参考,同时为各个医疗机构提供一个上传数据与评估的平台,以实现中医电子病历的标准同步,数据共享等功能,进一步促进中医数字化逐步向中医标准化发展,为其他标准化测试平台的建设提供一个可供借鉴的实例.},
  keywords = {中醫,中醫標準化,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\3VN28V7B\金木李由 等。 - 2020 - 中医电子病历标准符合性测试平台建设探索.pdf}
}

@inproceedings{jinseokleeKnowledgeQueryNetwork2019,
  title = {Knowledge {{Query Network}} for {{Knowledge Tracing}}: {{How Knowledge Interacts}} with {{Skills}}},
  shorttitle = {Knowledge {{Query Network}} for {{Knowledge Tracing}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Learning Analytics}} \& {{Knowledge}}},
  author = {{Jinseok Lee} and {Dit-Yan Yeung}},
  year = {3 月 4, 2019},
  series = {{{LAK19}}},
  pages = {491--500},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3303772.3303786},
  url = {https://dl.acm.org/doi/10.1145/3303772.3303786},
  urldate = {2023-08-23},
  abstract = {Knowledge Tracing (KT) is to trace the knowledge of students as they solve a sequence of problems represented by their related skills. This involves abstract concepts of students' states of knowledge and the interactions between those states and skills. Therefore, a KT model is designed to predict whether students will give correct answers and to describe such abstract concepts. However, existing methods either give relatively low prediction accuracy or fail to explain those concepts intuitively. In this paper, we propose a new model called Knowledge Query Network (KQN) to solve these problems. KQN uses neural networks to encode student learning activities into knowledge state and skill vectors, and models the interactions between the two types of vectors with the dot product. Through this, we introduce a novel concept called probabilistic skill similarity that relates the pairwise cosine and Euclidean distances between skill vectors to the odds ratios of the corresponding skills, which makes KQN interpretable and intuitive. On four public datasets, we have carried out experiments to show the following: 1. KQN outperforms all the existing KT models based on prediction accuracy. 2. The interaction between the knowledge state and skills can be visualized for interpretation. 3. Based on probabilistic skill similarity, a skill domain can be analyzed with clustering using the distances between the skill vectors of KQN. 4. For different values of the vector space dimensionality, KQN consistently exhibits high prediction accuracy and a strong positive correlation between the distance matrices of the skill vectors.},
  isbn = {978-1-4503-6256-6},
  langid = {english},
  keywords = {Deep Learning,Domain Modeling,Educational Data Mining,Intelligent Tutoring Systems,Knowledge Modeling,Knowledge Tracing,Learner Modeling,Learning Analytics,Massive Open Online Courses},
  annotation = {26 citations (Crossref) [2024-03-26]\\
titleTranslation: 用於知識追踪的知識查詢網絡：知識如何與技能交互\\
abstractTranslation:  知識追踪（KT）是在學生解決由其相關技能代表的一系列問題時追踪學生的知識。這涉及學生知識狀態的抽象概念以及這些狀態和技能之間的交互。因此，KT模型旨在預測學生是否會給出正確的答案並描述此類抽象概念。然而，現有的方法或相對較低的預測精度，或無法解釋這些概念。在本文中，我們提出了一種稱為知識查詢網絡(KQN )的新模型來解決這些問題。KQN利用神經網絡將學生的學習活動編碼為知識狀態和技能提供，並用點積對兩類提供之間的應答進行建模。通過這件事，在在四個公共數據集上，我們進行了實驗，結果表明： 1. KQN 在預測精度方面優於所有現有的KT 模型。2. 知識狀態和技能之間的交互可視化進行解釋。3. 基於技能概率相似性，可以使用KQN的技能通過咳嗽來分析技能域來提供距離之間的距離。4。對於不同的指揮空間維數值，KQN始終表現出較高的預測精度，並且技能指揮的距離矩陣之間具有優勢的正相關性。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\8QP6S5DH\\Lee 與 Yeung - 2019 - Knowledge Query Network for Knowledge Tracing How.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\J8862W6E\\Lee 與 Yeung - 2019 - Knowledge Query Network for Knowledge Tracing How.pdf}
}

@article{jinxuNovelFrameworkKnowledge2022,
  title = {A Novel Framework of Knowledge Transfer System for Construction Projects Based on Knowledge Graph and Transfer Learning},
  author = {{Jin Xu} and {Mengqi He} and {Ying Jiang}},
  date = {2022-08-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {199},
  pages = {116964},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2022.116964},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417422003906},
  urldate = {2023-09-18},
  abstract = {For construction enterprises, efficient knowledge sharing among projects not only effectively improves enterprise technology, level of management and competitiveness, but also promotes their sustainable development. Given the many benefits of knowledge management, enterprises have an urgent need for project knowledge sharing methods and tools. In this study, we build an automated and intelligent framework for a construction project knowledge transfer system based on knowledge graph and transfer learning. This framework aims to solve the problem of ineffective knowledge transfer that is encountered in the management of construction project knowledge sharing. First, to discover the relationship among knowledge and further obtain the relationships among projects, we design a domain knowledge graph ontology for construction projects and build an example. Then, based on this domain knowledge graph and combining construction data distance distribution with construction project knowledge background, we design a new construction project similarity measurement algorithm (PBG-MMD), which can guide the selection of the knowledge transfer source domain. Finally, a new transfer learning method is developed to automatically select the transfer source domain according to the domain context. The framework provides an effective answer for the problem of “what to transfer” in transfer learning and provides an effective solution to address the problem of “how to transfer” during knowledge transfer. Through the verification of practical case data, the proposed framework successfully realizes knowledge transfer among construction projects and provides an automated and intelligent knowledge sharing approach for construction enterprises.},
  langid = {english},
  keywords = {Construction project,Knowledge graph,Knowledge transfer,Project similarity,Transfer learning,已整理,知識分享,知識本體},
  annotation = {9 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於知識圖譜與遷移學習的建構專案知識遷移系統新框架\\
abstractTranslation:  對於建築企業來說，專案間高效的知識共享不僅有效提高企業技術、管理水平和競爭力，而且促進企業永續發展。鑑於知識管理的許多好處，企業迫切需要專案知識共享的方法和工具。在本研究中，我們基於知識圖譜和遷移學習，為建築專案知識遷移系統建構了一個自動化、智慧化的框架。該框架旨在解決在建設專案知識共享管理中遇到的知識轉移無效的問題。首先，為了發現知識之間的關係並進一步獲得專案之間的關係，我們設計了建設專案的領域知識圖譜本體並建立了範例。然後，基於該領域知識圖譜，結合施工資料距離分佈和施工專案知識背景，設計了新的施工專案相似度量演算法（PBG-MMD），可以指導知識遷移源領域的選擇。最後，開發了一種新的遷移學習方法，根據網域上下文自動選擇遷移源域。該框架為遷移學習中「遷移什麼」的問題提供了有效的答案，為解決知識遷移中「如何遷移」的問題提供了有效的解決方案。透過實際案例資料驗證，該框架成功實現了建築專案間的知識轉移，為建築企業提供了自動化、智慧化的知識共享途徑。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\HMPNVYQV\\Xu 等。 - 2022 - A novel framework of knowledge transfer system for.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\B5E7NZBE\\S0957417422003906.html}
}

@inproceedings{jinyuexiaStreamliningUserInteraction2013,
  title = {Streamlining User Interaction in Tag-Based Conversational Navigation of Knowledge Resource Libraries},
  booktitle = {Proceedings of the 13th {{ACM}}/{{IEEE-CS}} Joint Conference on {{Digital}} Libraries},
  author = {{Jinyue Xia} and {David C. Wilson}},
  year = {7 月 22, 2013},
  series = {{{JCDL}} '13},
  pages = {423--424},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2467696.2467769},
  url = {https://doi.org/10.1145/2467696.2467769},
  urldate = {2023-09-05},
  abstract = {This paper presents an approach for helping users more quickly discover relevant information resources in a tag based system, where each resource is associated with a number of descriptive meta-data tags. Our approach builds an adaptive conversational decision-tree structure to minimize the number of interactive cues required to help a user navigate to resources of interest. Initial experiments demonstrate the potential of the approach, with shallower decision trees supporting better overall interaction performance.},
  isbn = {978-1-4503-2077-1},
  langid = {english},
  keywords = {decision tree,knowledge navigation,recommender systems,user interaction},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 簡化知識資源庫基於標籤的會話導航中的用戶交互\\
abstractTranslation:  本文提出了一種幫助用戶在基於標籤的系統中更快地發現相關信息資源的方法，其中每個資源都與許多描述性元數據標籤相關聯。我們的方法構建了一個自適應會話決策樹結構，以最大限度地減少幫助用戶導航到感興趣的資源所需的交互線索的數量。初步實驗證明了該方法的潛力，較淺的決策樹支持更好的整體交互性能。},
  file = {C:\Users\BlackCat\Zotero\storage\U8PTNJJ4\Xia 與 Wilson - 2013 - Streamlining user interaction in tag-based convers.pdf}
}

@online{JiYuFusioninDecoderZhiZhongWenKaiFangLingYuWenDaYanJiu__TaiWanBoShuoShiLunWenZhiShiJiaZhiXiTong,
  title = {基於 Fusion-in-Decoder 之中文開放領域問答研究\_\_臺灣博碩士論文知識加值系統},
  url = {https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=key1YF/search?s=id=%22111NCHU5392009%22.&openfull=1&setcurrent=0},
  urldate = {2024-04-28},
  langid = {chinese},
  keywords = {問答系統,未整理,碩博士論文},
  file = {D\:\\Paper\\基於 Fusion-in-Decoder 之中文開放領域問答研究.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\223867G6\\search.html}
}

@online{JiYuShengChengZiLiaoJiHeJinYiBuYuXunLianZhiBaiKeWenDaXiTong,
  title = {基於生成資料集和進一步預訓練之百科問答系統},
  url = {https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=gSDQ4h/record?r1=5&h1=0#XXX},
  urldate = {2024-03-15},
  langid = {english},
  annotation = {titleTranslation: 基於生成資料集與進一步預訓練之百科問答系統},
  file = {D\:\\Paper\\基於生成資料集和進一步預訓練之百科問答系統.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\W9KZVU75\\record.html}
}

@online{JiYuShengChengZiLiaoJiHeJinYiBuYuXunLianZhiBaiKeWenDaXiTong__TaiWanBoShuoShiLunWenZhiShiJiaZhiXiTong,
  title = {基於生成資料集和進一步預訓練之百科問答系統\_\_臺灣博碩士論文知識加值系統},
  url = {https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=key1YF/search?s=id=%22111NCU05392069%22.&openfull=1&setcurrent=0},
  urldate = {2024-04-28},
  langid = {chinese},
  keywords = {問答系統,未整理,碩博士論文},
  file = {D\:\\Paper\\基於生成資料集和進一步預訓練之百科問答系統.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\IUTZXM2P\\search.html}
}

@online{JiYuShenJingWangLuoDeZhongWenDianZiBingLiMingMingShiTiShiBieBeiJingYouDianDaXue2018NianShuoShiLunWen,
  title = {基于神经网络的中文电子病历命名实体识别--《北京邮电大学》2018年硕士论文},
  url = {https://cdmd.cnki.com.cn/Article/CDMD-10013-1018125116.htm},
  urldate = {2022-08-14},
  file = {C:\Users\BlackCat\Zotero\storage\TC4KNPI3\CDMD-10013-1018125116.html}
}

@article{JiYuZhiShiTuPuDeGuoJiKeXueJiLiangXueYanJiuQianYanJiLiangFenXi,
  title = {基于知识图谱的国际科学计量学研究前沿计量分析},
  url = {https://d.wanfangdata.com.cn/periodical/kygl200901022},
  urldate = {2023-09-03},
  abstract = {通过绘制科学计量学研究前沿知识图谱,界定出七大国际科学计量学研究前沿领域:1、引文理论、科学交流、基础科研评价;2、科学知识图谱理论与方法、网络计量学;3、科学技术合作;4、科学计量学经典概率分布、文献计量学定律、信息计量学;5、科学计量学指标与国家科研绩效评价;6、知识的新产品--科技动力学;7、科学与技术的关系.其中前沿领域1、2、4属于理论与方法研究领域,而前沿领域3、5、6、7属于应用研究领域.经过比较发现科学计量学更加侧重于理论与方法研究.},
  langid = {chinese},
  keywords = {SCIENCE RESEARCH MANAGEMENT,共引分析,共词分析,研究前沿,科学知识图谱,科学计量学,科研管理},
  annotation = {2009-04-15 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 基於知識圖譜的國際科學計量學研究前沿計量分析\\
abstractTranslation:  通過近似科學計量學研究前沿知識圖譜，指南出七大國際科學計量學研究前沿領域：1、引文理論、科學交流、基礎科研評價；2、科學知識圖譜理論與方法、網絡計量學；3、科學技術合作;4、科學計量學經典概率分佈、文獻計量學、信息計量學;5、科學計量學指標與國家科研績效評價;6、知識的新產品--科技動力學;7、科學與技術其中前沿領域1、2、4屬於理論與方法研究領域，而前沿領域3、5、6、7屬於應用研究領域。經過比較發現科學計量學更側重於理論與方法研究。},
  file = {D:\Paper\基于知识图谱的国际科学计量学研究前沿计量分析.pdf}
}

@book{johnbiggamSucceedingYourMaster2008,
  title = {Succeeding with {{Your Master}}'s {{Dissertation}}: {{A Step-by-step Handbook}}},
  shorttitle = {Succeeding with {{Your Master}}'s {{Dissertation}}},
  author = {{John Biggam}},
  date = {2008},
  eprint = {j8VeMAEACAAJ},
  eprinttype = {googlebooks},
  publisher = {McGraw-Hill Education},
  abstract = {How do I plan a dissertation proposal? How do I clarify my research objectives? How do I conduct a literature review? What is essential to focus on while writing my Master's dissertation? This practical book offers straightforward guidance to help Master's students to clarify their objectives and structure their work in order to produce a successful dissertation.  Using case examples of both good and bad student practice, the handbook takes students through each step of the dissertation process, from their initial research proposal to the final submission. The author uses clear illustrations of what students need to do - or not do - to reach their potential, helping them to avoid the most common pitfalls.  This essential handbook covers: Producing focused and relevant research objectives Writing your literature review Citing your sources correctly Clearly explaining your use of research methods Writing up your findings Summarizing your work by linking your conclusions to your initial proposal Understanding marking schemes Aimed primarily at Master's students or students on short postgraduate courses in business, humanities and the social sciences, this book is also key reading for supervisors and undergraduates considering postgraduate study.},
  isbn = {978-0-335-23546-9},
  langid = {english},
  pagetotal = {280},
  keywords = {基本原理,已整理},
  annotation = {titleTranslation: 成功完成硕士论文：循序渐进手册\\
abstractTranslation:  如何规划论文提案？如何明确研究目标？如何进行文献综述？撰写硕士论文时必须关注哪些方面？这本实用的书籍提供了直截了当的指导，帮助硕士生明确目标、安排工作，从而成功撰写论文。  本手册使用好的和坏的学生实践案例，带领学生完成论文过程的每一步，从最初的研究计划到最后的提交。作者用清晰的图解说明了学生需要做什么--或者不需要做什么--以发挥他们的潜力，帮助他们避免最常见的陷阱。  这本手册的基本内容包括提出重点突出的相关研究目标 撰写文献综述 正确引用资料来源 明确解释研究方法的使用情况 撰写研究结果 将结论与最初的研究计划书联系起来，对工作进行总结 了解评分标准 本书主要针对硕士生或商科、人文和社会科学领域的短期研究生课程学生，也是导师和考虑攻读研究生课程的本科生的重要读物。}
}

@inproceedings{jonatlegullaInteractiveOntologyLearning2008,
  title = {An Interactive Ontology Learning Workbench for Non-Experts},
  booktitle = {Proceedings of the 2nd International Workshop on {{Ontologies}} and Information Systems for the Semantic Web},
  author = {{Jon Atle Gulla} and {Vijayan Sugumaran}},
  year = {10 月 30, 2008},
  series = {{{ONISW}} '08},
  pages = {9--16},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1458484.1458487},
  url = {https://dl.acm.org/doi/10.1145/1458484.1458487},
  urldate = {2023-09-16},
  abstract = {Ontologies are an integral part of Knowledge and Information Management systems and there is increased interest in using ontologies for organizational memory. Ontology learning workbenches are used for semi-automatic learning of ontologies from representative text collections. This paper presents a new interactive workbench that gives the users more freedom in their ontology engineering process and frees them from knowing any ontology language syntax. The workbench is implemented as part of a search project, in which ontologies are used to search for movie information on the web. New techniques are steadily being added to the workbench, though early testing has already confirmed the validity of the ontology learning approach.},
  isbn = {978-1-60558-255-9},
  langid = {english},
  keywords = {/unread,已整理,數據挖掘,本體建立,知識本體,語意分析},
  annotation = {3 citations (Crossref) [2024-03-26]\\
abstractTranslation:  本體是知識和資訊管理系統的一個組成部分，人們對使用本體進行組織記憶越來越感興趣。本體學習工作台用於從代表性文本集合中半自動學習本體。本文提出了一種新的互動式工作台，為使用者在本體工程過程中提供了更多自由，並使他們無需了解任何本體語言語法。該工作台是作為搜尋項目的一部分實現的，其中本體用於在網路上搜尋電影資訊。儘管早期測試已經證實了本體學習方法的有效性，但新技術正在穩步添加到工作台中。\\
titleTranslation: 適合非專家的互動式本體學習工作台},
  note = {說明知識本體建立時不該那麼依賴專業的本體人員。因此試圖將需要本體知識的部分隱藏起來，由系統來自動處理。研究仍未完成，值得追蹤後續。},
  file = {C:\Users\BlackCat\Zotero\storage\AANH7VT9\Gulla 與 Sugumaran - 2008 - An interactive ontology learning workbench for non.pdf}
}

@article{jooheechoiWillTooMany2018,
  title = {Will {{Too Many Editors Spoil The Tag}}? {{Conflicts}} and {{Alignment}} in {{Q}}\&{{A Categorization}}},
  shorttitle = {Will {{Too Many Editors Spoil The Tag}}?},
  author = {{Joohee Choi} and {Yla Tausczik}},
  year = {11 月 1, 2018},
  journaltitle = {Proceedings of the ACM on Human-Computer Interaction},
  shortjournal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {2},
  pages = {38:1--38:19},
  doi = {10.1145/3274307},
  url = {https://doi.org/10.1145/3274307},
  urldate = {2023-09-05},
  abstract = {Q\&A websites compile useful knowledge through user-generated questions and responses. Many Q\&As use collaborative tagging systems to improve search and discovery while distributing the work of categorizing and organization throughout the community. Although early work on collaborative tagging questioned whether consistent categorization schemes could emerge from large groups with little to no coordination, empirical studies have found surprising coherence among users' tags. We build on this research by testing whether coherence emerges in tag usage on Q\&As, a more challenging context, focusing in particular on mismatches in the specificity of tags (basic level disagreement). We found that some users shifted toward more specific tag usage over time slightly increasing conflict, but that moderators were instrumental in helping to resolve some of this conflict. This study highlights the importance of learning and moderation in the development of coherence in collaborative tagging systems.},
  issue = {CSCW},
  langid = {english},
  keywords = {categorization,collaborative tagging,distributed cognition,moderation,Q&As},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 太多的編輯會破壞標籤嗎？問答分類中的衝突和一致性\\
abstractTranslation:  問答網站通過用戶提出的問題和回答來彙編有用的知識。許多問答使用協作標記系統來改進搜索和發現，同時在整個社區中分配分類和組織工作。儘管協作標記的早期工作質疑是否可以從幾乎沒有協調的大型群體中產生一致的分類方案，但實證研究發現用戶標籤之間具有令人驚訝的一致性。我們在這項研究的基礎上，測試問答中的標籤使用是否出現連貫性，這是一個更具挑戰性的環境，特別關注標籤特異性的不匹配（基本級別的分歧）。我們發現，隨著時間的推移，一些用戶轉向更具體的標籤使用，略微增加了衝突，但版主在幫助解決部分衝突方面發揮了重要作用。這項研究強調了學習和調節在協作標記系統一致性發展中的重要性。},
  file = {C:\Users\BlackCat\Zotero\storage\GJEN89IN\Choi 與 Tausczik - 2018 - Will Too Many Editors Spoil The Tag Conflicts and.pdf}
}

@inproceedings{jordanbuchananpaffCognitiveFeedbackTheories2023,
  title = {Cognitive {{Feedback Theories}} and {{Artificial Intelligence}}: {{A Case}} for {{A Grammarly}} of {{UI}}/{{UX Design}}},
  shorttitle = {Cognitive {{Feedback Theories}} and {{Artificial Intelligence}}},
  author = {{Jordan Buchanan Paff}},
  date = {2023},
  url = {https://www.semanticscholar.org/paper/Cognitive-Feedback-Theories-and-Artificial-A-Case-A-Paff/52058ea628cc0cf6e3975788903658e2f9727656},
  urldate = {2023-10-23},
  abstract = {This thesis is concerned with utilizing artificial intelligence and machine learning (AI/ML) techniques and cognitive theories of feedback to enhance learning outcomes in the field of user interface and user experience (UI/UX) design. The capabilities of AI/ML have expanded immensely over the past several years, and it is now being effectively used in software programs like Grammarly, a tool that provides intelligent feedback on writing skills including grammar, tone, and clarity. Grammarly has been uniquely successful as a feedback tool because it relies on lessons from cognitive science regarding student feedback and learning outcomes. Currently, there is no comparable software available for UI/UX, making it a uniquely untapped area for effective learning tools. The question that this thesis attempts to answer, therefore, is: How can the successes of Grammarly and established cognitive feedback principles inform the design of an AI/ML-based feedback tool for UI/UX design? To answer that question, this thesis explores previous work on AI/ML techniques, cognitive feedback theories, structural similarities between grammar and design, and design heuristics in order to ultimately define the theoretical groundwork for a “Grammarly for UI/UX design.”},
  langid = {english},
  keywords = {No DOI found,人機互動,已整理,機器學習},
  annotation = {titleTranslation: 認知回饋理論與人工智慧：UI/UX 設計語法案例},
  note = {[TLDR] This thesis explores previous work on AI/ML techniques, cognitive feedback theories, structural similarities between grammar and design, and design heuristics in order to ultimately define the theoretical groundwork for a “Grammarly for UI/UX design.”}
}

@inproceedings{jorgenjaanusAligningKnowledgeDevelopment2013,
  title = {Aligning {{Knowledge Development}} between {{Innovation-Driven Context}} and {{Knowledge Organization Systems}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Knowledge Management}} and {{Knowledge Technologies}}},
  author = {{Jörgen Jaanus} and {Tobias Ley}},
  year = {9 月 4, 2013},
  series = {I-{{Know}} '13},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2494188.2494212},
  url = {https://doi.org/10.1145/2494188.2494212},
  urldate = {2023-09-11},
  abstract = {While semantic technologies are successfully used in some more static and business critical domains like public management or financial services the adoption in innovation driven contexts has been more hesitant. The challenge of aligning knowledge development between innovation-driven context and knowledge organization systems by introducing social semantic technologies requires the shift from creation centered to addition centered approach in dealing with new concepts. For any organizational context there is something existent with applied classification logic which has to be considered while creating new knowledge. In order to study an impact of stabilizing and dynamic forces on knowledge organization systems we investigate concept compounding as ontological change. As new concepts emerge through knowledge maturing process and have direct impact on enterprise knowledge organization systems, a longitudinal approach is needed that follows development of new concepts over time. We took this approach by analyzing ontology maturing in an international company which provides professional services to both institutional and private customers. We present examples of the compounding of six new concepts as interactions between dynamic forces which are functioning within social technologies and stabilizing forces in the form of arbitrary knowledge organization systems. Based on our conclusions we propose Knowledge Alignment Model and list the services which will be developed further for applying Social Semantic Technologies in an organizational context. The model is a connection between the conceptual considerations presented as a shift from creation to addition and between design activities to make this shift happen.},
  isbn = {978-1-4503-2300-0},
  langid = {english},
  keywords = {Business Rules,Knowledge Maturing,Knowledge Organization Systems,Social Semantic Technologies},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 協調創新驅動環境和知識組織系統之間的知識開發\\
abstractTranslation:  雖然語義技術成功地應用於一些更靜態和業務關鍵的領域，如公共管理或金融服務，但在創新驅動的環境中的採用卻更加猶豫。通過引入社會語義技術來協調創新驅動的背景和知識組織系統之間的知識發展的挑戰需要在處理新概念時從以創造為中心的方法轉變為以添加為中心的方法。對於任何組織環境，都存在一些應用分類邏輯的東西，在創建新知識時必須考慮這些邏輯。為了研究穩定力和動態力對知識組織系統的影響，我們研究概念複合作為本體論的變化。隨著新概念在知識成熟過程中出現並對企業知識組織系統產生直接影響，需要一種縱向方法來跟踪新概念隨著時間的推移的發展。我們通過分析一家為機構和私人客戶提供專業服務的國際公司成熟的本體論來採用這種方法。我們提出了六個新概念複合的例子，作為在社會技術中發揮作用的動態力量與任意知識組織系統形式的穩定力量之間的相互作用。根據我們的結論，我們提出了知識對齊模型，並列出了將進一步開發的服務，以便在組織環境中應用社會語義技術。該模型是作為從創造到添加的轉變而呈現的概念考慮因素與實現這種轉變的設計活動之間的聯繫。},
  file = {C:\Users\BlackCat\Zotero\storage\E8IEDHSN\Jaanus 與 Ley - 2013 - Aligning Knowledge Development between Innovation-.pdf}
}

@inproceedings{jorsielecerqueiraOntologyContextawareMiddleware2023,
  title = {An {{Ontology}} for {{Context-aware Middleware}} for {{Dependable Medical Systems}}},
  booktitle = {Proceedings of the 11th {{Latin-American Symposium}} on {{Dependable Computing}}},
  author = {{Jorsiele Cerqueira}},
  year = {1 月 17, 2023},
  series = {{{LADC}} '22},
  pages = {79--83},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3569902.3569947},
  url = {https://dl.acm.org/doi/10.1145/3569902.3569947},
  urldate = {2023-09-15},
  abstract = {In healthcare systems, there is an ecosystem of heterogeneous biosensors. A middleware is required for transmitting and establishing dependable communication with multiple integrations and information exchange through messages. However, in environments in which a distributed and dynamic network exists, a purely traditional middleware would not minimize the effect failures at data transmission and network congestion can cause. To preserve data integrity and provide relevant services as the environment changes, the use of context-aware middleware is recommended. This article describes an ontology for context-aware middleware to handle challenges faced by medical system networks and environment changes.},
  isbn = {978-1-4503-9737-7},
  langid = {english},
  keywords = {/unread,context aware middleware,dependable,modelling,ontology,已整理,知識本體,醫療},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 可靠醫療系統的上下文感知中間件本體\\
abstractTranslation:  在醫療保健系統中，存在一個異質生物感測器的生態系統。需要中間件來傳輸和建立與多個整合的可靠通訊以及透過訊息進行資訊交換。然而，在分散式和動態網路存在的環境中，純粹的傳統中間件並不能最大限度地減少資料傳輸失敗和網路擁塞可能造成的影響。為了在環境變化時保持資料完整性並提供相關服務，建議使用上下文感知中間件。本文描述了上下文感知中間件的本體，以應對醫療系統網路和環境變化所面臨的挑戰。},
  note = {以知識本體作為醫院中許多系統的中介站，以保有上下文訊息。},
  file = {C:\Users\BlackCat\Zotero\storage\8VGBSNND\Cerqueira - 2023 - An Ontology for Context-aware Middleware for Depen.pdf}
}

@article{josbragadevasconcelosApplicationKnowledgeManagement2017,
  title = {The Application of Knowledge Management to Software Evolution},
  author = {{Jos Braga de Vasconcelos} and {Chris Kimble} and {Paulo Carreteiro} and {lvaro Rocha}},
  year = {2 月 1, 2017},
  journaltitle = {International Journal of Information Management: The Journal for Information Professionals},
  shortjournal = {Int. J. Inf. Manag.},
  volume = {37},
  number = {1},
  pages = {1499--1506},
  issn = {0268-4012},
  doi = {10.1016/j.ijinfomgt.2016.05.005},
  url = {https://doi.org/10.1016/j.ijinfomgt.2016.05.005},
  urldate = {2023-09-28},
  abstract = {In complex software development projects, consistent planning and communication between the stakeholders is crucial for effective collaboration across the different stages in software construction. Taking the view of software development and maintenance as being part of the broader phenomenon of software evolution, this paper argues that the adoption of knowledge management practices in software engineering would improve both software construction and more particularly software maintenance. The research work presents a guidance model for both areas: knowledge management and software engineering, combining insights across corporate software projects as a means of evaluating the effects on people and organization, technology, workflows and processes.},
  langid = {english},
  keywords = {Collaborative work,Knowledge management,Software development process,Software engineering,Software maintenance,知識管理,軟體開發},
  annotation = {52 citations (Crossref) [2024-03-26]\\
abstractTranslation:  在複雜的軟體開發專案中，利害關係人之間一致的規劃和溝通對於軟體建構不同階段的有效協作至關重要。本文將軟體開發和維護視為更廣泛的軟體演化現象的一部分，認為在軟體工程中採用知識管理實踐將改善軟體構造，尤其是軟體維護。這項研究工作為知識管理和軟體工程這兩個領域提供了指導模型，結合了企業軟體專案的見解，作為評估對人員和組織、技術、工作流程和流程的影響的手段。}
}

@inproceedings{joshuay.kimComparisonAutomaticSpeech2022,
  title = {Comparison of {{Automatic Speech Recognition Systems}}},
  booktitle = {Conversational {{AI}} for {{Natural Human-Centric Interaction}}},
  author = {{Joshua Y. Kim} and {Chunfeng Liu} and {Rafael A. Calvo} and {Kathryn McCabe} and {Silas C. R. Taylor} and {Björn W. Schuller} and {Kaihang Wu}},
  editor = {{Svetlana Stoyanchev} and {Stefan Ultes} and {Haizhou Li}},
  date = {2022},
  series = {Lecture {{Notes}} in {{Electrical Engineering}}},
  pages = {123--131},
  publisher = {Springer Nature},
  location = {Singapore},
  doi = {10.1007/978-981-19-5538-9_8},
  abstract = {High-quality transcription systems are required for conversational analysis systems. We compared two manual transcribers with five automatic transcription systems using video conferences from a medical domain and found that (1) manual transcriptions significantly outperformed the automatic services, and (2) the automatic transcription of YouTube Captions significantly outperformed the other ASR services.},
  isbn = {978-981-19553-8-9},
  langid = {english},
  keywords = {Speech recognition,已整理,語音辨識,醫療},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 自動語音辨識系統的比較},
  file = {C:\Users\BlackCat\Zotero\storage\HNQSBFQN\Kim et al. - 2022 - Comparison of Automatic Speech Recognition Systems.pdf}
}

@article{jovanajovicMultidimensionalConceptMap2023,
  title = {Multidimensional {{Concept Map Representation}} of the {{Learning Objects Ontology Model}} for {{Personalized Learning}}},
  author = {{Jovana Jović} and {Miroslava Raspopović Milić} and {Svetlana Cvetanović}},
  date = {2023-09-01},
  journaltitle = {Journal of Internet Technology},
  volume = {24},
  number = {5},
  pages = {1043--1054},
  issn = {2079-4029},
  doi = {10.53106/160792642023092405003},
  url = {https://jit.ndhu.edu.tw/article/view/2954},
  urldate = {2023-10-18},
  abstract = {This work presents the creation and representation of an ontology model for the domain knowledge used for learning objects. The purpose of the developed ontology model is to define relations between learning objects that can be applied for their effective search and visualization. As the number of learning objects increases, the representation of the knowledge domain becomes challenging. In this paper, the authors propose the application of multidimensional concept maps (MCMs) for domain knowledge representation. The definition of different attributes used in the ontology model allow for defining the different dimensions needed for MCM ontology visualization. In order to achieve integration of the defined ontology model and MCMs, a software tool named Ontology-based system for learning objects retrieval (OBSLO) was developed. OBSLO’s role is to dynamically generate MCMs given the defined ontology with its relations and attributes, while also providing a content delivery environment and working space for learners. Proposed OBSLO architecture with integrated ontology model and MCMs was evaluated and compared to the learning management system where ontology and MCMs were not used. It was shown that learners using OBSLO showed better success rate in learning and positive level of satisfaction.},
  issue = {5},
  langid = {american},
  keywords = {Learning objects,Multidimensional concept maps,Ontology,Personalized e-learning,人機互動,使用者研究,知識本體},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 用于个性化学习的学习对象本体模型的多维概念图表示法\\
abstractTranslation:  本作品介绍了用于学习对象领域知识的本体模型的创建和表示。开发本体模型的目的是定义学习对象之间的关系，以便有效地搜索和可视化学习对象。随着学习对象数量的增加，知识领域的表征变得具有挑战性。在本文中，作者提出了应用多维概念图（MCM）来表示领域知识的方法。通过定义本体模型中使用的不同属性，可以定义 MCM 本体可视化所需的不同维度。为了实现已定义的本体模型与多维本体图的整合，开发了一个名为 "基于本体的学习对象检索系统（OBSLO）"的软件工具。OBSLO 的作用是根据已定义的本体及其关系和属性动态生成多 媒体内容，同时为学习者提供内容交付环境和工作空间。对拟议的集成了本体模型和多模块内容的 OBSLO 架构进行了评估，并与未使用本体和多模块内容的学习管理系统进行了比较。结果表明，使用 OBSLO 的学习者学习成功率更高，满意度也更高。},
  note = {將學習領域的知識本體可視化。},
  file = {C:\Users\BlackCat\Zotero\storage\MIZ4HDCK\Jović et al. - 2023 - Multidimensional Concept Map Representation of the.pdf}
}

@patent{JuChuanXiangYiZhongKeJiZhuanJiaZiYuanXuNiHuaYuYuYiTuiLiJianSuoFangFa2020,
  type = {patent},
  title = {一种科技专家资源虚拟化与语义推理检索方法},
  author = {{鞠传香} and {吴志勇} and {丁航奇} and {李松}},
  holder = {{Shandong University of Technology}},
  date = {2020-06-09},
  number = {CN111259041A},
  location = {CN},
  url = {https://patents.google.com/patent/CN111259041A/zh},
  urldate = {2023-07-28},
  abstract = {一种科技专家资源虚拟化与语义推理检索方法，属于科技服务技术领域。其特征在于：包括如下步骤：步骤1001，形式化定义科技专家资源；步骤1002，科技专家资源封装与虚拟化；步骤1003，构建科技专家领域本体模型；步骤1004，定义科技专家信息匹配语义推理规则；步骤1005，基于Jena推理机的科技专家检索。在本申请的科技专家资源虚拟化与语义推理检索方法中，能够实现跨区域科技专家资源共享和集成，利用本方法所形成的科技专家检索系统进行检索时，科技专家检索方法更便捷、信息更丰富，匹配结果更可靠、更智能，能够较好地满足选择科技专家的需求。},
  langid = {chinese},
  keywords = {expert,information,resource,scientific,technical,未整理},
  annotation = {titleTranslation: 一種科技專家資源虛擬化與推理推理檢索方法\\
abstractTranslation:  一種科技專家資源虛擬化與人工智能推理檢索方法，屬於科技服務技術領域。其特徵包括：包括如下步驟：步驟1001、形成定義科技專家資源；步驟1002、科技專家資源封裝與虛擬化；步驟1003 ，構建科技專家領域本體模型；步驟1004，定義科技專家信息匹配推理推理規則；步驟1005，基於耶拿推理機的科技專家搜索。在本申請的科技專家資源虛擬化與推理推理搜索方法中，能夠實現跨區域科技專家資源共享和集成，利用本方法所形成的科技專家檢索系統進行檢索時，科技專家檢索方法更便捷、信息更豐富，匹配結果更可靠、更智能，能夠很好地滿足選擇科技專家的需求。},
  file = {C:\Users\BlackCat\Zotero\storage\5E3F587X\鞠传香 等。 - 2020 - 一种科技专家资源虚拟化与语义推理检索方法.pdf}
}

@inproceedings{juhapuustjarviExploitingLearningObject2010,
  title = {Exploiting Learning Object Ontology in Building Personal Curricula for Pharmacists},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Information Integration}} and {{Web-based Applications}} \& {{Services}}},
  author = {{Juha Puustjärvi} and {Leena Puustjärvi}},
  year = {11 月 8, 2010},
  series = {{{iiWAS}} '10},
  pages = {367--372},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1967486.1967544},
  url = {https://dl.acm.org/doi/10.1145/1967486.1967544},
  urldate = {2023-09-16},
  abstract = {The role of continuing education and lifelong learning is becoming still more important as the fast development of technologies requires specialized skills that need to be renewed frequently. eLearning adopts well for continued education as it can be done in parallel to other work. However, in the context of continuing professional education building a personal curriculum is not just straightforward as there are several educational institutions that provide various courses and there are no unified ways of representing the content of the courses. We have investigated this problem in pharmacies where the building of personal curricula is dictated by regulations that are set by health care authorities. In particular, we have investigated how the educational information that is received from a variety of sources and that is in a variety of format can be managed in building personal curricula. The key idea in our developed solution is the learning object ontology, which is integrated with the pharmacy ontology. In this way the educational information can be stored in pharmacy's knowledge base that provides sophisticated ways for accessing educational information. The technologies we have used in implementing the pharmacy ontology are RDF and OWL.},
  isbn = {978-1-4503-0421-4},
  langid = {english},
  keywords = {/unread,continued education,curricula,eLearning,information filtering,learning objects,ontologies,OWL,RDF,semantic web,taxonomies,已整理,知識分類,知識本體,醫療},
  annotation = {3 citations (Crossref) [2024-03-26]\\
abstractTranslation:  由於科技的快速發展需要經常更新的專業技能，因此繼續教育和終身學習的角色變得更加重要。電子學習非常適合繼續教育，因為它可以與其他工作並行。然而，在繼續職業教育的背景下，建立個人課程並不簡單，因為有多個教育機構提供不同的課程，並且沒有統一的方式來表示課程內容。我們在藥局調查了這個問題，這些藥局的個人課程建設是由醫療保健當局製定的法規規定的。特別是，我們研究瞭如何在建立個人課程時管理從各種來源接收的各種格式的教育資訊。我們開發的解決方案的關鍵思想是學習對象本體，它與藥學本體整合。透過這種方式，教育資訊可以儲存在藥房的知識庫中，該知識庫提供了存取教育資訊的複雜方法。我們在實現藥學本體時使用的技術是RDF和OWL。\\
titleTranslation: 利用學習對象本體建構藥劑師個人課程},
  note = {看不懂何謂將藥房的學習資料放到知識本體中，所以不用學了嗎?},
  file = {C:\Users\BlackCat\Zotero\storage\8V3PP5UL\Puustjärvi 與 Puustjärvi - 2010 - Exploiting learning object ontology in building pe.pdf}
}

@inproceedings{junhuaduanGeneralSystematicMethod2022,
  title = {A {{General}} and {{Systematic Method}} for {{Constructing}} and {{Applying}} the {{Chinese Medical Knowledge Graph}}},
  booktitle = {2022 5th {{International Conference}} on {{Artificial Intelligence}} and {{Big Data}} ({{ICAIBD}})},
  author = {{Junhua Duan} and {Ziyuan Chen} and {Yi-An Zhu} and {Wei Lu}},
  date = {2022-05},
  pages = {547--552},
  doi = {10.1109/icaibd55127.2022.9820423},
  abstract = {In this paper, we propose a general systematic medical knowledge graph construction technique, which combines the methods of domain knowledge graph building and general knowledge graph building. Using ICD-11 as the disease knowledge system, the bottom-up method is used to collect medical data, then the multi-source heterogeneous data are fused to build a medical knowledge graph. Because knowledge graphs that are constructed from the bottom to the top may be incomplete, the knowledge reasoning method is used to complete the knowledge graph. Given the high requirement of knowledge accuracy in the medical field, only the reasoning results considered correct by experts can be added to the knowledge graph for knowledge completion. Finally, we create an application website for the medical knowledge graph, which realizes the functions of medical knowledge Q \& A and information recommendations. The approaches for medical knowledge graph construction, knowledge completion, and the Q \& A system devised in this paper can be used as a general framework that can be applied to other knowledge graph construction systems.},
  eventtitle = {2022 5th {{International Conference}} on {{Artificial Intelligence}} and {{Big Data}} ({{ICAIBD}})},
  langid = {english},
  keywords = {Chinese medical knowledge graph,Cognition,Hospitals,Knowledge based systems,knowledge completion,Knowledge engineering,knowledge fusion,Natural languages,Q & A system,Representation learning,Systematics,待讀,知識本體},
  annotation = {0 citations (Crossref) [2024-03-26]\\
0 citations (Semantic Scholar/DOI) [2023-06-24]\\
titleTranslation: 中醫知識圖譜構建與應用的通用系統方法\\
abstractTranslation:  在本文中，我們提出了一種通用系統醫學知識圖譜構建技術，該技術結合了領域知識圖譜構建和通用知識圖譜構建方法。以ICD-11作為疾病知識體系，採用自下而上的方法收集醫學數據，然後融合多源異構數據構建醫學知識圖譜。由於自下而上構建的知識圖譜可能不完整，因此採用知識推理的方法來完善知識圖譜。鑑於醫學領域對知識準確性的要求很高，只有專家認為正確的推理結果才能添加到知識圖譜中，完成知識補全。最後，我們創建了一個醫學知識圖譜的應用網站，實現了醫學知識問答和信息推薦的功能。本文設計的醫學知識圖譜構建、知識補全和問答系統的方法可以作為通用框架應用於其他知識圖譜構建系統。},
  note = {中醫知識本體，包含查詢功能阿哈哈},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\MK32GZU2\\Duan 等。 - 2022 - A General and Systematic Method for Constructing a.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\T2LBN4JM\\9820423.html}
}

@article{junzhaoPublishingChineseMedicine2010,
  title = {Publishing {{Chinese}} Medicine Knowledge as {{Linked Data}} on the {{Web}}},
  author = {{Jun Zhao}},
  date = {2010-07-27},
  journaltitle = {Chinese Medicine},
  shortjournal = {Chinese Medicine},
  volume = {5},
  number = {1},
  pages = {27},
  issn = {1749-8546},
  doi = {10.1186/1749-8546-5-27},
  url = {https://doi.org/10.1186/1749-8546-5-27},
  urldate = {2023-09-15},
  abstract = {Chinese medicine (CM) draws growing attention from Western healthcare practitioners and patients. However, the integration of CM knowledge and Western medicine (WM) has been hindered by a barrier of languages and cultures as well as a lack of scientific evidence for CM's efficacy and safety. In addition, most of CM knowledge published with relational database technology makes the integration of databases even more challenging.},
  langid = {english},
  keywords = {Chinese Medicine,Link Data,Resource Description Framework,SPARQL Query,Uniform Resource Identifier,中醫,已整理,本體設計,知識本體,重要},
  annotation = {9 citations (Crossref) [2024-03-26]\\
abstractTranslation:  中醫（CM）越來越受到西方醫療保健從業人員和患者的關注。然而，由於語言和文化的障礙以及缺乏證明中醫有效性和安全性的科學證據，中醫知識與西方醫學的融合受到阻礙。另外，CM知識多採用關聯式資料庫技術發布，使得資料庫的整合更具挑戰性。\\
titleTranslation: 將中醫知識作為關聯資料發佈在網路上},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\VUVEY8VB\\Zhao - 2010 - Publishing Chinese medicine knowledge as Linked Da.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\GH4UFNTC\\1749-8546-5-27.html}
}

@article{k.p.liaoDevelopmentPhenotypeAlgorithms2015,
  title = {Development of Phenotype Algorithms Using Electronic Medical Records and Incorporating Natural Language Processing},
  author = {{K. P. Liao} and {T. Cai} and {G. K. Savova} and {S. N. Murphy} and {E. W. Karlson} and {A. N. Ananthakrishnan} and {V. S. Gainer} and {S. Y. Shaw} and {Z. Xia} and {P. Szolovits} and {S. Churchill} and {I. Kohane}},
  date = {2015-04-24},
  journaltitle = {BMJ},
  shortjournal = {BMJ},
  volume = {350},
  pages = {h1885-h1885},
  issn = {1756-1833},
  doi = {10.1136/bmj.h1885},
  url = {https://www.bmj.com/lookup/doi/10.1136/bmj.h1885},
  urldate = {2022-09-19},
  issue = {apr24 11},
  langid = {english},
  annotation = {206 citations (Crossref) [2024-03-26]\\
206 citations (Semantic Scholar/DOI) [2022-10-26]\\
titleTranslation: 使用電子病歷並結合自然語言處理開發表型算法},
  file = {C:\Users\BlackCat\Zotero\storage\C9G8BYL4\Liao 等。 - 2015 - Development of phenotype algorithms using electron.pdf}
}

@inproceedings{kaifengguoChineseKnowledgeGraph2023,
  title = {A {{Chinese Knowledge Graph Q}}\&{{A System Based}} on {{Dense Relationship Retrieval}}},
  booktitle = {2023 {{IEEE}} 3rd {{International Conference}} on {{Software Engineering}} and {{Artificial Intelligence}} ({{SEAI}})},
  author = {{Kaifeng Guo} and {Ri Huang}},
  date = {2023-06},
  pages = {148--153},
  doi = {10.1109/SEAI59139.2023.10217649},
  url = {https://ieeexplore.ieee.org/document/10217649},
  urldate = {2023-11-23},
  abstract = {Currently, most knowledge graph question answering (KGQA) systems need to retrieve all entities when dealing with complex problems involving multiple entities and relationships, which results in high time complexity and resource consumption. The response time of KGQA systems is an important indicator for evaluating their performance. To address this issue, this paper proposes a Chinese KGQA system based on dense relationship retrieval. The system uses the Faiss index mechanism for vector similarity retrieval, quickly extracts the top K relationships from the pre-built vector relationship library, and constructs paths to reduce a large number of irrelevant semantic paths in most KGQA tasks, thereby improving the time cost and computational resource overhead caused by the exponential growth of path numbers in second-order and higher-order questions. By using this method, the response time of the QA system can be shortened to within 1 second with minimal loss of accuracy. This paper combines pre-training models to complete tasks such as text semantic similarity and entity mention recognition, and achieves an average F1 value of 71.3\% on the CCKS2019-CKBQA test set. Comparing with other systems demonstrates the superiority of our method in terms of accuracy and efficiency.},
  eventtitle = {2023 {{IEEE}} 3rd {{International Conference}} on {{Software Engineering}} and {{Artificial Intelligence}} ({{SEAI}})},
  langid = {english},
  keywords = {中文,問答系統,密集搜索,實體抽取,已整理,微讀,機器學習,知識圖譜,重要},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於密集關係檢索的中文知識圖譜問答系統\\
abstractTranslation:  目前，大多數知識圖譜問答（KGQA）系統在處理涉及多個實體和關係的複雜問題時需要檢索所有實體，這導致時間複雜度和資源消耗較高。 KGQA系統的反應時間是評估其性能的重要指標。針對這個問題，本文提出了一個基於密集關係檢索的中文KGQA系統。系統利用Faiss索引機制進行向量相似度檢索，從預先建立的向量關係庫中快速提取前K個關係，並建立路徑，減少大多數KGQA任務中大量不相關的語意路徑，從而提高時間成本和效率。二階和高階問題中路徑數指數成長所導致的計算資源開銷。透過使用這種方法，QA系統的響應時間可以縮短到1秒以內，並且精度損失最小。本文結合預訓練模型完成文本語意相似度、實體提及辨識等任務，在CCKS2019-CKBQA測試集上取得平均F1值71.3\%。與其他系統相比，證明了我們的方法在準確性和效率方面的優越性。},
  note = {通過向量搜尋大量減少遍歷整個KG所花費的時間，並且同時不會降低太多準確度。
\par
前面看完了，模型設計開始慢慢看不懂。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\D4Q3AS86\\Guo and Huang - 2023 - A Chinese Knowledge Graph Q&A System Based on Dens.pdf;D\:\\Paper\\A Chinese Knowledge Graph Q&A System Based on Dense Relationship Retrieval.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\FZX58I3T\\10217649.html}
}

@inproceedings{kanthashreemysoresathyendraContextualAdaptersPersonalized2022,
  title = {Contextual {{Adapters}} for {{Personalized Speech Recognition}} in {{Neural Transducers}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {{Kanthashree Mysore Sathyendra} and {Thejaswi Muniyappa} and {Feng-Ju Chang} and {Jing Liu} and {Jinru Su} and {Grant P. Strimel} and {Athanasios Mouchtaris} and {Siegfried Kunzmann}},
  date = {2022-05},
  pages = {8537--8541},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746126},
  url = {https://ieeexplore.ieee.org/abstract/document/9746126/citations#citations},
  urldate = {2023-10-30},
  abstract = {Personal rare word recognition in end-to-end Automatic Speech Recognition (E2E ASR) models is a challenge due to the lack of training data. A standard way to address this issue is with shallow fusion methods at inference time. However, due to their dependence on external language models and the deterministic approach to weight boosting, their performance is limited. In this paper, we propose training neural contextual adapters for personalization in neural transducer based ASR models. Our approach can not only bias towards user-defined words, but also has the flexibility to work with pretrained ASR models. Using an in-house dataset, we demonstrate that contextual adapters can be applied to any general purpose pretrained ASR model to improve personalization. Our method outperforms shallow fusion, while retaining functionality of the pretrained models by not altering any of the model weights. We further show that the adapter style training is superior to full-fine-tuning of the ASR models on datasets with user-defined content.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  langid = {english},
  keywords = {個人化語音辨識,已整理,語音辨識},
  annotation = {19 citations (Crossref) [2024-03-26]\\
titleTranslation: 用於神經感測器中個性化語音識別的上下文適配器},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\2D8GXRHS\\Sathyendra 等。 - 2022 - Contextual Adapters for Personalized Speech Recogn.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\8SHMDXTZ\\citations.html}
}

@article{kanzaTooManyTags2019,
  title = {Too Many Tags Spoil the Metadata: Investigating the Knowledge Management of Scientific Research with Semantic Web Technologies},
  shorttitle = {Too Many Tags Spoil the Metadata},
  author = {Kanza, Samantha and Gibbins, Nicholas and Frey, Jeremy G.},
  date = {2019-03-21},
  journaltitle = {Journal of Cheminformatics},
  shortjournal = {Journal of Cheminformatics},
  volume = {11},
  number = {1},
  pages = {23},
  issn = {1758-2946},
  doi = {10.1186/s13321-019-0345-8},
  url = {https://doi.org/10.1186/s13321-019-0345-8},
  urldate = {2023-12-21},
  abstract = {Scientific research is increasingly characterised by the volume of documents and data that it produces, from experimental plans and raw data to reports and papers. Researchers frequently struggle to manage and curate these materials, both individually and collectively. Previous studies of Electronic Lab Notebooks (ELNs) in academia and industry have identified semantic web technologies as a means for organising scientific documents to improve current workflows and knowledge management practices. In this paper, we present a qualitative, user-centred study of researcher requirements and practices, based on a series of discipline-specific focus groups. We developed a prototype semantic ELN to serve as a discussion aid for these focus groups, and to help us explore the technical readiness of a range of semantic web technologies. While these technologies showed potential, existing tools for semantic annotation were not well-received by our focus groups, and need to be refined before they can be used to enhance current researcher practices. In addition, the seemingly simple notion of “tagging and searching” documents appears anything but; the researchers in our focus groups had extremely personal requirements for how they organise their work, so the successful incorporation of semantic web technologies into their practices must permit a significant degree of customisation and personalisation.},
  langid = {english},
  keywords = {Document management,Ontologies,Scientific documents,Semantic tagging,待讀,數位實驗筆記,標籤,研究流程,重要},
  annotation = {11 citations (Crossref) [2024-03-26]\\
titleTranslation: 太多的標籤破壞了元資料：利用語意網路技術研究科學研究的知識管理\\
abstractTranslation:  科學研究越來越以其產生的文件和數據量為特徵，從實驗計劃和原始數據到報告和論文。研究人員經常難以單獨或集體管理和整理這些資料。學術界和工業界先前對電子實驗室筆記本 (ELN) 的研究已將語義網路技術確定為組織科學文件以改善當前工作流程和知識管理實踐的一種手段。在本文中，我們基於一系列特定學科焦點小組，對研究人員的要求和實踐進行了定性、以使用者為中心的研究。我們開發了一個語意 ELN 原型，作為這些焦點小組的討論輔助工具，並幫助我們探索一系列語意 Web 技術的技術準備。雖然這些技術顯示出潛力，但現有的語義註釋工具並未受到我們的焦點小組的歡迎，需要進行改進才能用於增強當前研究人員的實踐。此外，「標記與搜尋」文件這看似簡單的概念其實並非如此；我們焦點小組的研究人員對於如何組織他們的工作有著極其個人化的要求，因此語義網路技術成功地融入他們的實踐中必須允許很大程度的客製化和個性化。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\7HLQ4TTV\\Too many tags spoil the metadata： investigating the knowledge management of scientific research with semantic web technologies.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\SWQ4N2WL\\s13321-019-0345-8.html}
}

@thesis{karahansonIntegratingKnowledgeManagement2014,
  title = {Integrating {{Knowledge Management}} and {{E-Learning}} to {{Enhance Knowledge Creation}} and {{Conversion}} for {{Competitive Advantage}} in {{Small-}} and {{Medium-Sized Businesses}}},
  author = {{Kara Hanson}},
  date = {2014},
  institution = {ProQuest Dissertations Publishing},
  abstract = {The global business economy has established knowledge as the most important asset of an organization and a major factor in competitive advantage. A review of literature and research has established that small-and medium-sized businesses are rich in tacit knowledge, an essential piece of the knowledge creation and conversion process that supports organizational learning and knowledge—a factor in attaining and sustaining competitive advantage. Small- and medium sized businesses are inherently well-suited to informal learning, a collaborative function of sharing knowledge through conversations, dialogue, etc. However, this informal exchange may or may not naturally occur in SMBs. A more structured approach to the process that fosters informal knowledge sharing is needed to ensure knowledge creation occurs within the organization. Traditional attempts at KM and e-learning, which focused on knowledge as a static object rather than a dynamic process were unsuccessful in helping SMBs achieve competitive advantage. There is growing body of research that advocates for the integration of KM and e-learning to address past short-comings of each and create an environment that supports knowledge creation, sharing and dissemination. A framework to align the knowledge creation process and e-learning web tools was proposed to encourage knowledge sharing and build a learning organization to establish sustainable competitive advantage.},
  isbn = {9781303981746},
  langid = {english},
  keywords = {Architecture,Business education,Collaboration,Communication,Competitive advantage,Core competencies,Culture,Efficiency,Employees,Explicit knowledge,Information Technology,Innovations,Knowledge acquisition,Knowledge management,Knowledge sharing,Learning,Literature reviews,Management,Online instruction,Ontology,Organizational learning,Socialization,Success,Tacit knowledge,Taxonomy,Turnover,已整理,待讀,應用,知識系統,重要},
  annotation = {abstractTranslation:  全球商業經濟已將知識確立為組織最重要的資產和競爭優勢的主要因素。對文獻和研究的回顧表明，中小型企業擁有豐富的隱性知識，這是支持組織學習和知識的知識創造和轉換過程的重要組成部分，也是獲得和維持競爭優勢的因素。中小型企業本質上非常適合非正式學習，即透過對話、對話等共享知識的協作功能。然而，這種非正式交流可能會也可能不會自然地發生在中小企業中。需要一種更結構化的方法來促進非正式知識共享，以確保組織內部發生知識創造。傳統的知識管理和電子學習嘗試將知識視為靜態物件而不是動態過程，但未能成功幫助中小企業獲得競爭優勢。越來越多的研究主張將知識管理和電子學習結合，以解決兩者過去的缺點，並創造一個支持知識創造、分享和傳播的環境。提出了一個協調知識創造過程和電子學習網路工具的框架，以鼓勵知識共享並建立學習型組織以建立可持續的競爭優勢。\\
titleTranslation: 整合知識管理和電子學習，加強知識創造與轉化，提升中小型企業的競爭優勢}
}

@article{karamariefechoBiomedicalKnowledgeGraph2021,
  title = {A {{Biomedical Knowledge Graph System}} to {{Propose Mechanistic Hypotheses}} for {{Real-World Environmental Health Observations}}: {{Cohort Study}} and {{Informatics Application}}},
  shorttitle = {A {{Biomedical Knowledge Graph System}} to {{Propose Mechanistic Hypotheses}} for {{Real-World Environmental Health Observations}}},
  author = {{Karamarie Fecho} and {Chris Bizon} and {Frederick Miller} and {Shepherd Schurman} and {Charles Schmitt} and {William Xue} and {Kenneth Morton} and {Patrick Wang} and {Alexander Tropsha}},
  date = {2021-07-20},
  journaltitle = {JMIR Medical Informatics},
  shortjournal = {JMIR Med Inform},
  volume = {9},
  number = {7},
  pages = {e26714},
  issn = {2291-9694},
  doi = {10.2196/26714},
  url = {https://medinform.jmir.org/2021/7/e26714},
  urldate = {2023-10-13},
  abstract = {Background               Knowledge graphs are a common form of knowledge representation in biomedicine and many other fields. We developed an open biomedical knowledge graph–based system termed Reasoning Over Biomedical Objects linked in Knowledge Oriented Pathways (ROBOKOP). ROBOKOP consists of both a front-end user interface and a back-end knowledge graph. The ROBOKOP user interface allows users to posit questions and explore answer subgraphs. Users can also posit questions through direct Cypher query of the underlying knowledge graph, which currently contains roughly 6 million nodes or biomedical entities and 140 million edges or predicates describing the relationship between nodes, drawn from over 30 curated data sources.                                         Objective               We aimed to apply ROBOKOP to survey data on workplace exposures and immune-mediated diseases from the Environmental Polymorphisms Registry (EPR) within the National Institute of Environmental Health Sciences.                                         Methods               We analyzed EPR survey data and identified 45 associations between workplace chemical exposures and immune-mediated diseases, as self-reported by study participants (n= 4574), with 20 associations significant at P{$<$}.05 after false discovery rate correction. We then used ROBOKOP to (1) validate the associations by determining whether plausible connections exist within the ROBOKOP knowledge graph and (2) propose biological mechanisms that might explain them and serve as hypotheses for subsequent testing. We highlight the following three exemplar associations: carbon monoxide-multiple sclerosis, ammonia-asthma, and isopropanol-allergic disease.                                         Results               ROBOKOP successfully returned answer sets for three queries that were posed in the context of the driving examples. The answer sets included potential intermediary genes, as well as supporting evidence that might explain the observed associations.                                         Conclusions               We demonstrate real-world application of ROBOKOP to generate mechanistic hypotheses for associations between workplace chemical exposures and immune-mediated diseases. We expect that ROBOKOP will find broad application across many biomedical fields and other scientific disciplines due to its generalizability, speed to discovery and generation of mechanistic hypotheses, and open nature.},
  langid = {english},
  keywords = {問答系統,知識圖譜,醫學},
  annotation = {9 citations (Crossref) [2024-03-26]\\
titleTranslation: 為現實世界環境健康觀察提出機制假設的生物醫學知識圖系統：世代研究與資訊學應用\\
abstractTranslation:  背景知識圖是生物醫學和許多其他領域知識表示的常見形式。我們開發了一個基於開放生物醫學知識圖的系統，稱為「知識導向路徑中連結的生物醫學對象推理」（ROBOKOP）。 ROBOKOP 由前端使用者介面和後端知識圖組成。 ROBOKOP 使用者介面允許使用者提出問題並探索答案子圖。使用者還可以透過直接對底層知識圖進行Cypher 查詢來提出問題，該知識圖目前包含約600 萬個節點或生物醫學實體以及1.4 億條描述節點之間關係的邊或謂詞，這些資料來自30 多個精選資料來源。目的 我們旨在應用 ROBOKOP 調查國家環境健康科學研究所環境多態性登記處 (EPR) 中有關工作場所暴露和免疫介導疾病的數據。方法我們分析了EPR 調查數據，並根據研究參與者(n= 4574) 的自我報告，確定了工作場所化學品暴露與免疫介導疾病之間的45 種關聯，其中20 種關聯在錯誤發現率校正後顯著，P{$<$}.05。然後，我們使用 ROBOKOP 來（1）透過確定 ROBOKOP 知識圖譜中是否存在合理的聯繫來驗證關聯，以及（2）提出可以解釋它們並作為後續測試的假設的生物機制。我們強調以下三個典型關聯：一氧化碳-多發性硬化症、氨氣-氣喘和異丙醇-過敏性疾病。結果 ROBOKOP 成功傳回了在駕駛範例上下文中提出的三個查詢的答案集。答案集包括潛在的中間基因，以及可能解釋觀察到的關聯的支持證據。結論 我們展示了 ROBOKOP 在現實世界中的應用，以產生工作場所化學暴露與免疫介導疾病之間關聯的機制假設。我們預計 ROBOKOP 由於其普遍性、發現和生成機械假設的速度以及開放性，將在許多生物醫學領域和其他科學學科中廣泛應用。},
  file = {C:\Users\BlackCat\Zotero\storage\P6PSA5QV\Fecho et al. - 2021 - A Biomedical Knowledge Graph System to Propose Mec.pdf}
}

@online{karpukhinDensePassageRetrieval2020,
  title = {Dense {{Passage Retrieval}} for {{Open-Domain Question Answering}}},
  author = {Karpukhin, Vladimir and Oğuz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  date = {2020-09-30},
  eprint = {2004.04906},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2004.04906},
  url = {http://arxiv.org/abs/2004.04906},
  urldate = {2024-03-22},
  abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9\%-19\% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,未整理,經典,重要},
  annotation = {titleTranslation: 用於開放域問答的密集段落檢索},
  note = {Comment: EMNLP 2020},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\WKXRXE6C\\Karpukhin et al. - 2020 - Dense Passage Retrieval for Open-Domain Question A.pdf;D\:\\Paper\\Dense Passage Retrieval for Open-Domain Question Answering.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\5JVLFVVI\\2004.html}
}

@inproceedings{katiechapmanScholarlyDataShare2023,
  title = {Scholarly {{Data Share}} 2.0: {{Granular Access}} to {{Research Data}}},
  shorttitle = {Scholarly {{Data Share}} 2.0},
  booktitle = {Practice and {{Experience}} in {{Advanced Research Computing}}},
  author = {{Katie Chapman} and {Guangchen Ruan} and {Esen Tuna} and {Alan Walsh} and {Eric A. Wernert}},
  year = {9 月 10, 2023},
  series = {{{PEARC}} '23},
  pages = {177--180},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3569951.3597585},
  url = {https://dl.acm.org/doi/10.1145/3569951.3597585},
  urldate = {2023-09-11},
  abstract = {The Scholarly Data Share (SDS) is a lightweight web interface that facilitates access to large, curated research datasets in long-term storage. The first version, SDS 1.0, facilitated sharing public datasets without access restrictions. The new version, SDS 2.0, provides controlled access to datasets at various stages in the research data life cycle. This update enables granular and customizable access control for a variety of research domains and use cases. In this paper, we discuss the features, implementation, and use cases of SDS 2.0, as well as outlining our plans for future enhancements to the service.},
  isbn = {978-1-4503-9985-2},
  langid = {english},
  keywords = {collections,datasets,metadata,research data sharing,回收,資料集},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 學術數據共享2.0：研究數據的精細訪問\\
abstractTranslation:  學術數據共享 (SDS) 是一個輕量級 Web 界面，有助於訪問長期存儲中的大型、精選研究數據集。第一個版本 SDS 1.0 促進了公共數據集的共享，而沒有訪問限制。新版本 SDS 2.0 在研究數據生命週期的各個階段提供對數據集的受控訪問。此更新為各種研究領域和用例提供了精細且可定制的訪問控制。在本文中，我們討論了 SDS 2.0 的功能、實施和用例，並概述了我們未來增強該服務的計劃。},
  note = {只是開發一個資料集分享系統。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\GLLG3867\\Chapman 等。 - 2023 - Scholarly Data Share 2.0 Granular Access to Resea.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\WY43VSVM\\Scholarly Data Share 2.0： Granular Access to Research Data.pdf}
}

@incollection{kedengStructuralLearningGraphical2005,
  title = {Structural {{Learning}} of {{Graphical Models}} and {{Its Applications}} to {{Traditional Chinese Medicine}}},
  booktitle = {Fuzzy {{Systems}} and {{Knowledge Discovery}}},
  author = {{Ke Deng} and {Delin Liu} and {Shan Gao} and {Zhi Geng}},
  editor = {{Lipo Wang} and {Yaochu Jin}},
  editora = {{David Hutchison} and {Takeo Kanade} and {Josef Kittler} and {Jon M. Kleinberg} and {Friedemann Mattern} and {John C. Mitchell} and {Moni Naor} and {Oscar Nierstrasz} and {C. Pandu Rangan} and {Bernhard Steffen} and {Madhu Sudan} and {Demetri Terzopoulos} and {Dough Tygar} and {Moshe Y. Vardi} and {Gerhard Weikum}},
  editoratype = {redactor},
  date = {2005},
  volume = {3614},
  pages = {362--367},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11540007_45},
  url = {http://link.springer.com/10.1007/11540007_45},
  urldate = {2022-09-19},
  isbn = {978-3-540-28331-7 978-3-540-31828-6}
}

@online{keDevelopmentTestingRetrieval2024,
  title = {Development and {{Testing}} of {{Retrieval Augmented Generation}} in {{Large Language Models}} -- {{A Case Study Report}}},
  author = {Ke, YuHe and Jin, Liyuan and Elangovan, Kabilan and Abdullah, Hairil Rizal and Liu, Nan and Sia, Alex Tiong Heng and Soh, Chai Rick and Tung, Joshua Yi Min and Ong, Jasmine Chiat Ling and Ting, Daniel Shu Wei},
  date = {2024-01-29},
  eprint = {2402.01733},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.01733},
  url = {http://arxiv.org/abs/2402.01733},
  urldate = {2024-03-13},
  abstract = {Purpose: Large Language Models (LLMs) hold significant promise for medical applications. Retrieval Augmented Generation (RAG) emerges as a promising approach for customizing domain knowledge in LLMs. This case study presents the development and evaluation of an LLM-RAG pipeline tailored for healthcare, focusing specifically on preoperative medicine. Methods: We developed an LLM-RAG model using 35 preoperative guidelines and tested it against human-generated responses, with a total of 1260 responses evaluated. The RAG process involved converting clinical documents into text using Python-based frameworks like LangChain and Llamaindex, and processing these texts into chunks for embedding and retrieval. Vector storage techniques and selected embedding models to optimize data retrieval, using Pinecone for vector storage with a dimensionality of 1536 and cosine similarity for loss metrics. Human-generated answers, provided by junior doctors, were used as a comparison. Results: The LLM-RAG model generated answers within an average of 15-20 seconds, significantly faster than the 10 minutes typically required by humans. Among the basic LLMs, GPT4.0 exhibited the best accuracy of 80.1\%. This accuracy was further increased to 91.4\% when the model was enhanced with RAG. Compared to the human-generated instructions, which had an accuracy of 86.3\%, the performance of the GPT4.0 RAG model demonstrated non-inferiority (p=0.610). Conclusions: In this case study, we demonstrated a LLM-RAG model for healthcare implementation. The pipeline shows the advantages of grounded knowledge, upgradability, and scalability as important aspects of healthcare LLM deployment.},
  langid = {english},
  pubstate = {preprint},
  keywords = {ChatGPT,Langchain,LLama2,LLM,PyPDF2,RAG,公開程式碼,問答系統,已整理,已讀,醫療,重要},
  annotation = {titleTranslation: 大型語言模型中檢索增強生成的開發和測試——案例研究報告\\
abstractTranslation:  目的：大型語言模型 (LLM) 為醫療應用帶來了巨大的希望。檢索增強生成（RAG）作為法學碩士定制領域知識的一種有前途的方法而出現。本案例研究介紹了專為醫療保健量身定制的 LLM-RAG 管道的開發和評估，特別關注術前醫學。方法：我們使用 35 個術前指南開發了 LLM-RAG 模型，並針對人類生成的反應對其進行了測試，總共評估了 1260 個反應。 RAG 過程涉及使用 LangChain 和 Llamaindex 等基於 Python 的框架將臨床文件轉換為文本，並將這些文本處理成區塊以進行嵌入和檢索。向量儲存技術和選擇的嵌入模型來優化資料檢索，使用 Pinecone 進行向量存儲，維數為 1536，損失指標使用餘弦相似度。由初級醫生提供的人類生成的答案被用作比較。結果：LLM-RAG 模型平均在 15-20 秒內產生答案，明顯快於人類通常需要的 10 分鐘。在基礎LLM中，GPT4.0的準確率最高，達80.1\%。當使用 RAG 增強模型時，此準確率進一步提高到 91.4\%。與準確率為 86.3\% 的人類生成指令相比，GPT4.0 RAG 模型的表現表現出非劣效性 (p=0.610)。結論：在本案例研究中，我們展示了用於醫療保健實施的 LLM-RAG 模型。該管道展示了基礎知識、可升級性和可擴展性的優勢，作為醫療保健法學碩士部署的重要方面。},
  note = {資料:\\
RAG的資料: 35份術前指引，包含圖表及圖片的PDF
\par
輸入:
\par
14份臨床病例各6個項目、4位醫師
\par
架構:
\par
Python 3.11 + Langchain's DirectoryLoader、RecursiveCharacterTextSplitter + PyPDF2\\
chunk size: 1000 units with a 100-unit overlap, based on the cl100k\_base tokenizer
\par
向量資料庫: Pinecone
\par
嵌入模型: ada-002 ( 1536維 )
\par
結果gpt4屌打llama2，甚至不用RAG都能贏},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\LACVARCM\\Ke 等。 - 2024 - Development and Testing of Retrieval Augmented Gen.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\I5UQGXAP\\2402.html}
}

@article{kei-hoicheungSemanticWebData2010,
  title = {Semantic {{Web}} for Data Harmonization in {{Chinese}} Medicine},
  author = {{Kei-Hoi Cheung} and {Huajun Chen}},
  date = {2010-01-12},
  journaltitle = {Chinese Medicine},
  shortjournal = {Chinese Medicine},
  volume = {5},
  number = {1},
  pages = {2},
  issn = {1749-8546},
  doi = {10.1186/1749-8546-5-2},
  url = {https://doi.org/10.1186/1749-8546-5-2},
  urldate = {2023-09-15},
  abstract = {Scientific studies to investigate Chinese medicine with Western medicine have been generating a large amount of data to be shared preferably under a global data standard. This article provides an overview of Semantic Web and identifies some representative Semantic Web applications in Chinese medicine. Semantic Web is proposed as a standard for representing Chinese medicine data and facilitating their integration with Western medicine data.},
  langid = {english},
  keywords = {中西合併,中醫,已整理,知識本體},
  annotation = {5 citations (Crossref) [2024-03-26]\\
abstractTranslation:  研究中西醫的科學研究已經產生了大量數據，最好在全球數據標準下分享。本文對語意網進行了概述，並指出了一些具有代表性的語意網路在中醫中的應用。語意網路被提出作為表示中醫資料並促進其與西醫資料整合的標準。\\
titleTranslation: 用於中醫資料協調的語意網},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\3ZEJ65YD\\Cheung 與 Chen - 2010 - Semantic Web for data harmonization in Chinese med.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\72CB7N8Y\\1749-8546-5-2.html}
}

@inproceedings{kendallNationalDigitalTwin2021,
  title = {National {{Digital Twin}}: {{Integration Architecture Pattern}} and {{Principles}}},
  shorttitle = {National {{Digital Twin}}},
  author = {Kendall, John},
  namea = {{Apollo-University Of Cambridge Repository} and {University Of Cambridge}},
  nameatype = {collaborator},
  date = {2021},
  publisher = {[object Object]},
  doi = {10.17863/CAM.68207},
  url = {https://www.repository.cam.ac.uk/handle/1810/321334},
  urldate = {2024-04-16},
  langid = {english},
  keywords = {未整理},
  annotation = {titleTranslation: 國家數位孿生：整合架構模式與原則},
  file = {C:\Users\BlackCat\Zotero\storage\RBL7KKP2\Kendall - 2021 - National Digital Twin Integration Architecture Pa.pdf}
}

@inproceedings{kerstinbischoffCanAllTags2008,
  title = {Can All Tags Be Used for Search?},
  booktitle = {Proceedings of the 17th {{ACM}} Conference on {{Information}} and Knowledge Management},
  author = {{Kerstin Bischoff} and {Claudiu S. Firan} and {Wolfgang Nejdl} and {Raluca Paiu}},
  year = {10 月 26, 2008},
  series = {{{CIKM}} '08},
  pages = {193--202},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1458082.1458112},
  url = {https://doi.org/10.1145/1458082.1458112},
  urldate = {2023-09-02},
  abstract = {Collaborative tagging has become an increasingly popular means for sharing and organizing Web resources, leading to a huge amount of user generated metadata. These tags represent quite a few different aspects of the resources they describe and it is not obvious whether and how these tags or subsets of them can be used for search. This paper is the first to present an in-depth study of tagging behavior for very different kinds of resources and systems - Web pages (Del.icio.us), music (Last.fm), and images (Flickr) - and compares the results with anchor text characteristics. We analyze and classify sample tags from these systems, to get an insight into what kinds of tags are used for different resources, and provide statistics on tag distributions in all three tagging environments. Since even relevant tags may not add new information to the search procedure, we also check overlap of tags with content, with metadata assigned by experts and from other sources. We discuss the potential of different kinds of tags for improving search, comparing them with user queries posted to search engines as well as through a user survey. The results are promising and provide more insight into both the use of different kinds of tags for improving search and possible extensions of tagging systems to support the creation of potentially search-relevant tags.},
  isbn = {978-1-59593-991-3},
  langid = {english},
  keywords = {collaborative tagging,query classification,tag classification,tag search,tagging system analysis and comparison},
  annotation = {163 citations (Crossref) [2024-03-26]\\
titleTranslation: 所有標籤都可以用於搜索嗎？\\
abstractTranslation:  協作標記已成為一種日益流行的共享和組織 Web 資源的方式，從而產生大量用戶生成的元數據。這些標籤代表了它們所描述的資源的相當多的不同方面，並且這些標籤或它們的子集是否以及如何用於搜索並不明顯。本文首次對不同類型的資源和系統（網頁 (Del.icio.us)、音樂 (Last.fm) 和圖像 (Flickr)）的標記行為進行了深入研究，並比較了具有錨文本特徵的結果。我們對這些系統中的示例標籤進行分析和分類，以深入了解不同資源使用哪些類型的標籤，並提供所有三種標籤環境中標籤分佈的統計數據。由於即使是相關標籤也可能不會向搜索過程添加新信息，因此我們還會使用專家和其他來源分配的元數據檢查標籤與內容的重疊。我們討論了不同類型的標籤在改進搜索方面的潛力，將它們與發佈到搜索引擎的用戶查詢進行比較以及通過用戶調查。結果令人鼓舞，並為使用不同類型的標籤來改進搜索以及標籤系統的可能擴展以支持創建潛在的搜索相關標籤提供了更多見解。},
  file = {C:\Users\BlackCat\Zotero\storage\FZ9SDMAH\Bischoff 等。 - 2008 - Can all tags be used for search.pdf}
}

@inproceedings{keshavkolluruOpenIE6IterativeGrid2020,
  title = {{{OpenIE6}}: {{Iterative Grid Labeling}} and {{Coordination Analysis}} for {{Open Information Extraction}}},
  shorttitle = {{{OpenIE6}}},
  author = {{Keshav Kolluru} and {Vaibhav Adlakha} and {Samarth Aggarwal} and {Mausam} and {Soumen Chakrabarti}},
  date = {2020-11},
  pages = {3748--3761},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.emnlp-main.306},
  url = {https://aclanthology.org/2020.emnlp-main.306},
  urldate = {2023-04-19},
  abstract = {A recent state-of-the-art neural open information extraction (OpenIE) system generates extractions iteratively, requiring repeated encoding of partial outputs. This comes at a significant computational cost. On the other hand,sequence labeling approaches for OpenIE are much faster, but worse in extraction quality. In this paper, we bridge this trade-off by presenting an iterative labeling-based system that establishes a new state of the art for OpenIE, while extracting 10x faster. This is achieved through a novel Iterative Grid Labeling (IGL) architecture, which treats OpenIE as a 2-D grid labeling task. We improve its performance further by applying coverage (soft) constraints on the grid at training time. Moreover, on observing that the best OpenIE systems falter at handling coordination structures, our OpenIE system also incorporates a new coordination analyzer built with the same IGL architecture. This IGL based coordination analyzer helps our OpenIE system handle complicated coordination structures, while also establishing a new state of the art on the task of coordination analysis, with a 12.3 pts improvement in F1 over previous analyzers. Our OpenIE system - OpenIE6 - beats the previous systems by as much as 4 pts in F1, while being much faster.},
  eventtitle = {{{EMNLP}} 2020},
  langid = {english},
  keywords = {重要},
  annotation = {26 citations (Crossref) [2024-03-26]\\
49 citations (Semantic Scholar/DOI) [2023-04-20]\\
titleTranslation: OpenIE6：用於開放信息提取的迭代網格標記和協調分析\\
abstractTranslation:  最近最先進的神經開放信息提取（OpenIE）系統迭代地生成提取，需要對部分輸出進行重複編碼。這需要大量的計算成本。另一方面，OpenIE 的序列標記方法速度更快，但提取質量較差。在本文中，我們通過提出一種基於迭代標記的系統來彌合這種權衡，該系統為 OpenIE 建立了新的技術水平，同時提取速度提高了 10 倍。這是通過新穎的迭代網格標記 (IGL) 架構實現的，該架構將 OpenIE 視為 2-D 網格標記任務。我們通過在訓練時對網格應用覆蓋（軟）約束來進一步提高其性能。此外，根據觀察，最好的 OpenIE 系統在處理協調結構方面表現不佳，我們的 OpenIE 系統還集成了一個使用相同 IGL 架構構建的新協調分析器。這種基於 IGL 的協調分析器幫助我們的 OpenIE 系統處理複雜的協調結構，同時還在協調分析任務上建立了新的技術水平，F1 比以前的分析器提高了 12.3 點。我們的 OpenIE 系統 - OpenIE6 - 在 F1 中比以前的系統高出 4 分，同時速度也快得多。},
  file = {C:\Users\BlackCat\Zotero\storage\7BMCUS8M\Kolluru et al. - 2020 - OpenIE6 Iterative Grid Labeling and Coordination Analysis for Open Information Extraction.pdf}
}

@article{KeTingYuting-yukoJiYuZhengZhuangDeShuXingZuoWeiZhongYiZhengZhuangMiaoShuFaDeYanJiu2013,
  title = {基於症狀的屬性作為中醫症狀描述法的研究},
  author = {{柯廷諭(Ting-Yu Ko)} and {劉建宏(Chien-Hung Liu)} and {陳仁義(Zen-Yi Chen)} and {葉家舟(Chia-Chou Yeh)} and {林迺衛(Nai-Wei Lin)} and {葉明憲(Ming-Hsien Yeh)}},
  date = {2013},
  journaltitle = {台灣中醫臨床醫學雜誌},
  shortjournal = {台灣中醫臨床醫學雜誌},
  volume = {19},
  number = {1},
  pages = {39--48},
  publisher = {台灣中醫臨床醫學會},
  issn = {1817-6720},
  doi = {10.6968/tjccm},
  url = {https://www.AiritiLibrary.com/Publication/Index/18176720-201312-201502030023-201502030023-39-48},
  abstract = {症狀是中醫診病中最基本的資訊。經由標準化後的中醫症狀更能有效地整合中醫對症狀的定義、問診的方便性、辨證的準確性、電子病歷的記錄和各種統計分析的可能。因此，我們在本論文中提出了「中醫症狀及症狀屬性記錄」的概念，將《中醫證候學》書中高達16834筆中醫症狀敘述句，以下列方式呈現：1.以統一的格式表達所有的症狀單元。2.以各種屬性來囊括所有的症狀描述。3.將部分描述做適當的量化。4.意義相同的描述句必須統一為相同的標準化症狀。本文提出一種標準化症狀的格式。這種格式定義了一組統一的症狀屬性，來描述所有的症狀。這種症狀記錄方式可利醫師臨床對疾病症狀的收集，有利於電子病歷之資料庫儲存以及演算法分析，同時有利於統合各個辨證系統及各醫家對同一疾病的見解。最後我們再以自然語言分析的角度著手，將症狀詞彙進行斷詞、詞性標記、分析文法結構樹等處理過程，進行分析、記錄各種症狀詞彙的描述結構，並且應用自然語言處理技術來自動地將輸入的症狀對應到標準化症狀的方法。每一個字都有一組可能的詞性，每一個症狀都有一個文法來定義它的每一個字的詞性。這個方法可以顯著地提昇使用完整字串比對的方法，同時有利於大量地標準化中醫古籍成為現代可用之資訊。},
  langid = {zh\_CN},
  keywords = {Natural Language Processing,No DOI found,Symptom Attribute,Symptom Standardization,Traditional Chinese Medicine,中醫,實驗室,標準化,病歷分析,自然語言處理},
  annotation = {abstractTranslation:  中醫症狀是中醫診病中診的資訊。 全球標準化後的中醫症狀能夠更有效地整合中醫對症狀的定義、問診的方便性、辨證的準確性、電子病歷的記錄以及各種統計分析的可能。 ，我們在本論文中提出了「中醫症狀及症狀屬性記錄」的概念，將《中醫證候學》收錄多達16834筆中醫症狀敘述句，以下列方式呈現：1.以統一的格式表達所有的症狀單元。2.以各種屬性來括住所有的症狀描述。3.將部分描述做適當的量化。4.意義相同的描述句必須統一為相同的標準化症狀。本文提出了一種標準化症狀的格式。格式定義了一組統一的症狀屬性，來所有的症狀。這種症狀記錄方式可利醫師臨床對疾病症狀的收集，有利於電子病歷之數據庫存儲以及演算分析，同時有利於系統配合各個鑑別證據最後我們再以自然語言分析的角度著手，將症狀表進行斷詞、詞性標記、分析文法結構樹等處理過程，進行分析、記錄各種症狀表的描述結構，並且應用自然語言症狀處理技術來自動注射輸入的症狀對應於標準化症狀的方法。每一個字都有一組可能的詞性，每一個字都有一個文法來定義它的每一個字的詞性。這個方法可以顯著提升使用完整字串比對的方法，同時有利於大規模標準化中醫古籍成為現代可用之信息。\\
titleTranslation: 基於症狀屬性作為中醫症狀描述法的研究},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\2C9UNDZ7\\柯廷諭(Ting-Yu Ko) 等。 - 2013 - 基於症狀的屬性作為中醫症狀描述法的研究.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\45S6GNBW\\中醫症狀的結構化分析與標準化系統設計.pdf}
}

@online{keyuwangEmbeddingbasedApproachInconsistencytolerant2023,
  title = {An {{Embedding-based Approach}} to {{Inconsistency-tolerant Reasoning}} with {{Inconsistent Ontologies}}},
  author = {{Keyu Wang} and {Site Li} and {Jiaye Li} and {Guilin Qi} and {Qiu Ji}},
  date = {2023-04-04},
  eprint = {2304.01664},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.01664},
  url = {http://arxiv.org/abs/2304.01664},
  urldate = {2023-04-19},
  abstract = {Inconsistency handling is an important issue in knowledge management. Especially in ontology engineering, logical inconsistencies may occur during ontology construction. A natural way to reason with an inconsistent ontology is to utilize the maximal consistent subsets of the ontology. However, previous studies on selecting maximum consistent subsets have rarely considered the semantics of the axioms, which may result in irrational inference. In this paper, we propose a novel approach to reasoning with inconsistent ontologies in description logics based on the embeddings of axioms. We first give a method for turning axioms into distributed semantic vectors to compute the semantic connections between the axioms. We then define an embedding-based method for selecting the maximum consistent subsets and use it to define an inconsistency-tolerant inference relation. We show the rationality of our inference relation by considering some logical properties. Finally, we conduct experiments on several ontologies to evaluate the reasoning power of our inference relation. The experimental results show that our embedding-based method can outperform existing inconsistency-tolerant reasoning methods based on maximal consistent subsets.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,嵌入,已整理,未發表,知識本體,知識衝突},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-04-20]\\
0 citations (Semantic Scholar/DOI) [2023-04-20]\\
titleTranslation: 一種基於嵌入的方法來處理不一致本體的不一致推理\\
abstractTranslation:  不一致處理是知識管理中的一個重要問題。尤其是在本體工程中，本體構建過程中可能會出現邏輯不一致的情況。使用不一致本體進行推理的自然方法是利用本體的最大一致子集。然而，先前關於選擇最大一致子集的研究很少考慮公理的語義，這可能會導致不合理的推理。在本文中，我們提出了一種基於公理嵌入的描述邏輯中不一致本體的推理新方法。我們首先給出一種將公理轉化為分佈式語義向量的方法，以計算公理之間的語義聯繫。然後，我們定義一種基於嵌入的方法來選擇最大一致子集，並用它來定義不一致性容忍的推理關係。我們通過考慮一些邏輯屬性來證明我們的推理關係的合理性。最後，我們在幾個本體上進行實驗來評估我們的推理關係的推理能力。實驗結果表明，我們的基於嵌入的方法可以優於現有的基於最大一致子集的不一致性容忍推理方法。},
  note = {使用嵌入
\par
Comment: 18 pages, 1 figure
\par
Comment: 18 pages, 1 figure},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\QRXD3RXF\\Wang 等。 - 2023 - An Embedding-based Approach to Inconsistency-toler.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\6FI4J87C\\2304.html}
}

@article{khalidmahmoodmalikAutomatedDomainspecificHealthcare2020,
  title = {Automated Domain-Specific Healthcare Knowledge Graph Curation Framework: {{Subarachnoid}} Hemorrhage as Phenotype},
  shorttitle = {Automated Domain-Specific Healthcare Knowledge Graph Curation Framework},
  author = {{Khalid Mahmood Malik} and {Madan Krishnamurthy} and {Mazen Alobaidi} and {Maqbool Hussain} and {Fakhare Alam} and {Ghaus Malik}},
  date = {2020-05-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {145},
  pages = {113120},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2019.113120},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417419308371},
  urldate = {2023-09-12},
  abstract = {To derive meaningful insights from voluminous healthcare data, it is essential to convert it into machine understandable knowledge. Currently, machine understandable domain specific healthcare knowledge curation framework does not exist for complex neurological diseases such as subarachnoid hemorrhage stroke. We envisage futuristic clinical decision support systems and tools backed with such knowledge will aide in complex neurological disease prognosis, diagnosis, and treatment. Existing knowledge graphs (KGs) only contain concepts and relationships between them and offer this knowledge to information extraction and knowledge management applications. However, the proposed domain-specific automated KG curation framework enables extraction of concepts, relationships, individual and cohort graphs, and predictive knowledge. By employing ontology-based information extraction, ensemble learning and word embedding based on skip-gram techniques on structured and unstructured data from electronic health records of 1025 patients with an intracranial aneurysm, this paper proposes a novel fully automated framework to curate knowledge graph, consisting of concepts, different hierarchical and non-hierarchical relationships, and predictive rules for prediction of subarachnoid hemorrhage. The evaluation shows that proposed framework achieves 78\% precision and 71\% recall respectively, for concept extraction from clinical text. Taxonomic relationships evaluation had precision and recall of 68\%, and 95\%, respectively. Evaluation of knowledge to predict unruptured status using validation dataset shows accuracy, precision, recall, of 73\%, 76\%, and 90\% respectively.},
  langid = {english},
  keywords = {Association Rules,Electronic Health Records,Ensemble Learning,Intracranial Aneurysm,Knowledge Graph,Ontology,Subarachnoid Hemorrhage Stroke},
  annotation = {40 citations (Crossref) [2024-03-26]\\
abstractTranslation:  為了從大量的醫療數據中獲得有意義的見解，必須將其轉換為機器可理解的知識。目前，對於蛛網膜下腔出血性中風等複雜的神經系統疾病，尚不存在機器可理解的特定領域醫療保健知識管理框架。我們設想以這些知識為後盾的未來臨床決策支持系統和工具將有助於復雜的神經系統疾病的預後、診斷和治療。現有的知識圖（KG）僅包含概念和它們之間的關係，並將這些知識提供給信息提取和知識管理應用程序。然而，所提出的特定領域的自動化知識圖譜管理框架可以提取概念、關係、個體和群組圖以及預測知識。通過對1025 名顱內動脈瘤患者的電子健康記錄中的結構化和非結構化數據採用基於本體的信息提取、集成學習和基於Skip-Gram 技術的詞嵌入，本文提出了一種新穎的全自動框架來管理知識圖譜，包括蛛網膜下腔出血預測的概念、不同層次和非層次關係以及預測規則。評估表明，對於從臨床文本中提取概念，所提出的框架分別實現了 78\% 的精確度和 71\% 的召回率。分類關係評估的精確度和召回率分別為 68\% 和 95\%。使用驗證數據集預測未破裂狀態的知識評估顯示準確度、精確度、召回率分別為 73\%、76\% 和 90\%。\\
titleTranslation: 自動化特定領域醫療保健知識圖管理框架：蛛網膜下腔出血作為表型},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\HN7G5693\\Malik 等。 - 2020 - Automated domain-specific healthcare knowledge gra.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\N48G2T6K\\S0957417419308371.html}
}

@article{khanKnowledgeGraphsQuerying2023,
  title = {Knowledge {{Graphs Querying}}},
  author = {Khan, A.},
  date = {2023},
  journaltitle = {SIGMOD Record},
  volume = {52},
  number = {2},
  pages = {18--29},
  issn = {0163-5808},
  doi = {10.1145/3615952.3615956},
  abstract = {Knowledge graphs (KGs) such as DBpedia, Freebase, YAGO, Wikidata, and NELL were constructed to store large-scale, real-world facts as (subject, predicate, object) triples - that can also be modeled as a graph, where a node (a subject or an object) represents an entity with attributes, and a directed edge (a predicate) is a relationship between two entities. Querying KGs is critical in web search, question answering (QA), semantic search, personal assistants, fact checking, and recommendation. While significant progress has been made on KG construction and curation, thanks to deep learning recently we have seen a surge of research on KG querying and QA. The objectives of our survey are two-fold. First, research on KG querying has been conducted by several communities, such as databases, data mining, semantic web, machine learning, information retrieval, and natural language processing (NLP), with different focus and terminologies; and also in diverse topics ranging from graph databases, query languages, join algorithms, graph patterns matching, to more sophisticated KG embedding and natural language questions (NLQs). We aim at uniting different interdisciplinary topics and concepts that have been developed for KG querying. Second, many recent advances on KG and query embedding, multimodal KG, and KG-QA come from deep learning, IR, NLP, and computer vision domains. We identify important challenges of KG querying that received less attention by graph databases, and by the DB community in general, e.g., incomplete KG, semantic matching, multimodal data, and NLQs. We conclude by discussing interesting opportunities for the data management community, for instance, KG as a unified data model and vector-based query processing. © 2023 Copyright is held by the owner/author(s).},
  langid = {english},
  keywords = {未整理,知識圖譜},
  annotation = {1 citations (Crossref) [2024-03-26]},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\6Q47URGC\\Khan - 2023 - Knowledge Graphs Querying.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\7YKCCV8J\\Khan - 2023 - Knowledge Graphs Querying.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\RJWRVKEK\\display.html}
}

@online{khotQASCDatasetQuestion2020,
  title = {{{QASC}}: {{A Dataset}} for {{Question Answering}} via {{Sentence Composition}}},
  shorttitle = {{{QASC}}},
  author = {Khot, Tushar and Clark, Peter and Guerquin, Michal and Jansen, Peter and Sabharwal, Ashish},
  date = {2020-02-04},
  eprint = {1910.11473},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1910.11473},
  url = {http://arxiv.org/abs/1910.11473},
  urldate = {2024-03-15},
  abstract = {Composing knowledge from multiple pieces of texts is a key challenge in multi-hop question answering. We present a multi-hop reasoning dataset, Question Answering via Sentence Composition(QASC), that requires retrieving facts from a large corpus and composing them to answer a multiple-choice question. QASC is the first dataset to offer two desirable properties: (a) the facts to be composed are annotated in a large corpus, and (b) the decomposition into these facts is not evident from the question itself. The latter makes retrieval challenging as the system must introduce new concepts or relations in order to discover potential decompositions. Further, the reasoning model must then learn to identify valid compositions of these retrieved facts using common-sense reasoning. To help address these challenges, we provide annotation for supporting facts as well as their composition. Guided by these annotations, we present a two-step approach to mitigate the retrieval challenges. We use other multiple-choice datasets as additional training data to strengthen the reasoning model. Our proposed approach improves over current state-of-the-art language models by 11\% (absolute). The reasoning and retrieval problems, however, remain unsolved as this model still lags by 20\% behind human performance.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,問答系統,資料集},
  annotation = {titleTranslation: QASC：透過句子構成進行問答的資料集\\
abstractTranslation:  從多段文本中組合知識是多跳問答中的關鍵挑戰。我們提出了一個多跳推理資料集，即透過句子組合進行問答（QASC），它需要從大型語料庫中檢索事實並組合它們來回答多項選擇題。 QASC 是第一個提供兩個理想屬性的資料集：(a) 要組成的事實在大型語料庫中進行註釋，(b) 從問題本身來看，這些事實的分解並不明顯。後者使得檢索具有挑戰性，因為系統必須引入新的概念或關係才能發現潛在的分解。此外，推理模型必須學會使用常識推理來識別這些檢索到的事實的有效組成。為了幫助應對這些挑戰，我們提供了支持事實及其組成的註釋。在這些註釋的指導下，我們提出了一種兩步驟方法來減輕檢索挑戰。我們使用其他多項選擇資料集作為額外的訓練資料來加強推理模型。我們提出的方法比目前最先進的語言模型提高了 11\%（絕對）。然而，推理和檢索問題仍未解決，因為模型仍落後於人類 20\%。},
  note = {Comment: AAAI-20 Camera Ready Version},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\Y52FW2EI\\Khot 等。 - 2020 - QASC A Dataset for Question Answering via Sentenc.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\XRGKI88V\\1910.html}
}

@article{klaasandriesdegraafOntologybasedSoftwareArchitecture,
  title = {Ontology-Based {{Software Architecture Documentation}}},
  author = {{Klaas Andries de Graaf}},
  doi = {10.1109/WICSA-ECSA.212.20},
  langid = {english},
  annotation = {21 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於本體的軟體架構文檔},
  file = {C:\Users\BlackCat\Zotero\storage\F9JQZTFM\de Graaf - Ontology-based Software Architecture Documentation.pdf}
}

@inproceedings{knoblockMixedinitiativeMultisourceInformation2001,
  title = {Mixed-Initiative, Multi-Source Information Assistants},
  booktitle = {Proceedings of the 10th International Conference on {{World Wide Web}}},
  author = {Knoblock, Craig A. and Minton, Steven and Ambite, José Luis and Muslea, Maria and Oh, Jean and Frank, Martin},
  year = {4 月 1, 2001},
  series = {{{WWW}} '01},
  pages = {697--707},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/371920.372185},
  url = {https://dl.acm.org/doi/10.1145/371920.372185},
  urldate = {2024-04-10},
  isbn = {978-1-58113-348-6},
  langid = {english},
  keywords = {未整理},
  annotation = {titleTranslation: 混合主動、多源資訊助理},
  file = {C:\Users\BlackCat\Zotero\storage\JJUK9U57\Knoblock 等。 - 2001 - Mixed-initiative, multi-source information assista.pdf}
}

@article{kocamanSparkNLPNatural2021,
  title = {Spark {{NLP}}: {{Natural Language Understanding}} at {{Scale}}},
  shorttitle = {Spark {{NLP}}},
  author = {Kocaman, Veysel and Talby, David},
  date = {2021-05-01},
  journaltitle = {Software Impacts},
  shortjournal = {Software Impacts},
  volume = {8},
  pages = {100058},
  issn = {2665-9638},
  doi = {10.1016/j.simpa.2021.100058},
  url = {https://www.sciencedirect.com/science/article/pii/S2665963821000063},
  urldate = {2024-03-22},
  abstract = {Spark NLP is a Natural Language Processing (NLP) library built on top of Apache Spark ML. It provides simple, performant \& accurate NLP annotations for machine learning pipelines that can scale easily in a distributed environment. Spark NLP comes with 1100+ pretrained pipelines and models in more than 192+ languages. It supports nearly all the NLP tasks and modules that can be used seamlessly in a cluster. Downloaded more than 2.7 million times and experiencing 9x growth since January 2020, Spark NLP is used by 54\% of healthcare organizations as the world’s most widely used NLP library in the enterprise.},
  langid = {english},
  keywords = {Cluster,Deep learning,Natural language processing,NLP,Spark,Tensorflow,未整理},
  annotation = {33 citations (Crossref) [2024-03-26]\\
titleTranslation: Spark NLP：大規模自然語言理解},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\7AFHHIM7\\Kocaman and Talby - 2021 - Spark NLP Natural Language Understanding at Scale.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\HA8VHN64\\S2665963821000063.html}
}

@article{koconChatGPTJackAll2023,
  title = {{{ChatGPT}}: {{Jack}} of All Trades, Master of None},
  shorttitle = {{{ChatGPT}}},
  author = {Kocoń, Jan and Cichecki, Igor and Kaszyca, Oliwier and Kochanek, Mateusz and Szydło, Dominika and Baran, Joanna and Bielaniewicz, Julita and Gruza, Marcin and Janz, Arkadiusz and Kanclerz, Kamil and Kocoń, Anna and Koptyra, Bartłomiej and Mieleszczenko-Kowszewicz, Wiktoria and Miłkowski, Piotr and Oleksy, Marcin and Piasecki, Maciej and Radliński, Łukasz and Wojtasik, Konrad and Woźniak, Stanisław and Kazienko, Przemysław},
  date = {2023-11-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {99},
  pages = {101861},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2023.101861},
  url = {https://www.sciencedirect.com/science/article/pii/S156625352300177X},
  urldate = {2024-02-24},
  abstract = {OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. The first contact with the chatbot reveals its ability to provide detailed and precise answers in various areas. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT’s capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25\% for zero-shot and few-shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed that the more difficult the task (lower SOTA performance), the higher the ChatGPT loss. It especially refers to pragmatic NLP problems like emotion recognition. We also tested the ability to personalize ChatGPT responses for selected subjective tasks via Random Contextual Few-Shot Personalization, and we obtained significantly better user-based predictions. Additional qualitative analysis revealed a ChatGPT bias, most likely due to the rules imposed on human trainers by OpenAI. Our results provide the basis for a fundamental discussion of whether the high quality of recent predictive NLP models can indicate a tool’s usefulness to society and how the learning and validation procedures for such systems should be established.},
  langid = {english},
  keywords = {ChatGPT,Emotion recognition,GPT-4,Humor detection,Large language model,Model personalization,Natural language inference (NLI),Natural language processing (NLP),Offensive content,Pragmatic NLP tasks,Prompting,Question answering (QA),Semantic NLP tasks,Sentiment analysis,SOTA analysis,Stance detection,Subjective NLP tasks,Text classification,Word sense disambiguation (WSD),已整理,重要},
  annotation = {55 citations (Crossref) [2024-03-26]\\
titleTranslation: ChatGPT：萬事通，一無是處},
  note = {提到ChatGPT在許多任務上還沒辦法優於現有的SOTA方法。
\par
可能可以用於論文中},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\T393E7E9\\Kocoń 等。 - 2023 - ChatGPT Jack of all trades, master of none.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\JHGC2FD6\\S156625352300177X.html}
}

@article{KongLingWeiJiYuDuiKangXunLianDeZhongWenDianZiBingLiMingMingShiTiShiBie2022,
  title = {基于对抗训练的中文电子病历命名实体识别},
  author = {{孔令巍} and {朱艳辉} and {张旭} and {欧阳康} and {黄雅淋} and {金书川} and {沈加锐}},
  date = {2022},
  journaltitle = {湖南工业大学学报},
  shortjournal = {Journal of Hunan University of Technology},
  volume = {36},
  number = {3},
  pages = {36--43},
  doi = {10.3969/j.issn.1673-9833.2022.03.006},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhB6emd4eXhiMjAyMjAzMDA3GghhNGIzeWxxbA%3D%3D},
  urldate = {2022-08-14},
  abstract = {为提高传统命名实体识别模型在中文电子病历上的准确性,提出一种在基线模型B E RT-BiLSTM-CRF中加入对抗训练的方法,该方法在词嵌入层添加扰动因子从而生成对抗样本,并利用对抗样本进行迭代训练,从而优化模型参数.CCKS2021评测数据集实验结果表明,加入FGM和PGD两个对抗训练模型后,其精准率、召回率以及F1值相比于基线模型均有所提升.并且通过对比实验,验证了加入对抗训练能够提高模型的预测能力和鲁棒性.},
  langid = {zh\_CN},
  keywords = {BERT,BiLSTM,CRF,FGM,Journal of Hunan University of Technology,PGD,中文电子病历,命名实体识别,孔令巍,对抗训练,张旭,朱艳辉,欧阳康,沈加锐,湖南工业大学学报,金书川,黄雅淋},
  annotation = {湖南工业大学 计算机学院,湖南 株洲 412007;湖南省智能信息感知及处理技术重点实验室,湖南 株洲 412007湖南工业大学 计算机学院,湖南 株洲 412007湖南省智能信息感知及处理技术重点实验室,湖南 株洲 412007\\
湖南省教育厅科学研究基金资助重点项目 湖南省自然科学基金资助项目\\
2022-05-18 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 基於對抗訓練的中文電子病歷命名實體識別\\
abstractTranslation:  為提高傳統命名實體識別模型在中文電子病歷上的準確性，提出了一種在核心模型B E RT-BiLSTM-CRF中加入對抗訓練的方法，該方法在詞嵌入層添加擾動因子從而生成對抗樣本，並利用對抗樣本進行迭代訓練，從而優化模型參數。CCKS2021體育數據集實驗結果表明，加入FGM和PGD兩個對抗訓練模型後，其精准率、認知率以及F1值相比於基線模型均有所提升。並且通過對比實驗，驗證了加入對抗訓練能夠提高模型的預測能力和魯棒性。},
  file = {C:\Users\BlackCat\Zotero\storage\6N3EN678\孔 等。 - 2022 - 基于对抗训练的中文电子病历命名实体识别.pdf}
}

@inproceedings{krisztianbalogPersonalKnowledgeGraphs2019,
  title = {Personal {{Knowledge Graphs}}: {{A Research Agenda}}},
  shorttitle = {Personal {{Knowledge Graphs}}},
  booktitle = {Proceedings of the 2019 {{ACM SIGIR International Conference}} on {{Theory}} of {{Information Retrieval}}},
  author = {{Krisztian Balog} and {Tom Kenter}},
  year = {9 月 26, 2019},
  series = {{{ICTIR}} '19},
  pages = {217--220},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3341981.3344241},
  url = {https://dl.acm.org/doi/10.1145/3341981.3344241},
  urldate = {2023-09-16},
  abstract = {Knowledge graphs, organizing structured information about entities, and their attributes and relationships, are ubiquitous today. Entities, in this context, are usually taken to be anyone or anything considered to be globally important. This, however, rules out many entities people interact with on a daily basis. In this position paper, we present the concept of personal knowledge graphs: resources of structured information about entities personally related to its user, including the ones that might not be globally important. We discuss key aspects that separate them for general knowledge graphs, identify the main challenges involved in constructing and using them, and define a research agenda.},
  isbn = {978-1-4503-6881-0},
  langid = {english},
  keywords = {/unread,knowledge representation,personal information management,personal knowledge graphs,基礎理論,已整理,微讀,知識本體},
  annotation = {37 citations (Crossref) [2024-03-26]\\
titleTranslation: 個人知識圖：研究議程\\
abstractTranslation:  知識圖譜組織了有關實體及其屬性和關係的結構化訊息，如今已經無所不在。在這種情況下，實體通常被認為是被認為具有全球重要性的任何人或事物。然而，這排除了人們日常互動的許多實體。在這篇立場文件中，我們提出了個人知識圖的概念：與使用者個人相關的實體的結構化資訊資源，包括那些可能不具有全球重要性的資訊。我們討論了將它們區分為一般知識圖的關鍵方面，確定了建構和使用它們所涉及的主要挑戰，並定義了研究議程。},
  note = {定義個人知識圖譜PKG及其相關應用，以及與一般KG的區別。
\par
但是就這樣，沒有進一步的實現方式。},
  file = {C:\Users\BlackCat\Zotero\storage\WEEJY8CJ\Personal Knowledge Graphs A Research Agenda}
}

@inproceedings{kumarAutomationQuestionAnswerGeneration2021,
  title = {Automation of {{Question-Answer Generation}}},
  booktitle = {2021 {{Fourth International Conference}} on {{Computational Intelligence}} and {{Communication Technologies}} ({{CCICT}})},
  author = {Kumar, Alok and Singh, Deepika and Kharadi, Aditi and Kumari, Mala},
  date = {2021-07},
  pages = {175--180},
  doi = {10.1109/CCICT53244.2021.00043},
  url = {https://ieeexplore.ieee.org/document/9514975},
  urldate = {2024-04-16},
  abstract = {In this paper, an automatic system is proposed to generate different kinds of questions and answers from the input text. Question answer generation systems have been an interesting field of research for over decades. From generating questions for educational purposes to preparing answers to questions that could be asked in a legal proceeding, the purpose of question answer generation(QAG) systems is to reduce the tedious task of going through large texts. In our system, question-answer pairs from a given input text are generated using linguistic and statistical knowledge of text. Initially, those sentences are identified from the input text on which questions can be framed and in further steps, identified sentences are ranked in an order of importance. Built specifically for assessment purpose, the system generates multiple choices, fill-ups, true-false, binary and wh type questions based on high ranked sentences in the last step. In performance evaluation of the proposed system, it is found that the system is performing well and it is close to state of art research in the standard linguistic approach.},
  eventtitle = {2021 {{Fourth International Conference}} on {{Computational Intelligence}} and {{Communication Technologies}} ({{CCICT}})},
  langid = {english},
  keywords = {Automation,Communications technology,Dependency Parsing,Law,Linguistics,Named Entity Recognition(NER),Natural Language Generation(NLG),NLTK(Natural Language Toolkit),Performance evaluation,POS tagging( parts of speech),Speech recognition,Tagging},
  annotation = {titleTranslation: 問答產生自動化\\
abstractTranslation:  在本文中，提出了一種自動系統，可以根據輸入文字產生不同類型的問題和答案。幾十年來，問答產生系統一直是個有趣的研究領域。從教育目的生成問題到準備法律程序中可能提出的問題的答案，問答生成（QAG）系統的目的是減少瀏覽大量文本的繁瑣任務。在我們的系統中，給定輸入文字的問答對是使用文字的語言和統計知識產生的。最初，從可以在其上建立問題的輸入文字中識別出這些句子，並在進一步的步驟中，按照重要性順序對識別出的句子進行排名。該系統專為評估目的而構建，根據最後一步的高排名句子生成多項選擇、填充、真假、二元和wh類型問題。在對所提出系統的性能評估中，發現該系統表現良好，並且接近標準語言方法中的最新研究水平。},
  file = {C:\Users\BlackCat\Zotero\storage\L8FCV25Y\Kumar 等。 - 2021 - Automation of Question-Answer Generation.pdf}
}

@inproceedings{kurdiDevelopmentEvaluationWeb2014,
  title = {Development and {{Evaluation}} of a {{Web Based Question Answering System}} for {{Arabic Language}}},
  author = {Kurdi, Heba and Alkhaider, Sara and Alfaifi, Nada},
  date = {2014-02-21},
  volume = {4},
  pages = {187--202},
  doi = {10.5121/csit.2014.4216},
  abstract = {Question Answering (QA) systems are gaining great i mportance due to the increasing amount of web content and the high demand for digital informa tion that regular information retrieval techniques cannot satisfy. A question answering sys tem enables users to have a natural language dialog with the machine, which is required for virtually all emerging online service systems on the Internet. The need for such systems is higher in the context of the Arabic language. This is because of the scarcity of Arabic QA systems, which can be attributed to the great challenges they present to the research commu nity, including the particularities of Arabic, such as short vowels, absence of capital letters, c omplex morphology, etc. In this paper, we report the design and implementation of an Arabic w eb-based question answering system, which we called “JAWEB”, the Arabic word for the verb “an swer”. Unlike all Arabic question- answering systems, JAWEB is a web-based application , so it can be accessed at any time and from anywhere. Evaluating JAWEB showed when compare d to ask.com, the well-established web-based QA system, JAWEB provided 15-20\% higher r ecall. These promising results give clear evidence that JAWEB has great potential as a QA platform and is much needed by Arabic- speaking Internet users across the world.},
  eventtitle = {Computer {{Science}} \& {{Information Technology}}},
  isbn = {978-1-921987-27-4},
  langid = {english},
  keywords = {問答系統,未整理},
  annotation = {titleTranslation: 基於網路的阿拉伯語問答系統的發展和評估\\
abstractTranslation:  由於網路內容量的不斷增加以及常規資訊檢索技術無法滿足的對數位資訊的高需求，問答（QA）系統變得越來越重要。問答系統使用戶能夠與機器進行自然語言對話，這幾乎是網路上所有新興線上服務系統所必需的。在阿拉伯語環境中，對此類系統的需求更高。這是因為阿拉伯語問答系統的稀缺性，這可以歸因於它們給研究界帶來了巨大的挑戰，包括阿拉伯語的特殊性，如短元音、缺乏大寫字母、複雜的形態等。在本文中，我們報告了一個基於阿拉伯語網絡的問答系統的設計和實現，我們稱之為“JAWEB”，阿拉伯語中動詞“an swer”的意思。與所有阿拉伯語問答系統不同，JAWEB 是一個基於網路的應用程序，因此可以隨時隨地存取。對 JAWEB 的評估表明，與完善的基於網路的 QA 系統 Ask.com 相比，JAWEB 的召回率提高了 15-20\%。這些令人鼓舞的結果清楚地表明，JAWEB 作為 QA 平台具有巨大潛力，並且是世界各地阿拉伯語網路使用者的迫切需求。},
  file = {C:\Users\BlackCat\Zotero\storage\VI4IV4B9\Kurdi 等。 - 2014 - Development and Evaluation of a Web Based Question.pdf}
}

@inproceedings{kurtmillerComprehensionContextualSemantics2022,
  title = {Comprehension of {{Contextual Semantics Across Clinical Healthcare Domains}}},
  booktitle = {2022 {{IEEE}} 10th {{International Conference}} on {{Healthcare Informatics}} ({{ICHI}})},
  author = {{Kurt Miller}},
  date = {2022-06},
  pages = {479--480},
  issn = {2575-2634},
  doi = {10.1109/ICHI54592.2022.00077},
  url = {https://ieeexplore.ieee.org/document/9874473},
  urldate = {2023-10-12},
  abstract = {The widespread lack of adoption of clinical notetaking standards has rendered information retrieval from Electronic Health Records (EHRs) especially challenging using traditional Natural Language Processing (NLP) techniques. Clinical note authors too commonly adopt their own note-taking structures and styles, limiting the applicability of rule-based and statistical models. While the context of any given sentence within a note carries important implied information, context is notoriously difficult for a language model to infer. However, recent advances in deep learning NLP methods such as pre-training on domain-specific corpora, novel embedding structures, and transformer architectures have enabled an awareness of context not previously attainable. In this work, I study the application of these evidenced NLP approaches to a gold standard annotated corpus of primary care notes of multiple Mayo Clinic EHR systems. The strongly labelled data will be supplemented with large volumes of weakly labelled data curated using distant supervision. The combined dataset will be used to train and evaluate context classification and section boundary detection models that classify the current context of a sentence given adjacent text segments. Once validated against primary care corpora, transfer learning methods will enable access to shared knowledge across more specific clinical domains, enabling generalizability across clinical domains and a degree of transparency into the shared aspects of the integrated model.},
  eventtitle = {2022 {{IEEE}} 10th {{International Conference}} on {{Healthcare Informatics}} ({{ICHI}})},
  keywords = {機器學習,病歷分析,醫學},
  annotation = {0 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\7H57DTR5\9874473.html}
}

@article{kwiatkowskiNaturalQuestionsBenchmark2019,
  title = {Natural {{Questions}}: {{A Benchmark}} for {{Question Answering Research}}},
  shorttitle = {Natural {{Questions}}},
  author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
  editor = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
  date = {2019},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {452--466},
  publisher = {MIT Press},
  location = {Cambridge, MA},
  doi = {10.1162/tacl_a_00276},
  url = {https://aclanthology.org/Q19-1026},
  urldate = {2024-03-30},
  abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
  langid = {english},
  keywords = {問答系統,未整理},
  annotation = {titleTranslation: 自然問題：問答研究的基準\\
abstractTranslation:  我們提出了自然問題語料庫，一個問答資料集。問題包括向 Google 搜尋引擎發出的真實匿名聚合查詢。向註釋者提供一個問題以及來自前5 個搜尋結果的維基百科頁面，並註釋一個長答案（通常是一個段落）和一個短答案（一個或多個實體）（如果頁面上存在），或者如果沒有則標記為null存在長/短答案。公開版本包含 307,373 個帶有單一註釋的訓練範例； 7,830 個範例，附有 5 種開發資料註釋；以及另外 7,842 個帶有 5 路註釋的範例作為測試資料。我們提出了驗證數據品質的實驗。我們也描述了 302 個範例的 25 路註釋的分析，深入了解註釋任務上的人類變異性。我們引入穩健的指標來評估問答系統；展示這些指標的人類上限；並使用從相關文獻中提取的競爭方法建立基線結果。},
  file = {C:\Users\BlackCat\Zotero\storage\FV828DL6\Kwiatkowski 等。 - 2019 - Natural Questions A Benchmark for Question Answer.pdf}
}

@article{laalKnowledgeManagementHigher2011,
  title = {Knowledge Management in Higher Education},
  author = {Laal, Marjan},
  date = {2011-01-01},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  series = {World {{Conference}} on {{Information Technology}}},
  volume = {3},
  pages = {544--549},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2010.12.090},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050910004655},
  urldate = {2024-01-17},
  abstract = {This review article outlines the basic concepts of knowledge management (KM) in higher education (HE) institutes, and gives a summing up of previous scientific works to ensure providing an effective and efficient understanding of it for an ever-changing environment. KM is a systematic process by which knowledge needed for an organization to succeed is created, captured, shared and leveraged. Nowadays, the pace of evolution has entered a rapid speed, and those who can’t learn, adapt, and change from moment to moment simply won’t survive. Current HE institutes recognize their valuable intelligences and have adopted their changing role in a society.},
  langid = {english},
  keywords = {Higher education,Knowledge management,Process,未整理},
  annotation = {33 citations (Crossref) [2024-03-26]\\
titleTranslation: 高等教育中的知識管理\\
abstractTranslation:  這篇綜述文章概述了高等教育 (HE) 機構知識管理 (KM) 的基本概念，並總結了先前的科學工作，以確保在不斷變化的環境中提供對其有效且高效的理解。知識管理是一個系統化的過程，組織成功所需的知識透過這個過程被創造、獲取、分享和利用。如今，進化的步伐已進入快節奏，無法時時學習、適應、改變的人根本無法生存。目前的高等教育機構認識到他們寶貴的智力，並適應了他們在社會中不斷變化的角色。},
  file = {D\:\\Paper\\Knowledge management in higher education.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\H5GU8MG6\\S1877050910004655.html}
}

@thesis{LaiZhengJieJiYuShiYongZheJieMianYuanJianBaoSWTDeXianZhiShiCeShiAnLiChanSheng2022,
  title = {{{基於使用者介面元件包SWT的限制式測試案例產生}}},
  author = {{賴政傑}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2022},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/vmyy25},
  abstract = {軟體測試是確保軟體品質的主要方法。限制式測試案例產生是根據限制式軟體規格，使用解限制滿足問題的方法，自動產生測試案例的技術。本論文開發基於使用者介面元件包SWT的測試案例產生技術。本論文首先制定XML形式的限制式使用者介面規格，接著制定覆蓋使用者介面元件所有屬性及事件的測試覆蓋標準，最後運用限制式測試案例產生技術，根據使用者介面規格，自動產生滿足測試覆蓋標準的測試案例集。本論文針對每一個使用者介面元件，進行單元測試，以降低前端使用者介面元件和後端計算函式之間的耦合所導致的複雜度。},
  pagetotal = {326},
  keywords = {實驗室}
}

@online{lalaPaperQARetrievalAugmentedGenerative2023,
  title = {{{PaperQA}}: {{Retrieval-Augmented Generative Agent}} for {{Scientific Research}}},
  shorttitle = {{{PaperQA}}},
  author = {Lála, Jakub and O'Donoghue, Odhran and Shtedritski, Aleksandar and Cox, Sam and Rodriques, Samuel G. and White, Andrew D.},
  date = {2023-12-14},
  eprint = {2312.07559},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.07559},
  url = {http://arxiv.org/abs/2312.07559},
  urldate = {2024-05-02},
  abstract = {Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth. Retrieval-Augmented Generation (RAG) models have been proposed to reduce hallucinations and provide provenance for how an answer was generated. Applying such models to the scientific literature may enable large-scale, systematic processing of scientific knowledge. We present PaperQA, a RAG agent for answering questions over the scientific literature. PaperQA is an agent that performs information retrieval across full-text scientific articles, assesses the relevance of sources and passages, and uses RAG to provide answers. Viewing this agent as a question answering model, we find it exceeds performance of existing LLMs and LLM agents on current science QA benchmarks. To push the field closer to how humans perform research on scientific literature, we also introduce LitQA, a more complex benchmark that requires retrieval and synthesis of information from full-text scientific papers across the literature. Finally, we demonstrate PaperQA's matches expert human researchers on LitQA.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,RAG,問答系統,未整理},
  annotation = {titleTranslation: PaperQA：科學研究的檢索增強生成代理\\
abstractTranslation:  大型語言模型 (LLM) 可以很好地泛化各種語言任務，但會出現幻覺和不可解釋性，因此在沒有事實依據的情況下很難評估其準確性。人們提出了檢索增強生成（RAG）模型來減少幻覺並為答案的生成方式提供基礎。將此類模型應用於科學文獻可以實現科學知識的大規模、系統化處理。我們推出 PaperQA，一種用於回答科學文獻問題的 RAG 代理程式。 PaperQA 是一種代理，可在全文科學文章中執行資訊檢索、評估來源和段落的相關性，並使用 RAG 提供答案。將此代理視為問答模型，我們發現它在當前科學 QA 基準上超過了現有 LLM 和 LLM 代理的性能。為了推動該領域更接近人類如何對科學文獻進行研究，我們還引入了 LitQA，這是一個更複雜的基準，需要從文獻中的全文科學論文中檢索和綜合資訊。最後，我們展示了 PaperQA 與 LitQA 上的人類專家研究人員的匹配。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\RUNJWB33\\Lála 等。 - 2023 - PaperQA Retrieval-Augmented Generative Agent for .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\SS38K28Q\\2312.html}
}

@thesis{LanYiTingLeiBieTuYuWuJianXianZhiYuYanGuiGeZhiXianZhiLuoJiChengShiGuiGeDeZiDongZhuanHuan2015,
  title = {類別圖與物件限制語言規格至限制邏輯程式規格的自動轉換},
  author = {{藍翊庭}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2015},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/7r9b66},
  abstract = {軟體規格在軟體開發中占了一個舉足輕重的角色，軟體規格除了用來精確地定義一個軟體所該擁有的性質與行為外，更是開發人員之間或是開發人員與客戶之間的溝通媒介。本論文描述一個可以自動化的將原本不可執行的類別圖與物件限制語言規格轉換成可執行的限制邏輯程式規格的轉換器的設計與實作。這個自動轉換需要解決兩個主要問題。第一個問題是如何產生符合類別圖關聯關係中類別實例數量限制的物件。第二個問題是如何產生符合物件限制語言中類別恆定條件、函式前置條件、及函式後置條件的物件。限制邏輯程式的一致化機制和強大的解限制能力使得產生可執行規格的問題變得容易許多。產生出來的可執行限制邏輯程式規格可以應用在黑箱測試中做為測試案例產生器，同時產生測試輸入及預期輸出，或應用在白箱測試中做為預期輸出產生器，產生預期輸出。},
  pagetotal = {75}
}

@inproceedings{larssiposIdentifyingExplanationNeeds2023,
  title = {Identifying {{Explanation Needs}} of {{End-users}}: {{Applying}} and {{Extending}} the {{XAI Question Bank}}},
  shorttitle = {Identifying {{Explanation Needs}} of {{End-users}}},
  booktitle = {Mensch Und {{Computer}} 2023},
  author = {{Lars Sipos} and {Ulrike Schäfer} and {Katrin Glinka} and {Claudia Müller-Birn}},
  year = {9 月 3, 2023},
  series = {{{MuC}} '23},
  pages = {492--497},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3603555.3608551},
  url = {https://doi.org/10.1145/3603555.3608551},
  urldate = {2023-08-23},
  abstract = {Explainable Artificial Intelligence (XAI) is concerned with making the decisions of AI systems interpretable to humans. Explanations are typically developed by AI experts and focus on algorithmic transparency and the inner workings of AI systems. Research has shown that such explanations do not meet the needs of users who do not have AI expertise. As a result, explanations are often ineffective in making system decisions interpretable and understandable. We aim to strengthen a socio-technical view of AI by following a Human-Centered Explainable Artificial Intelligence (HC-XAI) approach, which investigates the explanation needs of end-users (i.e., subject matter experts and lay users) in specific usage contexts. One of the most influential works in this area is the XAI Question Bank (XAIQB) by Liao et al. The authors propose a set of questions that end-users might ask when using an AI system, which in turn is intended to help developers and designers identify and address explanation needs. Although the XAIQB is widely referenced, there are few reports of its use in practice. In particular, it is unclear to what extent the XAIQB sufficiently captures the explanation needs of end-users and what potential problems exist in the practical application of the XAIQB. To explore these open questions, we used the XAIQB as the basis for analyzing 12 think-aloud software explorations with subject matter experts, i.e., art historians. We investigated the suitability of the XAIQB as a tool for identifying explanation needs in a specific usage context. Our analysis revealed a number of explanation needs that were missing from the question bank, but that emerged repeatedly as our study participants interacted with an AI system. We also found that some of the XAIQB questions were difficult to distinguish and required interpretation during use. Our contribution is an extension of the XAIQB with 11 new questions. In addition, we have expanded the descriptions of all new and existing questions to facilitate their use. We hope that this extension will enable HCI researchers and practitioners to use the XAIQB in practice and may provide a basis for future studies on the identification of explanation needs in different contexts.},
  isbn = {9798400707711},
  langid = {english},
  keywords = {Explainable AI,Explanation needs,Human-AI collaboration,User study,使用者研究,可解釋性,回收,機器學習},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 確定最終用戶的解釋需求：應用和擴展 XAI 問題庫\\
abstractTranslation:  可解釋的人工智能（XAI）致力於使人工智能係統的決策能夠被人類解釋。解釋通常由人工智能專家製定，重點關注算法透明度和人工智能係統的內部運作。研究表明，這樣的解釋不能滿足不具備人工智能專業知識的用戶的需求。因此，解釋通常無法有效地使系統決策變得可解釋和理解。我們的目標是通過遵循以人為中心的可解釋人工智能（HC-XAI）方法來加強人工智能的社會技術觀點，該方法研究最終用戶（即主題專家和非專業用戶）在特定使用環境中的解釋需求。該領域最有影響力的作品之一是 Liao 等人的 XAI 題庫（XAIQB）。作者提出了最終用戶在使用人工智能係統時可能會問的一系列問題，旨在幫助開發人員和設計人員識別和解決解釋需求。儘管 XAIQB 被廣泛引用，但其實際使用的報導卻很少。特別是，目前尚不清楚XAIQB在多大程度上充分捕獲了最終用戶的解釋需求以及XAIQB在實際應用中存在哪些潛在問題。為了探索這些懸而未決的問題，我們使用 XAIQB 作為與主題專家（即藝術史學家）一起分析 12 個有聲思考軟件探索的基礎。我們研究了 XAIQB 作為識別特定使用上下文中的解釋需求的工具的適用性。我們的分析揭示了問題庫中缺失的許多解釋需求，但當我們的研究參與者與人工智能係統交互時，這些需求反復出現。我們還發現，XAIQB 的一些問題在使用過程中很難區分，需要解釋。我們的貢獻是 XAIQB 的擴展，增加了 11 個新問題。此外，我們還擴展了所有新問題和現有問題的描述，以方便其使用。我們希望這一擴展將使人機交互研究人員和從業者能夠在實踐中使用 XAIQB，並為未來在不同背景下識別解釋需求的研究提供基礎。},
  note = {本研究考慮以人為中心的可解釋性AI(HC-XAI)，並驗證此方法的效果。研究方法可能可以參考，但整體超出研究範圍。},
  file = {C:\Users\BlackCat\Zotero\storage\G9SKW2MR\Sipos 等。 - 2023 - Identifying Explanation Needs of End-users Applyi.pdf}
}

@inproceedings{layllad.c.renaultUsingOntologybasedApproach2018,
  title = {Using an {{Ontology-based Approach}} for {{Integrating Applications}} to Support {{Software Processes}}},
  booktitle = {Proceedings of the {{XVII Brazilian Symposium}} on {{Software Quality}}},
  author = {{Laylla D.C. Renault} and {Monalessa Perini Barcellos} and {Ricardo de Almeida Falbo}},
  year = {10 月 17, 2018},
  series = {{{SBQS}} '18},
  pages = {220--229},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3275245.3275269},
  url = {https://dl.acm.org/doi/10.1145/3275245.3275269},
  urldate = {2023-09-15},
  abstract = {軟件組織使用多種應用程序來支持其軟件流程。為了正確支持軟件流程，應在不同層（數據、服務和流程）集成應用程序。此外，集成應該涵蓋語義方面。因此，提供有關如何在解決語義方面的不同層上執行集成的指南的方法可能會有所幫助。本文提出了基於本體的語義集成方法（OBA-SI）的擴展，重點關注過程層的語義集成。該擴展建立了數據、服務和流程層集成之間的關係，並使用任務本體和流程本體來指導流程層集成。},
  isbn = {978-1-4503-6565-9},
  langid = {english},
  keywords = {/unread,process integration,system integration,回收,知識本體,語意分析},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用基於本體的方法整合應用程式以支援軟體流程\\
abstractTranslation:  軟體組織使用多個應用程式來支援其軟體流程。為了正確支援軟體流程，應在不同的層（資料、服務和流程）整合應用程式。此外，整合應該涵蓋語義方面。因此，提供有關如何在解決在語義方面的資訊不同層上執行集成的指南的方法可能會有所幫助。本文提出了基於本體的語義集成方法（OBA-SI）的擴展，重點關注過程層的語義集成。該擴展建立了數據、服務和流程層整合之間的關係，並使用任務本體和流程本體來指導流程層整合。},
  note = {與系統開發過程有關},
  file = {C:\Users\BlackCat\Zotero\storage\7B4IDC7E\Renault 等。 - 2018 - Using an Ontology-based Approach for Integrating A.pdf}
}

@online{LearningDistanceMetric,
  title = {Learning the Distance Metric in a Personal Ontology | {{Proceedings}} of the 2nd International Workshop on {{Ontologies}} and Information Systems for the Semantic Web},
  url = {https://dl.acm.org/doi/10.1145/1458484.1458488},
  urldate = {2023-09-16},
  langid = {english},
  keywords = {/unread},
  annotation = {titleTranslation: 學習個人本體中的距離測量 |第二屆語意網本體論與資訊系統國際研討會論文集}
}

@thesis{LeiYaoChengYiGeDianJiYuXunXuTuDeCeShiAnLiChanShengQi2008,
  title = {一個奠基於循序圖的測試案例產生器},
  author = {{雷曜誠}},
  namea = {{林迺衛} and {Nai-wei Lin}},
  nameatype = {collaborator},
  date = {2008},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/yv5j5w},
  abstract = {軟體於開發過程中通常無法完全確保實作的正確性，我們僅能透過一些檢 驗技術來搜尋軟體的錯誤，這個檢驗程序統稱為軟體測試。以往軟體測試僅在軟 體開發的最終步驟執行，亦即須等待實作全部完成之後，才開始測試。如此造成 軟體測試成效不盡理想且耗費的成本甚鉅。最近雖然在開發過程的每一個環節都 已結合軟體測試，大量提升軟體測試的成效，但是其耗費的成本卻依然可觀。因 此突顯自動化軟體測試的重要性。 本篇論文描述一個奠基於循序圖的半自動測試案例產生器。循序圖用於描 述物件之間的動態行為。這個工具首先分析這些動態行為來決定適當的測試路 徑。在使用者輸入執行每一條測試路徑所需的輸入資料和期望的輸出資料之後， 測試案例產生器將可自動產生符合JUnit 平台的Java 測試程式碼。},
  pagetotal = {72}
}

@online{lewisRetrievalAugmentedGenerationKnowledgeIntensive2021,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
  date = {2021-04-12},
  eprint = {2005.11401},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.11401},
  url = {http://arxiv.org/abs/2005.11401},
  urldate = {2024-03-21},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,RAG,未整理},
  annotation = {titleTranslation: 知識密集型 NLP 任務的檢索增強生成},
  note = {Comment: Accepted at NeurIPS 2020},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\776NWGBT\\Lewis 等。 - 2021 - Retrieval-Augmented Generation for Knowledge-Inten.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\QWKYCQD7\\2005.html}
}

@inproceedings{liangyaoTraditionalChineseMedicine2016,
  title = {Traditional {{Chinese}} Medicine Clinical Records Classification Using Knowledge-Powered Document Embedding},
  author = {{Liang Yao} and {Yin Zhang} and {Baogang Wei} and {Zherong Li} and {Xiangzhou Huang}},
  date = {2016-12-01},
  pages = {1926--1928},
  publisher = {IEEE Computer Society},
  doi = {10.1109/bibm.2016.7822817},
  url = {https://www.computer.org/csdl/proceedings-article/bibm/2016/07822817/12OmNyKJik2},
  urldate = {2022-08-03},
  abstract = {Text classification is one of the fundamental tasks in text mining. In the medical domain, there have been a number of studies on text classification in modern medicine clinical notes written in English. However, very limited text classification research has been conducted on clinical notes written in Chinese, especially traditional Chinese medicine (TCM) clinical records. The goal of this study was to investigate features and machine learning classification algorithms for TCM clinical text classification. We collected 7,037 TCM clinical records of famous TCM doctors as our dataset, and investigated the effects of different types of features and classification algorithms. Additionally, we proposed a novel method to combine deep learning text representation with TCM domain knowledge, which results in the best classification performance.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  isbn = {978-1-5090-1611-2},
  langid = {english},
  keywords = {embedded,中醫,原始碼,機器學習,深度學習,病歷分析,病歷分類,詞向量},
  annotation = {6 citations (Crossref) [2024-03-26]\\
abstractTranslation:  文本分類是文本挖掘的基本任務之一。在醫學領域，已經有許多關於現代醫學英文臨床筆記文本分類的研究。然而，對中文臨床記錄，特別是中醫臨床記錄進行的文本分類研究非常有限。本研究的目的是研究中醫臨床文本分類的特徵和機器學習分類算法。我們收集了 7,037 名名中醫的中醫臨床記錄作為我們的數據集，並研究了不同類型特徵和分類算法的效果。此外，我們提出了一種將深度學習文本表示與中醫領域知識相結合的新方法，從而獲得最佳的分類性能。\\
titleTranslation: 使用知識驅動的文檔嵌入進行中醫臨床記錄分類},
  note = {比較不同embedded方法及分器對分類中醫臨床病歷的幫助，其中包含利用概念來表示詞向量的方法（ESA）
\par
\begin{enumerate}

\item 未讀完，但感覺應該沒用
\item 有說明多種embedded的方法，但不是很清楚
\item 有原始程式碼的樣子
\item 所謂的分類是指分什麼類？分科別嗎？

\end{enumerate}},
  file = {C:\Users\BlackCat\Zotero\storage\G2FZXGTK\Liang Yao 等。 - 2016 - Traditional Chinese medicine clinical records clas.pdf}
}

@article{liangyaoTraditionalChineseMedicine2019,
  title = {Traditional {{Chinese}} Medicine Clinical Records Classification with {{BERT}} and Domain Specific Corpora},
  author = {{Liang Yao} and {Zhe Jin} and {Chengsheng Mao} and {Yin Zhang} and {Yuan Luo}},
  date = {2019-12-01},
  journaltitle = {Journal of the American Medical Informatics Association: JAMIA},
  shortjournal = {J Am Med Inform Assoc},
  volume = {26},
  number = {12},
  eprint = {31550356},
  eprinttype = {pmid},
  pages = {1632--1636},
  issn = {1527-974X},
  doi = {10.1093/jamia/ocz164},
  abstract = {Traditional Chinese Medicine (TCM) has been developed for several thousand years and plays a significant role in health care for Chinese people. This paper studies the problem of classifying TCM clinical records into 5 main disease categories in TCM. We explored a number of state-of-the-art deep learning models and found that the recent Bidirectional Encoder Representations from Transformers can achieve better results than other deep learning models and other state-of-the-art methods. We further utilized an unlabeled clinical corpus to fine-tune the BERT language model before training the text classifier. The method only uses Chinese characters in clinical text as input without preprocessing or feature engineering. We evaluated deep learning models and traditional text classifiers on a benchmark data set. Our method achieves a state-of-the-art accuracy 89.39\% ± 0.35\%, Macro F1 score 88.64\% ± 0.40\% and Micro F1 score 89.39\% ± 0.35\%. We also visualized attention weights in our method, which can reveal indicative characters in clinical text.},
  langid = {english},
  pmcid = {PMC7647141},
  keywords = {Benchmarking,BERT,clinical records classification,Datasets as Topic,Deep Learning,domain knowledge,Medical Records,Medicine Chinese Traditional,natural language processing,Natural Language Processing,TCM},
  annotation = {44 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用 BERT 和特定領域語料庫進行中醫臨床記錄分類\\
abstractTranslation:  中醫藥已有數千年的發展歷史，在中國人民的健康保健中發揮著重要作用。本文研究了中醫臨床病案分類為中醫5大病種的問題。我們探索了許多最先進的深度學習模型，發現最近 Transformers 的雙向編碼器表示可以比其他深度學習模型和其他最先進的方法取得更好的結果。在訓練文本分類器之前，我們進一步利用未標記的臨床語料庫對 BERT 語言模型進行微調。該方法僅使用臨床文本中的漢字作為輸入，沒有進行預處理或特徵工程。我們在基準數據集上評估了深度學習模型和傳統文本分類器。我們的方法達到了最先進的準確度 89.39\% ± 0.35\%，Macro F1 分數 88.64\% ± 0.40\% 和 Micro F1 分數 89.39\% ± 0.35\%。我們還在我們的方法中可視化了注意力權重，這可以揭示臨床文本中的指示性特徵。},
  file = {C:\Users\BlackCat\Zotero\storage\RC7462SH\Liang Yao 等。 - 2019 - Traditional Chinese medicine clinical records clas.pdf}
}

@thesis{LiaoChongRongYiGeZhenDuiShuangZiLiaoJiYiTiShuWeiXunHaoChuLiQiDeCBianYiQi2003,
  title = {{{一個針對雙資料記憶體數位訊號處理器的C編譯器}}},
  author = {{廖崇榮}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2003},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/4v9fzd},
  abstract = {相對於一般用途的處理器，數位訊號處理器擁有較特殊的指令集及記憶體結構來支援數位訊號處理。適當地配合此數位訊號處理器相關的特殊指令及記憶體結構可以明顯的改善數位訊號處理應用的效能。 依據LCC編譯器為基礎，本篇論文研製了一個針對擁有雙資料記憶體的數位訊號處理器的C編譯器。這個編譯器最主要的特色是支援程序間的雙資料記憶體變數配置分析，並將分析的結果，用於數位訊號處理相關特殊指令的最佳化。},
  pagetotal = {76}
}

@thesis{LiaoWeiShengYiGeJiYuZanCunQiZhuanYiYuYanDeChengShiZhiXingShiJianGuJiGongJu2012,
  title = {一個基於暫存器轉移語言的程式執行時間估計工具},
  author = {{廖韋勝}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2012},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/q4f8g2},
  abstract = {如何開發出高效能的程式碼,是程式設計人員的重要課題。將程式交給客戶 前,程式設計人員都會進行程式效能的評估,並探索最佳的編譯方式來盡可能的提 升程式的效能。本研究主要是提供一個奠基於編譯器中間語言-暫存器轉移語言的 程式效能評估工具。透由使用程式靜態分析,讓程式執行效能的評估盡可能不藉由 實際執行程式來獲得。經由初步和模擬器比較的實驗觀察,此程式效能評估工具的 平均準確率可達 72.03 \%,並在不同的編譯方式間呈現相當一致的趨勢。本研究並 運用這個程式效能評估工具提升重複式編譯的速度。透由使用控制流程樣式的快取 機制,初步的實驗顯示此程式效能評估工具可協助重複式編譯達到不錯的加速效果。},
  pagetotal = {29}
}

@thesis{LiaoWeiZhiJiYuYuJingHuaYuYanMoXingYuGuiZeKuZhiICD10BianMaXiTong2021,
  title = {基於語境化語言模型與規則庫之ICD-10編碼系統},
  author = {{廖偉智}},
  namea = {{賴飛羆}},
  nameatype = {collaborator},
  date = {2021},
  journaltitle = {臺灣大學生醫電子與資訊學研究所學位論文},
  pages = {66},
  institution = {國立臺灣大學},
  doi = {10.6342/NTU202102896},
  url = {http://dx.doi.org/10.6342/NTU202102896},
  abstract = {背景: 中央健康保險署要求特約醫療院所於2016年起用 ICD-10-CM/PCS編碼作為給付依據，然而目前的編碼作業透過專業疾病分類人員花費大量時間與精力解讀和分析病歷內容及診斷，以確保每件病歷疾病分類編碼的正確性。 目標: 本篇研究的目標是希望透過語境化語言模型與規則庫建構一個ICD-10 AI輔助系統，以提升編碼的時效性與正確性，透過歸入正確的DRG落點，取得醫療院所應有的醫務收入。 方法: 本篇研究中，我們使用亞東醫院的出院病摘資料並透過資料探勘與自然語言處裡的技術 (包含 Word2Vec, XLNet, BERT, AttentionXLM) 於深度學習網路中以實現 ICD-10 的自動編碼。並透過實務上與疾分師討論所建立的規則庫來優化原本的語境化語言模型。 結果: 在各個實驗結果中發現，語境化語言模型在多標籤分類任務中的表現的確比非語境化語言模型來的好，其中以BioBERT分類模型搭配資料探勘與規則庫的優化在 ICD-10-CM 與 ICD-10-PCS 的編碼上達到最好的結果 (F1-Score 0.77 與 0.69)。 結論: 在模型與系統的驗證上，透過亞東醫院疾分師的實際參與實驗，我們比較了沒有提供AI預測編碼與有提供AI預測編碼的編碼時間與一致性，並與成對的樣本進行了分析，結果顯示有提供預測的ICD代碼可以將疾分師的平均F1從中位數從0.832提高到0.922 （P {$<$} 0.05），但沒有減少其平均編碼時間（P = 0.64）。這些模型上的改變與規則庫的優化都會在未來持續於本ICD-10 的網頁服務中，以提供所有的 ICD-10 使用者自動編碼及訓練的服務。 關鍵詞: 自然語言處理、深度學習、多標籤分任務、語境化語言模型、規則庫、國際疾病分類標準},
  issue = {2021年},
  langid = {英文},
  pagetotal = {1},
  keywords = {Contextualized language model,Deep learning,ICD-10,Multi label classification,Natural language processing,Rule base,國際疾病分類標準,多標籤分任務,深度學習,自然語言處理,規則庫,語境化語言模型},
  annotation = {titleTranslation: 基於語境化語言模型與規則庫之ICD-10編碼系統\\
abstractTranslation:  背景：中央健康保險機構要求特約醫療所於2016年起用ICD-10-CM/PCS編碼作為給付依據，然而目前的編碼作業視覺專業疾病分類人員花費大量時間與情報解讀和分析病歷內容及診斷，以確保每件病歷疾病分類編碼的正確性。目標：本文研究的目標是希望穿透語境化語言模型與規則庫目前的一個ICD-10 AI輔助系統，以提升編碼的時效性與正確性、穿透性方法：本篇語言研究中，我們利用亞東醫院的出院病摘資料和透視數據探勘與自然處裡的技術（包含Word2Vec， XLNet, BERT, AttentionXLM) 於深度學習網路中以實現ICD- 10 的自動編碼。並交叉實務上與疾分師討論所建立的規則庫來優化具體的語境化語言模型。結果: 在各個實驗中結果中發現，語境語言模型在多標籤分類任務中的表現確實比非語境語言模型來的好，其中以BioBERT分類模型佈局數據探勘與規則庫的優化在ICD-10-CM與ICD -10-PCS 的編碼上達到最好的結果(F1-Score 0.77與0.69)。 結論實驗: 在模型與系統的驗證上，透過亞東醫院疾分師的實際參與，我們比較沒有提供AI預測編碼與提供AI 預測編碼的編碼時間與一致性，並與生成的樣本進行分析，結果顯示預測提供的ICD 代碼可以將快速分師的平均F1 插座容量從0.832 提高到0.922 (P {$<$} 0.05)，但沒有減少其平均編碼時間(P = 0.64)。這些模型上的改變與規則庫的優化都會在未來持續於本ICD-10的網頁服務中，以提供所有的ICD-10用戶自動編碼及訓練的服務。 關鍵詞: 自然語言處理、深度學習、多標籤分任務、語境化語言模型、規則庫、國際疾病分類標準}
}

@inproceedings{liAreTheyAll2023,
  title = {Are {{They All Good}}? {{Studying Practitioners}}' {{Expectations}} on the {{Readability}} of {{Log Messages}}},
  shorttitle = {Are {{They All Good}}?},
  booktitle = {2023 38th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Li, Zhenhao and Chen, An Ran and Hu, Xing and Xia, Xin and Chen, Tse-Hsun and Shang, Weiyi},
  date = {2023-09},
  pages = {129--140},
  issn = {2643-1572},
  doi = {10.1109/ASE56229.2023.00136},
  url = {https://ieeexplore.ieee.org/document/10298380},
  urldate = {2024-01-17},
  abstract = {Developers write logging statements to generate logs that provide run-time information for various tasks. The readability of log messages in the logging statements (i.e., the descriptive text) is rather crucial to the value of the generated logs. Immature log messages may slow down or even obstruct the process of log analysis. Despite the importance of log messages, there is still a lack of standards on what constitutes good readability of log messages and how to write them. In this paper, we conduct a series of interviews with 17 industrial practitioners to investigate their expectations on the readability of log messages. Through the interviews, we derive three aspects related to the readability of log messages, including Structure, Information, and Wording, along with several specific practices to improve each aspect. We validate our findings through a series of online questionnaire surveys and receive positive feedback from the participants. We then manually investigate the readability of log messages in large-scale open source systems and find that a large portion (38.1\%) of the log messages have inadequate readability. Motivated by such observation, we further explore the potential of automatically classifying the readability of log messages using deep learning and machine learning models. We find that both deep learning and machine learning models can effectively classify the readability of log messages with a balanced accuracy above 80.0\% on average. Our study provides comprehensive guidelines for composing log messages to further improve practitioners' logging practices.},
  eventtitle = {2023 38th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  langid = {english},
  keywords = {log,使用者研究,已整理},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 他們都好嗎？研究從業者對日誌訊息可讀性的期望},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\TP2H3SVW\\Li et al. - 2023 - Are They All Good Studying Practitioners' Expecta.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\YQA4JY2Z\\10298380.html}
}

@article{LiBoCaiYongTransformerCRFDeZhongWenDianZiBingLiMingMingShiTiShiBie2020,
  title = {采用Transformer-CRF的中文电子病历命名实体识别},
  author = {{李博} and {康晓东} and {张华丽} and {王亚鸽} and {陈亚媛} and {白放}},
  date = {2020},
  journaltitle = {计算机工程与应用},
  shortjournal = {Computer Engineering and Applications},
  volume = {56},
  number = {5},
  pages = {153--159},
  doi = {10.3778/j.issn.1002-8331.1909-0211},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhFqc2pnY3l5eTIwMjAwNTAyMRoIYTl3dXluajY%3D},
  urldate = {2022-08-14},
  abstract = {命名实体识别是自然语言处理的基本任务之一.针对中文电子病历命名实体识别传统模型识别效果不佳的问题,提出一种完全基于注意力机制的神经网络模型.实验采用自建真实中文电子病历数据集并对数据集进行人工标注、分词等预处理;对Transformer模型进行训练优化,以提取文本特征;利用条件随机场对提取到的文本特征进行分类识别.为验证所提方法的有效性,将构建的Transformer-CRF神经网络模型与其他7种传统模型进行比较研究,实验采用精确率、召回率和F1值三个指标评估模型的识别性能.实验结果显示,在同一语料集下,Transformer-CRF模型对身体部位类的命名实体识别效果较好,F1值高达95.02},
  langid = {zh\_CN},
  keywords = {中文電子病歷,中文電子病歷CEMR,命名實體,實體識別,條件隨機場(CRF),機器學習},
  annotation = {天津医科大学 医学影像学院,天津 300203天津医科大学 医学影像学院,天津 300203天津医科大学 医学影像学院,天津 300203天津医科大学 医学影像学院,天津 300203天津医科大学 医学影像学院,天津 300203天津医科大学 医学影像学院,天津 300203\\
京津冀协同创新项目\\
2020-03-31 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 採用Transformer-CRF的中文電子病歷命名識別實體\\
abstractTranslation:  實體命名歷識別是自然語言處理的基本任務之一。針對中文電子病歷命名實體識別傳統模型識別效果不佳的問題，提出一種完全基於注意力的神經機製網絡模型。實驗採用自建真實中文電子病歷數據集對數據集進行人工標記、分詞等剪切；對Transformer模型進行訓練優化，提取文本特徵；利用條件隨機場對提取到的文本特徵進行分類識別。將構建的Transformer-CRF神經網絡模型與其他7種傳統模型進行比較研究，實驗採用準確率、響應率和F1值三個指標評估模型的識別性能。實驗結果顯示，同語料集下，Transformer- CRF模型對身體部位類的命名實體識別效果較好，F1值高達95.02},
  file = {C:\Users\BlackCat\Zotero\storage\HUAPMRK2\李 等。 - 2020 - 采用Transformer-CRF的中文电子病历命名实体识别.pdf}
}

@inproceedings{liDenseRetrievalSystem2023,
  title = {A {{Dense Retrieval System}} and {{Evaluation Dataset}} for {{Scientific Computational Notebooks}}},
  booktitle = {2023 {{IEEE}} 19th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  author = {Li, Na and Zhang, Yangjun and Zhao, Zhiming},
  date = {2023-10},
  pages = {1--10},
  issn = {2325-3703},
  doi = {10.1109/e-Science58273.2023.10254859},
  url = {https://ieeexplore.ieee.org/abstract/document/10254859},
  urldate = {2024-03-22},
  abstract = {The discovery and reutilization of scientific codes are crucial in many research activities. Computational notebooks have emerged as a particularly effective medium for sharing and reusing scientific codes. Nevertheless, effectively locating relevant computational notebooks is a significant challenge. First, computational notebooks encompass multi-modal data comprising unstructured text, source code, and other media, posing complexities in representing such data for retrieval purposes. Second, the absence of evaluation datasets for the computational notebook search task hampers fair performance assessments within the research community. Prior studies have either treated computational notebook search as a code-snippet search problem or focused solely on content-based approaches for searching computational notebooks. To address the aforementioned difficulties, we present DeCNR, tackling the information needs of researchers in seeking computational notebooks. Our approach leverages a fused sparse-dense retrieval model to represent computational notebooks effectively. Additionally, we construct an evaluation dataset including actual scientific queries, computational notebooks, and relevance judgments for fair and objective performance assessment. Experimental results demonstrate that the proposed method surpasses baseline approaches in terms of F1@5 and NDCG@5. The proposed system has been implemented as a web service shipped with REST APIs, allowing seamless integration with other applications and web services.},
  eventtitle = {2023 {{IEEE}} 19th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  langid = {english},
  keywords = {Codes,Complexity theory,Computational modeling,computational notebook search,evaluation dataset,Jupyter Notebook,Media,RAG,scientific code reuse,Search problems,Source coding,virtual research environment,Web services,未整理,程式碼分析,資料集},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 科學計算筆記本的密集檢索系統和評估資料集},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\R6CLDIM6\\Li 等。 - 2023 - A Dense Retrieval System and Evaluation Dataset fo.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\SBPRM4XS\\10254859.html}
}

@inproceedings{liDidWeMiss2023,
  title = {Did {{We Miss Something Important}}? {{Studying}} and {{Exploring Variable-Aware Log Abstraction}}},
  shorttitle = {Did {{We Miss Something Important}}?},
  booktitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Li, Zhenhao and Luo, Chuan and Chen, Tse-Hsun and Shang, Weiyi and He, Shilin and Lin, Qingwei and Zhang, Dongmei},
  date = {2023-05},
  pages = {830--842},
  issn = {1558-1225},
  doi = {10.1109/ICSE48619.2023.00078},
  url = {https://ieeexplore.ieee.org/document/10172652},
  urldate = {2024-01-17},
  abstract = {Due to the sheer size of software logs, developers rely on automated techniques for log analysis. One of the first and most important steps of automated log analysis is log abstraction, which parses the raw logs into a structured format. Prior log abstraction techniques aim to identify and abstract all the dynamic variables in logs and output a static log template for automated log analysis. However, these abstracted dynamic variables may also contain important information that is useful to different tasks in log analysis. In this paper, we investigate the characteristics of dynamic variables and their importance in practice, and explore the potential of a variable-aware log abstraction technique. Through manual investigations and surveys with practitioners, we find that different categories of dynamic variables record various information that can be important depending on the given tasks, the distinction of dynamic variables in log abstraction can further assist in log analysis. We then propose a deep learning based log abstraction approach, named VALB, which can identify different categories of dynamic variables and preserve the value of specified categories of dynamic variables along with the log templates (i.e., variable-aware log abstraction). Through the evaluation on a widely used log abstraction benchmark, we find that VALB outperforms other state-of-the-art log abstraction techniques on general log abstraction (i.e., when abstracting all the dynamic variables) and also achieves a high variable-aware log abstraction accuracy that further identifies the category of the dynamic variables. Our study highlights the potential of leveraging the important information recorded in the dynamic variables to further improve the process of log analysis.},
  eventtitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  langid = {english},
  keywords = {log,已整理,機器學習},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 我們錯過了一些重要的事情嗎？研究和探索變數感知日誌抽象},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\XSAX2VZX\\Li et al. - 2023 - Did We Miss Something Important Studying and Expl.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\8J8IC2CD\\10172652.html}
}

@article{LiGuoLeiZhongWenBingLiWenBenFenCiFangFaYanJiu2016,
  title = {中文病历文本分词方法研究},
  author = {{李国垒} and {陈先来} and {夏冬} and {杨荣}},
  date = {2016},
  journaltitle = {中国生物医学工程学报},
  shortjournal = {Chinese Journal of Biomedical Engineering},
  volume = {35},
  number = {4},
  pages = {477--481},
  doi = {10.3969/j.issn.0258-8021.2016.04.012},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhN6Z3N3eXhnY3hiMjAxNjA0MDEyGghhcnE5aHZ1dA%3D%3D},
  urldate = {2022-08-14},
  abstract = {探索适合医学文本的分词方法,为医学数据挖掘和临床决策支持的语义分析奠定基础.分别使用单纯中科院ICTCLAS分词、ICTCLAS+自定义词典、ICTCLAS+统计分词和ICTCLAS+自定义词典结合互信息统计分词4种策略,对1 500份出院记录中的病历文本进行分词处理,并从准确率、召回率和综合指标值等3个方面对分词结果进行评价.以人工分词的50份出院记录结果为标准依据,4种分词策略的综合指标值分别为45.77％、58.76％、64.93％和78.06％.结果证实,自定义词典结合基于互信息的统计分词方法,能够有效地对病历中出院记录文本进行分词处理,可以满足临床数据分析的需求,具有良好的推广意义.},
  langid = {zh\_CN},
  keywords = {Chinese Journal of Biomedical Engineering,Chinese text segmentation,dictionary segmentation,discharge summary,electronic medical record,statistical word segmentation,中文分詞,病例文本,統計分詞,辭典分詞},
  annotation = {中南大学信息安全与大数据研究院,长沙,410013中南大学信息安全与大数据研究院,长沙410013;医学信息研究湖南省普通高等学校重点实验室(中南大学),长沙410013;湖南省高等学校医学大数据2011协同创新中心,长沙410013中国科学院成都文献情报中心,成都,610041中南大学湘雅医院,长沙,410078\\
国家社会科学基金\\
2016-10-13 （万方平台首次上网日期，不代表论文的发表时间）\\
abstractTranslation:  探索適合醫學文本的分詞方法，為醫學數據挖掘和臨床決策支持的語義分析奠定基礎。分別使用重症中科院ICTCLAS分詞、ICTCLAS+自定義詞典、ICTCLAS+統計分詞和ICTCLAS+自定義詞典結合互信息統計分詞4種策略,對1 500份出院記錄中的病歷文本進行分詞處理,並從準確率、識別率和綜合指標值等3個方面對分詞結果進行評價。以人工分詞的50份出院記錄結果為標準,4種分詞策略的綜合指標值分別為45.77\%、58.76\%、64.93\%和78.06\%。結果驗證，構建詞典結合基於互信息的統計分詞方法，能夠有效地對病歷中出院記錄文本進行分詞處理，滿足臨床分析數據需求，具有良好的推廣意義。\\
titleTranslation: 中文病歷文本分詞方法研究},
  file = {C:\Users\BlackCat\Zotero\storage\XLA828F7\李 等。 - 2016 - 中文病历文本分词方法研究.pdf}
}

@article{LiHaoMinLiYingDuanHuiLongLuXuDongZhongWenBingLiWenDangShuYuTiQuHeFouDingJianChuFangFa2008,
  title = {中文病历文档术语提取和否定检出方法},
  author = {{李昊旻 李莹 段会龙 吕旭东}},
  date = {2008},
  journaltitle = {Zhōngguó shengwú yīxué, gōngchéng xuébào},
  volume = {27},
  number = {5},
  pages = {716--721},
  publisher = {浙江大学生物医学工程与仪器科学学院生物医学工程教育部重点实验室,杭州},
  issn = {0258-8021},
  doi = {10.3969/j.issn.0258-8021.2008.05.016},
  abstract = {利用生物医学术语系统中的词汇和概念,为存有大量珍贵信息的非结构化临床文档建立有效的索引,以便进行信息挖掘和利用,国际上相关研究已经开展多年,而基于中文病历文档概念索引的研究尚属空白。本研究将现有的中文版的国际疾病分类（ICD）集成到统一医学语言系统（UMLS）中,依据中文语言处理的特殊性,对中文电子病历文档进行统计分析,提出了一套中文病历文档术语提取和否定检出的方法,可用于建立中文病历文档的概念索引。术语提取阶段利用高灵敏的最大匹配法并结合通用分词技术来控制假阳性;而在概念否定意义检出部分,充分利用中文特点并基于现有中文处理技术提出了一种简化的子句模式匹配方法。选取了两组医疗文本数据集对算法进行了验证,术语提取算法的灵敏性分别为99.51\%和100\%,误检率分别为1.46\%和1.66\%。否定检出算法的阳性预测值均为100\%,阴性预测值分别为100\%和98.99\%,除标点使用不规范等文书质量问题外,基本可以正确检出。},
  langid = {chi},
  keywords = {医学语言处理,否定检出,术语提取},
  annotation = {abstractTranslation:  利用生物醫學術語系統中的詞彙和概念，為存有大量重要信息的非整理臨床文檔建立有效的索引，以便進行信息挖掘和利用，國際上相關研究已經開展多年，並以中文病歷文檔概念索引為基礎本研究將現有的中文版國際疾病分類（ICD）集成到統一醫學語言系統（UMLS）中，借鑒中文語言處理的特殊性，對中文電子病歷文檔進行統計分析，提出建立了一套中文病歷文檔提取和否定檢出的方法，可用於建立中文病歷文檔的概念索引。檢出部分，充分利用中文特點並基於現有的中文處理技術提出了一種簡化的子句模式匹配方法。從而實現了醫療文本數據集對算法進行了驗證，分別提取算法的靈敏性為99.51 \%和100\%，正確檢出率分別為1.46\%和1.66\%。 否定檢出模型的積極預測值分別為100\%，陰性預測值分別為100\%和98.99\%，除標點使用不規範等文書質量問題外，基本可以正確檢出。\\
titleTranslation: 中文病歷文檔術語提取和否定檢測方法},
  file = {C:\Users\BlackCat\Zotero\storage\QGKA9R64\中文病历文档术语提取和否定检出方法.pdf}
}

@thesis{LiJiaHanZhiYuanDuoYaoCaiGongXiaoZhiZhongYiYaoCaiPeiWuXiTong2017,
  title = {支援多藥材功效之中醫藥材配伍系統},
  author = {{李佳翰}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2017},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/n85dr7},
  abstract = {本研究團隊先前研製一套基於中醫藥材知識本體的中醫藥材自動配伍系統。根據中醫方劑的君、臣、佐、使配伍原則，依據使用者指定的藥材功效，該系統可以自動推薦效力最佳的藥材。該系統共包含補氣類的35味藥材、28帖方劑、及65個標準化過的藥材功效。該系統的藥材功效標準化主要根據原始的藥材功效敘述，尚未考慮藥材的性、味及歸經。該系統開發一個強制多功效藥材配伍演算法。該系統依據君、臣、佐、使的順序推薦藥材。依據使用者指定的藥材功效，系統會強制性地加入順序在前的藥材功效，使用多目標最佳化演算法，計算效力最佳的藥材。 本論文在該系統的基礎上做了下列擴充。第一，本論文擴增其他虛證常用的方劑，總共包含155味藥材及112帖方劑。第二，本論文藥材功效標準化增加考慮藥材的性、味及歸經，擴增至221個標準藥材功效。第三，本論文開發一個彈性多功效藥材配伍演算法，使用者可以指定任意多個藥材功效，再使用多目標最佳化演算法，計算效力最佳的藥材。根據在這112帖方劑的初步使用經驗，確認新演算法不但顯著提升指定多藥材功效的彈性，也提高推薦最佳效力藥材的精確度。},
  pagetotal = {200}
}

@article{LiJingHuaZhongYiLinChuangZhiShiKuDeGouJianJiShuYanJiu2017,
  title = {中医临床知识库的构建技术研究},
  author = {{李敬华} and {于彤李宗友} and {王映辉} and {于琦} and {田野} and {孙晓峰} and {徐丽丽} and {高宏杰} and {张竹绿}},
  date = {2017},
  journaltitle = {中国数字医学},
  volume = {12},
  number = {1},
  pages = {3},
  abstract = {中医临床知识库系统旨在全面整合名医经验,临床指南,中医医案,术语系统,中医文献和方剂知识等多种知识资源,实现中医临床知识的共建和共享,促进中医临床研究,辅助中医临床决策.对整个中医临床知识体系进行分类认识,为进一步知识开发与应用奠定基础;建立中医临床知识的数据标准,规范化表达中医临床知识,使用关联数据技术,促进中医临床知识互融互通;制定中医临床知识分类标准,中医临床知识数据标准;构建中医临床知识库,实现客户端及嵌入式电子病历的决策支持应用服务.该知识库的内容已初具规模,成功部署并稳定运行,为广大中医药工作者提供及时,准确的知识服务.},
  keywords = {中醫},
  file = {C:\Users\BlackCat\Zotero\storage\2B9KUHTU\李敬华 等。 - 2017 - 中医临床知识库的构建技术研究.pdf}
}

@article{lijunliuCloudbasedFrameworkLargescale2018,
  title = {A Cloud-Based Framework for Large-Scale Traditional {{Chinese}} Medical Record Retrieval},
  author = {{Lijun Liu} and {Li Liu} and {Xiaodong Fu} and {Qingsong Huang} and {Xianwen Zhang} and {Yin Zhang}},
  date = {2018-01},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {Journal of Biomedical Informatics},
  volume = {77},
  pages = {21--33},
  issn = {15320464},
  doi = {10.1016/j.jbi.2017.11.013},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046417302587},
  urldate = {2022-09-19},
  langid = {english},
  annotation = {13 citations (Crossref) [2024-03-26]\\
10 citations (Semantic Scholar/DOI) [2022-10-26]\\
titleTranslation: 基於雲的大規模中醫病歷檢索框架},
  file = {C:\Users\BlackCat\Zotero\storage\ICY8B3MK\Liu 等。 - 2018 - A cloud-based framework for large-scale traditiona.pdf}
}

@article{LiLeiMenZhenYiShengGongZuoZhanJiDianZiBingLiZaiZhongYiLinChuangDeYingYong2010,
  title = {门诊医生工作站及电子病历在中医临床的应用},
  author = {{李磊}},
  date = {2010},
  journaltitle = {广州医药},
  number = {002},
  pages = {041},
  file = {C:\Users\BlackCat\Zotero\storage\D3JHLHNG\李磊 - 2010 - 门诊医生工作站及电子病历在中医临床的应用.pdf}
}

@article{LiLingFangJiYuBERTDeZhongWenDianZiBingLiMingMingShiTiShiBie2020,
  title = {基于BERT的中文电子病历命名实体识别},
  author = {{李灵芳} and {杨佳琦} and {李宝山} and {杜永兴} and {胡伟健}},
  date = {2020},
  journaltitle = {内蒙古科技大学学报},
  shortjournal = {Journal of Inner Mongolia University of Science and Technology},
  volume = {39},
  number = {1},
  pages = {71--77},
  doi = {10.16559/j.cnki.2095-2295.2020.01.015},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhFidGd0eHl4YjIwMjAwMTAxNRoIYTl3dXluajY%3D},
  urldate = {2022-08-14},
  abstract = {电子病历中蕴含着丰富的医学信息,这些医学信息对疾病的诊疗具有十分重要的意义.利用命名实体识别技术对电子病历进行信息抽取已成为研究的热点之一,为了更加高效准确的抽取中文电子病历中的实体,提出了BERT-BiLSTM-CRF命名实体识别模型.模型在传统BiLSTM-CRF模型基础上,融合了BERT字嵌入模型,更好的结合文章上下文,充分考虑了一词多义等问题.实验结果证明,该模型在中文电子病历命名实体识别任务中取得了良好的效果,较现有命名实体识别方法,从准确率、召回率、F1值3方面都有着明显的提升.电子病历命名实体识别任务准确度的提高,对进一步构建医学知识图谱、医学知识库等任务有着重大帮助.},
  langid = {zh\_CN},
  keywords = {BERT模型,Journal of Inner Mongolia University of Science and Technology,中文命名实体识别,中文电子病历,内蒙古科技大学学报,预训练语言模型},
  annotation = {内蒙古科技大学 信息工程学院,内蒙古 包头 014010内蒙古科技大学 信息工程学院,内蒙古 包头 014010内蒙古科技大学 信息工程学院,内蒙古 包头 014010内蒙古科技大学 信息工程学院,内蒙古 包头 014010内蒙古科技大学 信息工程学院,内蒙古 包头 014010\\
内蒙古自治区高等学校青年科技英才支持计划 国家自然科学基金~优秀青年科学基金~内蒙古自治区自然科学基金\\
2020-05-12 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 基於BERT的中文電子病歷命名實體識別\\
abstractTranslation:  電子病歷中蘊藏著豐富的醫學信息，這些醫學信息對疾病的診斷核心具有重要的意義，利用命名實體識別技術對電子病歷進行信息抽取已成為研究的熱點之一，以便更加高效準確的抽取中文電子病歷中的實體，提出了BERT-BiLSTM-CRF命名實體識別模型。模型在傳統BiLSTM-CRF模型基礎上，融合了BERT字嵌入模型，更好的結合上下文文章，充分考慮了動作多義等問題實驗結果證明，該模型在中文電子病歷命名實體識別任務中取得了良好的效果，較現有的命名實體識別方法，從準確率、識別率、F1值3方面都有著明顯的提升。切實識別任務準確度的提高，對進一步構建醫學知識圖譜、醫學知識庫等任務有重大幫助。},
  file = {C:\Users\BlackCat\Zotero\storage\ECJ69B9I\李 等。 - 2020 - 基于BERT的中文电子病历命名实体识别.pdf}
}

@article{LiMengDieZhongWenYiXueMingMingShiTiShiBieSuanFaYanJiu2022,
  title = {中文医学命名实体识别算法研究},
  author = {{李梦蝶} and {张平} and {李功利} and {姜伟} and {李科} and {蔡培强}},
  date = {2022},
  journaltitle = {医学信息学杂志},
  number = {043-003},
  abstract = {针对中文医学命名实体识别任务中实体细粒度较大,识别准确率不高等问题,提出一种融合特征Albert的中文医学命名实体识别算法,利用自建的真实标注语料对模型进行训练与测试,结果表明模型具有较好的识别效果.},
  keywords = {ALBERT,中文電子病歷CEMR,命名實體,融合特徵},
  file = {C:\Users\BlackCat\Zotero\storage\9TUR38RJ\李梦蝶 等。 - 2022 - 中文医学命名实体识别算法研究.pdf}
}

@article{LiMengXiangJiYuShenDuZhuDongXueXiDeZhongWenDianZiBingLiMingMingShiTiShiBie2022,
  title = {基于深度主动学习的中文电子病历命名实体识别},
  author = {{李梦翔} and {尤丽珏}},
  date = {2022},
  journaltitle = {微型电脑应用},
  number = {038-006},
  abstract = {电子病历(Electronic Medical Records,EMRs)的结构化对于提升医学记录的科研价值有着重要意义,准确,可靠的命名实体识别(Name Entity Recognition,NER)是EMRs结构化的重要组成.针对中文EMRs的高自由度及复杂性,该研究结合深度主动学习,给出了一种高资源利用,鲁棒的NER方法,以提升中文EMRs的结构化能力.冠心病中文EMRs的命名实体抽取实验表明,相较传统方法,结合主动学习的模型能将计算精度有效提升,F-Score的提升达到1.3\%.},
  keywords = {主動學習,命名實體,機器學習,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\VWAZJKA9\李梦翔 與 尤丽珏 - 2022 - 基于深度主动学习的中文电子病历命名实体识别.pdf}
}

@article{liMultisourceInformationFusion2023,
  title = {Multi-Source Information Fusion: {{Progress}} and Future},
  shorttitle = {Multi-Source Information Fusion},
  author = {Li, Xinde and Dunkin, Fir and Dezert, Jean},
  date = {2023-12},
  journaltitle = {Chinese Journal of Aeronautics},
  shortjournal = {Chinese Journal of Aeronautics},
  pages = {S1000936123004247},
  issn = {10009361},
  doi = {10.1016/j.cja.2023.12.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1000936123004247},
  urldate = {2024-04-11},
  abstract = {Semantic Scholar extracted view of "Multi-source information fusion: Progress and future" by Xinde Li et al.},
  langid = {english},
  keywords = {回收,多源資訊,未整理},
  annotation = {abstractTranslation:  語意學者摘錄了李新德等人的《多源資訊融合：進展與未來》觀點。\\
titleTranslation: 多源資訊融合：進展與未來},
  file = {D:\Paper\Multi-source information fusion Progress and future.pdf}
}

@inproceedings{liMultiSourceInformationManagement2010,
  title = {Multi-{{Source Information Management System}} of {{Railway Geological Environment Based}} on {{GIS Technology}}},
  booktitle = {2010 {{International Conference}} on {{E-Product E-Service}} and {{E-Entertainment}}},
  author = {Li, Weile and Tang, Chuan and Huang, Runqiu},
  date = {2010-01},
  pages = {1--4},
  doi = {10.1109/ICEEE.2010.5661103},
  url = {https://ieeexplore.ieee.org/document/5661103/keywords#keywords},
  urldate = {2024-04-14},
  abstract = {The selection of rural railway route requires the utilization of multi-source data such as topography, geology, environment and remote-sensing data, etc.. Based on the data capture, integrative management and spatial analysis of ARCGIS, the multi-source information management system on railway geological environment has been designed. In this system, multi-source data of different data type, scale and coordination are integrated to the same software platform, improving the utilization efficiency and share of data. Besides, the spatial analysis and 3D visualization functions of the system make the railway route selection become more visual and intelligent, and thus greatly raise the work efficiency.},
  eventtitle = {2010 {{International Conference}} on {{E-Product E-Service}} and {{E-Entertainment}}},
  langid = {english},
  keywords = {Data visualization,Geographic Information Systems,Geology,Rail transportation,Spatial databases,Terrain factors,Three dimensional displays},
  annotation = {titleTranslation: 基於GIS技術的鐵路地質環境多源資訊管理系統\\
abstractTranslation:  農村鐵路選線需要利用地形、地質、環境、遙感資料等多源資料。該系統將不同資料類型、規模和協調性的多來源資料整合到同一軟體平台上，提高了資料的利用效率和共享性。此外，系統的空間分析和3D視覺化功能使鐵路選線變得更加視覺化和智慧化，從而大大提高工作效率。},
  file = {C:\Users\BlackCat\Zotero\storage\2YTTK9CQ\Li 等。 - 2010 - Multi-Source Information Management System of Rail.pdf}
}

@inproceedings{lindamhadhbiDSOntologyDiseaseSymptomOntology2017,
  title = {{{DS-Ontology}}: {{A Disease-Symptom Ontology}} for {{General Diagnosis Enhancement}}},
  shorttitle = {{{DS-Ontology}}},
  booktitle = {Proceedings of the 2017 {{International Conference}} on {{Information System}} and {{Data Mining}}},
  author = {{Linda Mhadhbi} and {Jalel Akaichi}},
  year = {4 月 1, 2017},
  series = {{{ICISDM}} '17},
  pages = {99--102},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3077584.3077586},
  url = {https://dl.acm.org/doi/10.1145/3077584.3077586},
  urldate = {2023-09-15},
  abstract = {The diagnosis process is often challenging, it involves the correlation of various pieces of information followed by several possible conclusions and iterations of diseases that may overload physicians when facing urgent cases that may lead to bad consequences threatening people's lives. The physician is asked to search for all symptoms related to a specific disease. To make this kind of search possible, there is a strong need for an effective way to store and retrieve medical knowledge from various datasets in order to find links between human disease and symptoms. For this purpose, we propose in this work a new Disease-Symptom Ontology (DS-Ontology). Utilizing existing biomedical ontologies, we integrate all available disease-symptom relationships to create a DS-Ontology that will be used latter in an ontology-based Clinical Decision Support System to determine a highly effective medical diagnosis.},
  isbn = {978-1-4503-4833-1},
  langid = {english},
  keywords = {/unread,diagnosis,Disease,Ontology,Protègè,Symptom,已整理,機器學習,知識本體,西醫},
  annotation = {5 citations (Crossref) [2024-03-26]\\
titleTranslation: DS-Ontology：用於增強一般診斷的疾病症狀本體\\
abstractTranslation:  診斷過程通常具有挑戰性，它涉及各種資訊的相關性，然後是幾種可能的結論和疾病的迭代，這些可能會導致醫生在面對緊急病例時超負荷，從而導致威脅人們生命的不良後果。醫生被要求尋找與特定疾病相關的所有症狀。為了使這種搜尋成為可能，迫切需要一種有效的方法來儲存和檢索各種資料集中的醫學知識，以便找到人類疾病和症狀之間的關聯。為此，我們在這項工作中提出了一種新的疾病症狀本體論（DS-Ontology）。利用現有的生物醫學本體，我們整合所有可用的疾病-症狀關係來創建 DS 本體，該本體稍後將用於基於本體的臨床決策支援系統，以確定高效的醫學診斷。},
  file = {C:\Users\BlackCat\Zotero\storage\UJVCJUKG\Mhadhbi 與 Akaichi - 2017 - DS-Ontology A Disease-Symptom Ontology for Genera.pdf}
}

@article{lingchiISKEUnsupervisedAutomatic2021,
  title = {{{ISKE}}: {{An}} Unsupervised Automatic Keyphrase Extraction Approach Using the Iterated Sentences Based on Graph Method},
  shorttitle = {{{ISKE}}},
  author = {{Ling Chi} and {Liang Hu}},
  date = {2021-07-08},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowledge-Based Systems},
  volume = {223},
  pages = {107014},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2021.107014},
  url = {https://www.sciencedirect.com/science/article/pii/S095070512100277X},
  urldate = {2023-10-23},
  abstract = {The automatic extraction of key information is necessary for knowledge discovery in this era of rapid knowledge growth. The extraction of key information can also help researchers quickly obtain the information they want instead of reading through all potential documents. Recently, researchers have refocused their attention from words to sentences because utilizing sentences outperforms with respect to illustrating semantics and reduces the calculation complexity. We present a novel and lightweight automatic keyphrase extraction algorithm that does not depend on any external resources, including an external dictionary or corpus. Unlike traditional graph-based algorithms that iterate words to generate keyphrase lists, our proposal uses iterated sentences to rank words and generate keyphrase lists for the semantic information of sentences that are more complete than the word. We initialize the values of words with weighted information and generate a sentence score using these values. Then, we integrate sentences to update their values; hence, the values of the words are updated with the sentence information. We iterate this process until the values of the sentences and words converge. The proposed method is based on a measurement of the relations between sentences and an evaluation of the flow of these relations in an easily understood manner. These relationships are based on the hypothesis that the causality between adjacent sentences is semantically stronger than the causality between words. We not only increase the extraction accuracy, but also reduce the number of iterations of the algorithm. We compare our proposed method with five strong, popular baseline algorithms on four datasets. The results show that our proposed method performs better than the other algorithms on three evaluation metrics.},
  keywords = {Graph-based,Iterated sentence,Keyphrase extraction,Unsupervised learning,關鍵字},
  annotation = {3 citations (Crossref) [2024-03-26]},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\S756H5RZ\\Chi and Hu - 2021 - ISKE An unsupervised automatic keyphrase extracti.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\NSLB8KD5\\S095070512100277X.html}
}

@article{lingfengzhongComprehensiveSurveyAutomatic2023,
  title = {A {{Comprehensive Survey}} on {{Automatic Knowledge Graph Construction}}},
  author = {{Lingfeng Zhong} and {Jia Wu} and {Qian Li} and {Hao Peng} and {Xindong Wu}},
  year = {9 月 5, 2023},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  issn = {0360-0300},
  doi = {10.1145/3618295},
  url = {https://dl.acm.org/doi/10.1145/3618295},
  urldate = {2023-09-08},
  abstract = {Automatic knowledge graph construction aims to manufacture structured human knowledge. To this end, much effort has historically been spent extracting informative fact patterns from different data sources. However, more recently, research interest has shifted to acquiring conceptualized structured knowledge beyond informative data. In addition, researchers have also been exploring new ways of handling sophisticated construction tasks in diversified scenarios. Thus, there is a demand for a systematic review of paradigms to organize knowledge structures beyond data-level mentions. To meet this demand, we comprehensively survey more than 300 methods to summarize the latest developments in knowledge graph construction. A knowledge graph is built in three steps: knowledge acquisition, knowledge refinement, and knowledge evolution. The processes of knowledge acquisition are reviewed in detail, including obtaining entities with fine-grained types and their conceptual linkages to knowledge graphs; resolving coreferences; and extracting entity relationships in complex scenarios. The survey covers models for knowledge refinement, including knowledge graph completion, and knowledge fusion. Methods to handle knowledge evolution are also systematically presented, including condition knowledge acquisition, condition knowledge graph completion, and knowledge dynamic. We present the paradigms to compare the distinction among these methods along the axis of the data environment, motivation, and architecture. Additionally, we also provide briefs on accessible resources that can help readers to develop practical knowledge graph systems. The survey concludes with discussions on the challenges and possible directions for future exploration.},
  langid = {english},
  keywords = {deep learning,information extraction,knowledge fusion,knowledge graph completion,logic reasoning,Survey,待讀,知識圖譜,知識抽取,知識推理},
  annotation = {5 citations (Crossref) [2024-03-26]\\
Just Accepted\\
titleTranslation: 自動知識圖構建的綜合綜述\\
abstractTranslation:  自動知識圖譜構建旨在製造結構化的人類知識。為此，歷史上人們花費了大量精力從不同的數據源中提取信息豐富的事實模式。然而，最近，研究興趣已轉向獲取信息數據之外的概念化結構化知識。此外，研究人員還一直在探索在多樣化場景下處理複雜施工任務的新方法。因此，需要對范式進行系統審查，以組織數據級別之外的知識結構。為了滿足這一需求，我們綜合考察了300多種方法，總結了知識圖譜構建的最新進展。知識圖譜的構建分為三個步驟：知識獲取、知識細化和知識演化。詳細回顧了知識獲取的過程，包括獲取細粒度類型的實體及其與知識圖的概念聯繫；解決共指問題；並提取複雜場景下的實體關係。該調查涵蓋了知識細化的模型，包括知識圖補全和知識融合。還繫統地提出了處理知識演化的方法，包括條件知識獲取、條件知識圖補全和知識動態。我們提出了一些範式，以沿著數據環境、動機和架構的軸來比較這些方法之間的區別。此外，我們還提供了可訪問資源的簡介，可以幫助讀者開發實用的知識圖譜系統。調查最後討論了未來探索的挑戰和可能的方向。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\D3VRI3TL\\A Comprehensive Survey on Automatic Knowledge Graph Construction.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\K7SJHACL\\Zhong 等。 - 2023 - A Comprehensive Survey on Automatic Knowledge Grap.pdf;D\:\\download\\論文\\A Comprehensive Survey on Automatic Knowledge Graph Construction.pdf}
}

@online{LingYuZhiShiQianRuShiWenXianTanKanMoXingYingYongYuShengWuMingMingShiTiBianShi,
  title = {領域知識嵌入式文獻探勘模型應用於生物命名實體辨識},
  url = {https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/login?o=dnclcdr&s=id=%22110NYCU5114033%22.&searchmode=basic},
  urldate = {2024-03-12},
  langid = {english},
  keywords = {實體識別,知識本體,重要},
  annotation = {titleTranslation: 領域知識嵌入文獻探勘模型抽取生物命名實體},
  file = {C:\Users\BlackCat\Zotero\storage\5FINX56H\login.html}
}

@thesis{LinJiaHongKeChongZuShiZuiJiaHuaGCCBianYiQi2008,
  title = {{{可重組式最佳化GCC編譯器}}},
  author = {{林家弘}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2008},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/32et5v},
  abstract = {現今的編譯器不只是單純轉換高階語言到低階語言，同時也提供了釵h最佳化程序以提升程式的各式效能，例如:程式碼的大小、執行的時間、電源的消耗…等。GCC是一個普遍使用的最佳化編譯器，它的最佳化程序的套用順序是固定的。本論文探討改變最佳化程序的套用順序對程式效能的影響。本論文擴充GCC，使得它可以重組最佳化程序，本論文並且在Mibench的部份範例程式中進行了初步實驗，實驗顯示GCC的可重組式最佳化程序對於程式執行時間平均可提升6.5\%的結果。},
  pagetotal = {41}
}

@thesis{LinLiuYuZhongYiZhengZhuangDeBiaoZhunHuaChuTanYuYingYong2011,
  title = {中醫症狀的標準化初探與應用},
  author = {{林劉育}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2011},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/8mn8bt},
  abstract = {中醫的症狀是構成中醫核心觀念之證候的最重要的結構因素。症狀往往會因地域，氣候，習慣和醫家背景而有不同的敘述。這在古籍上常見。也因此造成中醫分析上的困擾，影響了中醫學的進步。 因此，我們在本論文中提出了「中醫症狀標準化」的概念，將《中醫證候學》書中高達3516筆中醫症狀敘述句，以下列方式呈現： 1.	每一個症狀單元只表示一種內涵 2.	每一個症狀單元都有其分類以及屬性跟其屬性值。 3.	將症狀描述的屬性做量化。 4.	將症狀描述的屬性統一。 5.	意義相同的描述句必須統一為相同的標準化症狀。 症狀標準化可用於產生中醫問診表以利於搜集各種病的症狀，也可用於演算法分析將症狀分群用來驗證中醫理論。本研究將描述症狀如何作標準化以及症狀標準化的步驟，利用統計機率將標準化症狀分群，透過分群結果驗證標準化的症狀是否正確。},
  pagetotal = {54},
  keywords = {中醫,實驗室,標準化,病歷分析},
  file = {C:\Users\BlackCat\Zotero\storage\5DFB6UWQ\林劉育 - 2011 - 中醫症狀的標準化初探與應用.pdf}
}

@thesis{LinMinXiongZhengGuiHuaGaiNianFenXiFangFaYingYongYuJiBingFenLeiDeZhiShiTanSuoJiYuCe2007,
  type = {thesis},
  title = {正規化概念分析方法應用於疾病分類的知識探索及預測},
  author = {{林敏雄} and {Min-Hsiung Lin}},
  date = {2007-06-03},
  url = {http://ir.lib.ncku.edu.tw/handle/987654321/31951},
  urldate = {2022-08-14},
  abstract = {健保局為了解決醫療財務的沈重壓力，控制不斷上漲的醫療費用，擬於民國97 年起，分 4 年導入診斷關係群(Diagnosis Related Group  DRG)包裹給付制度，DRG 的編碼是根據ICD-9-CM 碼(International Classification of Diseases Ninth Revision  Clinical Modification)來編碼，若病歷資料不完整、疾病分類人員編碼不正確，都可能影響醫院的財務收入，所以提昇疾病分類人員的編碼品質變的極為迫切且重要。希望藉由本研究萃取出隱含在出院病歷摘要編碼訊息來解決所面臨的新問題，期望以知識外顯化來協助疾病分類人員編碼。  本研究以模糊正規概念分析(FFCA)及自然語言處理(NLP)萃取出院病歷摘要內容，以圖形化的方式呈現疾病分類關鍵字詞的概念與關係，並提供疾病分類隱性知識的探索，強化整個疾病分類知識的架構、預測病歷可能編碼，藉以提升疾病分類人員編碼效率、增加正確性。},
  langid = {Chi},
  annotation = {Accepted: 2009-05-08T05:42:14Z\\
abstractTranslation:  健保局為了解決醫療財務的沉重壓力，控制不斷上漲的醫療費用，擬定於民國97年起分4年導入診斷關係群（診斷相關組DRG）包裹給付制度，DRG的編碼是根據ICD-9- CM碼(國際疾病分類第九版臨床修改)來編碼，若病歷數據不完整、疾病分類人員編碼不正確，都可能影響醫院的收入，所以提升疾病分類人員的編碼質量變的極為迫切且重要希望柬埔寨由本研究提取出隱含在出院病歷摘要編碼編碼來解決所面臨的新問題，希望以知識外顯化來幫助疾病分類人員編碼。本研究以模糊正規概念分析（FFCA）及語言自然處理(NLP)提取出院疾病歷摘要內容，以圖形化的方式呈現分類關鍵字詞的概念與關係，並提供疾病分類隱性知識的探索，強化整個疾病分類知識的架構、預測疾病歷可編碼、藉以提升疾病分類人員編碼效率、增加正確性。\\
titleTranslation: 正規化概念分析方法評估疾病分類的知識探索及預測},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\JNBN53M2\\林敏雄 與 Lin - 2007 - 正規化概念分析方法應用於疾病分類的知識探索及預測.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\N3IEEP94\\31951.html}
}

@thesis{LinQingChiZaiCilkZhiXingShiQiXiTongShangYanZhiYiGeKongZhiPingXingYuYan1999,
  title = {{{在Cilk執行時期系統上研製一個控制平行語言}}},
  author = {{林清池}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {1999},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/h7s3c2},
  abstract = {在這篇論文中，我們在Cilk執行時期系統上研製一個高階控制平行語言，稱之為CCC。CCC程式語言提出了平行區段的觀念，使得程式設計師對於程式的設計能更加具有結構性，以及訊息佇列的訊息溝通介面，增加了程式的擴展性。這些特性使得平行程式設計師不需要考慮到太多機器底層的東西，如工作分配、負載平衡以及訊息傳遞的問題。程式設計師得以專心發展平行演算法，而CCC編譯器則負責將高階的平行程式碼轉成低階的平行程式碼。},
  pagetotal = {71}
}

@online{linRADITRetrievalAugmentedDual2023,
  title = {{{RA-DIT}}: {{Retrieval-Augmented Dual Instruction Tuning}}},
  shorttitle = {{{RA-DIT}}},
  author = {Lin, Xi Victoria and Chen, Xilun and Chen, Mingda and Shi, Weijia and Lomeli, Maria and James, Rich and Rodriguez, Pedro and Kahn, Jacob and Szilvasy, Gergely and Lewis, Mike and Zettlemoyer, Luke and Yih, Scott},
  date = {2023-11-05},
  eprint = {2310.01352},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.01352},
  url = {http://arxiv.org/abs/2310.01352},
  urldate = {2024-04-16},
  abstract = {Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9\% in 0-shot setting and +1.4\% in 5-shot setting on average.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,LLM,RAG,RALM,已整理,待讀,微調,重要},
  annotation = {titleTranslation: RA-DIT：檢索增強雙指令調優\\
abstractTranslation:  檢索增強語言模型 (RALM) 透過從外部資料儲存存取長尾和最新知識來提高效能，但建構起來具有挑戰性。現有方法要么需要對 LM 預訓練進行昂貴的特定於檢索的修改，要么使用數據存儲的事後集成，從而導致性能不佳。我們引入檢索增強雙指令調優 (RA-DIT)，這是一種輕量級微調方法，透過對任何法學碩士進行檢索功能改造，提供了第三種選擇。我們的方法透過兩個不同的微調步驟進行操作：（1）一個更新預先訓練的LM 以更好地使用檢索到的信息，而（2）另一個更新檢索器以返回更相關的結果，這是LM 的首選。透過對需要知識利用和情境感知的任務進行微調，我們證明每個階段都會產生顯著的效能改進，並且使用這兩者會帶來額外的收益。我們的最佳模型 RA-DIT 65B 在一系列知識密集型零學習和小樣本學習基準中實現了最先進的性能，在 0 中顯著優於現有的上下文 RALM 方法高達 +8.9\% - 射擊設置，5 次射擊設定平均+1.4\%。},
  note = {Comment: v3: Add the performance of full RA-DIT model on commonsense reasoning tasks 24 pages},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\YSG3UJMX\\Lin 等。 - 2023 - RA-DIT Retrieval-Augmented Dual Instruction Tunin.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\CUVTYDKZ\\2310.html}
}

@thesis{LinShanZhiShiYongYiGeQuanChongShiJiYinYanSuanFaZiDongXuanZeGCCZuiJiaHuaXuanXiang2005,
  title = {{{使用一個權重式基因演算法自動選擇GCC最佳化選項}}},
  author = {{林善治}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2005},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/xyu398},
  abstract = {通常編譯器都會提供大量的最佳化選項，讓使用者選擇最適合自己程式的最佳化選項。因為大部分的使用者都不熟悉這些最佳化選項，所以提供一個能夠自動選擇最佳化選項的工具是很重要的。這類工具大多使用基因演算法來搜尋較適合的最佳化選項。本論文提出一個權重式的基因演算法來改善搜尋的效率。這個權重式的基因演算法除了利用傳統基因演算法的交配運算與突變運算外，再增加加權運算與群組運算。加權運算用來追蹤個別最佳化選項與被編譯程式的適用性。群組運算用來追蹤共用一群最佳化選項與被編譯程式的適用性。透由加權運算與群組運算可縮減基因演算法的搜尋空間及加快基因演算法的演進速度。本論文以gcc為研究之編譯器，以MiBench為研究之評量程式，進行初步效能評估，初步驗證加權運算與群組運算可縮減基因演算法的搜尋空間及加快基因演算法的演進速度。},
  pagetotal = {55}
}

@thesis{LinWeiHaoKeXueZhongYaoPeiWuXiTong2021,
  title = {科學中藥配伍系統},
  author = {{林威豪}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2021},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/4bcgh8},
  abstract = {本論文開發了一個科學中藥的配藥系統。台灣共有30多家科學中藥藥廠。針對同一個科學中藥方劑，每家科學中藥藥廠的方劑成分及藥材比重可能都會不同。因此，本論文建立一個科學中藥資料庫系統，可以讀取政府公開的科學中藥藥廠產品資訊，包含科學中藥產品、成分及比重。這個科學中藥資料庫系統也具備依照政府最新資訊，自動更新各家藥廠產品資訊的功能。醫院及診所也可以從此科學中藥資料庫系統下載資訊，建構該院所的藥房資料庫。 本科學中藥配藥系統是奠基於我們先前開發的傳統中藥配藥系統，使用多目標最佳化技術，先依多證候選擇療效最佳的複方，再依多療效選擇療效最佳的單方。本系統也支援彈性的多服藥頻率的藥單組合，及藥單的重量限制檢查。本系統執行效率也比之前的系統有明顯的改善。},
  pagetotal = {71},
  keywords = {實驗室}
}

@thesis{LinYanXunYiLiaoZhuSuJuXingYuCeJiZhongWenCiFanYiZhiYanJiu2011,
  type = {mathesis},
  title = {醫療主訴句型預測及中文詞翻譯之研究},
  author = {{林彥勳}},
  date = {2011-01},
  journaltitle = {長榮大學資訊管理學系(所)學位論文},
  institution = {長榮大學},
  doi = {10.6833/CJCU.2011.00194},
  url = {http://www.airitilibrary.com/Publication/alDetailedMesh1?DocID=U0015-2508201113393200&PublishTypeID=P003},
  urldate = {2022-08-14},
  abstract = {病歷摘要是醫護人員用以作為醫病構通橋梁的重要依據，撰寫中文化病歷摘要更是近年來衛生署為了提升病患對本身病情「知的權利」加以推廣之。目前病歷摘要中文化撰寫時所遭遇問題，如：1)中文輸入速度不夠快；2)撰寫花費過多時間；3)醫療詞彙難以明確翻譯成中文表達；4)中英語文法架構差異甚遠，轉譯時容易產生誤差等。 本研究的目的為針對病歷摘要主訴，發展建立一套可供醫護人員於撰寫中文化病歷摘要主訴時，參考之書撰建議系統，其功能包括醫療用語中文化輔助翻譯及主訴句型書寫預測提醒。其特定目標包括：(1) 蒐集並建置醫療專有名詞之中英平行詞庫，包含：病歷主訴資料的文字語料庫、中英平行語料庫、病歷主訴關鍵詞庫、POS-based(part of speech)平行語料庫、Class-based平行語料庫等工作；(2)資料分析與句型預測模型之建立：包含詞彙前後Bi-gram 語言模型、POS-based Bi-gram語言模型、Class-based Bi-gram語言模型以及Partial Sentence Tree 模型；(3) 建置病歷主訴撰寫介面；(4)系統評估。 本研究整合上述方法與模組，開發出一套病歷主訴撰寫系統，包括病歷主訴撰寫介面、句型預測模組副程式與中文翻譯模組副程式等子程式。並進行系統成效評估，主要測試系統的輸入效能及輸出之正確性。 實驗結果顯示，使用者透過POS-based Bigram下一字詞提示功能時，對於英文主訴撰寫之Keystroke數與構句時間能有小幅度改善；而使用完整語句預測模型時，能有效降低輸入之Keystroke數與構句時間，減少主訴撰寫時耗費的時間與人力。The medical records are important information for the communication of the illness condition. In recent years, to compose medical records in Chinese has increased demand since most patients are not aware of English medical terms. While composing a medical records in Chinese, several problems may arise: 1)input method of Chinese is not quick enough; 2)wasted too much time when writing of chief complaint; 3)not accurate translation of medical vocabulary; 4)the grammatical structure is very different. The goal of this paper is to construct a system for chief complaint composing with sentence pattern prediction. Two major modules are developed in this study: 1) chief complaint collection and construction module: to setup original chief complaint corpus, parallel corpus, medical keywords corpus, POS-based corpus, Class-based corpus; 2) data analysis and chief complaints prediction module: bi-gram language model, POS-based bi-gram language model, class-based bi-gram language model and Partial Sentence Tree Model. By integrating of the above modules, chief complaints composition supporting system with friendly input interface, sentence prediction function and chief complaints translation function was built. Several experiments and assessment of the effectiveness of the system was carried out. The experimental results showed that the Keystrokes and composing time of chief complaint composition are improved by adapting the proposed approach. It encourages us that the proposed system can be utilized on chief complaints composing and will be helpful in reducing the burden of medical staffs.},
  issue = {2011年},
  langid = {zh\_CN},
  pagetotal = {1-65},
  keywords = {Chief complaint,language model,partial sentence tree,Partial Sentence Tree,POS-based bi-gram,POS-based Bi-gram,病歷主訴,語言模型},
  annotation = {abstractTranslation:  病歷摘要是繼作為醫病構通橋樑的重要藉鑑，撰寫中文化病歷摘要近年來衛生署為了提升病患對自身疾病的「知的權利」進行推廣之。有時會遇到問題，如：1）中文輸入速度不夠快；2）編寫耗費時間過多；3）醫療文書難以明確翻譯成中文表達；4）中文法律架構差異甚遠，轉譯時容易產生文書等。研究目的為針對病歷摘要主訴，制定了一套供書寫以撰寫中文化病歷摘要主訴句時，參考之書撰稿建議系統，其功能包括醫療用語中文化輔助翻譯及主訴式書寫預測提醒。其特定目標包括：（1）徵集並建置醫療商標申請中英平行語庫，包含：病歷主訴資料的文字語料庫、中英平行語料庫、病歷主訴資料的文字語料庫、病歷主訴資料的文字語料庫、病歷主訴資料的文字語料庫、POS式（詞性） ）平行語料庫、基於類別的平行語料庫等工作；（2）數據分析與句型預測模型的建立：包含詞彙對照Bi-gram語言模型、基於POS的Bi-gram語言模型、基於類別的Bi-gram語言模型模型以及偏句樹模型；(3)建置病歷主訴撰寫介面；(4)系統評估。本研究整合上述方法與模組，開發出一套病歷主訴撰寫系統，包括病歷主訴撰寫介面、句型預測模組副程序與中文翻譯模組副程序等子程序。並進行系統成果評估，主要測試系統的輸入成功及輸出之正確性。實驗結果顯示，用戶使用基於POS的Bigram下一個詞提示功能對於中文主命令編寫時的擊鍵數與構句時間能有小幅改善；而使用完整的預測語句模型時，能有效降低輸入之擊鍵數與構句時間，減少主命令編寫時的繪圖時間與人力。病歷是溝通病情的重要信息。近年來，由於大多數患者不了解英文醫學術語，因此用中文撰寫病歷的需求不斷增加。在用中文書寫病歷時，可能會出現以下幾個問題：1）中文輸入法不夠快捷； 2）寫主訴浪費了太多時間； 3）醫學詞彙翻譯不准確； 4）語法結構差異很大。本文的目標是構建一個基於句型預測的主訴撰寫系統。本研究開發了兩大模塊：1）主訴採集與構建模塊：建立原始主訴語料庫、平行語料庫、醫學關鍵詞語料庫、詞性語料庫、類別語料庫； 2）數據分析和主訴預測模塊：二元語言模型、基於POS的二元語言模型、基於類別的二元語言模型和部分句子樹模型。通過對上述模塊的整合，構建了具有友好輸入界面、句子預測功能和主訴翻譯功能的主訴作文支撐系統。對系統的有效性進行了多次實驗和評估。實驗結果表明，採用該方法後主訴作文的擊鍵次數和撰寫時間得到了改善。這使我們感到鼓舞，所提出的系統可以用於主訴的撰寫，將有助於減輕醫務人員的負擔。\\
titleTranslation: 醫療主訴句型預測及中文詞翻譯之研究},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\LX4IDM6C\\醫療主訴句型預測及中文詞翻譯之研究.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\WU5ULA3L\\醫療主訴句型預測及中文詞翻譯之研究_doi.zip;C\:\\Users\\BlackCat\\Zotero\\storage\\JUUPZ8YQ\\alDetailedMesh1.html}
}

@thesis{LinYuXinZhongYiBingYinJiZangFuBianZhengXiTongDeYanZhi2013,
  title = {中醫病因及臟腑辨證系統的研製},
  author = {{林育欣}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2013},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/g5u6js},
  abstract = {辨證是中醫特有的臨床診斷形式。中醫的證型包含疾病的病因以及病位資訊，根據病患的症狀及體徵資訊來辨識疾病的病因及病位即是中醫的辨證。中醫證型的種類數量很多，每一證型所包含的症狀及體徵數量更是可觀，因此中醫自動辨證系統的研製是中醫實證醫學化的重要研究議題。本論文根據《中醫證候學》的證型架構及審因定位的辨證方法，研製出一個中醫自動辨證系統。此系統包含病因辯證(辨識病因)及臟腑辨證(辨識病位)。本論文也利用《中醫證候學》的證候資訊(包含疾病的病因、病位、及病機資訊)來評估《中醫證候學》的證型架構及審因定位的辨證方法的一致性。評估結果顯示《中醫證候學》的證型架構及審因定位的辨證方法有相當高的一致性。期望本研究所研製的中醫自動辨證系統未來可以作為中醫的研究平台，成為中醫實證醫學化的基礎建設。},
  pagetotal = {69}
}

@thesis{LinZhenDongFuHeLinChuangWenJianJiaGouBiaoZhunDeDianZiBingLiZhiYanZhi2004,
  title = {符合臨床文件架構標準的電子病歷之研製},
  author = {{林振冬}},
  namea = {{王明習} and {Ming-Shi Wang}},
  nameatype = {collaborator},
  date = {2004},
  journaltitle = {工程科學系碩博士班},
  volume = {碩士},
  institution = {國立成功大學},
  location = {台南市},
  url = {https://hdl.handle.net/11296/ks5n7q},
  abstract = {推動電子病歷的實現是醫療資訊發展的重要方向，希望經由病歷的交換與分享機制來合理運用醫療資源，並進而提昇整體醫療環境。在此方面，醫療資訊標準的採用與推廣扮演著相當重要的角色，故而有不少的研究投入於這個領域。但由於醫療院所之資訊系統各不相同，所呈現之電子病歷亦各異其趣，目前衛生署也尚未制定一套可供依循之標準，所以在交換的過程中必須依照交換的對象與內容進行特定之調整。 本研究計畫以臨床文件架構(Clinical Document Architecture, CDA)之架構標準為依據，提出一致性的方式，期能改善以上之缺點。CDA以醫療物件之角度來描述臨床文件，藉此構成電子病歷之元素，CDA可以是電子病歷的一小部分元件、或是外部來源、或是整體架構。因為是國際性的通用標準，也具有較高的擴展性和對未來演變的適應性。在本研究環境中，以醫院內依照時間軸之演進所產生的不同文件構成電子病歷，同時藉由跨院區之交叉查詢與文件調閱效果驗證CDA在交換過程之便利與彈性。},
  pagetotal = {74},
  file = {C:\Users\BlackCat\Zotero\storage\FQT2HNUF\林振冬 - 2004 - 符合臨床文件架構標準的電子病歷之研製.pdf}
}

@thesis{LinZhiYuanDianJiYuXianZhiLuoJiChengShiZhiHeiXiangShiHanShiCengJiCeShiAnLiChanShengQi2011,
  title = {奠基於限制邏輯程式之黑箱式函式層級測試案例產生器},
  author = {{林志遠}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2011},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/hh9hcx},
  abstract = {軟體測試是確保軟體品質最主要的方法。本論文開發一個針對Java程式的類別層級黑箱測試案例產生器。本論文使用類別圖與物件限制語言來描述函式層級的規格，定義函式之內的行為。軟體測試最困難的地方是測試案例的產生，測試案例包含測試輸入與預期輸出。奠基於限制邏輯程式，本論文可以使用一致的方法同時產生測試輸入與預期輸出，這個一致的方法成因於限制邏輯程式所擁有的強大限制式求解能力。這個方法分成三個步驟。第一個步驟系統化地條列出限制式圖上的測試路徑。第二個步驟將每一條測試路徑上的限制式轉換成一個限制邏輯程式的敘述式，這個敘述式的每一個解答就是一組測試輸入與預期輸出。第三個步驟將這些測試輸入與預期輸出轉換成Java測試函式。本論文使用JUnit測試架構自動執行產生的測試案例。},
  pagetotal = {111}
}

@thesis{LinZhongWeiYiGeKongZhiPingXingChengShiYuYanDeYanZhi1998,
  title = {一個控制平行程式語言的研製},
  author = {{林仲偉}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {1998},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/rqammd},
  abstract = {近幾年來，高階的資料平行語方在多處理器電腦或工作站群上，已經漸漸的普遍。不過，需要在不同實體處理器上，執行不一樣程式、處理不一樣資料的問題，並沒有辦法很有效地使用純粹的資料平行語方來實作。另外，有一些應用程式，特別是影像或是訊號處理方面的問題，在使用資料平行的觀念撰寫程式時，沒有很好的擴充性；還有一些科學或工程模擬的程式，需要依照數種不同的法則來模擬實驗。這一類的問題，就需要有支援控制平行的高階程式語方，才可以提供程式設計師簡單易用的使用界面，及有效地處理。 在本篇論文中，我們延伸傳統的C語言，定義了一個支援控制平行的程式語言，稱之為Task Parallel C (TPC)。在TPC這個語言中，我們提供了平行的控制機制，讓程式設計師可以讓一群工作同時地在平行環境中執行；另外，也提供了訊息的傳遞機制，讓需要協調溝通的工作，可以依此交換訊息。我們並實作了一個TPC程式語言編譯器，將TPC程式編譯成使用Message Passing Interface (MPI)當作訊息傳遞界面的C程式。初步的實驗結果，可做為我們未來改進的重要參考。},
  pagetotal = {84}
}

@thesis{LiPeiNingJiYuMoHuLiLunZhiZhongYiBianZheng2015,
  title = {基於模糊理論之中醫辨證},
  author = {{李培寗}},
  namea = {{林迺衛}},
  nameatype = {collaborator},
  date = {2015},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/e9r3b3},
  abstract = {中醫辨證體系非常龐雜，一般醫生臨床難以迅速掌握全貌。本研究希望藉由計算機龐大的記憶能力及迅速的分析能力，研製一個中醫自動辨證系統，做為中醫辨證的研究及教學平台。 《中醫證候學》[1]整理古今中醫辨證文獻，提出一個統一的辨證體系。它先依病因為綱將疾病分為15個證門及45個證類。它接著依病位為目將45個證類再細分為281證型。它最後依病機為科將281個證型細分為2344個證候。本論文根據《中醫證候學》[1]的虛證門，研製一個中醫自動辨證系統。 中醫自動辨證系統需要決定每一個證的特徵症狀集合及設計證的分類機制。本論文考量中醫辨證本具的模糊性，運用模糊集合來表示證的特徵症狀集合，並運用模糊分類來設計證的分類機制。本論文也做了一個初步的實驗來評估使用模糊理論及未使用模糊理論的辨證差異。},
  pagetotal = {79}
}

@thesis{LiPeiQinLeiBieCengJiDanYuanCeShiDeXianZhiShiCeShiAnLiChanShengQi2017,
  title = {類別層級單元測試的限制式測試案例產生器},
  author = {{李培琴}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2017},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/6r239b},
  abstract = {軟體測試自動化可以顯著地降低軟體的開發成本及提高軟體的品質。本論文開發一個針對Java程式的限制式類別層級單元測試的測試案例產生器。限制式測試案例產生技術將測試案例產生問題制定為一個限制滿足問題。本論文使用限制邏輯圖做為測試模型。這個工具會先將狀態圖轉換成限制邏輯圖，限制邏輯圖以圖形的方式描述類別物件的行為，或一串類別函式呼叫的輸入及輸出之間的限制邏輯關係。本論文接著透過物件行為的等價類別分割及測試覆蓋標準管理，來說明將測試案例產生問題轉換成限制滿足問題的過程。本論文也實作了一個資料流程類的全定義使用對測試覆蓋標準。},
  pagetotal = {51}
}

@inproceedings{liPersonalKnowledgeGraph2014,
  title = {Personal Knowledge Graph Population from User Utterances in Conversational Understanding},
  booktitle = {2014 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Li, Xiang and Tur, Gokhan and Hakkani-Tür, Dilek and Li, Qi},
  date = {2014-02},
  pages = {224--229},
  doi = {10.1109/SLT.2014.7078578},
  url = {https://ieeexplore.ieee.org/document/7078578},
  urldate = {2023-12-03},
  abstract = {Knowledge graphs provide a powerful representation of entities and the relationships between them, but automatically constructing such graphs from spoken language utterances presents the novelty and numerous challenges. In this paper, we introduce a statistical language understanding approach to automatically construct personal (user-centric) knowledge graphs in conversational dialogs. Such information has the potential to better understand the users' requests, fulfilling them, and enabling other technologies such as developing better inferences or proactive interactions. Knowledge encoded in semantic graphs such as Freebase has been shown to benefit semantic parsing and interpretation of natural language utterances. Hence, as a first step, we exploit the personal factual relation triples from Freebase to mine natural language snippets with a search engine, and the resulting snippets containing pairs of related entities to create the training data. This data is then used to build three key language understanding components: (1) Personal Assertion Classification identifies the user utterances that are relevant with personal facts, e.g., “my mother's name is Rosa”; (2) Relation Detection classifies the personal assertion utterance into one of the predefined relation classes, e.g., “parents”; and (3) Slot Filling labels the attributes or arguments of relations, e.g., “name(parents): Rosa”. Our experiments using the Microsoft conversational understanding system demonstrate the performance of this proposed approach on the population of personal knowledge graphs.},
  eventtitle = {2014 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  langid = {english},
  keywords = {回收,已整理,知識圖譜,語音識別},
  annotation = {10 citations (Crossref) [2024-03-26]\\
titleTranslation: 對話理解中使用者話語的個人知識圖譜},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\8YB8ALT7\\Li et al. - 2014 - Personal knowledge graph population from user utte.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\5DIJAGFY\\7078578.html}
}

@thesis{LiQiQingHeiXiangShiWangYeYingYongCeShiAnLiChanShengQi2013,
  title = {黑箱式網頁應用測試案例產生器},
  author = {{李麒慶}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2013},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/9m692f},
  abstract = {近代由於網際網路的發達，網頁應用軟體也隨之興起，使得確保網頁應用軟體的品質成為一個重要議題。軟體測試是確保軟體品質主要方法，若軟體測試純靠人力既費時又費力，且易出錯，本論文實作一個黑箱式網頁應用測試案例產生器，可以自動地產生網頁應用測試案例，以降低軟體開發成本與提高軟體品質。本工具首先分析以XML定義而成的規格書，將其轉換成網頁關係圖並交由測試路徑產生器來系統化地條列出網頁關係圖上的測試路徑。接著測試資料產生器會將每條路徑上的節點(web page)所對應到的功能分別產生測試資料，而若是動態網頁需要測試輸入的部分我們將藉由以定義好之類別圖與物件限制語言，轉換成限制邏輯程式的敘述式，來幫助我們求解得到測試輸入與預期輸出。最後根據這些測試資料可產生與JWebUnit網頁測試工具相符之Java測試案例程式碼，使用者便可利用JWebUnit測試平台自動執行所產生出來的測試案例碼。},
  pagetotal = {107}
}

@inproceedings{lisaehrlingerDefinitionKnowledgeGraphs2016,
  title = {Towards a {{Definition}} of {{Knowledge Graphs}}},
  author = {{Lisa Ehrlinger} and {Wolfram Wöß}},
  date = {2016-09-01},
  abstract = {Recently, the term knowledge graph has been used frequently in research and business, usually in close association with Semantic Web technologies, linked data, large-scale data analytics and cloud computing. Its popularity is clearly influenced by the introduction of Google's Knowledge Graph in 2012, and since then the term has been widely used without a definition. A large variety of interpretations has hampered the evolution of a common understanding of knowledge graphs. Numerous research papers refer to Google's Knowledge Graph, although no official documentation about the used methods exists. The prerequisite for widespread academic and commercial adoption of a concept or technology is a common understanding, based ideally on a definition that is free from ambiguity. We tackle this issue by discussing and defining the term knowledge graph, considering its history and diversity in interpretations and use. Our goal is to propose a definition of knowledge graphs that serves as basis for discussions on this topic and contributes to a common vision.},
  keywords = {已整理,知識圖譜,經典},
  note = {探討關於知識圖譜的定義，也提到有許多定義是衝突的。因此被多篇論文用來說明知識圖譜並沒有明確的定義。},
  file = {C:\Users\BlackCat\Zotero\storage\2QIG8UAY\Ehrlinger and Wöß - 2016 - Towards a Definition of Knowledge Graphs.pdf}
}

@thesis{LiuCaiJuMoNiZiLiaoPingXingChengShiYuYanDeXuNiChuLiQi1996,
  title = {模擬資料平行程式語言的虛擬處理器},
  author = {{劉財居}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {1996},
  journaltitle = {資訊工程學系},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/tyk7u2},
  abstract = {一般而言，撰寫平行程式要比傳統依序程式困難。目前的多處理機系統所 提供的程式語言是較低階的程式語言，使得多處理機系統的使用更加困難 。因此，高階平行程式語言對於多處理機系統的發展相當地重要。我們發 展了一個高階平行語言 --- CCC語言。CCC 語言提供了虛擬處理器、同步 執行模式、以及全域命名等特性。為了提供這些特性，CCC 語言提供了一 個結構 --- domain 結構。domain 結構是用於宣告及定義一個平行計算 的環境。讓使用者可以用很簡單的方式來表達平行演算法。    CCC 語言 的編譯器將 CCC 語言轉換成 nCube 多處理機系統上的 C 語言。一般多 處理機系統所擁有的處理器數目為固定的，而使用者所宣告的虛擬處理器 的數目，則隨應用而會有所不同，且往往會大於系統中所提供的處理器的 數目。因此，系統在計算時每個處理器常常必須模擬若干個虛擬處理器。 在本論文中，將探討如何將 CCC 程式中的虛擬處理器對應至 nCube 上的 真實處理器，而保持程式原來的語意。},
  pagetotal = {76}
}

@online{liuCoachLMAutomaticInstruction2024,
  title = {{{CoachLM}}: {{Automatic Instruction Revisions Improve}} the {{Data Quality}} in {{LLM Instruction Tuning}}},
  shorttitle = {{{CoachLM}}},
  author = {Liu, Yilun and Tao, Shimin and Zhao, Xiaofeng and Zhu, Ming and Ma, Wenbing and Zhu, Junhao and Su, Chang and Hou, Yutai and Zhang, Miao and Zhang, Min and Ma, Hongxia and Zhang, Li and Yang, Hao and Jiang, Yanfei},
  date = {2024-03-20},
  eprint = {2311.13246},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.13246},
  url = {http://arxiv.org/abs/2311.13246},
  urldate = {2024-05-04},
  abstract = {Instruction tuning is crucial for enabling Language Learning Models (LLMs) in responding to human instructions. The quality of instruction pairs used for tuning greatly affects the performance of LLMs. However, the manual creation of high-quality instruction datasets is costly, leading to the adoption of automatic generation of instruction pairs by LLMs as a popular alternative. To ensure the high quality of LLM-generated instruction datasets, several approaches have been proposed. Nevertheless, existing methods either compromise dataset integrity by filtering a large proportion of samples, or are unsuitable for industrial applications. In this paper, instead of discarding low-quality samples, we propose CoachLM, a novel approach to enhance the quality of instruction datasets through automatic revisions on samples in the dataset. CoachLM is trained from the samples revised by human experts and significantly increases the proportion of high-quality samples in the dataset from 17.7\% to 78.9\%. The effectiveness of CoachLM is further assessed on various real-world instruction test sets. The results show that CoachLM improves the instruction-following capabilities of the instruction-tuned LLM by an average of 29.9\%, which even surpasses larger LLMs with nearly twice the number of parameters. Furthermore, CoachLM is successfully deployed in a data management system for LLMs at Huawei, resulting in an efficiency improvement of up to 20\% in the cleaning of 40k real-world instruction pairs. We release various assets of CoachLM, including the training data, code and test set (https://github.com/lunyiliu/CoachLM).},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,未整理},
  annotation = {titleTranslation: CoachLM：自動指令修訂提高了 LLM 指令調優中的資料品質\\
abstractTranslation:  指令調整對於使語言學習模型 (LLM) 響應人類指令至關重要。用於調優的指令對的品質極大地影響了 LLM 的效能。然而，手動建立高品質的指令資料集成本高昂，因此法學碩士採用自動產生指令對作為流行的替代方案。為了確保法學碩士產生的教學數據集的高質量，人們提出了幾種方法。然而，現有的方法要麼透過過濾大部分樣本來損害資料集的完整性，要麼不適合工業應用。在本文中，我們沒有丟棄低品質的樣本，而是提出了 CoachLM，這是一種透過自動修改資料集中的樣本來提高教學資料集品質的新方法。 CoachLM 使用人類專家修改的樣本進行訓練，將資料集中高品質樣本的比例從 17.7\% 顯著提高到 78.9\%。 CoachLM 的有效性在各種實際教學測驗集上進一步評估。結果表明，CoachLM 將指令調校的 LLM 的指令追蹤能力平均提高了 29.9\%，甚至超過了參數數量接近兩倍的大型 LLM。此外，CoachLM已成功部署在華為法學碩士資料管理系統中，在4萬個真實指令對的清理中，效率提升高達20\%。我們發布了CoachLM的各種資產，包括訓練資料、程式碼和測試集（https://github.com/lunyiliu/CoachLM）。},
  note = {Comment: Accepted by ICDE 2024},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\M8KD2UXI\\Liu 等。 - 2024 - CoachLM Automatic Instruction Revisions Improve t.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\UB4FXU9I\\2311.html}
}

@article{liuExplainablePersonalizedCognitive2023,
  title = {An {{Explainable}} and {{Personalized Cognitive Reasoning Model Based}} on {{Knowledge Graph}}: {{Toward Decision Making}} for {{General Practice}}},
  shorttitle = {An {{Explainable}} and {{Personalized Cognitive Reasoning Model Based}} on {{Knowledge Graph}}},
  author = {Liu, Qianghua and Tian, Yu and Zhou, Tianshu and Lyu, Kewei and Wang, Zhixiao and Zheng, Yixiao and Liu, Ying and Ren, Jingjing and Li, Jingsong},
  date = {2023},
  journaltitle = {IEEE Journal of Biomedical and Health Informatics},
  pages = {1--12},
  issn = {2168-2208},
  doi = {10.1109/JBHI.2023.3312154},
  url = {https://ieeexplore.ieee.org/abstract/document/10239383?casa_token=HHUADuqSrDQAAAAA:KO6OTjLBkP8v1pmJwVKTpShAbQMum1sNMY01-yeo2ByziQMuxnFkdBSv-ja0VEH19S73EKgBdw},
  urldate = {2024-01-17},
  abstract = {General practice plays a prominent role in primary health care (PHC). However, evidence has shown that the quality of PHC is still unsatisfactory, and the accuracy of clinical diagnosis and treatment must be improved in China. Decision making tools based on artificial intelligence can help general practitioners diagnose diseases, but most existing research is not sufficiently scalable and explainable. An explainable and personalized cognitive reasoning model based on knowledge graph (CRKG) proposed in this paper can provide personalized diagnosis, perform decision making in general practice, and simulate the mode of thinking of human beings utilizing patients' electronic health records (EHRs) and knowledge graph. Taking abdominal diseases as the application point, an abdominal disease knowledge graph is first constructed in a semiautomated manner. Then, the CRKG designed referring to dual process theory in cognitive science involves the update strategy of global graph representations and reasoning on a personal cognitive graph by adopting the idea of graph neural networks and attention mechanisms. For the diagnosis of diseases in general practice, the CRKG outperforms all the baselines with a precision@1 of 0.7873, recall@10 of 0.9020 and hits@10 of 0.9340. Additionally, the visualization of the reasoning process for each visit of a patient based on the knowledge graph enhances clinicians' comprehension and contributes to explainability. This study is of great importance for the exploration and application of decision making based on EHRs and knowledge graph.},
  eventtitle = {{{IEEE Journal}} of {{Biomedical}} and {{Health Informatics}}},
  langid = {english},
  keywords = {可解釋性,已整理,略讀,知識圖譜,辨證,醫療},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於知識圖譜的可解釋和個性化認知推理模型：面向一般實踐的決策},
  note = {還沒看完
\par
很完整的研究，使用知識圖譜建立圖神經網路後，結合病患資料可以做到辨證推理。此外結合注意力機制能彈性調整感性與知性(?)},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\XFP2NGSW\\Liu 等。 - 2023 - An Explainable and Personalized Cognitive Reasonin.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\VSATXFU4\\10239383.html}
}

@article{LiuHongYanJiYuDianZiBingLiJieGouHuaDeMingLaoZhongYiJingYanShuJuWaJueYanJiu2015,
  title = {基于电子病历结构化的名老中医经验数据挖掘研究},
  author = {{刘鸿燕} and {胡红濮} and {张越}},
  date = {2015},
  journaltitle = {医学信息学杂志},
  number = {12},
  pages = {6},
  doi = {10.3969/j.issn.1673-6036.2015.12.003},
  abstract = {数据挖掘在中医药信息化领域的重要地位日益突显,在名老中医经验传承方面尤为突出.名老中医经验数据挖掘的基础为结构化的中医电子病历,详细阐述中医电子病历的表达架构,数据采集技术,名老中医经验数据挖掘中对显性知识和隐性知识的常用挖掘方法,提出存在的问题及相应建议.},
  keywords = {中醫,數據挖掘,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\ZD3WJ23J\刘鸿燕 等。 - 2015 - 基于电子病历结构化的名老中医经验数据挖掘研究.pdf}
}

@thesis{LiuJianHongDianJiYuBeiShiWangLuDeZhongYiZhengXingBianShi2012,
  title = {奠基於貝氏網路的中醫證型辨識},
  author = {{劉建宏}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2012},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/5v3ajf},
  abstract = {中醫的證型含攝了一個疾病的病因與病位的資訊，因此，根據病人的症狀辨識證型是中醫論治的核心。中醫證型的種類為數不少，症狀的種類也非常龐大，而且證型和症狀之間的關係更是錯綜複雜。若要完整的記憶這些錯綜複雜的關係常常超出系統的資訊儲存容量，而證型和症狀之間錯綜複雜的關係，目前仍然有許多的不確定性存在。本論文將運用貝氏網路的近似演算法來克服中醫證型辨識資訊容量過大及資訊不確定性的問題。 本論文將參考《中醫證候學》的證型架構。《中醫證候學》在證型與症狀間加了一層證象，含攝一個疾病的病機的資訊。因此，每一個證型是由一些證象組合而成，每一個證象是由一些症狀組合而成。此外，證象在證型裡出現的機率不盡相同，依機率由高至低，分別以主證象、副證象、及賓證象表示。同一個證象在不同的證型裡，它的症狀組合也會稍有不同。 本論文將分別比較兩種辨證方式的效果。第一種辨證分式是建構證型、證象與症狀三層的貝氏網路，給予症狀集合經貝氏網路求得各個證型的機率值。第二種辨證分式是建構證象與症狀兩層的貝氏網路，給予症狀集合經貝氏網路求得各個證象機率值，而證型的機率值為加總證象標準化權重值與證象機率值的乘積(證象標準化權重值依據證象主副賓關係給定)。本論文研究的成果希望有助於奠定中醫證型標準化及證象標準化的基礎。},
  pagetotal = {39}
}

@article{LiuLiuMingMingShiTiShiBieYanJiuZongShu2018,
  title = {命名实体识别研究综述},
  author = {{刘浏} and {王东波}},
  date = {2018},
  journaltitle = {情报学报},
  shortjournal = {Journal of the China Society for Scientific andTechnical Information},
  volume = {37},
  number = {3},
  pages = {329--340},
  doi = {10.3772/j.issn.1000-0135.2018.03.010},
  url = {http://d.wanfangdata.com.cn/periodical/qbxb201803010},
  urldate = {2022-08-14},
  abstract = {命名实体识别一直以来都是信息抽取、自然语言处理等领域中重要的研究任务,随着机器学习技术的新发展,数字人文研究的兴起,事件知识和实体知识变得越发重要,命名实体识别焕发出新的发展动力.本文详细梳理了命名实体识别从提出至今的发展脉络,从实体的定义、重要的评测会议、主流的研究方法研究的应用价值等角度,全面考察了该领域的研究现状,并分析了命名实体识别未来的发展趋势.},
  langid = {zh\_CN},
  keywords = {entity extraction,information extraction,Journal of the China Society for Scientific andTechnical Information,Named Entity Recognition,信息抽取,命名实体识别,实体挖掘,情报学报,王东波},
  annotation = {南京大学信息管理学院,南京 210023;江苏省数据工程与知识服务重点实验室(南京大学),南京 210023南京农业大学信息科学技术学院,南京 210095;江苏省数据工程与知识服务重点实验室(南京大学),南京 210023\\
国家社会科学基金 国家自然科学基金~江苏省普通高等学校学术学位研究生科研创新计划\\
2018-07-04 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 實體識別研究綜述\\
abstractTranslation:  實體命名識別一直以來都是信息採集、自然語言處理等領域中重要的研究任務，隨著機器學習技術的新發展、數字人文研究的興起，事件知識和實體知識生成越發重要，命名實體識別煥發論文詳細梳理了命名實體識別，從提出迄今為止的發展脈絡，從實體的定義、重要的股市會議、主流的研究方法研究的應用等角度，全面照亮了該領域的研究現狀，並分析了命名識別實體未來的發展趨勢。},
  file = {C:\Users\BlackCat\Zotero\storage\MVEMY376\刘 與 王 - 2018 - 命名实体识别研究综述.pdf}
}

@article{LiuYiBinJiYuPoSuBeiYeSiHeword2vecDeZhongYiDianZiBingLiWenBenXinXiChouQu2020,
  title = {基于朴素贝叶斯和word2vec的中医电子病历文本信息抽取},
  author = {{刘一斌} and {叶辉} and {易珺} and {曹东}},
  date = {2020},
  journaltitle = {世界科学技术：中医药现代化},
  volume = {22},
  number = {10},
  pages = {6},
  abstract = {目的本文主要讨论如何用机器学习算法对非结构化的中医电子病历文本进行信息的分类抽取,抽取出症状,处方,治法等有用信息.方法先将电子病历文本进行分词,然后进行标签标注,采用朴素贝叶斯和word2vec算法训练形成模型,最后进行模型测试.结果实验结果表明,该算法模型的信息抽取查准率可达80\%以上.结论该研究在中医电子病历文本信息抽取领域做出了初步探索,为进一步进行中医药领域的数据挖掘和科研工作提供了良好的基础.},
  keywords = {中醫,樸素貝葉斯,訊息抽取,詞向量,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\GAHANTF6\刘一斌 等。 - 2020 - 基于朴素贝叶斯和word2vec的中医电子病历文本信息抽取.pdf}
}

@thesis{LiuYiLunZhongYiYaoCaiZhiShiBenTiJiZhongYiYaoCaiCiKuZhiYanZhi2018,
  title = {中醫藥材知識本體及中醫藥材辭庫之研製},
  author = {{劉羿綸}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2018},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/dk4ukn},
  abstract = {中醫藥從古至今已有二千多年歷史，由於經歷不同朝代的不同書寫及記錄方式，使得中醫藥之敘述豐富多樣。為了促進中醫藥的資訊化，本研究團隊首先進行中醫藥材之標準化，並建構一個中醫藥材知識本體。為了方便持續進行中醫藥材之標準化，本研究團隊也建構一個中醫藥材辭庫。中醫藥材知識本體及中醫藥材辭庫的開發是中醫藥資訊化的基礎建設。 本論文以本研究團隊先前建構的包含221個藥材功效、155味藥材與112帖方劑的中醫藥材辭庫為基底，新增了7個藥材功效、77味藥材與94帖方劑，主要是中醫實證相關的藥材功效、藥材與方劑。根據新增的藥材功效及藥材，使用本研究團隊開發的藥材配伍系統，新增的94帖方劑中，93帖方劑的所有藥材都能以首選藥材配伍出來。剩下1帖方劑為養生方劑，強調食物性藥材，而非藥材功效的效力，因此有一個藥材沒能以首選藥材配伍出來。另外，本論文也加強了本研究團隊先前建構的中醫藥材辭庫的查詢及維護功能。},
  pagetotal = {76},
  keywords = {實驗室}
}

@thesis{LiuZongXinYiGeDianJiYuHuoDongTuDeCeShiAnLiChanShengQi2008,
  title = {一個奠基於活動圖的測試案例產生器},
  author = {{劉宗鑫}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2008},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/w55569},
  abstract = {軟體測試是用來確保軟體品質的主要活動。本論文使用統一塑模語言的活動圖和物件限制語言來定義 Java 方法的規格。本論文發展了一套半自動的工具來產生 Java 方法的測試程式。這個工具是由四個部份所組成：圖形讀取器、測試路徑產生器、測試路徑執行器、和測試程式產生器。圖形讀取器讀入活動圖並將它轉換成內部的資料結構。測試路徑產生器列舉所有可能的測試路徑。根據使用者給定的輸入值，測試路徑執行器模擬執行一條測試路徑。它可以判斷此測試路徑是否是可實現的和已選取的可實現路徑滿足了何種測試覆遞郱ョC測試程式產生器產生 JUnit Java 測試類別。},
  pagetotal = {57}
}

@inproceedings{liWhereShallWe2021,
  title = {Where Shall We Log? Studying and Suggesting Logging Locations in Code Blocks},
  shorttitle = {Where Shall We Log?},
  booktitle = {Proceedings of the 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Li, Zhenhao and Chen, Tse-Hsun (Peter) and Shang, Weiyi},
  year = {1 月 27, 2021},
  series = {{{ASE}} '20},
  pages = {361--372},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3324884.3416636},
  url = {https://dl.acm.org/doi/10.1145/3324884.3416636},
  urldate = {2024-01-16},
  abstract = {Developers write logging statements to generate logs and record system execution behaviors to assist in debugging and software maintenance. However, deciding where to insert logging statements is a crucial yet challenging task. On one hand, logging too little may increase the maintenance difficulty due to missing important system execution information. On the other hand, logging too much may introduce excessive logs that mask the real problems and cause significant performance overhead. Prior studies provide recommendations on logging locations, but such recommendations are only for limited situations (e.g., exception logging) or at a coarse-grained level (e.g., method level). Thus, properly helping developers decide finer-grained logging locations for different situations remains an unsolved challenge. In this paper, we tackle the challenge by first conducting a comprehensive manual study on the characteristics of logging locations in seven open-source systems. We uncover six categories of logging locations and find that developers usually insert logging statements to record execution information in various types of code blocks. Based on the observed patterns, we then propose a deep learning framework to automatically suggest logging locations at the block level. We model the source code at the code block level using the syntactic and semantic information. We find that: 1) our models achieve an average of 80.1\% balanced accuracy when suggesting logging locations in blocks; 2) our cross-system logging suggestion results reveal that there might be an implicit logging guideline across systems. Our results show that we may accurately provide finer-grained suggestions on logging locations, and such suggestions may be shared across systems.},
  isbn = {978-1-4503-6768-4},
  langid = {english},
  keywords = {log,已整理,機器學習},
  annotation = {20 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\PWP9RWYX\Li et al. - 2021 - Where shall we log studying and suggesting loggin.pdf}
}

@thesis{LiXinLongZhongYiShiBianZhengLunZhiShiMianZhengZhiShiTuPuGouJianFangFaYanJiu2018,
  title = {中医师辨证论治失眠症知识图谱构建方法研究},
  author = {{李新龙}},
  date = {2018},
  institution = {中国中医科学院},
  url = {http://cdmd.cnki.com.cn/Article/CDMD-84502-1018284602.htm},
  urldate = {2022-10-18},
  abstract = {研究目的梳理中医师辨证论治知识本体框架,探索基于真实世界观察性研究数据的中医师辨证论治知识图谱构建流程,展示医师个体化的辨证论治知识体系,为挖掘和分析医师隐性思维及其诊疗特点提供新的工具和系统流程,为中医师经验传承和创新奠定方法学基础.研究方法1)基于辨证论治失眠症真实诊疗过程的数据采集和预处理:依托国家自然基金重点项目"阶梯递进的辨证论治疗效评价方法研究",在真实世界诊疗环境下,基于临床科研信息共享系统,前瞻性连续采集不同中医师一段时间内辨证论治失眠症实际诊疗数据,采用ETL软件Medical Integrator Studio,通过数据的抽取,数据转换及数据装载功能,完成对不同中医师诊疗数据的预处理.2)基于文献研究的中医辨证论治知识本体框架构建:以"中医","医案","病案","病历","病例报告",和"指南","规范","标准","路径"为检索词,检索中国学术期刊全文数据库,中国生物医学文献数据库,万方数据库,并通过人工检索包含中医药辨证论治知识体系的相关教材.文献管理软件采用NoteExpress,通过初筛和全文筛选后纳入合格文献,提取辨证论治基本元素,采用Gephi软件对其进行共现分析和可视化展示.3)融合复杂网络优化策略的辨证论治知识图谱构建:以真实临床诊疗数据为主,通过数据导出,共现矩阵构建,数据标注,数据导入,数据核查,图谱构建等步骤,构建不同中医师辨证论治初步知识图谱;综合运用专家访谈,文献研究,复杂网络等方法对初步图谱进行修订和优化.知识图谱构建采用Gephi软件,复杂网络构建采用Liquorice软件.研究结果1)三位中医师辨证论治诊疗数据库建立:截止至2016年12月31日,共纳入三个队列431例,1022诊次数据,其中医师A纳入失眠患者85例,259诊次;医师B纳入失眠患者101例,245诊次;医师C纳入失眠患者245例,518诊次.形成了"人机结合,以人为主"的个性化预处理策略:构建了包括数据调取,核查,预处理规则制订,规则反馈与修订,规则导入与批量处理,处理后核查等步骤在内的预处理流程,共建立规则11119条,对症状,诊断,中药/针灸治法,中药/针灸方名,中药,西药/中成药,实验室检查,物理检查等数据进行了不同程度的个体化预处理.2)辨证论治知识本体的框架形成:共检索到文献5555篇,剔除库间和库内重复文献1912篇,通过标题和摘要初筛得到文献370篇,通过全文筛选剔除文献328篇,最终纳入42篇文献,涉及涉及指南22项(占52.4\%),标准17项(占40.5\%),规范1项(占2.4\%),教材2部(占4.8\%).形成包含"临床表现","诊断","治疗","疗效"在内的4个域,53个基本元素,9类元素间联系在内的辨证论治知识本体框架.3)构建并优化三位中医师辨证论治失眠症的知识图谱:构建了三位中医师辨证论治失眠症的临床表现,诊断,治疗图谱;融合复杂网络,文献研究,专家访谈等方法对三位中医师核心方药及其有效人群特征进行了挖掘,并基于核心方有效人群构建了优化后的辨证论治知识图谱;初步形成一套构建中医师辨证论治知识图谱的系统流程.研究结论本研究以三位中医师辨治失眠症为例,构建了其个性化的辨证论治知识图谱,并初步形成了一套中医辨证论治知识图谱的构建流程,研究表明基于真实世界临床诊疗数据,采用知识图谱技术是可视化展示中医师个体化辨证论治失眠知识体系的有效手段.中医临床科研信息共享系统能够提供结构化,同源的真实世界临床诊疗数据,为中医师知识图谱的构建奠定良好的数据基础.在此基础上,通过"人机结合,以人为主"的数据预处理,辨证论治知识本体构建,知识本体与临床诊疗实体的映射,知识图谱的构建与优化,疗效评价与核心方药刻画等流程,可以构建出中医师辨证论治知识图谱.将知识图谱构建技术与辨证论治疗效评价相结合,建立中医师个体诊疗知识图谱,能够为挖掘,展示医师隐性思维及其诊疗特点提供新的工具和系统流程,为中医师经验传承和创新奠定方法学基础.},
  file = {C:\Users\BlackCat\Zotero\storage\UHGT4WAQ\李 - 2018 - 中医师辨证论治失眠症知识图谱构建方法研究.pdf}
}

@thesis{LiYiHongYiJinSiZiCiBiDuiFangFaFuZhuZhongYiZhengZhuangBiaoZhunHua2012,
  title = {以近似字詞比對方法輔助中醫症狀標準化},
  author = {{李宜泓}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2012},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/bwz4hy},
  abstract = {中醫擁有眾多診斷和治療疾病的經典古籍，是一個龐大且有價值的知識寶庫。診斷疾病時，症狀是非常重要的參考因素。由於各家經典古籍記載的症狀用詞不同，且相同的症狀名稱其定義也有差異，我們稱其是症狀尚未標準化。症狀尚未標準化是使得後人欲進行系統性的科學研究，遭遇龐大的困難之一。若中醫使用的症狀名稱與定義能趨於一致，中醫將能更順利且從不同觀點深入地研究前人的智慧結晶，電腦科學亦能協助中醫進行各種規模之研究。 本論文提出一套架構、流程與電腦系統，用電腦科學領域發展已久的完整和近似字詞比對方法，輔以資料庫的案例式推論（Case-based reasoning）學習機制和中醫師諮詢的三方協同合作，協助中醫症狀標準化。我們已經整理《中醫證候學》記載的一部分症狀，透過林劉育製作的問診式階層式標準化症狀清單，完成此書燥證門證象的部分症狀之標準化，以及建立一部分的中醫症狀同義詞之資料庫。 中醫症狀同義詞資料庫將可用於製作臨床看診的問診表，用以探索疾病作用於身體的路徑，亦能持續標準化各家經典古籍的症狀，使資料保持一致性且更為豐富。症狀同義詞資料庫將會促成更深入地研究中醫症狀之間的差異，也會使電腦科學能輔助中醫進行辨證和分析資料，也能提供現代中醫電子病歷欠缺的症狀轉換到統一標準所需的資料。本研究未來可以結合數位典藏的資料，透過擷取和分析古籍記載的資訊，和歷史學家一起研究中國各朝代和各個地區的流行病學，讓我們從不同角度了解古人的生活百態。},
  pagetotal = {54},
  keywords = {中醫,實驗室,標準化,病歷分析},
  file = {C:\Users\BlackCat\Zotero\storage\MNH8B9JX\李宜泓 - 2012 - 以近似字詞比對方法輔助中醫症狀標準化.pdf}
}

@thesis{LiYiLingZhiYuanShiYongZheDingYiXingTaiDeHeiXiangCeShiAnLiChanSheng2020,
  title = {支援使用者定義型態的黑箱測試案例產生},
  author = {{黎怡伶}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2020},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/byanes},
  abstract = {軟體測試自動化可以同時降低軟體的開發成本及提高軟體的品質，本研究團隊已開發一個針對Java整數型態及整數陣列型態的黑箱測試案例產生器。此測試案例產生器將測試案例產生問題定義為一個限制滿足問題。此測試案例產生器使用統一塑模語言及物件限制語言做為規格語言。它將規格轉換成限制邏輯圖做為測試模型，系統地條列圖上的路徑來滿足測試覆蓋標準，將路徑上的限制式轉換成限制邏輯程式，並求解得到測試資料，最後再根據測試資料產生測試腳本。 為了擴充之前的系統，本論文定義了一個支援Java語言的系統定義型態及使用者定義型態的型態系統。本論文根據此型態系統，擴充及開發可以描述及分析此型態系統的OCL語言及OCL處理器。本論文也根據此型態系統，擴充黑箱測試案例產生器，支援使用者定義型態的測試案例產生。},
  pagetotal = {215},
  keywords = {實驗室}
}

@thesis{LiYiXuanVoiceComposerYiGeYuYinYingYongChengShiDeKaiFaGongJu,
  title = {{{VoiceComposer}}:一個語音應用程式的開發工具},
  author = {{李宜軒}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/s68y6t},
  abstract = {語音應用程式提供使用者透過語音的方式得到便捷的資訊與服務。快速地開發語音應用程式變成釵h提供語音資訊與服務的公司的迫切期望。這篇論文描述一個語音應用程式的開發工具Voice Composer。Voice Composer 能夠幫助開發者快速的開發及維護語音應用程式。 Voice Composer的特性包括使用視覺化的方式編輯語音對話流程，使用元件化的方式開發可重複使用的對話元件，及使用系統化的方式存取資料庫},
  pagetotal = {139}
}

@article{LiYouPingGuanYuZhongYiYaoLiaoXiaoPingJieHeJiLiYanJiuDeSiKao2003,
  title = {关于中医药疗效评价和机理研究的思考},
  author = {{李幼平} and {程峰} and {冯莉}},
  date = {2003},
  journaltitle = {世界科学技术-中药现代化 2003年5卷2期  17-20页  ISTIC PKU CSCD},
  doi = {10.3969/j.issn.1674-3849.2003.02.005},
  url = {http://doc.paperpass.com/journal/20030065sjkxjs-zyxdh.html},
  urldate = {2022-10-23},
  abstract = {中药和西药两种疗效评价体系各有其优缺点,应当相互借鉴,取长补短,循证医学疗效评价体系可作为二者有益的补充和借鉴.选择西药无计可施而中医药疗效确切的常见病,多发病,疑难病,借鉴循证医学的原理和方法,收集,整理中医药治疗的现有临床研究资料,建立中医药临床疗效数据库,对其中高质量的文献进行系统评价;根据回顾性研究的结果,针对现有研究的不足设计严格规范的前瞻性临床试验(RCT),以常用西医疗法为对照,从有效性,安全性,卫生经济学,伦理学等方面综合评价中医药临床疗效.此举将最大限度地展示中医药疗效优势,有助于建立能体现中医药治疗的优势和特色,同时被国际认可的疗效评价体系.经循证医学严格评价确证中医药临床疗效后,充分运用基因组,蛋白质组,生物信息学等现代先进科技手段,探讨中药的复合作用,最终以"国际通用的医学语言"阐明中药,特别是复方中药的作用机制和规律.},
  keywords = {中医药,循证医学,机理研究,疗效评价},
  file = {C:\Users\BlackCat\Zotero\storage\T6AAEDHE\李 等。 - 2003 - 关于中医药疗效评价和机理研究的思考.pdf}
}

@thesis{LiYueXunHeiXiangHanShiCengJiCeShiAnLiChanShengQiZhiHanShiHuJiaoDeChuLi2012,
  title = {黑箱函式層級測試案例產生器之函式呼叫的處理},
  author = {{李岳勳}},
  namea = {{林迺衛} and {Naiwei Lin}},
  nameatype = {collaborator},
  date = {2012},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/gs2ejv},
  abstract = {軟體測試是確保軟體品質的主要方法，在軟體測試中，最重要的關鍵在於測試案例的產生。我們之前已研製一個黑箱函式層級測試案例產生器的雛型，該雛型提出一個自動產生測試案例的架構，該架構使用UML類別圖與物件限制語言作為規格語言來描述並定義函式的行為，並使用限制邏輯程式語言強大的求解能力自動產生包含測試輸入與預期輸出的測試案例。然而，該雛型尚未處理函式呼叫的敘述。本論文擴充此黑箱函式層級測試案例產生器的雛型，處理物件限制語言規格中有函式呼叫的敘述。本論文提出的方法是針對每一個函式的物件限制語言敘述產生一個對應的限制邏輯程式敘述來模擬執行此函式的行為，藉此模擬執行來自動產生同時滿足呼叫及被呼叫函式之限制式的測試案例。此功能的增加大量地擴充測試案例產生器可處理的應用程式。},
  pagetotal = {85}
}

@article{LiYueYanWangHaoDengSanHongChenYanMianXiangShiJianBenTiDeYiXueWenBenYuYiGuanLianHuaYanJiu2022,
  title = {面向事件本体的医学文本语义关联化研究},
  author = {{李跃艳王昊邓三鸿陈艳}},
  date = {2022},
  journaltitle = {情报学报},
  volume = {41},
  number = {5},
  pages = {497--511},
  url = {https://www.zhangqiaokeyan.com/academic-journal-cn_journal-china-society-scientific-technical-information_thesis/0201299189353.html},
  urldate = {2023-10-16},
  abstract = {随着互联网医疗的快速发展,数字经济和智能经济成为未来必然发展趋势,医学知识的语义化和规范化是实现智慧医疗和数字医学的重要手段.但现阶段较为成熟的医学本体仅仅描述了一些既定的静态知识,无法揭示医学知识之间的动态关联.因此,以知识表示和知识组织为出发点,构建符合叙事性文本特征的医学知识结构化表示方法具有十分重要的意义.本文在梳理叙事学理论,事件知识表示的基础上,按照是否具有叙事性特征,将医学文本分为叙事性文本和概念性文本;然后,分别对概念性医学文本和叙事性医学文本进行语义建模与表示,构建基于事件本体的医学知识本体模型;最后,根据本文提出的概念模型,实现SARS-CoV-2病毒入侵过程的语义结构化表示.初步标注的实验结果表明,将事件本体模型迁移到医学文本语义结构化描述中,有助于实现医学文本的深层次表示和知识发现,能更好地描述医学知识之间的动态关联,更好地表征医学对象在时间和空间的动态发展特点.},
  keywords = {No DOI found,事件本体,叙事性文本,规范化,语义化}
}

@article{LLMRoboticBrain2023,
  title = {{{LLM}} as {{A Robotic Brain}}: {{Unifying Egocentric Memory}} and {{Control}}},
  shorttitle = {{{LLM}} as {{A Robotic Brain}}},
  date = {2023-04-18},
  publisher = {arXiv (Cornell University)},
  doi = {10.48550/arxiv.2304.09349},
  url = {https://typeset.io/papers/llm-as-a-robotic-brain-unifying-egocentric-memory-and-3f7j0n5z},
  urldate = {2024-04-16},
  abstract = {Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The active exploration tasks require the robot to extensively explore an unknown environment within a limited number of actions. Meanwhile, the embodied question answering tasks necessitate that the robot answers questions based on observations acquired during prior explorations.},
  langid = {english},
  keywords = {未整理},
  annotation = {titleTranslation: 法學碩士作為機器人大腦：統一以自我為中心的記憶與控制\\
abstractTranslation:  具身人工智慧專注於研究和開發擁有實體或虛擬實體（即機器人）並能夠與環境動態互動的智慧系統。記憶體和控制是具體係統的兩個基本部分，通常需要單獨的框架來對它們進行建模。在本文中，我們提出了一種新穎且可推廣的框架，稱為 LLM-Brain：使用大規模語言模型作為機器人大腦來統一以自我為中心的記憶和控制。 LLM-Brain 框架利用零樣本學習方法，整合了用於機器人任務的多種多模式語言模型。 LLM-Brain 中的所有組件都使用自然語言在閉環多輪對話中進行通信，其中包括感知、規劃、控制和記憶。該系統的核心是一個具體化的法學碩士，用於維持以自我為中心的記憶並控制機器人。我們透過檢查兩個下游任務來演示 LLM-Brain：主動探索和具體問題回答。主動探索任務要求機器人在有限的動作範圍內廣泛探索未知的環境。同時，具體的問答任務需要機器人根據先前探索期間獲得的觀察結果來回答問題。}
}

@article{longbiaochengEstimationReliabilityFunction2021,
  title = {Estimation {{Reliability Function Assisted Sound Source Localization With Enhanced Steering Vector Phase Difference}}},
  author = {{Longbiao Cheng} and {Xingwei Sun} and {Dingding Yao} and {Junfeng Li} and {Yonghong Yan}},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {421--435},
  issn = {2329-9304},
  doi = {10.1109/taslp.2020.3043107},
  abstract = {The performance of the traditional direction-of-arrival (DOA) estimation algorithms greatly degrades in noisy and reverberant environments. Recently, deep learning has been applied to sound source localization and provided the substantial improvement in robustness for DOA estimation. In this paper, we propose a sound source localization approach using the deep learning-based steering vector phase difference enhancement. The steering vectors and their estimation reliability functions (ERFs) are first estimated under the guidance of the time-frequency masks that are predicted using deep neural network (DNN). The phase difference of the steering vectors is further enhanced with a second DNN model, which is trained with the ERF-weighted mean square error (MSE) loss. The DOA of the sound source is finally determined by the ERF-weighted histogram analysis. Experimental results with various types and levels of noise and various reverberant conditions show that the proposed approach outperforms the state-of-the-art sound source localization algorithms in utterance and frame-level DOA estimation.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  langid = {english},
  keywords = {Deep learning,deep neural network,Direction-of-arrival estimation,Estimation,Histograms,Microphones,Sound source localization,Speech enhancement,steering vector phase difference,Time-frequency analysis,time-frequency mask,weighted histogram analysis,聲源定位},
  annotation = {4 citations (Crossref) [2024-03-26]\\
titleTranslation: 具有增強導向矢量相位差的估計可靠性函數輔助聲源定位\\
abstractTranslation:  傳統的波達方向 (DOA) 估計算法的性能在噪聲和混響環境中會大大降低。最近，深度學習已應用於聲源定位，並為 DOA 估計的魯棒性提供了顯著的改進。在本文中，我們提出了一種使用基於深度學習的轉向矢量相位差增強的聲源定位方法。首先在使用深度神經網絡（DNN）預測的時頻掩模的指導下估計引導向量及其估計可靠性函數（ERF）。第二個 DNN 模型進一步增強了轉向矢量的相位差，該模型使用 ERF 加權均方誤差 (MSE) 損失進行訓練。通過ERF加權直方圖分析最終確定聲源的DOA。各種類型和級別的噪聲以及各種混響條件的實驗結果表明，所提出的方法在言語和幀級 DOA 估計方面優於最先進的聲源定位算法。},
  file = {C:\Users\BlackCat\Zotero\storage\I4FM6P7Y\9286560.html}
}

@inproceedings{loS2ORCSemanticScholar2020,
  title = {{{S2ORC}}: {{The Semantic Scholar Open Research Corpus}}},
  shorttitle = {{{S2ORC}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney and Weld, Daniel},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  date = {2020-07},
  pages = {4969--4983},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.acl-main.447},
  url = {https://aclanthology.org/2020.acl-main.447},
  urldate = {2023-11-30},
  abstract = {We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.},
  eventtitle = {{{ACL}} 2020},
  langid = {english},
  keywords = {學術論文,已整理,資料集},
  annotation = {103 citations (Crossref) [2024-03-26]\\
titleTranslation: S2ORC：語意學者開放研究語料庫},
  file = {C:\Users\BlackCat\Zotero\storage\SWBUV3GQ\Lo et al. - 2020 - S2ORC The Semantic Scholar Open Research Corpus.pdf}
}

@article{LuBingJiYuSuiJiSenLinDeGaoWeiShuJuKeShiHua2014,
  title = {基于随机森林的高维数据可视化},
  author = {{吕兵} and {王华珍}},
  date = {2014},
  journaltitle = {计算机应用},
  volume = {34},
  number = {6},
  pages = {1613--1617},
  url = {http://www.cqvip.com/qk/94832x/201406/49821890.html},
  urldate = {2022-08-05},
  abstract = {目前对高维数据进行挖掘的方法大多是基于数学理论而非可视化的直觉。为便于直观分析和评价高维数据,提出引入随机森林(RF)方法对高维数据进行数据可视化。首先,采用RF进行有监督学习得到样本间的相似度度量,并采用主坐...},
  keywords = {No DOI found},
  file = {C:\Users\BlackCat\Zotero\storage\6EPBLDHV\吕兵 與 王华珍 - 2014 - 基于随机森林的高维数据可视化.pdf}
}

@thesis{LuBoWeiZhongYiZhenJiuPeiXueXiTongDeYanZhi2018,
  title = {中醫針灸配穴系統的研製},
  author = {{盧柏瑋}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2018},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/p3r4n7},
  abstract = {本研究團隊先前研製一套中醫辨證系統，可根據病人的症狀，辨識病人的證候。本論文研製一個中醫針灸配穴系統，可根據中醫辨證系統辨識的證候，提供針灸處方的建議。本中醫針灸配穴系統目前使用130個腧穴，包含五輸穴、原穴、絡穴、背俞穴、腹募穴，以及常用腧穴。本中醫針灸配穴系統目前提供4個種類的針灸配穴方法的針灸處方建議，包含五行配穴方法(子母配穴法、瀉南補北法、五門十變法、針灸聚英之井滎輸經和配穴法)、臟腑配穴方法(表裏配穴法、俞募配穴法)、時間配穴方法(子午流注法、四時分刺法)以及常用腧穴方法(證候常用腧穴、八總穴、六腑下合穴)。目前還非常缺乏根據證候提供針灸處方的研究，期待本論文可以開啟這個方向的研究。},
  pagetotal = {75},
  keywords = {實驗室}
}

@article{lundChatGPTNewAcademic2023,
  title = {{{ChatGPT}} and a New Academic Reality: {{Artificial Intelligence-written}} Research Papers and the Ethics of the Large Language Models in Scholarly Publishing},
  shorttitle = {{{ChatGPT}} and a New Academic Reality},
  author = {Lund, Brady D. and Wang, Ting and Mannuru, Nishith Reddy and Nie, Bing and Shimray, Somipam and Wang, Ziang},
  date = {2023},
  journaltitle = {Journal of the Association for Information Science and Technology},
  volume = {74},
  number = {5},
  pages = {570--581},
  issn = {2330-1643},
  doi = {10.1002/asi.24750},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24750},
  urldate = {2024-02-24},
  abstract = {This article discusses OpenAI's ChatGPT, a generative pre-trained transformer, which uses natural language processing to fulfill text-based user requests (i.e., a “chatbot”). The history and principles behind ChatGPT and similar models are discussed. This technology is then discussed in relation to its potential impact on academia and scholarly research and publishing. ChatGPT is seen as a potential model for the automated preparation of essays and other types of scholarly manuscripts. Potential ethical issues that could arise with the emergence of large language models like GPT-3, the underlying technology behind ChatGPT, and its usage by academics and researchers, are discussed and situated within the context of broader advancements in artificial intelligence, machine learning, and natural language processing for research and scholarly publishing.},
  langid = {english},
  keywords = {ChatGPT,已整理,研究流程},
  annotation = {151 citations (Crossref) [2024-03-26]\\
titleTranslation: ChatGPT 與新的學術現實：人工智慧撰寫的研究論文以及學術出版中大型語言模型的倫理},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\J957B63Y\\Lund 等。 - 2023 - ChatGPT and a new academic reality Artificial Int.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\XKQJFQCZ\\asi.html}
}

@online{LunWenYanJiuLiChengYuZhiShiFenXiangPingTaiJianZhiZhiYanJiu__TaiWanBoShuoShiLunWenZhiShiJiaZhiXiTong,
  title = {論文研究歷程與知識分享平台建置之研究\_\_臺灣博碩士論文知識加值系統},
  url = {https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=bPDywe/record?r1=60&h1=0},
  urldate = {2024-01-14},
  langid = {chinese},
  keywords = {使用者研究,已整理,紙本論文,重要},
  annotation = {titleTranslation: 論文研究歷程與知識分享平台建立研究\_\_台灣博論文碩士知識加值系統},
  file = {C:\Users\BlackCat\Zotero\storage\ECZ8Z9P3\record.html}
}

@thesis{LuoHaoFengShiYongJingMoDianXingFenXiChangNianXingBiYanZhiZhengZhuang2009,
  title = {使用經脈電性分析常年性鼻炎之症狀},
  author = {{羅皓丰}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2009},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/rqrjz2},
  abstract = {常年性鼻炎是一種非常常見的疾病，根據近年的研究報告，常年性鼻炎在成人及兒童的盛行率分別約為30\%和40\%。近年來利用穴道檢測儀(Electro Dermal Screen Device)測量人體電位值作為疾病診斷的研究逐漸增多。本研究探討利用穴道檢測儀診斷常年性鼻炎的可行性。 本研究利用穴道檢測儀測量人體與常年性鼻炎密切相關的20個穴位的電位值，並應用分類演算法(classification algorithms)及群聚演算法(clustering algorithms)分析所測量的結果，探討是否可以使用穴道檢測的測量值來區分常年性鼻炎的患者及非患者，及是否可以利用穴道檢測儀的測量值來區分常年性鼻炎的嚴重度。本研究之實驗是在慈濟大林醫院中醫科自然醫學實驗室進行，共蒐集了178筆的受測資料，含111筆的常年性鼻炎資料及67筆的非常年性鼻炎資料。 實驗的第一項結果顯示，使用支援向量機(Support Vector Machine)分類演算法區分常年性鼻炎患者及非常年性鼻炎時，平均準確率可達97.78\%；使用最近K個鄰居(K Nearest Neighbor)分類演算法時，平均準確率也可達93.26\%。 實驗的第二項結果顯示，分別使用K個中心點(K-means) 群聚演算法及華氏階層 (Ward’s Hierarchical) 群聚演算法將受測資料分成三群（分別代表非患者、輕度患者、及中重度患者）時，有87\%受測資料（或154筆資料）的分群是一致的，而且所有的非患者資料都被正確地分到非患者那一群。根據具有一致性分群的154筆資料，使用支援向量機分類演算法區分常年性鼻炎的輕度患者及中重度患者時，平均準確率可達99.57\%。 實驗的第三項結果顯示，參照三群受測者的綜合鼻腔症狀評分表(Total Nasal Symptoms Score，以下簡稱TNSS)時，發現三群受測者的TNSS平均分數和分群結果是一致的。第一群受測者沒有鼻炎症狀出現，第二群受測者有輕度鼻炎症狀出現，而第三群受測者有中重度鼻炎症狀出現。另外我們也發現第二群受測者和第三群受測者最顯著的差異是在肝經穴位及脾經穴位，這項發現剛好和長期研究常年性鼻炎的馬光亞中醫師的臨床經驗相符合。 根據本研究的實驗結果顯示，利用穴道檢測儀來診斷常年性鼻炎具有非常高的可行性。},
  pagetotal = {43}
}

@article{LuoXiJieHeDuoTouZiZhuYiLiJiZhiYuBiLSTMCRFDeZhongWenLinChuangShiTiShiBie2021,
  title = {结合多头自注意力机制与BiLSTM-CRF的中文临床实体识别},
  author = {{罗熹} and {夏先运} and {安莹} and {陈先来}},
  date = {2021},
  journaltitle = {湖南大学学报（自然科学版）},
  shortjournal = {Journal of Hunan University(Natural Sciences)},
  volume = {48},
  number = {4},
  pages = {45--55},
  doi = {10.16339/j.cnki.hdxbzkb.2021.04.006},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5Eg9obmR4eGIyMDIxMDQwMDYaCDh2OW9ibHoy},
  urldate = {2022-08-14},
  abstract = {命名实体是电子病历中相关医学知识的主要载体,因此,临床命名实体识别(Clinical Named Entity Recognition,CNER)也就成为了临床文本分析处理的基础性任务之一.由于文本结构和语言等方面的特殊性,面向中文电子病历(Electronic Medical Records,EMRs)的临床命名实体识别依然存在着巨大的挑战.本文提出了一种基于多头自注意力神经网络的中文临床命名实体识别方法.该方法使用了一种新颖的融合领域词典的字符级特征表示方法,并在BiLSTM-CRF模型的基础上,结合多头自注意力机制来准确地捕获字符间潜在的依赖权重、语境和语义关联等多方面的特征,从而有效地},
  langid = {zh\_CN},
  keywords = {Journal of Hunan University(Natural Sciences),中文电子病历,命名实体识别,夏先运,多头自注意力,湖南大学学报（自然科学版）,长短期记忆,陈先来},
  annotation = {中南大学 大数据研究院,湖南 长沙 410083;湖南警察学院 网络侦查技术湖南省重点实验室,湖南 长沙410138中南大学 大数据研究院,湖南 长沙 410083\\
高新技术产业科技创新引领计划 湖南省自然科学基金~湖南省科技重大专项~国家重点研发计划~国家重点实验室开放基金\\
2021-04-30 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 結合多頭自註意力機制與BiLSTM-CRF的中文臨床實體識別\\
abstractTranslation:  實體命名是電子病歷中相關醫學知識的主要載體，因此，臨床命名實體識別（Clinical Named Entity Recognition，CNER）因而成為臨床文本分析處理的基礎性任務之一。針對特殊性、中文電子病歷(Electronic Medical Records,EMRs)的臨床命名實體識別仍然存在著巨大的挑戰。本文提出了一種基於多頭神經網絡的中文臨床命名實體識別方法。一種新穎的融合領域搜索的字符級特徵表示方法，並在BiLSTM -CRF模型的基礎上，結合多頭自機制注意力來準確捕獲字符間潛在的依賴權重、語境和語義關聯等多方面的特徵，從而有效地},
  file = {C:\Users\BlackCat\Zotero\storage\WPJCCULC\罗 等。 - 2021 - 结合多头自注意力机制与BiLSTM-CRF的中文临床实体识别.pdf}
}

@thesis{LuQiAnShuShenYuanXingXiTongZhiSheJiYuShiZuo2014,
  title = {書審原型系統之設計與實作},
  author = {{盧祈安}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2014},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/8k2kr7},
  abstract = {每年各校申請入學作業都會耗費大量的審查資料的影印及紙張成本。教育部希望可以用較環保的方式來進行申請入學的資料審查作業，因此積極地推動線上電子資料審查作業，委託大學甄選入學委員會開發線上審查原型系統，提供各大學進行申請入學的資料審查作業。 這篇論文描述此線上審查原型系統的設計及實作。此系統達成下列的品質需求：利用完全虛擬化技術達成易移植性，以適應各校不同的電腦機型及作業系統。利用模組化及原型技術達成易維護性，以適應各校不同的資料審查作業模式，讓各大學能依實際作業上的需求來調整及擴充系統功能。利用最新的動態網頁設計技術達成易使用性，讓網頁程式在操作上更人性化及直覺化，能像一般電腦應用程式一樣方便操作，而且可以支援不同的瀏覽器。除了作業系統本身的防火牆外，也增加網路位址的限定，讓學校及學系可以限制進入系統的機器，來達成系統的高安全性。最後本論文也以壓力測試的結果來佐證本系統達成規格所需求的效能。},
  pagetotal = {75}
}

@article{luzhouNaturalLanguageProcessing2021,
  title = {Natural {{Language Processing Algorithms}} for {{Normalizing Expressions}} of {{Synonymous Symptoms}} in {{Traditional Chinese Medicine}}},
  author = {{Lu Zhou} and {Shuangqiao Liu} and {Caiyan Li} and {Yuemeng Sun} and {Yizhuo Zhang} and {Yuda Li} and {Huimin Yuan} and {Yan Sun} and {Fengqin Xu} and {Yuhang Li}},
  editor = {{Mediani Ahmed}},
  date = {2021-10-11},
  journaltitle = {Evidence-Based Complementary and Alternative Medicine},
  shortjournal = {Evidence-Based Complementary and Alternative Medicine},
  volume = {2021},
  pages = {1--12},
  issn = {1741-4288, 1741-427X},
  doi = {10.1155/2021/6676607},
  url = {https://www.hindawi.com/journals/ecam/2021/6676607/},
  urldate = {2022-09-19},
  abstract = {Background. The modernization of traditional Chinese medicine (TCM) demands systematic data mining using medical records. However, this process is hindered by the fact that many TCM symptoms have the same meaning but different literal expressions (i.e., TCM synonymous symptoms). This problem can be solved by using natural language processing algorithms to construct a high-quality TCM symptom normalization model for normalizing TCM synonymous symptoms to unified literal expressions. Methods. Four types of TCM symptom normalization models, based on natural language processing, were constructed to find a high-quality one: (1) a text sequence generation model based on a bidirectional long short-term memory (Bi-LSTM) neural network with an encoder-decoder structure; (2) a text classification model based on a Bi-LSTM neural network and sigmoid function; (3) a text sequence generation model based on bidirectional encoder representation from transformers (BERT) with sequence-to-sequence training method of unified language model (BERT-UniLM); (4) a text classification model based on BERT and sigmoid function (BERT-Classification). The performance of the models was compared using four metrics: accuracy, recall, precision, and F1-score. Results. The BERT-Classification model outperformed the models based on Bi-LSTM and BERT-UniLM with respect to the four metrics. Conclusions. The BERT-Classification model has superior performance in normalizing expressions of TCM synonymous symptoms.},
  langid = {english},
  annotation = {2 citations (Crossref) [2024-03-26]\\
1 citations (Semantic Scholar/DOI) [2022-10-26]\\
titleTranslation: 中醫同義症狀表達規範化的自然語言處理算法\\
abstractTranslation:  背景。中醫現代化需要利用病歷進行系統的數據挖掘。然而，這一過程受到許多中醫症狀具有相同含義但字面表達不同（即中醫同義症狀）這一事實的阻礙。該問題可以通過利用自然語言處理算法構建高質量的中醫症狀歸一化模型，將中醫同義症狀歸一化為統一的文字表達來解決。方法。為了找到高質量的模型，構建了四種基於自然語言處理的中醫症狀歸一化模型：（1）基於雙向長短期記憶（Bi-LSTM）神經網絡的文本序列生成模型，編碼器-解碼器結構； (2)基於Bi-LSTM神經網絡和sigmoid函數的文本分類模型； （3）基於Transformer雙向編碼器表示（BERT）的文本序列生成模型，採用統一語言模型（BERT-UniLM）的序列到序列訓練方法； (4)基於BERT和sigmoid函數的文本分類模型(BERT-Classification)。使用四個指標來比較模型的性能：準確度、召回率、精確度和 F1 分數。結果。 BERT-Classification 模型在四個指標方面均優於基於 Bi-LSTM 和 BERT-UniLM 的模型。結論。 BERT分類模型在中醫同義症狀表達標準化方面具有優越的性能。},
  note = {\section{用於規範化中醫同義癥狀表達的自然語言處理演算法}},
  file = {C:\Users\BlackCat\Zotero\storage\7976LXES\Zhou 等。 - 2021 - Natural Language Processing Algorithms for Normali.pdf}
}

@online{lyuCRUDRAGComprehensiveChinese2024,
  title = {{{CRUD-RAG}}: {{A Comprehensive Chinese Benchmark}} for {{Retrieval-Augmented Generation}} of {{Large Language Models}}},
  shorttitle = {{{CRUD-RAG}}},
  author = {Lyu, Yuanjie and Li, Zhiyu and Niu, Simin and Xiong, Feiyu and Tang, Bo and Wang, Wenjin and Wu, Hao and Liu, Huanyong and Xu, Tong and Chen, Enhong and Luo, Yi and Cheng, Peng and Deng, Haiying and Wang, Zhonghao and Lu, Zijia},
  date = {2024-02-18},
  eprint = {2401.17043},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.17043},
  url = {http://arxiv.org/abs/2401.17043},
  urldate = {2024-04-10},
  abstract = {Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content. However, the evaluation of RAG systems is challenging, as existing benchmarks are limited in scope and diversity. Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where RAG could prove advantageous. Moreover, they only evaluate the performance of the LLM component of the RAG pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we have categorized the range of RAG applications into four distinct types-Create, Read, Update, and Delete (CRUD), each representing a unique use case. "Create" refers to scenarios requiring the generation of original, varied content. "Read" involves responding to intricate questions in knowledge-intensive situations. "Update" focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. "Delete" pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed comprehensive datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, the context length, the knowledge base construction, and the LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Benchmark,Computer Science - Computation and Language,RAG,中文,問答系統,已整理,待讀,數據集,重要},
  annotation = {abstractTranslation:  檢索增強生成 (RAG) 是一種透過整合外部知識來源來增強大型語言模型 (LLM) 功能的技術。該方法解決了法學碩士的常見限制，包括過時的資訊和產生不準確的「幻覺」內容的傾向。然而，RAG 系統的評估具有挑戰性，因為現有基準的範圍和多樣性有限。目前大多數基準測試主要評估問答應用程序，而忽略了 RAG 可能具有優勢的更廣泛的情況。而且，他們在實驗中僅評估RAG管道的LLM組件的性能，而忽略了檢索組件和外部知識資料庫的影響。為了解決這些問題，本文建構了一個大規模、更全面的基準，並在各種 RAG 應用情境中評估 RAG 系統的所有元件。具體來說，我們將 RAG 應用程式範圍分為四種不同的類型 - 建立、讀取、更新和刪除 (CRUD)，每種類型代表一個獨特的用例。 「創造」是指需要產生原創的、多樣化的內容的場景。 「閱讀」涉及在知識密集的情況下回答複雜的問題。 「更新」著重於修改和修正現有文本中的不準確或不一致之處。 「刪除」涉及將大量文字總結為更簡潔形式的任務。對於每個 CRUD 類別，我們開發了全面的資料集來評估 RAG 系統的性能。我們也分析了 RAG 系統各個元件的影響，例如檢索器、上下文長度、知識庫建構和法學碩士。最後，我們提供了針對不同場景優化 RAG 技術的有用見解。\\
titleTranslation: CRUD-RAG：大型語言模型檢索增強生成的綜合中文基準},
  note = {Comment: 26 Pages},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\9MCLWUKN\\Lyu 等。 - 2024 - CRUD-RAG A Comprehensive Chinese Benchmark for Re.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\NPFMBDJ4\\2401.html}
}

@article{m.ramighorabPersonalisedInformationRetrieval2013,
  title = {Personalised {{Information Retrieval}}: Survey and Classification},
  shorttitle = {Personalised {{Information Retrieval}}},
  author = {{M. Rami Ghorab} and {Dong Zhou} and {Alexander O’Connor} and {Vincent Wade}},
  date = {2013-09},
  journaltitle = {User Modeling and User-Adapted Interaction},
  shortjournal = {User Model User-Adap Inter},
  volume = {23},
  number = {4},
  pages = {381--443},
  issn = {0924-1868, 1573-1391},
  doi = {10.1007/s11257-012-9124-1},
  url = {http://link.springer.com/10.1007/s11257-012-9124-1},
  urldate = {2023-10-23},
  abstract = {Information Retrieval (IR) systems assist users in finding information from the myriad of information resources available on the Web. A traditional characteristic of IR systems is that if different users submit the same query, the system would yield the same list of results, regardless of the user. Personalised Information Retrieval (PIR) systems take a step further to better satisfy the user’s specific information needs by providing search results that are not only of relevance to the query but are also of particular relevance to the user who submitted the query. PIR has thereby attracted increasing research and commercial attention as information portals aim at achieving user loyalty by improving their performance in terms of effectiveness and user satisfaction. In order to provide a personalised service, a PIR system maintains information about the users and the history of their interactions with the system. This information is then used to adapt the users’ queries or the results so that information that is more relevant to the users is retrieved and presented. This survey paper features a critical review of PIR systems, with a focus on personalised search. The survey provides an insight into the stages involved in building and evaluating PIR systems, namely: information gathering, information representation, personalisation execution, and system evaluation. Moreover, the survey provides an analysis of PIR systems with respect to the scope of personalisation addressed. The survey proposes a classification of PIR systems into three scopes: individualised systems, community-based systems, and aggregate-level systems. Based on the conducted survey, the paper concludes by highlighting challenges and future research directions in the field of PIR.},
  langid = {english},
  keywords = {PIR,人機互動,已整理,研究流程,資訊超載},
  annotation = {77 citations (Crossref) [2024-03-26]\\
titleTranslation: 个性化信息检索：调查与分类\\
abstractTranslation:  信息检索（IR）系统可帮助用户从网络上无数的信息资源中查找信息。信息检索系统的一个传统特点是，如果不同用户提交相同的查询，无论用户是谁，系统都会给出相同的结果列表。个性化信息检索（PIR）系统则更进一步，通过提供不仅与查询相关，而且与提交查询的用户特别相关的搜索结果，更好地满足用户的特定信息需求。由于信息门户网站的目标是通过提高效率和用户满意度来获得用户忠诚度，因此 PIR 引起了越来越多的研究和商业关注。为了提供个性化服务，PIR 系统会保存用户信息及其与系统交互的历史记录。然后利用这些信息来调整用户的查询或结果，以便检索和提供与用户更相关的信息。本调查报告对 PIR 系统进行了严格审查，重点是个性化搜索。调查深入探讨了建立和评估 PIR 系统所涉及的各个阶段，即：信息收集、信息表示、个性化执行和系统评估。此外，调查还根据所涉及的个性化范围对 PIR 系统进行了分析。调查建议将 PIR 系统分为三个范围：个性化系统、基于社区的系统和总体级系统。在调查的基础上，本文最后强调了 PIR 领域面临的挑战和未来的研究方向。},
  note = {[TLDR] The survey provides an insight into the stages involved in building and evaluating PIR systems, namely: information gathering, information representation, personalisation execution, and system evaluation, and proposes a classification of Pir systems into three scopes: individualised systems, community-based systems, and aggregate-level systems.}
}

@article{m.scherfNextGenerationLiterature2005,
  title = {The next Generation of Literature Analysis: {{Integration}} of Genomic Analysis into Text Mining},
  shorttitle = {The next Generation of Literature Analysis},
  author = {{M. Scherf} and {A. Epple} and {T. Werner}},
  date = {2005-01-01},
  journaltitle = {Briefings in Bioinformatics},
  shortjournal = {Briefings in Bioinformatics},
  volume = {6},
  number = {3},
  pages = {287--297},
  issn = {1467-5463, 1477-4054},
  doi = {10.1093/bib/6.3.287},
  url = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/6.3.287},
  urldate = {2022-09-19},
  langid = {english},
  annotation = {73 citations (Crossref) [2024-03-26]\\
108 citations (Semantic Scholar/DOI) [2022-10-26]\\
titleTranslation: 下一代文獻分析：將基因組分析整合到文本挖掘中},
  file = {C:\Users\BlackCat\Zotero\storage\277JE6WL\Scherf 等。 - 2005 - The next generation of literature analysis Integr.pdf}
}

@article{madhuneupanebastolaActivityTheoreticalPerspective2023,
  title = {An {{Activity Theoretical Perspective}} on {{Writing}} and {{Supervising}} a {{Master}}’s {{Thesis}}},
  author = {{Madhu Neupane Bastola}},
  date = {2023-08-30},
  journaltitle = {English Language Teaching Perspectives},
  volume = {8},
  number = {1-2},
  pages = {31--44},
  issn = {2961-1822},
  doi = {10.3126/eltp.v8i1-2.57855},
  url = {https://www.nepjol.info/index.php/eltp/article/view/57855},
  urldate = {2023-10-23},
  abstract = {For students as novice researchers, a master’s thesis is the most demanding component because it requires them to display and learn research skills and work independently. Unfortunately, master thesis supervision has remained far from the limelight of university pedagogy. Drawing on mixed-methods research, this paper characterizes the writing and supervising of a master’s thesis from the perspective of cultural-historical activity theory. It presents various components of the activity system and how the interaction between these systems creates conflicts and contradictions. The implications of such an understanding to effective master’s thesis writing and supervision have been provided.},
  issue = {1-2},
  langid = {english},
  keywords = {supervisory feedback,研究流程},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 撰写和指导硕士论文的活动理论视角\\
abstractTranslation:  对于作为研究新手的学生来说，硕士论文是要求最高的部分，因为它需要他们展示和学习研究技能并独立工作。遗憾的是，硕士论文指导一直远离大学教育学的视线。本文以混合方法研究为基础，从文化历史活动理论的角度对硕士论文的写作和指导进行了分析。它介绍了活动系统的各个组成部分，以及这些系统之间的互动如何产生冲突和矛盾。这种理解对有效的硕士论文写作和指导具有重要意义。},
  file = {C:\Users\BlackCat\Zotero\storage\U6UEBXEU\Bastola - 2023 - An Activity Theoretical Perspective on Writing and.pdf}
}

@article{madhuneupanebastolaEffectiveThesisWriting2021,
  title = {({{In}})Effective {{Thesis Writing}} and {{Supervision}}: ({{Lost}}) {{Opportunity}} for {{Learning}}},
  shorttitle = {({{In}})Effective {{Thesis Writing}} and {{Supervision}}},
  author = {{Madhu Neupane Bastola} and {Bal Mukunda Bhandari}},
  date = {2021-10-31},
  journaltitle = {Studies in ELT and Applied Linguistics},
  volume = {1},
  number = {1},
  pages = {8--28},
  issn = {2795-1871},
  doi = {10.3126/seltal.v1i1.40604},
  url = {https://www.nepjol.info/index.php/seltal/article/view/40604},
  urldate = {2023-10-23},
  abstract = {Correction: The first author's name was mis-spelt on the webpage. On 18th November 2021, 'Bastika' was changed to 'Bastola'. The PDF remains correct. Thesis writing requires a wide range of reading, the skill of critiquing, a good skill of academic writing, and a proper collaboration of student and supervisor; however, it is poorly understood, less explored, and replete with problems. In this paper, we present the merits of thesis writing, supervisors' and students' commonly held perceptions, the effectiveness of supervisory feedback, and the value of student engagement. Then we present two components of thesis writing (i.e., introduction and literature review, including theoretical framework). Considering the need of the novice researchers (i.e., master's students) who are writing their thesis for the first time, we present these two components' introduction and provide suggestions for supervisors. We also present commonly used language features and examples. This paper is expected to be beneficial to students and supervisors alike.},
  issue = {1},
  langid = {english},
  keywords = {student engagement,研究流程},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: (无效的论文写作与指导：（丧失的）学习机会\\
abstractTranslation:  更正：网页上第一作者的名字拼写错误。2021 年 11 月 18 日，"Bastika "改为 "Bastola"。PDF 文件仍然正确。论文写作需要广泛的阅读、点评技巧、良好的学术写作技巧以及学生和导师的适当合作；然而，人们对论文写作的理解不深，探索较少，问题重重。在本文中，我们将介绍毕业论文写作的优点、导师和学生的普遍看法、导师反馈的有效性以及学生参与的价值。然后，我们介绍了论文写作的两个组成部分（即引言和文献综述，包括理论框架）。考虑到首次撰写论文的新手研究人员（即硕士生）的需要，我们介绍了这两个部分的引言，并为指导教师提供了建议。我们还介绍了常用的语言特点和示例。希望本文能对学生和导师有所帮助。},
  file = {C:\Users\BlackCat\Zotero\storage\PIRCJW9Z\Bastola and Bhandari - 2021 - (In)effective Thesis Writing and Supervision (Los.pdf}
}

@inproceedings{mahdiInformationOverloadEffects2020,
  title = {Information {{Overload}}: {{The Effects}} of {{Large Amounts}} of {{Information}}},
  shorttitle = {Information {{Overload}}},
  booktitle = {2020 1st. {{Information Technology To Enhance}} e-Learning and {{Other Application}} ({{IT-ELA}}},
  author = {Mahdi, Mohammed N. and Ahmad, Abdul R. and Ismail, Roslan and Subhi, Mohammed A. and Abdulrazzaq, Mohammed M. and Qassim, Qais S.},
  date = {2020-07},
  pages = {154--159},
  doi = {10.1109/IT-ELA50150.2020.9253082},
  url = {https://ieeexplore.ieee.org/document/9253082},
  urldate = {2024-02-24},
  abstract = {There is an immense amount of information available on the web. In no small part, this wealth of knowledge has challenged scientists continued to search for ways to make the information as easy as possible for end-users. Exploratory quest is transparent and has many characteristics that allow users to find relevant information and to improve their comprehension of the task involved. A comprehensive study is performed on various findings focused on the solutions to the overload issue and has shown that the use of faceted filters minimizes the abundance of information. This paper also discusses information overload and exploratory testing features, such as facial scanning, which can mitigate the effect of clear marine filters.},
  eventtitle = {2020 1st. {{Information Technology To Enhance}} e-Learning and {{Other Application}} ({{IT-ELA}}},
  langid = {english},
  keywords = {Electronic learning,Exploratory search,Faceted energy,Faceted search,Information filtering,Information filters,Information overload,Information technology,Search engines,Task analysis,Testing,已整理,資訊超載},
  annotation = {7 citations (Crossref) [2024-03-26]\\
titleTranslation: 資訊過載：大量資訊的影響},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\GEX8WBN7\\Mahdi 等。 - 2020 - Information Overload The Effects of Large Amounts.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\8CXUC24T\\9253082.html}
}

@article{MaHuanHuanZhongWenDianZiBingLiMingMingShiTiShiBieFangFaYanJiu2020,
  title = {中文电子病历命名实体识别方法研究},
  author = {{马欢欢} and {孔繁之} and {高建强}},
  date = {2020},
  journaltitle = {医学信息学杂志},
  shortjournal = {Journal of Medical Intelligence},
  volume = {41},
  number = {4},
  pages = {24--29},
  doi = {10.3969/j.issn.1673-6036.2020.04.005},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5Eg95eHFiZ3oyMDIwMDQwMDYaCGE5d3V5bmo2},
  urldate = {2022-08-14},
  abstract = {针对中文电子病历命名实体识别任务中存在的边界划分不准确、实体识别率不高等问题,提出基于深度学习的CNN-BiLSTM-CRF模型,详细阐述模型结构与原理,采集3 127份中文电子病历数据进行实验以验证模型性能,结果表明该模型具有较好的识别效果及性能.},
  langid = {zh\_CN},
  keywords = {Journal of Medical Intelligence,中文电子病历,医学信息学杂志,卷积神经网络,命名实体识别,孔繁之},
  annotation = {曲阜师范大学软件学院 曲阜273100济宁医学院医学信息工程学院 日照276826\\
教育部卓越工程师教育培养计划项目产学合作协同育人项目\\
2020-07-23 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 中文電子病歷命名實體方法研究\\
abstractTranslation:  針對中文電子病歷命名實體任務中存在的邊界劃分不准確、實體識別率不高等問題，提出基於深度學習的CNN-BiLSTM-CRF模型，詳細闡述模型結構與原理，採集3 127份中文電子病歷數據對模型性能進行實驗驗證，結果表明該模型具有較好的識別效果及性能。},
  file = {C:\Users\BlackCat\Zotero\storage\TR6UYXVH\马 等。 - 2020 - 中文电子病历命名实体识别方法研究.pdf}
}

@article{mairemaischRestructuringMasterDegree2003,
  title = {Restructuring a {{Master}}'s Degree Dissertation as a {{Patchwork Text}}},
  author = {{Maire Maisch}},
  date = {2003-05},
  journaltitle = {Innovations in Education and Teaching International},
  shortjournal = {Innovations in Education and Teaching International},
  volume = {40},
  number = {2},
  pages = {194--201},
  issn = {1470-3297, 1470-3300},
  doi = {10.1080/1470329031000089058},
  url = {http://www.tandfonline.com/doi/abs/10.1080/1470329031000089058},
  urldate = {2023-10-20},
  abstract = {This paper describes the introduction of the Patchwork Text format into the dissertation on a Master's degree in Social Work. The rationale for rethinking the conventional dissertation is discussed and the ‘patches’ making up the revised dissertation format are detailed. The paper draws upon the completed dissertations of two students on the programme to illustrate how the characteristics of the Patchwork Text have provided a very different learning experience for the students involved; and in doing so offers a preliminary evaluation of the Patchwork Text as an alternative mode of supervising and assessing dissertation students.},
  langid = {english},
  keywords = {已整理,拼湊文本,研究流程},
  annotation = {6 citations (Crossref) [2024-03-26]\\
titleTranslation: 将硕士学位论文重组为拼凑文本\\
abstractTranslation:  本文介绍了在社会工作硕士学位论文中引入 "拼凑文本 "格式的情况。论文讨论了重新思考传统学位论文的理由，并详细介绍了构成修订后学位论文格式的 "补丁"。论文以该课程两名学生已完成的毕业论文为基础，说明了 "拼凑文本 "的特点如何为相关学生提供了截然不同的学习体验，并以此对 "拼凑文本 "作为指导和评估毕业论文学生的另一种模式进行了初步评估。}
}

@online{mallenWhenNotTrust2023,
  title = {When {{Not}} to {{Trust Language Models}}: {{Investigating Effectiveness}} of {{Parametric}} and {{Non-Parametric Memories}}},
  shorttitle = {When {{Not}} to {{Trust Language Models}}},
  author = {Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
  date = {2023-07-02},
  eprint = {2212.10511},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.10511},
  url = {http://arxiv.org/abs/2212.10511},
  urldate = {2024-04-30},
  abstract = {Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only when necessary. Experimental results show that this significantly improves models' performance while reducing the inference costs.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,LLM,幻覺,未整理},
  note = {Comment: ACL 2023; Code and data available at https://github.com/AlexTMallen/adaptive-retrieval},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\YP66RU55\\Mallen 等。 - 2023 - When Not to Trust Language Models Investigating E.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\C62KC624\\2212.html}
}

@article{marcavandrielTextminingAnalysisHuman2006,
  title = {A Text-Mining Analysis of the Human Phenome},
  author = {{Marc A van Driel} and {Jorn Bruggeman} and {Gert Vriend} and {Han G Brunner} and {Jack A M Leunissen}},
  date = {2006-05},
  journaltitle = {European Journal of Human Genetics},
  shortjournal = {Eur J Hum Genet},
  volume = {14},
  number = {5},
  pages = {535--542},
  issn = {1018-4813, 1476-5438},
  doi = {10.1038/sj.ejhg.5201585},
  url = {http://www.nature.com/articles/5201585},
  urldate = {2022-09-19},
  langid = {english},
  annotation = {505 citations (Crossref) [2024-03-26]\\
593 citations (Semantic Scholar/DOI) [2022-10-26]\\
titleTranslation: 人類現象的文本挖掘分析},
  file = {C:\Users\BlackCat\Zotero\storage\5QAYMW9U\van Driel 等。 - 2006 - A text-mining analysis of the human phenome.pdf}
}

@article{mareklipczakEfficientTagRecommendation2011,
  title = {Efficient {{Tag Recommendation}} for {{Real-Life Data}}},
  author = {{Marek Lipczak} and {Evangelos Milios}},
  year = {10 月 1, 2011},
  journaltitle = {ACM Transactions on Intelligent Systems and Technology},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  volume = {3},
  number = {1},
  pages = {2:1--2:21},
  issn = {2157-6904},
  doi = {10.1145/2036264.2036266},
  url = {https://doi.org/10.1145/2036264.2036266},
  urldate = {2023-09-03},
  abstract = {Despite all of the advantages of tags as an easy and flexible information management approach, tagging is a cumbersome task. A set of descriptive tags has to be manually entered by users whenever they post a resource. This process can be simplified by the use of tag recommendation systems. Their objective is to suggest potentially useful tags to the user. We present a hybrid tag recommendation system together with a scalable, highly efficient system architecture. The system is able to utilize user feedback to tune its parameters to specific characteristics of the underlying tagging system and adapt the recommendation models to newly added content. The evaluation of the system on six real-life datasets demonstrated the system’s ability to combine tags from various sources (e.g., resource content or tags previously used by the user) to achieve the best quality of recommended tags. It also confirmed the importance of parameter tuning and content adaptation. A series of additional experiments allowed us to better understand the characteristics of the system and tagging datasets and to determine the potential areas for further system development.},
  langid = {english},
  keywords = {broad folksonomies,collaborative tagging,folksonomies,hybrid systems,narrow folksonomies,Tag recommendation},
  annotation = {18 citations (Crossref) [2024-03-26]\\
titleTranslation: 真實數據的高效標籤推薦\\
abstractTranslation:  儘管標籤作為一種簡單而靈活的信息管理方法具有所有優點，但標記仍然是一項繁瑣的任務。用戶每次發布資源時都必須手動輸入一組描述性標籤。這個過程可以通過使用標籤推薦系統來簡化。他們的目標是向用戶建議潛在有用的標籤。我們提出了一個混合標籤推薦系統以及一個可擴展、高效的系統架構。該系統能夠利用用戶反饋來調整其參數以適應底層標記系統的特定特徵，並使推薦模型適應新添加的內容。對系統在六個現實數據集上的評估證明了系統能夠組合來自各種來源的標籤（例如，資源內容或用戶之前使用的標籤），以實現推薦標籤的最佳質量。這也證實了參數調整和內容適配的重要性。一系列額外的實驗使我們能夠更好地了解系統和標記數據集的特徵，並確定進一步系統開發的潛在領域。},
  file = {C:\Users\BlackCat\Zotero\storage\ZTS29ZGY\Lipczak 與 Milios - 2011 - Efficient Tag Recommendation for Real-Life Data.pdf}
}

@inproceedings{mariaauxiliomedinaVisualizationRecordsClassified2012,
  title = {Visualization of Records Classified with the 1998 {{ACM CCS}}},
  booktitle = {Proceedings of the 4th {{Mexican Conference}} on {{Human-Computer Interaction}}},
  author = {{María Auxilio Medina} and {J. Alfredo Sánchez} and {Jorge de la C. Mora} and {Antonio Benítez Ruiz}},
  year = {10 月 3, 2012},
  series = {{{MexIHC}} '12},
  pages = {55--62},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2382176.2382189},
  url = {https://dl.acm.org/doi/10.1145/2382176.2382189},
  urldate = {2023-09-13},
  abstract = {This paper proposes a visualization scheme for large-scale collections of documents organized hierarchically. The scheme provides a concise and customizable view of OAI-PMH records. We assume that records are classified with a hierarchical algorithm or taxonomy. As a way of illustration, we chose the 1998 ACM Computing Classification System. A two-dimensional interface maps record attributes to visual elements: X- and Y-axes. Color, size, place and shape in the interface have a predefined intention. The scheme implementation is based of SVG charts. Users can explore data, metadata and to know the relationships between the records derived from the classification scheme. A collection of documents of the Universidad Politécnica de Puebla (UPPuebla) is used as a test bed for the proposed scheme. The paper reports preliminary results of out prototype version.},
  isbn = {978-1-4503-1659-0},
  langid = {english},
  keywords = {document collections,information visualization,OAI-PMH protocol,scalable vector graphics,可視化,已整理,知識分類},
  annotation = {0 citations (Crossref) [2024-03-26]\\
abstractTranslation:  本文提出了一種用於分層組織的大規模文件集合的可視化方案。此方案提供了 OAI-PMH 記錄的簡潔且可自訂的視圖。我們假設記錄是使用分層演算法或分類法進行分類的。作為說明，我們選擇了 1998 年 ACM 計算分類系統。二維介面將記錄屬性對應到視覺元素：X 軸和 Y 軸。介面中的顏色、大小、位置和形狀都有預先定義的意圖。此方案的實作是基於SVG圖表。使用者可以探索資料、元資料並了解從分類方案得出的記錄之間的關係。普埃布拉理工大學 (UPPuebla) 的文件集被用作擬議方案的測試平台。該論文報告了原型版本的初步結果。\\
titleTranslation: 使用 1998 ACM CCS 分類的記錄的可視化},
  file = {C:\Users\BlackCat\Zotero\storage\3AH4HRJG\Medina 等。 - 2012 - Visualization of records classified with the 1998 .pdf}
}

@article{marianofernandezMethontologyOntologicalArt,
  title = {Methontology: {{From Ontological Art Towards Ontological Engineering}}},
  author = {{Mariano Fernandez} and {Asuncion Gomez-Pearez} and {Natalia Juristo}},
  langid = {english},
  keywords = {本體建立,知識本體,經典},
  annotation = {titleTranslation: 方法論：從本體藝術到本體工程},
  file = {C:\Users\BlackCat\Zotero\storage\4R5IYWND\Fernandez 等。 - Methontology From Ontological Art Towards Ontolog.pdf}
}

@inproceedings{marielatapia-leonExtensionBiDOOntology2019,
  title = {Extension of the {{BiDO Ontology}} to {{Represent Scientific Production}}},
  booktitle = {Proceedings of the 2019 8th {{International Conference}} on {{Educational}} and {{Information Technology}}},
  author = {{Mariela Tapia-Leon} and {Idafen Santana-Perez} and {María Poveda-Villalón} and {Paola Espinoza-Arias} and {Janneth Chicaiza} and {Oscar Corcho}},
  year = {3 月 2, 2019},
  series = {{{ICEIT}} 2019},
  pages = {166--172},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3318396.3318422},
  url = {https://doi.org/10.1145/3318396.3318422},
  urldate = {2023-09-26},
  abstract = {The SPAR Ontology Network is a suite of complementary ontology modules to describe the scholarly publishing domain. BiDO Standard Bibliometric Measures is part of its set of ontologies. It allows describing of numerical and categorical bibliometric data such as h-index, author citation count, journal impact factor. These measures may be used to evaluate scientific production of researchers. However, they are not enough. In a previous study, we determined the lack of some terms to provide a more complete representation of scientific production. Hence, we have built an extension using the NeOn Methodology to restructure the BiDO ontology. With this extension, it is possible to represent and measure the number of documents from research, the number of citations from a paper and the number of publications in high impact journals according to its area and discipline.},
  isbn = {978-1-4503-6267-2},
  keywords = {BiDO,Ontology,RDF,Scholarly Publishing,Scientific Production,SPAR Ontology Network,SPARQL,未整理},
  annotation = {3 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\E8L856LG\Tapia-Leon 等。 - 2019 - Extension of the BiDO Ontology to Represent Scient.pdf}
}

@inproceedings{marielatapia-leonOntologyPersonalLearning2017,
  title = {Ontology of Personal Learning Environments in the Development of Thesis Project},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Education Technology}} and {{Computers}}},
  author = {{Mariela Tapia-León} and {Teresa Santamaría} and {Janneth Chicaiza} and {Sergio Luján-Mora}},
  date = {2017-12-20},
  series = {{{ICETC}} '17},
  pages = {183--187},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3175536.3175555},
  url = {https://dl.acm.org/doi/10.1145/3175536.3175555},
  urldate = {2023-09-16},
  abstract = {The thesis is the final step in academic formation of students. Its development may experience some difficulties that cause delays in delivery times. The internet allows students to access relevant and large amounts of information for use in the development of their theses. The internet also allows students to interact with others in order to create knowledge networks. However, exposure to too much information can produce infoxication. Therefore, there is a need to organise such information and technological resources. Through a personal learning environment (PLE), students can use current technology and online resources to develop their projects. Furthermore, by means of an ontological model, the underlying knowledge in the domain and environment can be represented in a readable format for machines. This paper presents an ontological model called PLET4Thesis, which has been designed in order to organise the process of thesis development using the elements required to create a PLE.},
  isbn = {978-1-4503-5435-6},
  langid = {english},
  keywords = {Inference rule,ontology,PLE,Protégé,thesis,已整理,已讀,本體建立,知識本體,重要},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 論文項目開發中的個人學習環境本體\\
abstractTranslation:  論文是學生學術形成的最後一步。它的開發可能會遇到一些困難，導致交付時間延遲。互聯網使學生能夠訪問大量相關信息，用於撰寫論文。互聯網還允許學生與他人互動以創建知識網絡。然而，接觸過多的信息會產生信息中毒。因此，需要組織這些信息和技術資源。通過個人學習環境（PLE），學生可以使用當前技術和在線資源來開發他們的項目。此外，通過本體模型，領域和環境中的底層知識可以以機器可讀的格式表示。本文提出了一個名為 PLET4Thesis 的本體模型，該模型的設計目的是為了使用創建 PLE 所需的元素來組織論文開發過程。},
  file = {C:\Users\BlackCat\Zotero\storage\FP45323V\Ontology of personal learning environments in the development of thesis project.pdf}
}

@article{marjatalikkaGuidedProcessEnhance2022,
  title = {Guided Process to Enhance Undergraduate Engineering Students’ Thesis Work},
  author = {{Marja Talikka} and {Johanna Naukkarinen} and {Katriina Mielonen} and {Harri Eskelinen}},
  date = {2022-10-08},
  journaltitle = {2022 IEEE Frontiers in Education Conference (FIE)},
  pages = {1--6},
  publisher = {IEEE},
  location = {Uppsala, Sweden},
  doi = {10.1109/FIE56618.2022.9962441},
  url = {https://ieeexplore.ieee.org/document/9962441/},
  urldate = {2023-10-23},
  abstract = {This innovative practice paper examines the impact of structured guidance on students’ thesis work process. Many engineering students focus on their professional future, and therefore do not understand the need to learn to research. Yet, the universities’ strong emphasis on scientific research, and the need for new generations of researchers requires that research skills are embedded also in the engineering curricula even at undergraduate level. As many engineering students are reluctant to read and write, the development of their scientific literacy skills requires many kinds of support. This paper describes pedagogical interventions implemented in a mechanical engineering bachelor’s thesis course to improve the quality of references of the theses and the completion of the thesis process in time. It also discusses the effects of systematic scaffolding of information literacy. Results show that the guided thesis process significantly improved the completion of bachelor’s thesis in the given time frame. However, statistically significant improvement in the quality and quantity of cited references could not be observed. This indicates that adding teaching sessions on information retrieval, scientific referencing, and scientific writing is not sufficient without close connection to the actual thesis supervision process performed by faculty.},
  eventtitle = {2022 {{IEEE Frontiers}} in {{Education Conference}} ({{FIE}})},
  isbn = {9781665462440},
  langid = {english},
  keywords = {使用者研究,研究流程},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 加強本科工程學生論文工作的指導流程\\
abstractTranslation:  這篇創新的實務論文探討了結構化指導對學生論文工作過程的影響。許多工科學生專注於自己的職業未來，因此不明白學習研究的必要性。然而，大學對科學研究的高度重視，以及對新一代研究人員的需求，要求將研究技能納入工程課程，甚至在本科階段也是如此。由於許多工程系學生不願意閱讀和寫作，他們的科學素養技能的發展需要多種支持。本文描述了在機械工程學士論文課程中實施的教學幹預措施，以提高論文參考文獻的品質和及時完成論文過程。它還討論了資訊素養的系統支架的影響。結果表明，指導論文過程顯著提高了學士論文在給定時間內的完成率。然而，沒有觀察到引用參考文獻的品質和數量具有統計意義的顯著改善。這表明，如果不與教師實際的論文監督過程緊密結合，增加資訊檢索、科學參考和科學寫作的教學課程是不夠的。}
}

@inproceedings{martia.hearstUntanglingTextData1999,
  title = {Untangling Text Data Mining},
  booktitle = {Proceedings of the 37th Annual Meeting of the {{Association}} for {{Computational Linguistics}} on {{Computational Linguistics}}  -},
  author = {{Marti A. Hearst}},
  date = {1999},
  pages = {3--10},
  publisher = {Association for Computational Linguistics},
  location = {College Park, Maryland},
  doi = {10.3115/1034678.1034679},
  url = {http://portal.acm.org/citation.cfm?doid=1034678.1034679},
  urldate = {2022-09-19},
  eventtitle = {The 37th Annual Meeting of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {已概覽,数据分析与知识发现,数据挖掘,知識挖掘,知识挖掘},
  annotation = {355 citations (Crossref) [2024-03-26]\\
922 citations (Semantic Scholar/DOI) [2022-10-26]\\
titleTranslation: 理清文本數據挖掘},
  note = {→不需要全自動化，僅將文本處理的繁瑣部份自動化，由使用者決策引導方向
\par
→系统提出假设和调查这些假设的策略，而用户或者使用或者忽略这些建议并决定下一步行动。},
  file = {C:\Users\BlackCat\Zotero\storage\CSPDFR6R\Hearst - 1999 - Untangling text data mining.pdf}
}

@inproceedings{masayukiokamotoAnnotatingKnowledgeWork2011,
  title = {Annotating Knowledge Work Lifelog: Term Extraction from Sensor and Operation History},
  shorttitle = {Annotating Knowledge Work Lifelog},
  booktitle = {Proceedings of the 20th {{ACM}} International Conference on {{Information}} and Knowledge Management},
  author = {{Masayuki Okamoto} and {Nayuko Watanabe} and {Shinichi Nagano} and {Kenta Cho}},
  year = {10 月 24, 2011},
  series = {{{CIKM}} '11},
  pages = {2581--2584},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2063576.2064025},
  url = {https://doi.org/10.1145/2063576.2064025},
  urldate = {2023-09-11},
  abstract = {We present a system that supports review of a knowledge work lifelog as an activity history. Since knowledge workers often review their own activity histories, gathering each user's activities on his/her terminal as a lifelog is a promising approach. However, readability of the stored lifelog is a large problem of lifelog-based application. We propose a term extraction method to add annotation labels to the stored lifelog for supporting knowledge workers, exploiting text data acquired from desktop activities. Our prototype system monitors a user's desktop activities after combining raw events, and then extracts possible annotation labels with LDA and C-value techniques from documents and text data in sensor events. In this paper, we introduce a lifelogging module and a lifelog annotation method based on term extraction techniques. According to an empirical evaluation for three weeks, we found that the current method is useful for one-week review.},
  isbn = {978-1-4503-0717-8},
  langid = {english},
  keywords = {knowledge work,lifelog,term extraction},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 註釋知識工作生活日誌：從傳感器和操作歷史中提取術語\\
abstractTranslation:  我們提出了一個系統，支持將知識工作生活日誌作為活動歷史進行審查。由於知識工作者經常回顧自己的活動歷史，因此在他/她的終端上收集每個用戶的活動作為生活日誌是一種很有前途的方法。然而，存儲的生活日誌的可讀性是基於生活日誌的應用程序的一個大問題。我們提出了一種術語提取方法，將註釋標籤添加到存儲的生活日誌中，以支持知識工作者，利用從桌面活動獲取的文本數據。我們的原型系統在組合原始事件後監視用戶的桌面活動，然後使用 LDA 和 C 值技術從傳感器事件中的文檔和文本數據中提取可能的註釋標籤。在本文中，我們介紹了一種生活日誌模塊和一種基於術語提取技術的生活日誌註釋方法。根據三週的實證評估，我們發現目前的方法對於一周的複習是有用的。},
  file = {C:\Users\BlackCat\Zotero\storage\7H4EY9X2\Okamoto 等。 - 2011 - Annotating knowledge work lifelog term extraction.pdf}
}

@thesis{matthiasmayerVisualizingWebSessions2007,
  type = {phdthesis},
  title = {Visualizing Web Sessions: Improving Web Browser History by a Better Understanding of Web Page Revisitation and a New Session-and Task-Based, Visual Web History Approach},
  shorttitle = {Visualizing Web Sessions},
  author = {{Matthias Mayer}},
  date = {2007},
  institution = {Staats-und Universitätsbibliothek Hamburg Carl von Ossietzky},
  langid = {english},
  annotation = {titleTranslation: 可視化 Web 會話：通過更好地理解網頁重訪以及新的基於會話和任務的可視化 Web 歷史記錄方法來改進 Web 瀏覽器歷史記錄},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\GLLIFTVA\\Mayer - 2007 - Visualizing web sessions improving web browser hi.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\WE2ZJL2Y\\1955.html}
}

@article{matthiassamwaldIntegratingFindingsTraditional2010,
  title = {Integrating Findings of Traditional Medicine with Modern Pharmaceutical Research: The Potential Role of Linked Open Data},
  shorttitle = {Integrating Findings of Traditional Medicine with Modern Pharmaceutical Research},
  author = {{Matthias Samwald} and {Michel Dumontier} and {Jun Zhao} and {Joanne S. Luciano} and {Michael Scott Marshall} and {Kei Cheung}},
  date = {2010-12-17},
  journaltitle = {Chinese Medicine},
  shortjournal = {Chinese Medicine},
  volume = {5},
  number = {1},
  pages = {43},
  issn = {1749-8546},
  doi = {10.1186/1749-8546-5-43},
  url = {https://doi.org/10.1186/1749-8546-5-43},
  urldate = {2023-09-15},
  abstract = {One of the biggest obstacles to progress in modern pharmaceutical research is the difficulty of integrating all available research findings into effective therapies for humans. Studies of traditionally used pharmacologically active plants and other substances in traditional medicines may be valuable sources of previously unknown compounds with therapeutic actions. However, the integration of findings from traditional medicines can be fraught with difficulties and misunderstandings. This article proposes an approach to use linked open data and Semantic Web technologies to address the heterogeneous data integration problem. The approach is based on our initial experiences with implementing an integrated web of data for a selected use-case, i.e., the identification of plant species used in Chinese medicine that indicate potential antidepressant activities.},
  langid = {english},
  keywords = {Aristolochic Acid,Chinese Medicine,PubMed Abstract,Resource Description Framework,Semantic Technology,中醫,人機互動,已整理,知識本體},
  annotation = {10 citations (Crossref) [2024-03-26]\\
abstractTranslation:  現代藥物研究進步的最大障礙之一是難以將所有可用的研究成果整合到人類的有效療法中。對傳統藥物中傳統使用的藥理活性植物和其他物質的研究可能是以前未知的具有治療作用的化合物的寶貴來源。然而，整合傳統醫學的發現可能充滿困難和誤解。本文提出了一種使用連結開放資料和語意Web技術來解決異質資料整合問題的方法。此方法是基於我們為選定用例實施綜合數據網路的初步經驗，即識別中藥中使用的表明潛在抗憂鬱活性的植物物種。\\
titleTranslation: 將傳統醫學的發現與現代藥物研究結合：關聯開放資料的潛在作用},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\7EERM9P2\\Samwald 等。 - 2010 - Integrating findings of traditional medicine with .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\RQUVVP79\\1749-8546-5-43.html}
}

@inproceedings{meganmaConceptualQuestionsDeveloping2023,
  title = {Conceptual {{Questions}} in {{Developing Expert-Annotated Data}}},
  booktitle = {Proceedings of the {{Nineteenth International Conference}} on {{Artificial Intelligence}} and {{Law}}},
  author = {{Megan Ma} and {Brandon Waldon} and {Julian Nyarko}},
  year = {9 月 7, 2023},
  series = {{{ICAIL}} '23},
  pages = {427--431},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3594536.3595139},
  url = {https://doi.org/10.1145/3594536.3595139},
  urldate = {2023-09-11},
  abstract = {In this paper, we argue that nuanced expert annotation often requires a significant rethinking of the traditional paradigms of data annotation. In a small pilot study, we find that even the most highly trained experts demonstrate significant heterogeneity in their evaluation of the document-level coherence of bespoke contracts. The outcomes of our study provide preliminary considerations of how paradigms of document annotation should fully utilize expert annotations in bespoke contexts.},
  isbn = {9798400701979},
  langid = {english},
  keywords = {contract review,data annotation paradigms,domain expertise,large language models,legal NLP,使用者研究,已整理,監督式學習,資料標記},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 開發專家註釋數據中的概念問題\\
abstractTranslation:  在本文中，我們認為細緻入微的專家註釋通常需要對傳統數據註釋範式進行重大重新思考。在一項小型試點研究中，我們發現即使是訓練有素的專家在評估定制合同的文檔級一致性時也表現出顯著的異質性。我們的研究結果為文檔註釋範式應如何在定制環境中充分利用專家註釋提供了初步考慮。},
  note = {經過觀察不同律師對資料集的標注發現，不同律師間的標注有相當大的不同。論文提出，也許以後標注資料時需要考慮不同的標注者，並盡力保存不同標注者的個人想法。此外，再AI技術不斷進步的當下，也許舊有的資料標注方法已經過時了。},
  file = {C:\Users\BlackCat\Zotero\storage\CWZ7A38V\Ma 等。 - 2023 - Conceptual Questions in Developing Expert-Annotate.pdf}
}

@inproceedings{mei-lingchenHowPersonalExperience2018,
  title = {How {{Personal Experience}} and {{Technical Knowledge Affect Using Conversational Agents}}},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Intelligent User Interfaces Companion}}},
  author = {{Mei-Ling Chen} and {Hao-Chuan Wang}},
  year = {3 月 5, 2018},
  series = {{{IUI}} '18 {{Companion}}},
  pages = {1--2},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3180308.3180362},
  url = {https://dl.acm.org/doi/10.1145/3180308.3180362},
  urldate = {2023-09-16},
  abstract = {Conversational agents (CA) use dialogues to interact with users so as to offer an experience of naturalistic interaction. However, due to the low transparency and poor explanability of mechanism inside CA, individual's understanding of CA's capabilities may affect how the individual interacts with CA and the sustainability of CA use. To examine how users' understanding affect perceptions and experiences of using CA, we conducted a laboratory study asking 41 participants performed a set of tasks using Apple Siri. We independently manipulated two factors: (1) personal experience of using CA, and (2) technical knowledge about CA's system model. We conducted mixed-method analyses of post-task usability measures and interviews, and confirmed that use experience and technical knowledge affects perceived usability and mental models differently.},
  isbn = {978-1-4503-5571-1},
  langid = {english},
  keywords = {/unread,Conversational agents,explainable intelligent user interfaces,mental models,人機互動,使用者調查,問答系統,已整理,機器學習},
  annotation = {16 citations (Crossref) [2024-03-26]\\
titleTranslation: 個人經驗和技術知識如何影響會話代理的使用\\
abstractTranslation:  會話代理程式（CA）使用對話與使用者交互，以提供自然互動的體驗。然而，由於CA內部機制透明度低、可解釋性差，個人對CA能力的理解可能會影響個人與CA的互動方式以及CA使用的可持續性。為了研究使用者的理解如何影響使用 CA 的看法和體驗，我們進行了一項實驗室研究，要求 41 名參與者使用 Apple Siri 執行一組任務。我們獨立操縱兩個因素：（1）使用CA的個人經驗，以及（2）關於CA系統模型的技術知識。我們對任務後可用性測量和訪談進行了混合方法分析，並證實使用經驗和技術知識對感知可用性和心理模型有不同的影響。},
  file = {C:\Users\BlackCat\Zotero\storage\BVWHLMBN\Chen 與 Wang - 2018 - How Personal Experience and Technical Knowledge Af.pdf}
}

@inproceedings{meloniAIDABotEnhancingConversational2023,
  title = {{{AIDA-Bot}} 2.0: {{Enhancing Conversational Agents}} with~{{Knowledge Graphs}} for~{{Analysing}} the~{{Research Landscape}}},
  shorttitle = {{{AIDA-Bot}} 2.0},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2023},
  author = {Meloni, Antonello and Angioni, Simone and Salatino, Angelo and Osborne, Francesco and Birukou, Aliaksandr and Reforgiato Recupero, Diego and Motta, Enrico},
  editor = {Payne, Terry R. and Presutti, Valentina and Qi, Guilin and Poveda-Villalón, María and Stoilos, Giorgos and Hollink, Laura and Kaoudi, Zoi and Cheng, Gong and Li, Juanzi},
  date = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {400--418},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-47243-5_22},
  abstract = {The crucial task of analysing the complex dynamics of the research landscape and uncovering the latest insights from the scientific literature is of paramount importance to researchers, governments, and commercial organizations. Springer Nature, one of the leading academic publishers worldwide, plays a significant role in this domain and regularly integrates and processes a variety of data sources to inform strategic decisions. Since exploring the resulting data is a challenging task, in 2021 we developed AIDA-Bot, a chatbot that addresses inquiries about the research landscape by utilising a large-scale knowledge graph of scholarly data. This paper presents the novel AIDA-Bot 2.0, which can both 1) support a set of predetermined question types by automatically translating them to formal queries on the knowledge graph, and 2) answer open questions by summarising information from relevant articles. We evaluated the performance of AIDA-Bot 2.0 through a comparative assessment against alternative architectures and an extensive user study. The results indicate that the novel features provide more accurate information and an excellent user experience.},
  isbn = {978-3-031-47243-5},
  langid = {english},
  keywords = {AIDA,Conversational Agents,Knowledge Graphs,Scholarly Analytics,Scholarly Data,Science of Science,問答系統,學術問答,已整理,待讀,機器學習,知識圖譜,重要},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: AIDA-Bot 2.0：利用知識圖增強對話代理人以分析研究前景},
  file = {C:\Users\BlackCat\Zotero\storage\69VW3CZY\Meloni 等。 - 2023 - AIDA-Bot 2.0 Enhancing Conversational Agents with.pdf}
}

@article{meloniIntegratingConversationalAgents2023,
  title = {Integrating {{Conversational Agents}} and {{Knowledge Graphs Within}} the {{Scholarly Domain}}},
  author = {Meloni, Antonello and Angioni, Simone and Salatino, Angelo and Osborne, Francesco and Reforgiato Recupero, Diego and Motta, Enrico},
  date = {2023},
  journaltitle = {IEEE Access},
  volume = {11},
  pages = {22468--22489},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3253388},
  url = {https://ieeexplore.ieee.org/document/10061222},
  urldate = {2023-12-03},
  abstract = {In the last few years, chatbots have become mainstream solutions adopted in a variety of domains for automatizing communication at scale. In the same period, knowledge graphs have attracted significant attention from business and academia as robust and scalable representations of information. In the scientific and academic research domain, they are increasingly used to illustrate the relevant actors (e.g., researchers, institutions), documents (e.g., articles, patents), entities (e.g., concepts, innovations), and other related information. Following the same direction, this paper describes how to integrate conversational agents with knowledge graphs focused on the scholarly domain, a.k.a. Scientific Knowledge Graphs. On top of the proposed architecture, we developed AIDA-Bot, a simple chatbot that leverages a large-scale knowledge graph of scholarly data. AIDA-Bot can answer natural language questions about scientific articles, research concepts, researchers, institutions, and research venues. We have developed four prototypes of AIDA-Bot on Alexa products, web browsers, Telegram clients, and humanoid robots. We performed a user study evaluation with 15 domain experts showing a high level of interest and engagement with the proposed agent.},
  eventtitle = {{{IEEE Access}}},
  langid = {english},
  keywords = {AIDA,NLP,問答系統,已整理,待讀,機器學習,略讀,知識圖譜,重要},
  annotation = {6 citations (Crossref) [2024-03-26]\\
titleTranslation: 在學術領域內整合對話代理與知識圖\\
abstractTranslation:  在過去幾年中，聊天機器人已成為各種領域採用的大規模自動化通訊的主流解決方案。在同一時期，知識圖譜作為強大且可擴展的資訊表示，引起了商界和學術界的極大關注。在科學和學術研究領域，它們越來越多地用於說明相關參與者（例如研究人員、機構）、文件（例如文章、專利）、實體（例如概念、創新）和其他相關資訊。遵循相同的方向，本文描述如何將會話代理與專注於學術領域的知識圖（又\hspace{0pt}\hspace{0pt}稱科學知識圖）整合。在所提出的架構之上，我們開發了 AIDA-Bot，這是一個利用大規模學術資料知識圖譜的簡單聊天機器人。 AIDA-Bot可以回答有關科學文章、研究概念、研究人員、機構和研究場所的自然語言問題。我們在 Alexa 產品、網頁瀏覽器、Telegram 用戶端和人形機器人上開發了四種 AIDA-Bot 原型。我們與 15 名領域專家進行了用戶研究評估，顯示出對擬議代理的高度興趣和參與。},
  note = {設計一個基於NLP及複雜判斷的問答機器人，可以基於AIDA學術知識圖譜做問答。
\par
基於不同的問答模板關鍵字設計相對應的處理(自動機)，如果有缺少的資料再和使用者確認。
\par
未來展望包含如何將transformers模型應用到該框架中。
\par
相關研究中包含問答機器人、學術知識圖譜等重要內容。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\4WN4AKL4\\Meloni 等。 - 2023 - Integrating Conversational Agents and Knowledge Gr - 複製.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\7SZVX6K2\\10061222.html}
}

@article{mengchenVoiceCloakAdversarialExample2023,
  title = {{{VoiceCloak}}: {{Adversarial Example Enabled Voice De-Identification}} with {{Balanced Privacy}} and {{Utility}}},
  shorttitle = {{{VoiceCloak}}},
  author = {{Meng Chen} and {Li Lu} and {Junhao Wang} and {Jiadi Yu} and {Yingying Chen} and {Zhibo Wang} and {Zhongjie Ba} and {Feng Lin} and {Kui Ren}},
  year = {6 月 12, 2023},
  journaltitle = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  shortjournal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {7},
  number = {2},
  pages = {48:1--48:21},
  doi = {10.1145/3596266},
  url = {https://dl.acm.org/doi/10.1145/3596266},
  urldate = {2023-09-16},
  abstract = {Faced with the threat of identity leakage during voice data publishing, users are engaged in a privacy-utility dilemma when enjoying the utility of voice services. Existing machine-centric studies employ direct modification or text-based re-synthesis to de-identify users' voices but cause inconsistent audibility for human participants in emerging online communication scenarios, such as virtual meetings. In this paper, we propose a human-centric voice de-identification system, VoiceCloak, which uses adversarial examples to balance the privacy and utility of voice services. Instead of typical additive examples inducing perceivable distortions, we design a novel convolutional adversarial example that modulates perturbations into real-world room impulse responses. Benefiting from this, VoiceCloak could preserve user identity from exposure by Automatic Speaker Identification (ASI), while remaining the voice perceptual quality for non-intrusive de-identification. Moreover, VoiceCloak learns a compact speaker distribution through a conditional variational auto-encoder to synthesize diverse targets on demand. Guided by these pseudo targets, VoiceCloak constructs adversarial examples in an input-specific manner, enabling any-to-any identity transformation for robust de-identification. Experimental results show that VoiceCloak could achieve over 92\% and 84\% successful de-identification on mainstream ASIs and commercial systems with excellent voiceprint consistency, speech integrity, and audio quality.},
  langid = {english},
  keywords = {/unread,adversarial examples,voice de-identification,voice privacy preservation,回收},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: VoiceCloak：對抗性範例啟用語音去識別，平衡隱私和實用性\\
abstractTranslation:  面對語音資料發布過程中身分外洩的威脅，使用者在享受語音服務的效用時陷入了隱私與效用的困境。現有的以機器為中心的研究採用直接修改或基於文字的重新合成來識別使用者的聲音，但會導致人類參與者在新興的線上通訊場景（例如虛擬會議）中聽得不一致。在本文中，我們提出了一種以人為中心的語音去識別系統VoiceCloak，它使用對抗性範例來平衡語音服務的隱私和實用性。我們設計了一種新穎的捲積對抗範例，將擾動調製為現實世界的房間脈衝響應，而不是引入可感知失真的典型加法範例。受益於這一點，VoiceCloak 可以透過自動說話者識別 (ASI) 保護使用者身分免於暴露，同時保持非侵入式去身分識別的語音感知品質。此外，VoiceCloak 透過條件變分自動編碼器學習緊湊的說話者分佈，以根據需要合成不同的目標。在這些偽目標的指導下，VoiceCloak 以特定於輸入的方式建構對抗性範例，從而實現任意到任意的身份轉換，從而實現穩健的去識別。實驗結果表明，VoiceCloak在主流ASI和商業系統上可以實現92\%和84\%以上的去識別成功率，具有出色的聲紋一致性、語音完整性和音訊品質。},
  note = {語音隱私性。},
  file = {C:\Users\BlackCat\Zotero\storage\USBDR4QH\Chen 等。 - 2023 - VoiceCloak Adversarial Example Enabled Voice De-I.pdf}
}

@incollection{mengcuiCurrentStatusTraditional2014,
  title = {Current {{Status}} of {{Traditional Chinese Medicine Language System}}},
  booktitle = {Frontier and {{Future Development}} of {{Information Technology}} in {{Medicine}} and {{Education}}},
  author = {{Meng Cui} and {Lirong Jia} and {Tong Yu} and {Shuo Yang} and {Lihong liu. Ling Zhu} and {Jinghua Li} and {Bo Gao} and {Yan Dong}},
  editor = {{Shaozi Li} and {Qun Jin} and {Xiaohong Jiang} and {James J. Park}},
  date = {2014},
  volume = {269},
  pages = {2287--2292},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-94-007-7618-0_280},
  url = {http://link.springer.com/10.1007/978-94-007-7618-0_280},
  urldate = {2022-09-19},
  isbn = {978-94-007-7617-3 978-94-007-7618-0}
}

@inproceedings{mengxuezhaoPersonalizedAbstractiveOpinion2022,
  title = {Personalized {{Abstractive Opinion Tagging}}},
  booktitle = {Proceedings of the 45th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {{Mengxue Zhao} and {Yang Yang} and {Miao Li} and {Jingang Wang} and {Wei Wu} and {Pengjie Ren} and {Maarten de Rijke} and {Zhaochun Ren}},
  year = {7 月 7, 2022},
  series = {{{SIGIR}} '22},
  pages = {1066--1076},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3477495.3532037},
  url = {https://doi.org/10.1145/3477495.3532037},
  urldate = {2023-09-06},
  abstract = {An opinion tag is a sequence of words on a specific aspect of a product or service. Opinion tags reflect key characteristics of product reviews and help users quickly understand their content in e-commerce portals. The task of abstractive opinion tagging has previously been proposed to automatically generate a ranked list of opinion tags for a given review. However, current models for opinion tagging are not personalized, even though personalization is an essential ingredient of engaging user interactions, especially in e-commerce. In this paper, we focus on the task of personalized abstractive opinion tagging. There are two main challenges when developing models for the end-to-end generation of personalized opinion tags: sparseness of reviews and difficulty to integrate multi-type signals, i.e., explicit review signals and implicit behavioral signals. To address these challenges, we propose an end-to-end model, named POT, that consists of three main components: (1) a review-based explicit preference tracker component based on a hierarchical heterogeneous review graph to track user preferences from reviews; (2)a behavior-based implicit preference tracker component using a heterogeneous behavior graph to track the user preferences from implicit behaviors; and (3) a personalized rank-aware tagging component to generate a ranked sequence of personalized opinion tags. In our experiments, we evaluate POT on a real-world dataset collected from e-commerce platforms and the results demonstrate that it significantly outperforms strong baselines.},
  isbn = {978-1-4503-8732-3},
  langid = {english},
  keywords = {回收},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 個性化抽象意見標籤\\
abstractTranslation:  意見標籤是關於產品或服務特定方面的一系列單詞。意見標籤反映了產品評論的關鍵特徵，幫助用戶快速了解其在電子商務門戶中的內容。抽象意見標籤的任務先前已被提出來自動生成給定評論的意見標籤的排名列表。然而，當前的意見標記模型並不是個性化的，儘管個性化是參與用戶交互的重要組成部分，尤其是在電子商務中。在本文中，我們重點關注個性化抽象意見標記的任務。在開發端到端生成個性化意見標籤的模型時存在兩個主要挑戰：評論的稀疏性和難以集成多類型信號，即顯式評論信號和隱式行為信號。為了應對這些挑戰，我們提出了一種名為POT 的端到端模型，它由三個主要組件組成：（1）基於分層異構評論圖的基於評論的顯式偏好跟踪器組件，用於跟踪評論中的用戶偏好； (2)基於行為的隱式偏好跟踪器組件，使用異構行為圖來跟踪隱式行為的用戶偏好； (3)個性化排名感知標籤組件，用於生成個性化意見標籤的排名序列。在我們的實驗中，我們在從電子商務平台收集的真實數據集上評估了 POT，結果表明它顯著優於強大的基線。},
  file = {C:\Users\BlackCat\Zotero\storage\US5BVUSL\Zhao 等。 - 2022 - Personalized Abstractive Opinion Tagging.pdf}
}

@incollection{meuschkeBenchmarkPDFInformation2023,
  title = {A {{Benchmark}} of {{PDF Information Extraction Tools}} Using a {{Multi-Task}} and {{Multi-Domain Evaluation Framework}} for {{Academic Documents}}},
  author = {Meuschke, Norman and Jagdale, Apurva and Spinde, Timo and Mitrović, Jelena and Gipp, Bela},
  date = {2023},
  volume = {13972},
  eprint = {2303.09957},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {383--405},
  doi = {10.1007/978-3-031-28032-0_31},
  url = {http://arxiv.org/abs/2303.09957},
  urldate = {2023-11-30},
  abstract = {Extracting information from academic PDF documents is crucial for numerous indexing, retrieval, and analysis use cases. Choosing the best tool to extract specific content elements is difficult because many, technically diverse tools are available, but recent performance benchmarks are rare. Moreover, such benchmarks typically cover only a few content elements like header metadata or bibliographic references and use smaller datasets from specific academic disciplines. We provide a large and diverse evaluation framework that supports more extraction tasks than most related datasets. Our framework builds upon DocBank, a multi-domain dataset of 1.5M annotated content elements extracted from 500K pages of research papers on arXiv. Using the new framework, we benchmark ten freely available tools in extracting document metadata, bibliographic references, tables, and other content elements from academic PDF documents. GROBID achieves the best metadata and reference extraction results, followed by CERMINE and Science Parse. For table extraction, Adobe Extract outperforms other tools, even though the performance is much lower than for other content elements. All tools struggle to extract lists, footers, and equations. We conclude that more research on improving and combining tools is necessary to achieve satisfactory extraction quality for most content elements. Evaluation datasets and frameworks like the one we present support this line of research. We make our data and code publicly available to contribute toward this goal.},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,已整理,機器學習,論文分析},
  annotation = {titleTranslation: 使用學術文件多任務和多領域評估框架的 PDF 資訊擷取工具基準},
  note = {Comment: iConference 2023},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\7Y8AXDKZ\\Meuschke et al. - 2023 - A Benchmark of PDF Information Extraction Tools us.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\2BAIVTEM\\2303.html}
}

@book{michaela.petersBioinformationalPhilosophyPostdigital2022,
  title = {Bioinformational {{Philosophy}} and {{Postdigital Knowledge Ecologies}}},
  editor = {{Michael A. Peters} and {Petar Jandrić} and {Sarah Hayes}},
  date = {2022},
  series = {Postdigital {{Science}} and {{Education}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-95006-4},
  url = {https://link.springer.com/10.1007/978-3-030-95006-4},
  urldate = {2023-10-18},
  isbn = {978-3-030-95005-7 978-3-030-95006-4},
  langid = {english},
  keywords = {Anthropocene,Artificial Intelligence,Big Data,Bioinformation,Bioinformational Capitalism,Biopolitics,Covid-19,Environment,Epistemology,Knowledge Socialism,Openness,Philosophy,Postdigital,Viral Modernity,已整理,知識本體,醫學},
  annotation = {titleTranslation: 生物信息哲学与后数字知识生态学},
  note = {一本關於醫學與現代知識ˋ的書，2022年出版，很新。},
  file = {C:\Users\BlackCat\Zotero\storage\2QEMHBI2\Peters et al. - 2022 - Bioinformational Philosophy and Postdigital Knowle.pdf}
}

@article{michaelconlonVIVOSystemResearch2019,
  title = {{{VIVO}}: A System for Research Discovery},
  shorttitle = {{{VIVO}}},
  author = {{Michael Conlon} and {Andrew Woods} and {Graham Triggs} and {Ralph O'Flinn} and {Muhammad Javed} and {Jim Blake} and {Benjamin Gross} and {Qazi Ahmad} and {Sabih Ali} and {Martin Barber} and {Don Elsborg} and {Kitio Fofack} and {Christian Hauschke} and {Violeta Ilik} and {Huda Khan} and {Ted Lawless} and {Jacob Levernier} and {Brian Lowe} and {Jose Martin} and {Rebecca Younes}},
  date = {2019-07-26},
  journaltitle = {Journal of Open Source Software},
  shortjournal = {Journal of Open Source Software},
  volume = {4},
  pages = {1182},
  doi = {10.21105/joss.01182},
  keywords = {未整理,重要},
  annotation = {10 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\R5BMHBIN\Conlon et al. - 2019 - VIVO a system for research discovery.pdf}
}

@article{miguelcerianiSemanticIntegrationAudio2023,
  title = {Semantic Integration of Audio Content Providers through the {{Audio Commons Ontology}}},
  author = {{Miguel Ceriani} and {Fabio Viola} and {Saša Rudan} and {Francesco Antoniazzi} and {Mathieu Barthet} and {György Fazekas}},
  year = {7 月 13, 2023},
  journaltitle = {Web Semantics: Science, Services and Agents on the World Wide Web},
  shortjournal = {Web Semant.},
  volume = {77},
  number = {C},
  issn = {1570-8268},
  doi = {10.1016/j.websem.2023.100787},
  url = {https://doi.org/10.1016/j.websem.2023.100787},
  urldate = {2023-09-27},
  abstract = {A broad variety of audio content is available online through an increasing number of repositories and platforms. Resources such as music tracks, recorded sounds or instrument samples may be accessed by users for tasks ranging from customised music listening and exploration, to music making and sound design using existing sounds and samples. However, each online repository offers its own API and represents information through its own data model, making it difficult for applications to exploit the plurality of online audio and music content on the web. A crucial step toward integrating audio repositories in a flexible manner is a shared basis for modelling the data therein. This paper describes and extends the Audio Commons Ontology, a common data model designed to integrate existing repositories in the audio media domain. The ontology is designed with the involvement of users through surveys and requirements analyses, and evaluated in-use, by demonstrating how it supports the integration of four relevant repositories with heterogeneous APIs and data models. While this work proves the concept in the audio domain, our proposed methodology may transfer across a broad range of media integration tasks.},
  keywords = {Audio content,Ontology,Web API integration,未整理},
  annotation = {1 citations (Crossref) [2024-03-26]}
}

@article{minchenImprovingWebsiteStructure2018,
  title = {Improving Website Structure through Reducing Information Overload},
  author = {{Min Chen}},
  date = {2018-06-01},
  journaltitle = {Decision Support Systems},
  shortjournal = {Decision Support Systems},
  volume = {110},
  pages = {84--94},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2018.03.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0167923618300575},
  urldate = {2023-10-23},
  abstract = {It is well known that website success relies heavily on its usability. Previous studies find that website usability depends greatly upon its visual complexity which has significant effects on users' psychological perception and cognitive load. In this study, we use a page's outdegree as one measurement for its visual complexity. In general, outdegrees should be kept not too high in page design as large outdegrees are often signs of high page complexity which can adversely affect user navigation. This is particularly desirable and critical for maintaining website structures, because as a website evolves over time, the need for information also changes. Website structures must be updated periodically to align with users' information needs. In this process, obsolete links should be removed to avoid clustering of links that could cause information overload to users. However, the need to slim down website structures is understudied in the literature. In this paper, we propose a mathematical programming model that reduces information load by removing links from highly clustered pages while minimizing the impact to users. Results from tests on a real dataset indicate that the model not only significantly reduces page complexity with little impact on user navigation, but also can be solved effectively. The model is also tested on large synthetic datasets to demonstrate its remarkable scalability.},
  langid = {english},
  keywords = {Information overload,Mathematical programming,Visual complexity,Website usability,人機互動},
  annotation = {27 citations (Crossref) [2024-03-26]\\
titleTranslation: 透過減少資訊過載來改善網站結構},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\H9W2VZNW\\Chen - 2018 - Improving website structure through reducing infor.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\QKF5UXMT\\S0167923618300575.html}
}

@inproceedings{ming-hsiangsuAutomaticOntologyPopulation2019,
  title = {Automatic {{Ontology Population Using Deep Learning}} for {{Triple Extraction}}},
  booktitle = {2019 {{Asia-Pacific Signal}} and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA ASC}})},
  author = {{Ming-Hsiang Su} and {Chung-Hsien Wu} and {Po-Chen Shih}},
  date = {2019-01},
  pages = {262--267},
  issn = {2640-0103},
  doi = {10.1109/APSIPAASC47483.2019.9023113},
  url = {https://ieeexplore.ieee.org/document/9023113},
  urldate = {2023-10-13},
  abstract = {Ontology is a kind of representation used to represent knowledge in a form that computers can derive the content meaning. The purpose of this work is to automatically populate an ontology using deep neural networks for updating an ontology with new facts from an input knowledge resource. In this study for automatic ontology population, a bi-LSTM-based term extraction model based on character embedding is proposed to extract the terms from a sentence. The extracted terms are regarded as the concepts of the ontology. Then, a multi-layer perception network is employed to decide the predicates between the pairs of the extracted concepts. The two concepts (one serves as subject and the other as object) along with the predicate form a triple. The number of occurrences of the dependency relations between the concepts and the predicates are estimated. The predicates with low occurrence frequency are filtered out to obtain precise triples for ontology population. For evaluation of the proposed method, we collected 46,646 sentences from Ontonotes 5.0 for training and testing the bi-LSTM-based term extraction model. We also collected 404,951 triples from ConceptNet 5 for training and testing the multilayer perceptron-based triple extraction model. From the experimental results, the proposed method could extract the triples from the documents, achieving 74.59\% accuracy for ontology population.},
  eventtitle = {2019 {{Asia-Pacific Signal}} and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA ASC}})},
  keywords = {已整理,機器學習,深度學習,知識本體,資料挖掘},
  annotation = {3 citations (Crossref) [2024-03-26]},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\9H8I3WGY\\Su et al. - 2019 - Automatic Ontology Population Using Deep Learning .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\9FVWQ8WR\\9023113.html}
}

@article{mingsongyangSemiautomaticRepresentationDesign2023,
  title = {Semi-Automatic Representation of Design Code Based on Knowledge Graph for Automated Compliance Checking},
  author = {{Mingsong Yang} and {Qin Zhao} and {Lei Zhu} and {Haining Meng} and {Kehai Chen} and {Zongjian Li} and {Xinhong Hei}},
  date = {2023-09-01},
  journaltitle = {Computers in Industry},
  shortjournal = {Computers in Industry},
  volume = {150},
  pages = {103945},
  issn = {0166-3615},
  doi = {10.1016/j.compind.2023.103945},
  url = {https://www.sciencedirect.com/science/article/pii/S0166361523000957},
  urldate = {2023-09-18},
  abstract = {Automated compliance checking (ACC) intends to verify the compliance of designs in construction industry by design codes. The ability to interpret and represent semantic information of design codes determines the maximum application scope of ACC. However, design codes are clause texts written in natural languages and existing ACC studies usually use relatively low-complexity code clause samples. At present, the lack of an accurate representation model for design codes leads to difficulties in representing the implicit information, nested logic, and complex relations contained in high-complexity clauses in codes. To address this problem, this research establishes a new representation model based on knowledge graph (KG). Four schemas are proposed into the model including order, complex, event and integration schemas. Further, an accompanying methodology for semi-automatic construction of design code KG (DCKG) is proposed. It includes four parts: interpretation, reconstruction, organization, and implementation. Where the implementation part develops a code annotation platform. In the case study and experiment, a scenario of checking a building information model (BIM) of metro station by GB50157–2013 Code for Design of Metro is adopted to validate the newly proposed representation model and the automated compliance process. The results show that the proposed model and method are correct and feasible, and our model outperforms other models in the representation ability of design codes.},
  langid = {english},
  keywords = {Automated compliance checking (ACC),Building design,Building information model (BIM),Design code representation,Knowledge graph (KG)},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於知識圖譜的設計代碼半自動表示，用於自動合規性檢查\\
abstractTranslation:  自動合規性檢查（ACC）旨在透過設計規範驗證建築業設計的合規性。解釋和表示設計程式碼語意資訊的能力決定了ACC的最大應用範圍。然而，設計程式碼是用自然語言編寫的條款文本，現有的 ACC 研究通常使用複雜度相對較低的程式碼條款樣本。目前，設計程式碼缺乏準確的表示模型，導致程式碼中高複雜性子句所包含的隱含資訊、嵌套邏輯和複雜關係難以表示。針對此問題，本研究建立了一個基於知識圖譜（KG）的新表示模型。模型中提出了四種模式，包括順序模式、複雜模式、事件模式和整合模式。此外，也提出了一種半自動建置設計程式碼 KG (DCKG) 的方法。它包括解釋、重構、組織和實施四個部分。其中實作部分開發了一個程式碼註解平台。在案例研究和實驗中，採用了根據GB50157-2013《地鐵設計規範》檢查地鐵站建築資訊模型（BIM）的場景來驗證新提出的表示模型和自動化合規流程。結果表明，所提出的模型和方法是正確可行的，並且我們的模型在設計程式碼的表示能力方面優於其他模型。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\6INJE667\\Yang 等。 - 2023 - Semi-automatic representation of design code based.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\5B4JLRSG\\S0166361523000957.html}
}

@article{mohamadyaserjaradehInformationExtractionPipelines2023,
  title = {Information Extraction Pipelines for Knowledge Graphs},
  author = {{Mohamad Yaser Jaradeh} and {Kuldeep Singh} and {Markus Stocker} and {Andreas Both} and {Sören Auer}},
  date = {2023-05-01},
  journaltitle = {Knowledge and Information Systems},
  shortjournal = {Knowl Inf Syst},
  volume = {65},
  number = {5},
  pages = {1989--2016},
  issn = {0219-3116},
  doi = {10.1007/s10115-022-01826-x},
  url = {https://doi.org/10.1007/s10115-022-01826-x},
  urldate = {2023-10-25},
  abstract = {In the last decade, a large number of knowledge graph (KG) completion approaches were proposed. Albeit effective, these efforts are disjoint, and their collective strengths and weaknesses in effective KG completion have not been studied in the literature. We extend Plumber, a framework that brings together the research community’s disjoint efforts on KG completion. We include more components into the architecture of Plumber~ to comprise 40~reusable components for various KG completion subtasks, such as coreference resolution, entity linking, and relation extraction. Using these components, Plumber dynamically generates suitable knowledge extraction pipelines and offers overall 432~distinct pipelines. We study the optimization problem of choosing optimal pipelines based on input sentences. To do so, we train a transformer-based classification model that extracts contextual embeddings from the input and finds an appropriate pipeline. We study the efficacy of Plumber for extracting the KG triples using standard datasets over three KGs: DBpedia, Wikidata, and Open Research Knowledge Graph. Our results demonstrate the effectiveness of Plumber in dynamically generating KG completion pipelines, outperforming all baselines agnostic of the underlying KG. Furthermore, we provide an analysis of collective failure cases, study the similarities and synergies among integrated components and discuss their limitations.},
  langid = {english},
  keywords = {Information extraction,NLP pipelines,Semantic search,Semantic web,Software reusability,問答系統,已整理,機器學習,知識圖譜,重要},
  annotation = {6 citations (Crossref) [2024-03-26]\\
titleTranslation: 知識圖譜的資訊擷取管道},
  file = {C:\Users\BlackCat\Zotero\storage\KUXTAFPK\Jaradeh et al. - 2023 - Information extraction pipelines for knowledge gra.pdf}
}

@inproceedings{mohamadyaserjaradehOpenResearchKnowledge2019,
  title = {Open {{Research Knowledge Graph}}: {{Next Generation Infrastructure}} for {{Semantic Scholarly Knowledge}}},
  shorttitle = {Open {{Research Knowledge Graph}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Knowledge Capture}}},
  author = {{Mohamad Yaser Jaradeh} and {Allard Oelen} and {Kheir Eddine Farfar} and {Manuel Prinz} and {Jennifer D'Souza} and {Gábor Kismihók} and {Markus Stocker} and {Sören Auer}},
  year = {9 月 23, 2019},
  series = {K-{{CAP}} '19},
  pages = {243--246},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3360901.3364435},
  url = {https://dl.acm.org/doi/10.1145/3360901.3364435},
  urldate = {2023-08-23},
  abstract = {Despite improved digital access to scholarly knowledge in recent decades, scholarly communication remains exclusively document-based. In this form, scholarly knowledge is hard to process automatically. We present the first steps towards a knowledge graph based infrastructure that acquires scholarly knowledge in machine actionable form thus enabling new possibilities for scholarly knowledge curation, publication and processing. The primary contribution is to present, evaluate and discuss multi-modal scholarly knowledge acquisition, combining crowdsourced and automated techniques. We present the results of the first user evaluation of the infrastructure with the participants of a recent international conference. Results suggest that users were intrigued by the novelty of the proposed infrastructure and by the possibilities for innovative scholarly knowledge processing it could enable.},
  isbn = {978-1-4503-7008-0},
  langid = {english},
  keywords = {information science,knowledge capture,knowledge graph,research infrastructure,scholarly communication},
  annotation = {126 citations (Crossref) [2024-03-26]\\
titleTranslation: 開放研究知識圖：下一代語義學術知識基礎設施\\
abstractTranslation:  儘管近幾十年來學術知識的數字化取得了一些改善，但學術交流仍然完全基於文檔。在這種形式下，學術知識很難自動處理。我們提出了基於知識圖譜的基礎設施的第一步，該以機器可操作形式的學術知識獲取為基礎，從而為學術知識的管理、發布和處理提供了新的可能性。主要貢獻是結合眾包設施和自動化技術來呈現、評估和討論學術知識獲取的多模式。我們與最近一次國際會議的參與者一起展示了基礎設施的首次用戶評估結果。},
  file = {C:\Users\BlackCat\Zotero\storage\4FJ47M49\Jaradeh 等。 - 2019 - Open Research Knowledge Graph Next Generation Inf.pdf}
}

@article{mohamedaminebelabbesInformationOverloadConcept2023,
  title = {Information Overload: A Concept Analysis},
  shorttitle = {Information Overload},
  author = {{Mohamed Amine Belabbes} and {Ian Ruthven} and {Yashar Moshfeghi} and {Diane Rasmussen Pennington}},
  date = {2023-01-10},
  journaltitle = {Journal of Documentation},
  shortjournal = {JD},
  volume = {79},
  number = {1},
  pages = {144--159},
  issn = {0022-0418},
  doi = {10.1108/JD-06-2021-0118},
  url = {https://www.emerald.com/insight/content/doi/10.1108/JD-06-2021-0118/full/html},
  urldate = {2023-10-23},
  abstract = {Purpose               With the shift to an information-based society and to the de-centralisation of information, information overload has attracted a growing interest in the computer and information science research communities. However, there is no clear understanding of the meaning of the term, and while there have been many proposed definitions, there is no consensus. The goal of this work was to define the concept of “information overload”. In order to do so, a concept analysis using Rodgers' approach was performed.                                         Design/methodology/approach               A concept analysis using Rodgers' approach based on a corpus of documents published between 2010 and September 2020 was conducted. One surrogate for “information overload”, which is “cognitive overload” was identified. The corpus of documents consisted of 151 documents for information overload and ten for cognitive overload. All documents were from the fields of computer science and information science, and were retrieved from three databases: Association for Computing Machinery (ACM) Digital Library, SCOPUS and Library and Information Science Abstracts (LISA).                                         Findings               The themes identified from the authors’ concept analysis allowed us to extract the triggers, manifestations and consequences of information overload. They found triggers related to information characteristics, information need, the working environment, the cognitive abilities of individuals and the information environment. In terms of manifestations, they found that information overload manifests itself both emotionally and cognitively. The consequences of information overload were both internal and external. These findings allowed them to provide a definition of information overload.                                         Originality/value               Through the authors’ concept analysis, they were able to clarify the components of information overload and provide a definition of the concept.},
  langid = {english},
  keywords = {已整理,微讀,研究流程,資訊超載},
  annotation = {8 citations (Crossref) [2024-03-26]\\
titleTranslation: 信息超载：概念分析\\
abstractTranslation:  目的 随着社会向信息化和信息去中心化的转变，信息超载引起了计算机和信息科学研究界越来越多的关注。然而，人们对这一术语的含义并没有明确的理解，虽然提出了许多定义，但并没有达成共识。这项工作的目标是定义 "信息超载 "的概念。为此，我们采用罗杰斯的方法进行了概念分析。                                         设计/方法/途径 基于 2010 年至 2020 年 9 月期间发布的文件语料库，采用罗杰斯方法进行了概念分析。确定了 "信息超载 "的一个替代概念，即 "认知超载"。文件语料库包括 151 篇信息超载文件和 10 篇认知超载文件。所有文件都来自计算机科学和信息科学领域，并从三个数据库中检索到：计算机协会（ACM）数字图书馆、SCOPUS 和图书馆与信息科学文摘（LISA）。                                         研究结果 作者通过概念分析确定的主题使我们能够提取信息超载的诱因、表现和后果。他们发现诱因与信息特征、信息需求、工作环境、个人认知能力和信息环境有关。在表现形式方面，他们发现信息超载既表现在情绪上，也表现在认知上。信息超载的后果既有内部的，也有外部的。这些发现使他们能够提供信息超载的定义。                                         独创性/价值 作者通过概念分析，阐明了信息超载的组成部分，并给出了这一概念的定义。},
  note = {通過文獻分析，定義訊息超載的具體定義
\par
[TLDR] The authors were able to clarify the components of information overload and provide a definition of the concept through a concept analysis based on a corpus of documents published between 2010 and September 2020.},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\38UB3T35\\Belabbes et al. - 2023 - Information overload a concept analysis.pdf;D\:\\Paper\\Dealing With Information Overload in Multifaceted Personal Informatics Systems.pdf}
}

@article{mohamedyassinelandolsiInformationExtractionElectronic2023,
  title = {Information Extraction from Electronic Medical Documents: State of the Art and Future Research Directions},
  shorttitle = {Information Extraction from Electronic Medical Documents},
  author = {{Mohamed Yassine Landolsi} and {Lobna Hlaoua} and {Lotfi Ben~Romdhane}},
  date = {2023-02-01},
  journaltitle = {Knowledge and Information Systems},
  shortjournal = {Knowl Inf Syst},
  volume = {65},
  number = {2},
  pages = {463--516},
  issn = {0219-3116},
  doi = {10.1007/s10115-022-01779-1},
  url = {https://doi.org/10.1007/s10115-022-01779-1},
  urldate = {2023-10-25},
  abstract = {In the medical field, a doctor must have a comprehensive knowledge by reading and writing narrative documents, and he is responsible for every decision he takes for patients. Unfortunately, it is very tiring to read all necessary information about drugs, diseases and patients due to the large amount of documents that are increasing every day. Consequently, so many medical errors can happen and even kill people. Likewise, there is such an important field that can handle this problem, which is the information extraction. There are several important tasks in this field to extract the important and desired information from unstructured text written in natural language. The main principal tasks are named entity recognition and relation extraction since they can structure the text by extracting the relevant information. However, in order to treat the narrative text we should use natural language processing techniques to extract useful information and features. In our paper, we introduce and discuss the several techniques and solutions used in these tasks. Furthermore, we outline the challenges in information extraction from medical documents. In our knowledge, this is the most comprehensive survey in the literature with an experimental analysis and a suggestion for some uncovered directions.},
  langid = {english},
  keywords = {Electronic medical records,Information extraction,Medical named entities recognition,Medical relation extraction,Section detection,實體抽取,已整理,機器學習,資料挖掘,醫療,電子病歷},
  annotation = {19 citations (Crossref) [2024-03-26]\\
titleTranslation: 從電子醫療文件中提取資訊：最新技術和未來的研究方向},
  file = {C:\Users\BlackCat\Zotero\storage\FDQTZQ7I\Landolsi et al. - 2023 - Information extraction from electronic medical doc.pdf}
}

@inproceedings{mohammadaftabalamkhanWeScribeIntelligentMeeting2023,
  title = {{{WeScribe}}: {{An Intelligent Meeting Transcriber}} and {{Analyzer Application}}},
  shorttitle = {{{WeScribe}}},
  booktitle = {Proceedings of {{Third International Conference}} on {{Computing}}, {{Communications}}, and {{Cyber-Security}}},
  author = {{Mohammad Aftab Alam Khan} and {Maryam AlAyat} and {Jumana AlGhamdi} and {Shahad Mohammed AlOtaibi} and {Maha AlZahrani} and {Malak AlQahtani} and {Atta-ur-Rahman} and {Mona Altassan} and {Farmanullah Jan}},
  editor = {{Pradeep Kumar Singh} and {Sławomir T. Wierzchoń} and {Sudeep Tanwar} and {Joel J. P. C. Rodrigues} and {Maria Ganzha}},
  date = {2023},
  series = {Lecture {{Notes}} in {{Networks}} and {{Systems}}},
  pages = {755--766},
  publisher = {Springer Nature},
  location = {Singapore},
  doi = {10.1007/978-981-19-1142-2_59},
  abstract = {In all existing organizations regardless of their type or size, meetings are conducted on the regular basis to invite discussions for organizational decision making. While many organizations, even large ones, still hire employees to perform these tasks, there is no doubt that the results are exposed to human error. Documenting meetings’ minutes is essential for its success and keeping track of the work progress and decisions flow, approvals, while keeping it complete, consistent, and coherent. This project idea was proposed by Aramco to develop a suitable solution for a hectic problem. The process of documenting and taking minutes can be tedious, so we aim to automate audio meeting transcription with the use of technologies that convert speech to text while recognizing the speaker and then process and analyze the most valuable information tagged based on persons in the meeting. This goal can be accomplished through the development of an app that uses speech recognition for conversion, voice recognition for identification of speakers, and natural language processing (NLP) for analysis and then combines them all in a transcription form with considerable accuracy. Further, the proposed system identifies potential events, deadlines, and follow-ups and adds them to the speaker’s calendar upon approval. In the future, we aspire to expand it with some features such as increasing the number of meeting members, creating special sections for each department in the company which adopt WeScribe, and feed our NLP model with more data to develop its performance and increase its accuracy.},
  isbn = {978-981-19114-2-2},
  langid = {english},
  keywords = {Information extraction,Meeting transcriber,Named entity recognition,NLP,Text to speech,已整理,會議記錄},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: WeScribe：智慧會議轉錄器和分析器應用程序\\
abstractTranslation:  在所有現有組織中，無論其類型或規模如何，都會定期舉行會議，邀請討論組織決策。儘管許多組織，甚至是大型組織，仍然僱用員工來執行這些任務，但毫無疑問，結果會受到人為錯誤的影響。記錄會議記錄對於會議的成功至關重要，並追蹤工作進度、決策流程、批准情況，同時保持會議的完整、一致和連貫性。這個專案想法是由阿美石油公司提出的，旨在為繁忙的問題開發合適的解決方案。記錄和記錄會議記錄的過程可能很乏味，因此我們的目標是使用技術將音頻會議轉錄自動化，在識別發言者的同時將語音轉換為文本，然後處理和分析根據會議人員標記的最有價值的信息。這一目標可以透過開發應用程式來實現，該應用程式使用語音識別進行轉換，使用語音識別來識別說話者，使用自然語言處理(NLP) 進行分析，然後將它們以相當準確的轉錄形式組合起來。此外，建議的系統會識別潛在的事件、截止日期和後續行動，並在批准後將其添加到演講者的日曆中。未來，我們希望透過一些功能來擴展它，例如增加會議成員的數量，為採用 WeScribe 的公司每個部門創建專門的部分，並為我們的 NLP 模型提供更多數據以提高其性能並提高其準確性。},
  file = {C:\Users\BlackCat\Zotero\storage\35H8HSVC\Khan et al. - 2023 - WeScribe An Intelligent Meeting Transcriber and A.pdf}
}

@article{mohammadbaghernegahbanInformationOverloadRealTime2018,
  title = {Information {{Overload}} in {{Real-Time Mobile Web Applications}}: {{Student Viewpoint}}},
  shorttitle = {Information {{Overload}} in {{Real-Time Mobile Web Applications}}},
  author = {{Mohammad Bagher Negahban} and {Vg Talawar}},
  date = {2018-10-24},
  journaltitle = {Interdisciplinary Journal of Virtual Learning in Medical Sciences},
  shortjournal = {Interdiscip J Virtual Learn Med Sci},
  volume = {In Press},
  issn = {2476-7263, 2476-7271},
  doi = {10.5812/ijvlms.84176},
  url = {http://ijvlms.com/en/articles/84176.html},
  urldate = {2023-10-23},
  abstract = {Context: The main goal of this research is to explore the views of post-graduate students in Kerman regarding information overload in real-time mobile web applications in order to provide better, more appropriate solutions for eliminating and controlling information overload. Methods: The views of post-graduate students regarding information overload in real-time mobile web applications were assessed by sending/receiving questionnaires to the emails of 330 post-graduate students from the Kerman government universities who were chosen using Morgan's table sampling technique. The reliability of research questionnaires were analyzed in the SPSS software using Cronbach's alpha coefficient, which was 0.908 for this research. Results: The findings showed that the students had positive views about influencing factors of information overload, moderate views about personal influencing factors in dealing with information overload, positive views about effective tools for controlling information overload, moderate views about personal influencing and environmental factors in eliminating information overload, positive views about solutions for mitigating information overload, and negative views about the effects of information overload on users (P = 0.01). This means that all the independent variables directly affect the viewpoints of graduate students. Conclusions: The research results show that irrelevant information for the user, the amount of monitoring and evaluation of individuals and management of discussion groups, wrong and incorrect information, and violation of privacy are the main factors responsible for information overload. Furthermore, training programs on user interaction with information are considered as personal and environmental influencing factors in eliminating information overload. Management of personal knowledge, referring the work to professionals, and filtering the information are considered as important solutions for mitigating information overload, and informational stress and anxiety, despair and hopelessness in retrieving relevant information, informational fatigue, expenditure loss, reduced ability to think deeply, and reduced use of mental skills are considered as the most important effects of information overload on users.},
  issue = {In Press},
  langid = {english},
  keywords = {使用者研究,已整理,資訊超載},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 即時移動 Web 應用程式中的資訊過載：學生觀點\\
abstractTranslation:  背景：本研究的主要目標是探討克爾曼研究生對即時行動 Web 應用程式中資訊過載的看法，以便為消除和控制資訊過載提供更好、更合適的解決方案。方法：透過向來自克爾曼政府大學的 330 名研究生的電子郵件發送/接收調查問卷，評估研究生對即時行動 Web 應用程式中資訊過載的看法，這些研究生是使用摩根表格抽樣技術選出的。在SPSS軟體中使用Cronbach's α係數對研究問卷的信度進行分析，本研究的α係數為0.908。結果：調查結果顯示，學生對資訊超載的影響因素持正面態度，對應對資訊超載的個人影響因素持中度態度，對控制資訊超載的有效工具持正面態度，對消除資訊超載的個人影響因素和環境因素持中度態度。資訊過載、對緩解資訊過載解決方案的正面看法以及對資訊過載對使用者影響的負面看法（P = 0.01）。這意味著所有自變數都直接影響研究生的觀點。結論：研究結果表明，與使用者無關的資訊、對個人的監控評估量和討論群組的管理、錯誤和不正確的資訊以及侵犯隱私是造成資訊過載的主要因素。此外，關於使用者與資訊互動的訓練計畫被認為是消除資訊過載的個人和環境影響因素。個人知識管理、將工作轉介給專業人員、過濾資訊被認為是緩解資訊過載、資訊壓力和焦慮、檢索相關資訊時的絕望和絕望、資訊疲勞、支出損失、深入思考能力下降的重要解決方案，而心理技能的減少被認為是資訊過載對使用者最重要的影響。},
  note = {[TLDR] The research results show that irrelevant information for the user, the amount of monitoring and evaluation of individuals and management of discussion groups, wrong and incorrect information, and violation of privacy are the main factors responsible for information overload.},
  file = {C:\Users\BlackCat\Zotero\storage\PPF6GKND\Negahban and Talawar - 2018 - Information Overload in Real-Time Mobile Web Appli.pdf}
}

@article{mohammednajahmahdiSolutionInformationOverload2020,
  title = {Solution for {{Information Overload Using Faceted Search}}–{{A Review}}},
  author = {{Mohammed Najah Mahdi} and {Abdul Rahim Ahmad} and {Roslan Ismail} and {Hayder Natiq} and {Mohammed Abdulameer Mohammed}},
  date = {2020},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {8},
  pages = {119554--119585},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3005536},
  url = {https://ieeexplore.ieee.org/document/9127450/},
  urldate = {2023-10-23},
  abstract = {In the modern society, Internet provides massive amounts of heterogeneous information, hence Information overload has become an ubiquitous issue. In this paper, we conduct a large scale quantitative study for articles dealing with (1) information overloading; (2) faceted search; and (3) filtering the data in three major databases, namely, Web of Science, ScienceDirect, and IEEE Explore. These three databases have presented 172 articles, which can be classified into four categories. The first category contains review and survey papers related to information overload. The second category includes papers that concentrate on developing theoretical frameworks to reduce information overloading. The third category contains papers dealing with improving structure or architectural of software for filtering the huge data. The fourth category includes papers that provide criteria to evaluate filtering techniques. Finally, our contribution provides further understanding of information overload, and gives an important basis for future research. Moreover, we illustrate that the dynamic faceted filters are more efficient to reduce the information overload.},
  langid = {english},
  keywords = {Review,已整理,文獻,資訊超載,重要},
  annotation = {15 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用分面搜尋解決資訊過載問題 - 回顧\\
abstractTranslation:  現代社會，網路提供了大量異質訊息，資訊過載成為普遍存在的問題。在本文中，我們對涉及（1）資訊超載的文章進行了大規模的量化研究； (2)分面搜尋； (3)過濾Web of Science、ScienceDirect、IEEE Explore這三大資料庫中的資料。這三個資料庫共收錄了 172 篇文章，可分為四類。第一類包含與資訊過載相關的評論和調查論文。第二類包括專注於發展理論架構以減少資訊過載的論文。第三類包含涉及改進用於過濾大量資料的軟體結構或體系結構的論文。第四類包括提供評估過濾技術標準的論文。最後，我們的貢獻提供了對資訊過載的進一步理解，並為未來的研究提供了重要基礎。此外，我們也說明動態多面過濾器可以更有效地減少資訊過載。},
  note = {第三章的C節有比較多可以參考的內容
\par
[TLDR] A large scale quantitative study for articles dealing with information overloading; faceted search; and filtering the data in three major databases, namely, Web of Science, ScienceDirect, and IEEE Explore, shows that the dynamic faceted filters are more efficient to reduce the information overload.},
  file = {D:\Paper\Solution for Information Overload Using Faceted Search–A Review.pdf}
}

@article{mohdhafizulafifiabdullahSystematicLiteratureReview2023,
  title = {Systematic {{Literature Review}} of {{Information Extraction From Textual Data}}: {{Recent Methods}}, {{Applications}}, {{Trends}}, and {{Challenges}}},
  shorttitle = {Systematic {{Literature Review}} of {{Information Extraction From Textual Data}}},
  author = {{Mohd Hafizul Afifi Abdullah} and {Norshakirah Aziz} and {Said Jadid Abdulkadir} and {Hitham Seddig Alhassan Alhussian} and {Noureen Talpur}},
  date = {2023},
  journaltitle = {IEEE Access},
  volume = {11},
  pages = {10535--10562},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3240898},
  url = {https://ieeexplore.ieee.org/document/10032132},
  urldate = {2023-10-31},
  abstract = {Information extraction (IE) is a challenging task, particularly when dealing with highly heterogeneous data. State-of-the-art data mining technologies struggle to process information from textual data. Therefore, various IE techniques have been developed to enable the use of IE for textual data. However, each technique differs from one another because it is designed for different data types and has different target information to be extracted. This study investigated and described the most contemporary methods for extracting information from textual data, emphasizing their benefits and shortcomings. To provide a holistic view of the domain, this comprehensive systematic literature review employed a systematic mapping process to summarize studies published in the last six years (from 2017 to 2022). It covers fundamental concepts, recent approaches, applications, and trends, in addition to challenges and future research prospects in this domain area. Based on an analysis of 161 selected studies, we found that the state-of-the-art models employ deep learning to extract information from textual data. Finally, this study aimed to guide novice and experienced researchers in future research and serve as a foundation for this research area.},
  eventtitle = {{{IEEE Access}}},
  langid = {english},
  keywords = {Review,已整理,待讀,知識抽取,重要},
  annotation = {5 citations (Crossref) [2024-03-26]\\
titleTranslation: 從文字資料中提取資訊的系統性文獻綜述：最新方法、應用、趨勢和挑戰\\
abstractTranslation:  資訊擷取（IE）是一項具有挑戰性的任務，特別是在處理高度異質的資料時。最先進的資料探勘技術難以處理文字資料中的資訊。因此，已經開發了各種IE技術來使得能夠將IE用於文字資料。然而，每種技術都彼此不同，因為它們是針對不同的資料類型而設計的，並且具有不同的要提取的目標資訊。這項研究調查並描述了從文字資料中提取資訊的最現代方法，強調了它們的優點和缺點。為了提供該領域的整體視圖，這篇全面的系統文獻綜述採用了系統映射過程來總結過去六年（2017 年至 2022 年）發表的研究。它涵蓋了基本概念、最新方法、應用和趨勢，以及該領域的挑戰和未來研究前景。基於 161 項選定研究的分析，我們發現最先進的模型採用深度學習從文字資料中提取資訊。最後，本研究旨在指導新手和經驗豐富的研究人員進行未來的研究，並作為該研究領域的基礎。},
  note = {研究問題1：從文字資料來看，與 IE 相關的概念有哪些？動機：從文本資料中討論與 IE 相關的基本概念和知識。 
\par
研究問題2：目前從非結構化資料中提取文字資料的技術是什麼？動機：討論目前可用於文字資料 IE 的技術。
\par
研究問題3：IE 在文字資料中的實際應用是什麼？動機：透過文字資料研究 IE 在各領域的當前應用，並提出未來潛在的方向。
\par
研究問題4：從文字資料來看，與 IE 相關的出版品的強度是多少？},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\7JG4BAFB\\Abdullah et al. - 2023 - Systematic Literature Review of Information Extrac.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\VDMGATWI\\10032132.html}
}

@inproceedings{mounirm.bendouchAddressingScalabilityIssues2022,
  title = {Addressing {{Scalability Issues}} in {{Semantics-Driven Recommender Systems}}},
  booktitle = {{{IEEE}}/{{WIC}}/{{ACM International Conference}} on {{Web Intelligence}} and {{Intelligent Agent Technology}}},
  author = {{Mounir M. Bendouch} and {Flavius Frasincar} and {Tarmo Robal}},
  year = {4 月 13, 2022},
  series = {{{WI-IAT}} '21},
  pages = {56--63},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3486622.3493963},
  url = {https://doi.org/10.1145/3486622.3493963},
  urldate = {2023-09-26},
  abstract = {Content-based semantics-driven recommender systems are often used in the small-scale news recommendation domain. These recommender systems improve over TF-IDF by taking into account (domain) semantics through semantic lexicons or domain ontologies. Our work explores the application of such recommender systems to other domains, using the case of large-scale movie recommendations. We propose new methods to extract semantic features from various item descriptions, and for scaling up the semantics-driven approach with pre-computation of the cosine similarities and gradient learning of the model. The results of the study on a large-scale dataset of user ratings demonstrate that semantics-driven recommenders can be extended to more complex domains and outperform TF-IDF on ROC, PR, F1, and Kappa metrics.},
  isbn = {978-1-4503-9115-3},
  keywords = {ontology,scalability,semantics-driven recommendation,未整理},
  annotation = {2 citations (Crossref) [2024-03-26]}
}

@online{muchengrenTCMSDBenchmarkProbing2022,
  title = {{{TCM-SD}}: {{A Benchmark}} for {{Probing Syndrome Differentiation}} via {{Natural Language Processing}}},
  shorttitle = {{{TCM-SD}}},
  author = {{Mucheng Ren} and {Heyan Huang} and {Yuxiang Zhou} and {Qianwen Cao} and {Yuan Bu} and {Yang Gao}},
  date = {2022-08-02},
  eprint = {2203.10839},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.10839},
  url = {http://arxiv.org/abs/2203.10839},
  urldate = {2023-06-24},
  abstract = {Traditional Chinese Medicine (TCM) is a natural, safe, and effective therapy that has spread and been applied worldwide. The unique TCM diagnosis and treatment system requires a comprehensive analysis of a patient's symptoms hidden in the clinical record written in free text. Prior studies have shown that this system can be informationized and intelligentized with the aid of artificial intelligence (AI) technology, such as natural language processing (NLP). However, existing datasets are not of sufficient quality nor quantity to support the further development of data-driven AI technology in TCM. Therefore, in this paper, we focus on the core task of the TCM diagnosis and treatment system -- syndrome differentiation (SD) -- and we introduce the first public large-scale dataset for SD, called TCM-SD. Our dataset contains 54,152 real-world clinical records covering 148 syndromes. Furthermore, we collect a large-scale unlabelled textual corpus in the field of TCM and propose a domain-specific pre-trained language model, called ZY-BERT. We conducted experiments using deep neural networks to establish a strong performance baseline, reveal various challenges in SD, and prove the potential of domain-specific pre-trained language model. Our study and analysis reveal opportunities for incorporating computer science and linguistics knowledge to explore the empirical validity of TCM theories.},
  langid = {english},
  pubstate = {preprint},
  keywords = {BERT,中醫,已整理,資料集,辨證},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-06-24]\\
0 citations (Semantic Scholar/DOI) [2023-06-24]\\
titleTranslation: TCM-SD：通過自然語言處理探查辨證的基準\\
abstractTranslation:  中醫藥是一種天然、安全、有效的療法，已在世界範圍內傳播和應用。獨特的中醫診療體系需要對隱藏在自由文本臨床記錄中的患者症狀進行全面分析。此前的研究表明，借助自然語言處理（NLP）等人工智能（AI）技術，該系統可以實現信息化、智能化。然而，現有數據集的質量和數量不足以支持中醫數據驅動的人工智能技術的進一步發展。因此，在本文中，我們聚焦於中醫診療體系的核心任務——辨證論治（SD），並引入了第一個公開的大規模SD數據集，稱為TCM-SD。我們的數據集包含 54,152 條真實世界臨床記錄，涵蓋 148 種綜合症。此外，我們收集了中醫領域的大規模未標記文本語料庫，並提出了一種特定領域的預訓練語言模型，稱為 ZY-BERT。我們使用深度神經網絡進行了實驗，以建立強大的性能基線，揭示 SD 中的各種挑戰，並證明特定領域預訓練語言模型的潛力。我們的研究和分析揭示了結合計算機科學和語言學知識來探索中醫理論的實證有效性的機會。},
  note = {收集65000比臨床病例及對應的辨證資料，並用其微調BERT成ZY-BERT，在辨證上有良好的結果。有81\%的準確度但F1僅50\%，是因為資料分布不均勻造成的。
\par
\href{https://github.com/Borororo/ZY-BERT/tree/main}{Borororo/ZY-BERT (github.com)}
\par
Comment: 10 main pages + 2 reference pages, to appear at CCL2022},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\DIJDLZ5H\\Ren 等。 - 2022 - TCM-SD A Benchmark for Probing Syndrome Different.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\NCTGF7JP\\2203.html}
}

@article{mudithatisseraOntologicalKnowledgeInferring2023,
  title = {Ontological Knowledge Inferring Approach: {{Introducing Directed Collocations}} ({{DC}}) and {{Joined Directed Collocations}} ({{JDC}})},
  shorttitle = {Ontological Knowledge Inferring Approach},
  author = {{Muditha Tissera} and {Ruvan Weerasinghe}},
  date = {2023-07-13},
  journaltitle = {International Journal of Knowledge-based and Intelligent Engineering Systems},
  shortjournal = {KES},
  volume = {27},
  number = {1},
  pages = {113--132},
  issn = {13272314, 18758827},
  doi = {10.3233/KES-221516},
  url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/KES-221516},
  urldate = {2023-09-18},
  abstract = {The growing need of utilizing unstructured knowledge embedded in open-domain natural language text into machine-processable forms requires the induction of hardly extracted structured knowledge into knowledge bases which makes the Semantic Web vision a reality. In this context, ontologies, and ontological knowledge (triples) plays a vital role. This research introduces two novel concepts named Directed Collocation (DC) and Joined Directed Collocation (JDC) along with a methodical application of them to infer new ontological knowledge. Introduced Quality-Threshold-Value (QTV) parameter improves the quality of the inferred ontological knowledge. Having set a moderate value (3) for QTV, this approach inferred 95,491 new ontological knowledge from 43,100 triples of open domain Sri Lankan English news corpus. Indeed, the outcome was approximately doubled in size as the source corpus. Some inferred ontological knowledge was identical with the original corpus content, which evidences the accuracy of this approach. The remaining were validated using inter-rater agreement method (high reliability) and out of which around 56\% were estimated as effective. The inferred outcome which is in the triple format may use in any knowledge base. The proposed approach is domain independent. Thus, helps to construct/extend ontologies for any domain with the help of less or no human specialists.},
  langid = {english},
  keywords = {已整理,待讀,本體建立,語意分析,重要},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 本體知識推論方法：引入定向搭配（DC）與連結定向搭配（JDC）\\
abstractTranslation:  將開放域自然語言文本中嵌入的非結構化知識利用為機器可處理形式的需求日益增長，需要將難以提取的結構化知識引入知識庫，從而使語義網願景成為現實。在這種背景下，本體論和本體論知識（三元組）起著至關重要的作用。這項研究引入了兩個新概念，即定向搭配（DC）和聯合定向搭配（JDC），並系統地應用它們來推斷新的本體知識。引入的品質閾值（QTV）參數提高了推斷本體知識的品質。為 QTV 設定適中值 (3) 後，該方法從開放域斯里蘭卡英語新聞語料庫的 43,100 個三元組中推斷出 95,491 個新的本體論知識。事實上，結果的大小大約是來源語料庫的兩倍。一些推斷的本體知識與原始語料內容相同，這證明了該方法的準確性。其餘的使用評估者間一致性方法（高可靠性）進行驗證，其中約 56\% 被估計為有效。三重格式的推論結果可以在任何知識庫中使用。所提出的方法是領域無關的。因此，有助於在很少或沒有人類專家的幫助下建立/擴展任何領域的本體。},
  file = {C:\Users\BlackCat\Zotero\storage\E6S3JCND\Tissera 與 Weerasinghe - 2023 - Ontological knowledge inferring approach Introduc.pdf}
}

@online{muennighoffMTEBMassiveText2022,
  title = {{{MTEB}}: {{Massive Text Embedding Benchmark}}},
  shorttitle = {{{MTEB}}},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Loïc and Reimers, Nils},
  date = {2022-10-13},
  url = {https://arxiv.org/abs/2210.07316v3},
  urldate = {2024-02-29},
  abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.},
  langid = {english},
  organization = {arXiv.org},
  file = {C:\Users\BlackCat\Zotero\storage\R8JJ9DJX\Muennighoff et al. - 2022 - MTEB Massive Text Embedding Benchmark.pdf}
}

@online{muennighoffMTEBMassiveText2023,
  title = {{{MTEB}}: {{Massive Text Embedding Benchmark}}},
  shorttitle = {{{MTEB}}},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Loïc and Reimers, Nils},
  date = {2023-03-19},
  eprint = {2210.07316},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.07316},
  url = {http://arxiv.org/abs/2210.07316},
  urldate = {2024-03-07},
  abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning,問答系統,資料集,重要},
  annotation = {titleTranslation: MTEB：海量文本嵌入基準},
  note = {Comment: 24 pages, 14 tables, 6 figures},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\FG7BLVII\\Muennighoff 等。 - 2023 - MTEB Massive Text Embedding Benchmark.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\4J6XF688\\2210.html}
}

@article{muhammadnabeelasimSurveyOntologyLearning2018,
  title = {A Survey of Ontology Learning Techniques and Applications},
  author = {{Muhammad Nabeel Asim} and {Muhammad Wasim} and {Muhammad Usman Ghani Khan} and {Waqar Mahmood} and {Hafiza Mahnoor Abbasi}},
  date = {2018-10-05},
  journaltitle = {Database: The Journal of Biological Databases and Curation},
  shortjournal = {Database (Oxford)},
  volume = {2018},
  eprint = {30295720},
  eprinttype = {pmid},
  pages = {bay101},
  issn = {1758-0463},
  doi = {10.1093/database/bay101},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6173224/},
  urldate = {2023-10-18},
  abstract = {Ontologies have gained a lot of popularity and recognition in the semantic web because of their extensive use in Internet-based applications. Ontologies are often considered a fine source of semantics and interoperability in all artificially smart systems. Exponential increase in unstructured data on the web has made automated acquisition of ontology from unstructured text a most prominent research area. Several methodologies exploiting numerous techniques of various fields (machine learning, text mining, knowledge representation and reasoning, information retrieval and natural language processing) are being proposed to bring some level of automation in the process of ontology acquisition from unstructured text. This paper describes the process of ontology learning and further classification of ontology learning techniques into three classes (linguistics, statistical and logical) and discusses many algorithms under each category. This paper also explores ontology evaluation techniques by highlighting their pros and cons. Moreover, it describes the scope and use of ontology learning in several industries. Finally, the paper discusses challenges of ontology learning along with their corresponding future directions.},
  pmcid = {PMC6173224},
  keywords = {已整理,本體建立,機器學習,知識本體,資料集,重要},
  annotation = {97 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\QJTAF6BB\Asim et al. - 2018 - A survey of ontology learning techniques and appli.pdf}
}

@inproceedings{murillocastroOntologySupportKnowledge2023,
  title = {An {{Ontology}} to Support {{Knowledge Management Solutions}} for {{Human-Computer Interaction Design}}},
  booktitle = {Proceedings of the {{XXI Brazilian Symposium}} on {{Software Quality}}},
  author = {{Murillo Castro} and {Monalessa Barcellos}},
  year = {1 月 27, 2023},
  series = {{{SBQS}} '22},
  pages = {1--10},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3571473.3571502},
  url = {https://doi.org/10.1145/3571473.3571502},
  urldate = {2023-09-26},
  abstract = {Developing interactive systems is a challenging task that involves concerns related to the human-computer interaction (HCI), such as usability and user experience. Therefore, HCI design is a core issue to the quality of such systems. HCI design often involves people with different backgrounds (e.g., Arts, Software Engineering, Design). This makes knowledge transfer a challenging issue due to the lack of a common conceptualization about HCI design, leading to semantic interoperability problems, such as ambiguity and imprecision when interpreting shared information. Ontologies have been acknowledged as a successful approach to represent domain knowledge and support knowledge-based solutions. Hence, in this work, we propose to explore the use of ontologies to represent structured knowledge of HCI design and improve knowledge sharing in this context. We developed the Human-Computer Interaction Design Ontology (HCIDO), which is part of the Human-Computer Interaction Ontology Network (HCI-ON) and is connected to the Software Engineering Ontology Network (SEON). By making knowledge related to the HCI design domain explicit and structured, HCIDO helped us to develop KTID, a tool that aims to support capturing and sharing knowledge to aid in HCI design by allowing HCI designers to annotate information about design choices in design artifacts shared with HCI design stakeholders. Preliminary results indicate that the tool can be particularly useful for novice HCI designers.},
  isbn = {978-1-4503-9999-9},
  keywords = {HCI Design,Knowledge Management,Ontology,User Interface,人機互動,已整理,知識本體},
  annotation = {0 citations (Crossref) [2024-03-26]}
}

@inproceedings{murtaNoWorkflowCapturingAnalyzing2015,
  title = {{{noWorkflow}}: {{Capturing}} and {{Analyzing Provenance}} of {{Scripts}}},
  shorttitle = {{{noWorkflow}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Murta, Leonardo and Braganholo, Vanessa and Chirigati, Fernando and Koop, David and Freire, Juliana},
  editor = {Ludäscher, Bertram and Plale, Beth},
  date = {2015},
  pages = {71--83},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-16462-5_6},
  abstract = {We propose noWorkflow, a tool that transparently captures provenance of scripts and enables reproducibility. Unlike existing approaches, noWorkflow is non-intrusive and does not require users to change the way they work – users need not wrap their experiments in scientific workflow systems, install version control systems, or instrument their scripts. The tool leverages Software Engineering techniques, such as abstract syntax tree analysis, reflection, and profiling, to collect different types of provenance, including detailed information about the underlying libraries. We describe how noWorkflow captures multiple kinds of provenance and the different classes of analyses it supports: graph-based visualization; differencing over provenance trails; and inference queries.},
  isbn = {978-3-319-16462-5},
  langid = {english},
  keywords = {Function Activation,Function Call,Function Definition,Hash Code,Python,Python Script,未整理},
  annotation = {53 citations (Crossref) [2024-03-26]\\
titleTranslation: noWorkflow：捕捉和分析腳本的來源\\
abstractTranslation:  我們提出了 noWorkflow，這是一種可以透明地捕捉腳本來源並實現可重複性的工具。與現有方法不同，noWorkflow 是非侵入式的，不需要使用者改變他們的工作方式——使用者不需要將他們的實驗包裝在科學工作流程系統中、安裝版本控制系統或檢測他們的腳本。該工具利用軟體工程技術（例如抽象語法樹分析、反射和分析）來收集不同類型的來源，包括有關底層庫的詳細資訊。我們描述了 noWorkflow 如何捕捉多種來源及其支援的不同類別的分析：基於圖形的視覺化；起源路徑的差異；和推理查詢。},
  file = {C:\Users\BlackCat\Zotero\storage\7CNH8VIA\Murta 等。 - 2015 - noWorkflow Capturing and Analyzing Provenance of .pdf}
}

@article{mustafajarrarMethodologicalPrinciplesOntology2005,
  title = {Towards {{Methodological Principles}} for {{Ontology Engineering}}.},
  author = {{Mustafa Jarrar}},
  date = {2005},
  journaltitle = {Available at SSRN 1117002},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1117002},
  urldate = {2023-10-18},
  keywords = {No DOI found,已整理,本體建立,知識本體},
  note = {說明建立本體的方法，有148次引用，應該低於那些經典方法，可能要檢查這些方法之間有什麼不同。},
  file = {C:\Users\BlackCat\Zotero\storage\H6SGRXGL\Jarrar - 2005 - Towards Methodological Principles for Ontology Eng.pdf}
}

@article{nammeekimTeachingMethodsStructural2022,
  title = {A {{Teaching Methods}} of {{Structural Mapping}} as a {{Visualization Process}} of {{Summary Thinking}}},
  author = {{Nammee Kim}},
  date = {2022-02-28},
  journaltitle = {The Korean Association of General Education},
  shortjournal = {Korean J General Edu},
  volume = {16},
  number = {1},
  pages = {227--240},
  issn = {1976-3212, 2714-1101},
  doi = {10.46392/kjge.2022.16.1.227},
  url = {https://j-kagedu.or.kr/journal/view.php?doi=10.46392/kjge.2022.16.1.227},
  urldate = {2023-10-23},
  abstract = {The purpose of this thesis is to propose a method of teaching abstract thinking through structural drawing that visualizes the macro structure in a liberal arts writing class at a university. First, by analyzing the results of the learner’s structure diagram from 2018 to 2021, we calculated notable issues for summary thinking training, and based on this, we proposed a practical class plan. First, it was important to clearly maintain the distance between the learner, who is the subject of the text summary, and the original author of the text. This is an important principle for accurately understanding the text and also for understanding the learners themselves, who again are the subjects of the summary. Second, it was important to adjust the level of abstraction and generalization of text information. This abstraction generalization process is a process of considering abstract thinking and the relationship between texts by reflecting the key elements constituting the text in the structural diagram. Third, it was important to understand the text composition principle through the activity to check the hierarchy among the core information constituting the text. Some examples of possible classes to help us achieve the main goal for this summary thinking were introduced. First, we presented the process of internalizing the thinking process involved in abstract thinking through multi-faceted feedback activities on the structural diagram using abstract thinking. Second, in order to strengthen the process of understanding the internal relationship of the text structure, a method for internalizing the concept and thought process used in actual thinking was suggested by comparatively analyzing the meta-activities identified in the previous feedback process. Finally, a method to strengthen the openness of summary thinking was suggested by imparting that the structure drawing activity itself has an open possibility.},
  langid = {english},
  keywords = {研究流程},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 结构图教学法作为总结性思维的可视化过程\\
abstractTranslation:  本论文旨在提出一种在大学文科写作课中通过结构图将宏观结构可视化的抽象思维教学方法。首先，通过分析2018年至2021年学习者结构图的绘制结果，计算出总结性思维训练的显著问题，在此基础上提出切实可行的课堂教学方案。首先，要明确保持作为文本总结主体的学习者与文本原作者之间的距离。这是准确理解文本的重要原则，也是理解作为摘要主体的学习者本身的重要原则。其次，要调整文本信息的抽象概括程度。这个抽象概括的过程，就是通过在结构图中体现构成文本的关键要素，思考抽象思维和文本之间关系的过程。第三，要通过检查构成文本的核心信息之间的层次关系的活动来理解文本构成原理。我们介绍了一些可能的类例，以帮助我们实现本次总结思考的主要目标。首先，我们通过对结构图的多方位反馈活动，介绍了运用抽象思维内化抽象思维所涉及的思维过程。其次，为了加强对文本结构内部关系的理解过程，通过对比分析之前反馈过程中发现的元活动，提出了内化实际思考中使用的概念和思维过程的方法。最后，通过传授结构图绘制活动本身具有开放性的可能性，提出了加强总结思维开放性的方法。},
  note = {[TLDR] A method of teaching abstract thinking through structural drawing that visualizes the macro structure in a liberal arts writing class at a university is proposed and a practical class plan is proposed for this summary thinking training.},
  file = {C:\Users\BlackCat\Zotero\storage\ULI5G9S9\Kim - 2022 - A Teaching Methods of Structural Mapping as a Visu.pdf}
}

@inproceedings{nardosalemuFrameworkDevelopAutomatic2023,
  title = {A {{Framework}} to {{Develop Automatic Speech Recognition}} for {{Low Resource Languages}}},
  booktitle = {Proceedings of the 54th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 2},
  author = {{Nardos Alemu} and {Chelsea Hua} and {Phuc H. Le} and {Khoi P. N. Nguyen} and {Melat Ali} and {Nanette Veilleux}},
  year = {3 月 6, 2023},
  series = {{{SIGCSE}} 2023},
  pages = {1228},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3545947.3573271},
  url = {https://doi.org/10.1145/3545947.3573271},
  urldate = {2023-09-03},
  abstract = {Current Automatic Speech Recognition (ASR) systems, like Google Assistant, Apple's Siri, or Amazon's Alexa, continue to only support a small number of languages (English, Mandarin, Arabic, etc.), primarily those spoken in developed nations with abundant resources. While these languages have been able to reap the benefits of having such technology at their disposal, places like Ethiopia, and Vietnam are still far behind. This work represents a global collaboration to create a framework for customizing ASR systems for low resource languages (LRLs), or languages with limited human and financial resources. This paper describes the methodology for using an existing application (Kaldi) to implement an ASR system for two such languages, Amharic and Vietnamese, with the least amount of annotated speech. The languages are chosen to leverage available student expertise and create cross-cultural connections. The objective of the research is to create a procedure by which, given enough training records and annotation, any language can be added.},
  isbn = {978-1-4503-9433-8},
  langid = {english},
  keywords = {low resource languages,無法取得,語音辨識},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 為低資源語言開發自動語音識別的框架\\
abstractTranslation:  當前的自動語音識別 (ASR) 系統，如 Google Assistant、蘋果的 Siri 或亞馬遜的 Alexa，仍然僅支持少數語言（英語、普通話、阿拉伯語等），主要是資源豐富的發達國家所使用的語言。雖然這些語言已經能夠從掌握此類技術中獲益，但埃塞俄比亞和越南等國家仍然遠遠落後。這項工作代表了一項全球合作，旨在創建一個框架，為低資源語言 (LRL) 或人力和財力資源有限的語言定制 ASR 系統。本文描述了使用現有應用程序 (Kaldi) 為阿姆哈拉語和越南語這兩種語言實現 ASR 系統的方法，並使用最少的註釋語音。選擇這些語言是為了利用現有的學生專業知識並建立跨文化聯繫。該研究的目標是創建一個程序，只要有足夠的訓練記錄和註釋，就可以添加任何語言。}
}

@article{naseerahmedsajidSingleVsMultiLabel2023,
  title = {Single vs. {{Multi-Label}}: {{The Issues}}, {{Challenges}} and {{Insights}} of {{Contemporary Classification Schemes}}},
  shorttitle = {Single vs. {{Multi-Label}}},
  author = {{Naseer Ahmed Sajid} and {Atta Rahman} and {Munir Ahmad} and {Dhiaa Musleh} and {Mohammed Imran Basheer Ahmed} and {Reem Alassaf} and {Sghaier Chabani} and {Mohammed Salih Ahmed} and {Asiya Abdus Salam} and {Dania AlKhulaifi}},
  date = {2023-01},
  journaltitle = {Applied Sciences},
  volume = {13},
  number = {11},
  pages = {6804},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app13116804},
  url = {https://www.mdpi.com/2076-3417/13/11/6804},
  urldate = {2023-10-25},
  abstract = {Over the decades, a tremendous increase has been witnessed in the production of documents available in digital form. The increased production of documents has gained so much momentum that their rate of production jumps two-fold every five years. These articles are searched over the internet via search engines, digital libraries, and citation indexes. However, the retrieval of relevant research papers for user queries is still a pipedream. This is because scientific documents are not indexed based on some subject classification hierarchies. Hence, the classification of these documents becomes a challenging task for the researchers. Classification of the documents can be two-fold: one way is to assign a single label to each document and the other is to assign multi-labels to each document based on its belonging domains. Classification of the documents can be performed by using either the available metadata or the whole content of the documents. While performing classification, there are many challenges which may belong to the dataset, feature selection technique, preprocessing methodology, and which classification model is suitable for the classification of the documents. This paper highlights the issues for single-label and multi-label classification by using either metadata or content of the documents and why metadata-based approaches are better than content-based approaches in terms of feasibility.},
  issue = {11},
  langid = {english},
  keywords = {classification,data mining and ML,digital libraries,multi-label,single label,回收,已整理,文獻分類,知識分類},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 單一標籤與多標籤：當代分類方案的問題、挑戰與見解},
  file = {C:\Users\BlackCat\Zotero\storage\YBMLDWTR\Sajid et al. - 2023 - Single vs. Multi-Label The Issues, Challenges and.pdf}
}

@article{naserahmadiRuleHubPublicCorpus2020,
  title = {{{RuleHub}}: {{A Public Corpus}} of {{Rules}} for {{Knowledge Graphs}}},
  shorttitle = {{{RuleHub}}},
  author = {{Naser Ahmadi} and {Thi-Thuy-Duyen Truong} and {Le-Hong-Mai Dao} and {Stefano Ortona} and {Paolo Papotti}},
  year = {10 月 15, 2020},
  journaltitle = {數據與信息質量雜誌},
  shortjournal = {J. Data and Information Quality},
  volume = {12},
  number = {4},
  pages = {21:1--21:22},
  issn = {1936-1955},
  doi = {10.1145/3409384},
  url = {https://dl.acm.org/doi/10.1145/3409384},
  urldate = {2023-08-24},
  abstract = {以實體為中心的知識圖（KG）現在很流行用於收集有關實體的事實。KG 具有豐富的模式，具有大量不同的類型和謂詞來描述實體及其關係。在這些豐富的模式上，邏輯規則用於表示數據元素之間的依賴關係。雖然規則在查詢回答、數據管理和其他任務中很有用，但它們通常不隨知識圖譜一起提供。這些規則必須手動定義或借助規則挖掘方法發現。我們認為這個規則收集任務應該集體完成，以更好地利用我們對數據的理解，並避免在相同的知識圖譜上進行重複的工作。為此，我們引入RuleHub，我們可擴展的公共 KG 規則集。RuleHub 為所有用戶提供規則歸檔和檢索功能，其可擴展架構不限制 KG 或支持的規則類型。我們正在用來自最流行的知識圖譜的數千條規則填充語料庫，並報告我們通過統計測量自動表徵規則質量的實驗。},
  langid = {english},
  keywords = {graph dependencies,完整公開,知識圖譜,規則挖掘,資料挖掘},
  annotation = {5 citations (Crossref) [2024-03-26]\\
titleTranslation: RuleHub：知識圖規則的公共語料庫\\
abstractTranslation:  以實體為中心的知識圖(KG)現在很流行收集有關實體的事實。KG具有豐富的模式，具有大量不同的類型和謂詞來描述實體關係。在這些豐富的模式上，邏輯規則用於表示數據元素之間的依賴。雖然規則在查詢回答、數據管理和其他任務中很有用，但它們通常不隨知識圖譜關係一起提供。這些規則必須手動定義或借助規則挖掘方法發現。我們認為這個規則收集任務應該集體完成，以便更好地利用我們對數據的理解，並避免在相同的知識圖譜上進行重複的工作。為此，我們引入RuleHub，我們可擴展的公共知識庫規則集。RuleHub 為全部提供用戶規則歸檔和檢索功能，其可擴展架構不限制KG或支持的規則類型。我們正在使用來自最流行的知識圖譜的數千條規則填充語料庫，並報告我們通過統計測量自動表徵規則質量的實驗。},
  file = {C:\Users\BlackCat\Zotero\storage\7DYEA7Y3\Ahmadi 等。 - 2020 - RuleHub A Public Corpus of Rules for Knowledge Gr.pdf}
}

@inproceedings{natalyaf.noyMechanicalTurkOntology2013,
  title = {Mechanical Turk as an Ontology Engineer? Using Microtasks as a Component of an Ontology-Engineering Workflow},
  shorttitle = {Mechanical Turk as an Ontology Engineer?},
  booktitle = {Proceedings of the 5th {{Annual ACM Web Science Conference}}},
  author = {{Natalya F. Noy} and {Jonathan Mortensen} and {Mark A. Musen} and {Paul R. Alexander}},
  year = {5 月 2, 2013},
  series = {{{WebSci}} '13},
  pages = {262--271},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2464464.2464482},
  url = {https://dl.acm.org/doi/10.1145/2464464.2464482},
  urldate = {2023-09-15},
  abstract = {Ontology evaluation has proven to be one of the more difficult problems in ontology engineering. Researchers proposed numerous methods to evaluate logical correctness of an ontology, its structure, or coverage of a domain represented by a corpus. However, evaluating whether or not ontology assertions correspond to the real world remains a manual and time-consuming task. In this paper, we explore the feasibility of using microtask crowdsourcing through Amazon Mechanical Turk to evaluate ontologies. Specifically, we look at the task of verifying the subclass--superclass hierarchy in ontologies. We demonstrate that the performance of Amazon Mechanical Turk workers (turkers) on this task is comparable to the performance of undergraduate students in a formal study. We explore the effects of the type of the ontology on the performance of turkers and demonstrate that turkers can achieve accuracy as high as 90\% on verifying hierarchy statements form common-sense ontologies such as WordNet. Finally, we compare the performance of turkers to the performance of domain experts on verifying statements from an ontology in the biomedical domain. We report on lessons learned about designing ontology-evaluation experiments on Amazon Mechanical Turk. Our results demonstrate that microtask crowdsourcing can become a scalable and efficient component in ontology-engineering workflows.},
  isbn = {978-1-4503-1889-1},
  langid = {english},
  keywords = {/unread,Amazon mechanical turk,crowdsourcing,human computation,ontology,semantic web,已整理,本體驗證,知識本體},
  annotation = {27 citations (Crossref) [2024-03-26]\\
abstractTranslation:  本體評估已被證明是本體工程中較困難的問題之一。研究人員提出了多種方法來評估本體、其結構或語料庫所代表的領域的覆蓋範圍的邏輯正確性。然而，評估本體論斷言是否對應於現實世界仍然是一項手動且耗時的任務。在本文中，我們探討了透過 Amazon Mechanical Turk 使用微任務眾包來評估本體的可行性。具體來說，我們研究驗證本體中的子類別-超類別層次結構的任務。我們證明，亞馬遜 Mechanical Turk 工人 (turker) 在這項任務上的表現與正式研究中本科生的表現相當。我們探討了本體類型對 turker 效能的影響，並證明 turker 在驗證諸如 WordNet 等常識本體的層次結構語句時可以達到高達 90\% 的準確率。最後，我們將 turker 的表現與領域專家在驗證生物醫學領域本體的陳述方面的表現進行比較。我們報告了在 Amazon Mechanical Turk 上設計本體評估實驗的經驗教訓。我們的結果表明，微任務眾包可以成為本體工程工作流程中的可擴展且高效的組件。\\
titleTranslation: 機械土耳其人當本體工程師？使用微任務作為本體工程工作流程的組成部分},
  note = {探討在知識本體驗證階段使用工人智慧的可能性。},
  file = {C:\Users\BlackCat\Zotero\storage\R93TKJ6K\Noy 等。 - 2013 - Mechanical turk as an ontology engineer using mic.pdf}
}

@inproceedings{nicoletapredaActiveKnowledgeDynamically2010,
  title = {Active Knowledge: Dynamically Enriching {{RDF}} Knowledge Bases by Web Services},
  shorttitle = {Active Knowledge},
  booktitle = {Proceedings of the 2010 {{ACM SIGMOD International Conference}} on {{Management}} of Data},
  author = {{Nicoleta Preda} and {Gjergji Kasneci} and {Fabian M. Suchanek} and {Thomas Neumann} and {Wenjun Yuan} and {Gerhard Weikum}},
  year = {6 月 6, 2010},
  series = {{{SIGMOD}} '10},
  pages = {399--410},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1807167.1807212},
  url = {https://dl.acm.org/doi/10.1145/1807167.1807212},
  urldate = {2023-08-23},
  abstract = {The proliferation of knowledge-sharing communities and the advances in information extraction have enabled the construction of large knowledge bases using the RDF data model to represent entities and relationships. However, as the Web and its latently embedded facts evolve, a knowledge base can never be complete and up-to-date. On the other hand, a rapidly increasing suite of Web services provide access to timely and high-quality information, but this is encapsulated by the service interface. We propose to leverage the information that could be dynamically obtained from Web services in order to enrich RDF knowledge bases on the fly whenever the knowledge base does not suffice to answer a user query. To this end, we develop a sound framework for appropriately generating queries to encapsulated Web services and efficient algorithms for query execution and result integration. The query generator composes sequences of function calls based on the available service interfaces. As Web service calls are expensive, our method aims to reduce the number of calls in order to retrieve results with sufficient recall. Our approach is fully implemented in a complete prototype system named ANGIE1. The user can query and browse the RDF knowledge base as if it already contained all the facts from the Web services. This data, however, is gathered and integrated on the fly, transparently to the user. We demonstrate the viability and efficiency of our approach in experiments based on real-life data provided by popular Web services.},
  isbn = {978-1-4503-0032-2},
  langid = {english},
  keywords = {binding patterns,information integration,knowledge bases,query mediation,rdf,semantics,warehousing,知識圖譜,知識本體},
  annotation = {26 citations (Crossref) [2024-03-26]\\
titleTranslation: 主動知識：通過Web服務動態豐富RDF知識庫\\
abstractTranslation:  知識共享社區的激增和信息提取的進步使得能夠使用 RDF 數據模型來構建大型知識庫來表示實體和關係。然而，隨著網絡及其潛在嵌入事實的發展，知識庫永遠不可能是完整和最新的。另一方面，快速增長的 Web 服務套件提供了對及時、高質量信息的訪問，但這是由服務接口封裝的。我們建議利用可以從 Web 服務動態獲取的信息，以便在知識庫不足以回答用戶查詢時動態豐富 RDF 知識庫。為此，我們開發了一個健全的框架，用於適當生成對封裝的 Web 服務的查詢，以及用於查詢執行和結果集成的高效算法。查詢生成器根據可用的服務接口組成函數調用序列。由於 Web 服務調用的成本很高，因此我們的方法旨在減少調用次數，以便檢索具有足夠召回率的結果。我們的方法完全在名為 ANGIE1 的完整原型系統中實現。用戶可以查詢和瀏覽 RDF 知識庫，就好像它已經包含了來自 Web 服務的所有事實一樣。然而，這些數據是動態收集和集成的，對用戶來說是透明的。我們在基於流行 Web 服務提供的現實生活數據的實驗中證明了我們的方法的可行性和效率。},
  file = {C:\Users\BlackCat\Zotero\storage\BBCL3QDV\Preda 等。 - 2010 - Active knowledge dynamically enriching RDF knowle.pdf}
}

@article{NiFeiJiYuZhenShiShiJieYanJiuFangFaGouJianZhongYiXinXiJiBingZhengZhuangShuYuZhiShiTiXi2022,
  title = {基于真实世界研究方法构建中医心系疾病症状术语知识体系},
  author = {{倪菲} and {袁东超} and {杨茗茜} and {陈彦君} and {曲金桥} and {郭鹤} and {郑一} and {于睿} and {辛华}},
  date = {2022},
  journaltitle = {辽宁中医杂志},
  volume = {49},
  number = {3},
  pages = {3},
  url = {http://qikan.cqvip.com/Qikan/Article/Detail?id=00002G8IK53G7JP0MLDO6JP16NR},
  urldate = {2022-10-18},
  abstract = {目的基于真实世界研究方法构建中医心系疾病临床症状术语知识体系,不仅为中医症状术语规范化及标准化奠定基础,还为中医辨证论治心血管系统疾病提供规范化诊疗思路.方法以辽宁中医药大学图书馆藏书为基础,全面检索近现代中医临床名家医案专著,共检索符合要求医案150例,根据纳入标准及排除标准,最终纳入医案80例,应用Excel 2019对医案数据以及高频症状术语(出现频率≥15\%的症状)频数及频次,四诊类别,五脏归属,功能分类情况进行描述性统计分析.结果共确立内涵最小的独立症状术语194个,限定出现频率≥15\%为高频症状术语,为胸闷,乏力,气短等27个,累计使用频次709次.结论通过真实世界数据研究,分别从"主症-次症-兼症";"五脏系统之心-肝-脾-肺-肾"以及"神志-情绪-饮食-呼吸"3条主线构建了中医临床心系疾病症状术语知识结构体系,初步规范了中医心系疾病症状术语.},
  keywords = {No DOI found,中医心系疾病,應該有用,术语,症状,真实世界研究,知识体系}
}

@inproceedings{nigamReviewPaperApplication2020,
  title = {A {{Review Paper On The Application Of Knowledge Graph On Various Service Providing Platforms}}},
  booktitle = {2020 10th {{International Conference}} on {{Cloud Computing}}, {{Data Science}} \& {{Engineering}} ({{Confluence}})},
  author = {Nigam, Vanshika Vikas and Paul, Shreya and Agrawal, Arun Prakash and Bansal, Rishabh},
  date = {2020-01},
  pages = {716--720},
  doi = {10.1109/Confluence47617.2020.9058298},
  url = {https://ieeexplore.ieee.org/document/9058298},
  urldate = {2023-12-03},
  abstract = {During the past decade or so, knowledge graphs have stealthily made way into our daily lives, whether through voice assistants (the likes of Alexa, Google Assistant or Siri), spontaneous search results or customized personal shopping experiences through online stores recommenders. Today, we are surrounded and constantly interacting with knowledge graphs on a regular basis. The scope and impact of knowledge graphs and underlying graph databases are still a mystery to people. Considering its smooth entry into our lives, most of us are unaware of how dependent we actually are on the technology. In this paper, we have shown the working of the Google knowledge graph and how the knowledge graph works as the most effective recommendation system. We have discussed how it finds its way through daily life in various fields like Banking, Social media (Facebook, LinkedIn, Netflix), Food application (Uber Eats) and the Healthcare sector.},
  eventtitle = {2020 10th {{International Conference}} on {{Cloud Computing}}, {{Data Science}} \& {{Engineering}} ({{Confluence}})},
  langid = {english},
  keywords = {Review,已整理,知識圖譜},
  annotation = {6 citations (Crossref) [2024-03-26]\\
titleTranslation: 知識圖譜在各種服務提供平台上的應用綜述論文},
  note = {說明知識圖譜的應用，但主要著重在推薦的部分。
\par
可以做為知識圖譜的介紹使用。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\MLZ4TFAY\\Nigam et al. - 2020 - A Review Paper On The Application Of Knowledge Gra.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\RDZZABAA\\9058298.html}
}

@misc{nigelzankerReviewSucceedingYour2009,
  title = {Review: Succeeding with Your Master's Dissertation: A Step-by-Step Handbook},
  shorttitle = {Review},
  author = {{Nigel Zanker}},
  date = {2009},
  abstract = {Review: succeeding with your master's dissertation: a step-by-step handbook},
  langid = {english},
  keywords = {Design not elsewhere classified,Design Practice and Management not elsewhere classified},
  annotation = {titleTranslation: 评论：成功完成硕士论文：循序渐进手册\\
abstractTranslation:  评论：成功完成硕士论文：循序渐进手册}
}

@thesis{NiLinFengLiuChengXiangGuanZhiChengXuJianBieMingFenXi1996,
  title = {流程相關之程序間別名分析},
  author = {{倪林峰}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {1996},
  journaltitle = {資訊工程學系},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/uf6979},
  abstract = {程式的最佳化與平行化有賴於精確的資料流程分析。精確的別名資訊可以 明顯地提高資料流程分析的精確度,尤其對提供指標型別的程式語言,精確 的別名分析更顯其重要性。在這篇論文中,我們對C程式提出一個流程相關 的程序間別名分析的演算法。為了能精確地計算出每個程式點上的別名關 係,我們採取儘可能完整地記錄控制流程資訊的策略。我們已完成別名分 析演算法的製作及初步的測試以驗証演算法的精確性。},
  pagetotal = {66}
}

@inproceedings{nimbalkarSemanticInterpretationStructured2016,
  title = {Semantic {{Interpretation}} of {{Structured Log Files}}},
  booktitle = {2016 {{IEEE}} 17th {{International Conference}} on {{Information Reuse}} and {{Integration}} ({{IRI}})},
  author = {Nimbalkar, Piyush and Mulwad, Varish and Puranik, Nikhil and Joshi, Anupam and Finin, Tim},
  date = {2016-07},
  pages = {549--555},
  doi = {10.1109/IRI.2016.81},
  url = {https://ieeexplore.ieee.org/document/7785790},
  urldate = {2024-01-30},
  abstract = {Data from computer log files record traces of events involving user activity, applications, system software and network traffic. Logs are usually intended for diagnostic and debugging purposes, but their data can be extremely useful in system audits and forensic investigations. Logs created by intrusion detection systems, Web servers, antivirus and anti-malware systems, firewalls and network devices have information that can reconstruct the activities of malware or a malicious agent, help plan for remediation and prevent attacks by revealing probes or intrusions before damage has been done. While existing tools like Splunk can help analyze logs with known schemas, understanding log whose format is unfamiliar or associated with new device or custom application can be challenging. We describe a framework for analyzing logs and automatically generating a semantic description of their schema and content in RDF. The framework begins by normalizing the log into columns and rows using regular expression-based and dictionary-based classifiers. Leveraging our existing work on inferring the semantics of tables, we associate semantic types with columns and, when possible, map them to concepts in general knowledge-bases (e.g. DBpedia) and domain specific ones (e.g., Unified Cybersecurity Ontology). We link cell values to known type instances (e.g., an IP address) and suggest relationships between columns. Converting large and verbose log files into such semantic representations reveals their meaning and supports search, integration and reasoning over the data.},
  eventtitle = {2016 {{IEEE}} 17th {{International Conference}} on {{Information Reuse}} and {{Integration}} ({{IRI}})},
  langid = {english},
  keywords = {Computer security,cybersecurity,knowledge graph,linked-data,log,log files,Ontologies,Resource description framework,Semantics,Web servers,已整理,知識本體},
  annotation = {12 citations (Crossref) [2024-03-26]\\
titleTranslation: 結構化日誌檔的語意解釋\\
abstractTranslation:  電腦日誌檔案中的資料記錄了涉及使用者活動、應用程式、系統軟體和網路流量的事件痕跡。日誌通常用於診斷和調試目的，但它們的數據在系統審計和取證調查中非常有用。由入侵偵測系統、Web 伺服器、防毒軟體和反惡意軟體系統、防火牆和網路裝置建立的日誌所包含的資訊可以重建惡意軟體或惡意代理程式的活動，幫助規劃補救措施，並透過在損害發生之前揭示探測或入侵來防止攻擊。已經完成了。雖然 Splunk 等現有工具可以幫助分析具有已知模式的日誌，但理解格式不熟悉或與新設備或自訂應用程式相關的日誌可能具有挑戰性。我們描述了一個用於分析日誌並自動產生 RDF 中的模式和內容的語義描述的框架。此框架首先使用基於正規表示式和基於字典的分類器將日誌規範化為列和行。利用我們現有的推斷表語義的工作，我們將語義類型與列相關聯，並在可能的情況下將它們映射到通用知識庫（例如DBpedia）和特定領域知識庫（例如統一網路安全本體）中的概念。我們將單元格值連結到已知類型實例（例如 IP 位址）並建議列之間的關係。將大型且詳細的日誌檔案轉換為此類語義表示可以揭示其含義並支援對資料的搜尋、整合和推理。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\XM2ZFB6D\\Nimbalkar et al. - 2016 - Semantic Interpretation of Structured Log Files.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\VZKX82JI\\7785790.html}
}

@online{ningyuzhangConceptualizedRepresentationLearning2020,
  title = {Conceptualized {{Representation Learning}} for {{Chinese Biomedical Text Mining}}},
  author = {{Ningyu Zhang} and {Qianghuai Jia} and {Kangping Yin} and {Liang Dong} and {Feng Gao} and {Nengwei Hua}},
  date = {2020-08-25},
  eprint = {2008.10813},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2008.10813},
  url = {http://arxiv.org/abs/2008.10813},
  urldate = {2023-06-24},
  abstract = {Biomedical text mining is becoming increasingly important as the number of biomedical documents and web data rapidly grows. Recently, word representation models such as BERT has gained popularity among researchers. However, it is difficult to estimate their performance on datasets containing biomedical texts as the word distributions of general and biomedical corpora are quite different. Moreover, the medical domain has long-tail concepts and terminologies that are difficult to be learned via language models. For the Chinese biomedical text, it is more difficult due to its complex structure and the variety of phrase combinations. In this paper, we investigate how the recently introduced pre-trained language model BERT can be adapted for Chinese biomedical corpora and propose a novel conceptualized representation learning approach. We also release a new Chinese Biomedical Language Understanding Evaluation benchmark (\textbackslash textbf\{ChineseBLUE\}). We examine the effectiveness of Chinese pre-trained models: BERT, BERT-wwm, RoBERTa, and our approach. Experimental results on the benchmark show that our approach could bring significant gain. We release the pre-trained model on GitHub: https://github.com/alibaba-research/ChineseBLUE.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  annotation = {38 citations (Semantic Scholar/arXiv) [2023-06-24]\\
titleTranslation: 中文生物醫學文本挖掘的概念化表示學習\\
abstractTranslation:  隨著生物醫學文檔和網絡數據數量的快速增長，生物醫學文本挖掘變得越來越重要。最近，諸如 BERT 之類的單詞表示模型在研究人員中越來越受歡迎。然而，由於一般語料庫和生物醫學語料庫的單詞分佈有很大不同，因此很難估計它們在包含生物醫學文本的數據集上的性能。此外，醫學領域存在難以通過語言模型學習的長尾概念和術語。對於中文生物醫學文本來說，由於其結構複雜、短語組合多樣，難度較大。在本文中，我們研究了最近引入的預訓練語言模型 BERT 如何適用於中國生物醫學語料庫，並提出了一種新穎的概念化表示學習方法。我們還發布了新的中文生物醫學語言理解評估基準（\textbackslash textbf\{ChineseBLUE\}）。我們研究了中國預訓練模型的有效性：BERT、BERT-wwm、RoBERTa 和我們的方法。基準測試的實驗結果表明，我們的方法可以帶來顯著的收益。我們在 GitHub 上發布了預訓練模型：https://github.com/alibaba-research/ChineseBLUE。},
  note = {\href{https://blog.csdn.net/ld326/article/details/118685409}{[论文阅读笔记45]ChineseBLUE[MC-BERT]\_chineseblue数据集\_happyprince的博客-CSDN博客}
\par
\href{https://github.com/alibaba-research/ChineseBLUE/tree/master}{alibaba-research/ChineseBLUE: Chinese Biomedical Language Understanding Evaluation benchmark (ChineseBLUE) (github.com)}
\par
Comment: WSDM2020 Health Day
\par
\href{https://blog.csdn.net/ld326/article/details/118685409}{[论文阅读笔记45]ChineseBLUE[MC-BERT]\_chineseblue数据集\_happyprince的博客-CSDN博客}},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\WPPG7DTP\\Zhang 等。 - 2020 - Conceptualized Representation Learning for Chinese.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\EJD5D984\\2008.html}
}

@thesis{niyativyasApproachUsingEmbodied2022,
  type = {M.Sc.},
  title = {An {{Approach}} of {{Using Embodied Conversational Agent}} for {{Personalized Tutoring}}},
  author = {{Niyati Vyas}},
  date = {2022},
  journaltitle = {ProQuest Dissertations and Theses},
  institution = {University of Windsor (Canada)},
  location = {Canada -- Ontario, CA},
  url = {https://www.proquest.com/docview/2665153687/abstract/F2AAC7CBE05C4944PQ/1},
  urldate = {2023-10-18},
  abstract = {Since the onset of internet, the world had been slowly and steadily turning digital, taking its time. The introduction of the deadly coronavirus has forced this transformation to pickup its pace. These unprecedented times have forced all daily activities to be carried out maintaining a social distance and everything from shopping to studying is being preferred to be done online, which has given a surge to new and effective educational technologies. While many studies are actively working on linear and collaborative learning in a virtual learning environment, few have tried to exploit the use of Embodied Conversational Agent (ECA) for personalized tutoring. This thesis research develops a novel approach that is able to not only allow ECA to perform the role of human tutors but also interact with users in an adaptive manner, according to their level of knowledge. This approach extends the research previously accomplished by our research group that uses self-adjusted POMDP policies to guide the interaction between ECA and users. Furthermore, the construction of a generic, scalable tutoring ontology enables the dialogue management of ECA to track the history of belief states and use the trend of changes to estimate the level of user’s knowledge for the control of feedback and the tasks presented to the user. It is anticipated that the research work helps improving online learning with the ECA acting as a personal tutor.},
  isbn = {9798438731856},
  langid = {english},
  pagetotal = {87},
  keywords = {Embodied conversational agent,Tutoring ontology,Virtual reality,已整理},
  annotation = {titleTranslation: 使用嵌入式对话代理进行个性化辅导的方法\\
abstractTranslation:  自互联网出现以来，世界一直在慢慢地、稳步地实现数字化。致命冠状病毒的出现迫使这一转变加快了步伐。这个史无前例的时代迫使所有日常活动都要保持一定的社交距离，从购物到学习，人们都喜欢在网上完成，这也催生了新的、有效的教育技术。虽然许多研究都在积极研究虚拟学习环境中的线性学习和协作学习，但很少有人尝试利用嵌入式对话代理（ECA）进行个性化辅导。本论文研究开发了一种新方法，不仅能让 ECA 扮演人类辅导员的角色，还能根据用户的知识水平，以自适应的方式与用户互动。这种方法扩展了我们研究小组之前完成的研究，即使用自我调整的 POMDP 策略来指导 ECA 与用户之间的互动。此外，通过构建一个通用的、可扩展的辅导本体，ECA 的对话管理可以跟踪信念状态的历史，并利用变化趋势来估计用户的知识水平，从而控制反馈和向用户提出的任务。预计这项研究工作将有助于改善由 ECA 充当个人导师的在线学习。},
  note = {使用個人代理 ( ECA ) 來實現虛擬的導師。},
  file = {C:\Users\BlackCat\Zotero\storage\F9YFP4HN\Vyas - 2022 - An Approach of Using Embodied Conversational Agent.pdf}
}

@inproceedings{nourhenealayaWhatMakesOntology2015,
  title = {What {{Makes Ontology Reasoning}} so {{Arduous}}? {{Unveiling}} the Key Ontological Features},
  shorttitle = {What {{Makes Ontology Reasoning}} so {{Arduous}}?},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Web Intelligence}}, {{Mining}} and {{Semantics}}},
  author = {{Nourhène Alaya} and {Sadok Ben Yahia} and {Myriam Lamolle}},
  year = {7 月 13, 2015},
  series = {{{WIMS}} '15},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2797115.2797117},
  url = {https://dl.acm.org/doi/10.1145/2797115.2797117},
  urldate = {2023-09-15},
  abstract = {Reasoning with ontologies is one of the core fields of research in Description Logics. A variety of efficient reasoner with highly optimized algorithms have been developed to allow inference tasks on expressive ontology languages such as OWL(DL). However, reasoner reported computing times have exceeded and sometimes fall behind the expected theoretical values. From an empirical perspective, it is not yet well understood, which particular aspects in the ontology are reasoner performance degrading factors. In this paper, we conducted an investigation about state of art works that attempted to portray potential correlation between reasoner empirical behaviour and particular ontological features. These works were analysed and then broken down into categories. Further, we proposed a set of ontology features covering a broad range of structural and syntactic ontology characteristics. We claim that these features are good indicators of the ontology hardness level against reasoning tasks. In order to assess the worthiness of our proposals, we adopted a supervised machine learning approach. Features served as the bases to learn predictive models of reasoners robustness. These models was trained for 6 well known reasoners and using their evaluation results during the ORE'2014 competition. Our prediction models showed a high accuracy level which witness the effectiveness of our set of features.},
  isbn = {978-1-4503-3293-4},
  langid = {english},
  keywords = {/unread,已整理,監督式學習,知識推理,知識本體},
  annotation = {5 citations (Crossref) [2024-03-26]\\
abstractTranslation:  本體推理是描述邏輯研究的核心領域之一。各種具有高度最佳化演算法的高效推理器已經被開發出來，以允許在表達性本體語言（例如 OWL（DL））上執行推理任務。然而，reasoner 報告的計算時間已經超過甚至有時落後於預期的理論值。從實證角度來看，尚不清楚本體中的哪些特定方面是推理機性能下降的因素。在本文中，我們對藝術作品進行了一項調查，試圖描繪推理者經驗行為與特定本體論特徵之間的潛在相關性。對這些作品進行分析，然後將其分為幾類。此外，我們提出了一組涵蓋廣泛的結構和句法本體特徵的本體特徵。我們聲稱這些特徵是針對推理任務的本體硬度水準的良好指標。為了評估我們提案的價值，我們採用了監督機器學習方法。特徵是學習推理器穩健性預測模型的基礎。這些模型接受了 6 位知名推理者的訓練，並在 ORE'2014 競賽期間使用了他們的評估結果。我們的預測模型顯示出很高的準確度，這證明了我們的特徵集的有效性。\\
titleTranslation: 是什麼讓本體推理如此艱鉅？揭示關鍵本體論特徵},
  note = {訓練一個模型從多個推理器學到推理方法，大概看。},
  file = {C:\Users\BlackCat\Zotero\storage\S7AG25AT\Alaya 等。 - 2015 - What Makes Ontology Reasoning so Arduous Unveilin.pdf}
}

@inproceedings{nupapKnowledgeManagementSystem2022,
  title = {Knowledge {{Management System}} by Applying {{Knowledge Creating Company}}: {{Transforming Tacit}} to {{Explicit Knowledge}}},
  shorttitle = {Knowledge {{Management System}} by Applying {{Knowledge Creating Company}}},
  booktitle = {2022 {{Joint International Conference}} on {{Digital Arts}}, {{Media}} and {{Technology}} with {{ECTI Northern Section Conference}} on {{Electrical}}, {{Electronics}}, {{Computer}} and {{Telecommunications Engineering}} ({{ECTI DAMT}} \& {{NCON}})},
  author = {Nupap, Soontarin},
  date = {2022-01},
  pages = {439--444},
  issn = {2768-4644},
  doi = {10.1109/ECTIDAMTNCON53731.2022.9720388},
  url = {https://ieeexplore.ieee.org/document/9720388},
  urldate = {2023-11-28},
  abstract = {In this era, knowledge management system (KMS) has been accredited as a knowledge repository for utilisation in order to sustain knowledge sharing, capturing, and institutionalising from organisational stakeholders as well as to maintain long-term competitive advantage and sustainable development. Relied on my preceding study that indicated an appropriate KM framework for quality assurance (QA) in management for education, this study endeavour to advocate a framework of KMS implementation based on knowledge creating company notation for a case study which is a group of research team within a university department in Thailand confronting the problems of failing to meet with a national QA standard, topic namely, “research”, by developing a KMS based on the prepared IC system from the preceding study including; knowledge creating company theory, software development life cycle(SDLC), and joint application development(JAD) in order to transform tacit knowledge to explicit knowledge and organise the explicit knowledge to a knowledge repository as a KMS. The initial findings disclose the suggested methodology wherein the KMS primarily applied for resolving problem in QA in management for education was appropriate to sustenance knowledge sharing and learning atmosphere for long-term competitive advantage and sustainable development.},
  eventtitle = {2022 {{Joint International Conference}} on {{Digital Arts}}, {{Media}} and {{Technology}} with {{ECTI Northern Section Conference}} on {{Electrical}}, {{Electronics}}, {{Computer}} and {{Telecommunications Engineering}} ({{ECTI DAMT}} \& {{NCON}})},
  langid = {english},
  keywords = {企業知識管理系統,已整理},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 應用知識創造公司的知識管理系統：將隱性知識轉化為顯性知識},
  note = {注重企業中使用的知識管理，因此在權限控管、成果驗證方面較為不同。
\par
但開發軟體的需求及過程能夠參考。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\SAWS7X9K\\Nupap - 2022 - Knowledge Management System by applying Knowledge .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\BLL6KDBQ\\9720388.html}
}

@article{o-shenlinpSyndromeDifferentiationDeficiency2023,
  title = {Syndrome {{Differentiation}} for {{Deficiency Syndromes}} in {{Traditional Chinese Medicine Based}} on {{Fuzzy Sets}}},
  author = {{O-SHENLIN P} and {AI-WEILIN N} and {ING-HSIENYEH M} and {HIA-CHOUYEH C} and {UNG-PINCHIU H} and {EI-CHUNWU M}},
  date = {2023-09},
  journaltitle = {Journal of Information Science and Engineering},
  volume = {39},
  number = {5},
  pages = {1079--1100},
  publisher = {社團法人中華民國計算語言學學會},
  issn = {1016-2364},
  doi = {10.6688/JISE.202309_39(5).0005},
  url = {https://www.airitilibrary.com/Publication/alDetailedMesh?docid=10162364-N202304210013-00005},
  urldate = {2023-09-19},
  abstract = {A disease in traditional Chinese medicine is defined as a sequence of syndromes. The diagnosis of syndromes in traditional Chinese medicine is called syndrome differentiation. The construction of a syndrome differentiation system directly from clinical medical records using machine learning is still infeasible due to the lack of standardization of symptoms and syndromes in current clinical medical records. This article proposes a sophisticated approach to developing a syndrome differentiation system for 18 deficiency syndromes according to the knowledge of textbooks. This approach defines the syndrome differentiation problem as a membership problem of fuzzy sets. This approach designs a number of membership functions for fuzzy sets of syndromes based on a symptom grouping scheme and a symptom weighing scheme. Symptoms are grouped according to syndrome location, cause, and mechanism in the symptom grouping scheme. The symptom weighing scheme assigns exponentially decreasing weights to symptoms in each symptom group. An experimental evaluation based on a benchmark of 50 case reports shows that the proposed membership functions are very practical based on three differentiation metrics. This syndrome differentiation system can produce clinical medical records with standard symptoms and syndromes. In the future, these standard clinical medical records can be utilized to construct syndrome differentiation systems using machine learning.},
  langid = {english},
  keywords = {deficiency syndromes,fuzzy sets,syndrome differentiation,traditional Chinese medicine},
  annotation = {titleTranslation: 基於模糊集的中醫虛證辨證\\
abstractTranslation:  中醫將疾病定義為一連串的證候。中醫對證候的診斷稱為辨證。由於目前臨床病歷缺乏症狀和證候的標準化，利用機器學習直接從臨床病歷建構辨證系統仍然不可行。本文提出了一種根據教科書知識發展 18 種虛證辨證系統的複雜方法。此方法將證候鑑別問題定義為模糊集合的隸屬度問題。此方法基於症狀分組方案和症狀權重方案為模糊症狀集設計了多個隸屬函數。症狀分組方案中根據症狀部位、原因和機制將症狀分組。症狀權重方案為每個症狀組中的症狀分配指數遞減的權重。基於 50 個案例報告基準的實驗評估表明，基於三個差異化指標，所提出的隸屬函數非常實用。此辨證系統可以產生具有標準症狀和證候的臨床病歷。未來，這些標準的臨床病歷可以用來利用機器學習來建構辨證系統。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\T4VLI7WG\\P 等。 - 2023 - Syndrome Differentiation for Deficiency Syndromes .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\26WX3QZA\\alDetailedMesh.html}
}

@software{Obsidianlocalrestapi,
  title = {Obsidian-Local-Rest-Api},
  namea = {, coddingtonbear},
  nameatype = {collaborator},
  url = {https://github.com/coddingtonbear/obsidian-local-rest-api},
  version = {2.3.0}
}

@article{oelenCreatingValidatingScholarly2023,
  title = {Creating and Validating a Scholarly Knowledge Graph Using Natural Language Processing and Microtask Crowdsourcing},
  author = {Oelen, Allard and Stocker, Markus and Auer, Sören},
  date = {2023-04-05},
  journaltitle = {International Journal on Digital Libraries},
  shortjournal = {Int J Digit Libr},
  issn = {1432-1300},
  doi = {10.1007/s00799-023-00360-7},
  url = {https://doi.org/10.1007/s00799-023-00360-7},
  urldate = {2024-03-22},
  abstract = {Due to the growing number of scholarly publications, finding relevant articles becomes increasingly difficult. Scholarly knowledge graphs can be used to organize the scholarly knowledge presented within those publications and represent them in machine-readable formats. Natural language processing (NLP) provides scalable methods to automatically extract knowledge from articles and populate scholarly knowledge graphs. However, NLP extraction is generally not sufficiently accurate and, thus, fails to generate high granularity quality data. In this work, we present TinyGenius, a methodology to validate NLP-extracted scholarly knowledge statements using microtasks performed with crowdsourcing. TinyGenius is employed to populate a paper-centric knowledge graph, using five distinct NLP methods. We extend our previous work of the TinyGenius methodology in various ways. Specifically, we discuss the NLP tasks in more detail and include an explanation of the data model. Moreover, we present a user evaluation where participants validate the generated NLP statements. The results indicate that employing microtasks for statement validation is a promising approach despite the varying participant agreement for different microtasks.},
  langid = {english},
  keywords = {Crowdsourcing microtasks,Knowledge graph validation,NLP,Scholarly knowledge graphs,UI,User interface evaluation,使用者研究,學術知識圖譜,已整理,已讀,評估參考},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用自然語言處理和微任務眾包創建和驗證學術知識圖},
  note = {比起會議論文的內容，關於任務詳情與系統評估的描述較完整。
\par
提到web的反應時間最好在2秒內。
\par
提到SUS在人少時也可信。
\par
提到雖然使用者同質性較高但貼近現實。
\par
與研究較ˊ無直接相關，但系統評估的部分可以參考。},
  file = {C:\Users\BlackCat\Zotero\storage\V7LYN3EU\Oelen et al. - 2023 - Creating and validating a scholarly knowledge grap.pdf}
}

@inproceedings{oelenTinyGeniusIntertwiningNatural2022,
  title = {{{TinyGenius}}: {{Intertwining Natural Language Processing}} with {{Microtask Crowdsourcing}} for {{Scholarly Knowledge Graph Creation}}},
  shorttitle = {{{TinyGenius}}},
  booktitle = {2022 {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Oelen, Allard and Stocker, Markus and Auer, Sören},
  date = {2022-06},
  pages = {1--5},
  url = {https://ieeexplore.ieee.org/document/9852949},
  urldate = {2023-11-30},
  abstract = {As the number of published scholarly articles grows steadily each year, new methods are needed to organize scholarly knowledge so that it can be more efficiently discovered and used. Natural Language Processing (NLP) techniques are able to autonomously process scholarly articles at scale and to create machine readable representations of the article content. However, autonomous NLP methods are by far not sufficiently accurate to create a high-quality knowledge graph. Yet quality is crucial for the graph to be useful in practice. We present TinyGenius, a methodology to validate NLP-extracted scholarly knowledge statements using microtasks performed with crowdsourcing. The scholarly context in which the crowd workers operate has multiple challenges. The explainability of the employed NLP methods is crucial to provide context in order to support the decision process of crowd workers. We employed TinyGenius to populate a paper-centric knowledge graph, using five distinct NLP methods. In the end, the resulting knowledge graph serves as a digital library for scholarly articles. CCS CONCEPTS • Information systems → Crowdsourcing; • Applied computing → Digital libraries and archives; • Human-centered computing → Graphical user interfaces.},
  eventtitle = {2022 {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  langid = {english},
  keywords = {人機互動,已整理,已讀,會議論文},
  annotation = {titleTranslation: TinyGenius：將自然語言處理與微任務眾包結合起來，用於創建學術知識圖譜},
  note = {建立良好的UI介面及良好的溝通方式(RDF、GraphDB)，來將NLP分析後的學術知識圖譜拆成小任務，由人類投票來驗證準確性。用於評估NLP抽取實體自動建立知識圖譜的效果
\par
和學術論文關係較小。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\VAUFUN82\\Oelen et al. - 2022 - TinyGenius Intertwining Natural Language Processi.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\6TR9HLRG\\9852949.html}
}

@article{okoloefestanleyInformationOverloadCauses2021,
  title = {Information {{Overload}}: {{Causes}}, {{Symptoms}}, {{Consequences}} and {{Solutions}}},
  shorttitle = {Information {{Overload}}},
  author = {{Okolo Efe Stanley}},
  date = {2021-11-05},
  journaltitle = {Asian Journal of Information Science and Technology},
  volume = {11},
  number = {2},
  pages = {1--6},
  issn = {2231-6108},
  doi = {10.51983/ajist-2021.11.2.2887},
  url = {https://ojs.trp.org.in/index.php/ajist/article/view/2887},
  urldate = {2023-10-23},
  abstract = {This paper looks at the concept of Information, over, load and information overload respectively, a brief history of how information overload came into existence. It also outlined some salient factors that are responsible for or causes information overload and they entail the followings as enshrined in the study: Multiple sources of information; the availability of too much information; the difficulty in managing information; the information's irrelevance or insignificance; The inability to comprehend the material due to a lack of time. Furthermore, massive amounts of fresh information are constantly being created on a daily basis; pressure to create and compete in the provision of knowledge, particularly in the academic setting. The lack of complexity and nascent simplicity of creating, duplicating, and sharing information online, leading to a quantity over quality effect in many institutions and businesses; the absence of complexity and nascent simplicity of creating, copying, and sharing information online; The exponential development of information delivery methods, such as radio, television, print media, websites, e-mail, mobile telephony, RSS feeds, and so on; the growing weight of historical data available to us; a plethora of inconsistent, contradicting, and simply wrong information; the lack of clear structure in groups of information and poor clues as to the relationships between those groups; the lack of simple procedures for quickly processing, comparing, and evaluating information sources; the lack of clear structure in groups of information and poor clues as to the relationships between those groups. The study pointed out some symptoms that can be seen as evidence or signs indicating that there is the presence of information overload thereafter, it brought out some consequences of information overload specifically to an individual and collectively to an organization. In this paper the researcher ended the work by outlining some remedies on how to combat information overload since it has become a matter that has come to stay.},
  issue = {2},
  langid = {english},
  keywords = {Solutions,已整理,文獻,資訊超載},
  annotation = {6 citations (Crossref) [2024-03-26]\\
titleTranslation: 信息超载：原因、症状、后果和解决方案\\
abstractTranslation:  本文分别探讨了 "信息"、"过载 "和 "信息超载 "的概念，以及信息超载产生的简要历史。本文还概述了造成或导致信息超载的一些突出因素，这些因素包括研究报告中所载的以下几点：信息来源多；可获得的信息太多；信息管理困难；信息无关紧要或无足轻重；由于缺乏时间而无法理解材料。此外，每天都有大量的新鲜信息不断产生；在提供知识方面，尤其是在学术环境中，存在着创造和竞争的压力。在网上创建、复制和共享信息既不复杂，也初显简单，导致许多机构和企业出现重 量轻质的现象；在网上创建、复制和共享信息既不复杂，也初显简单；广播、电 视、印刷媒体、网站、电子邮件、移动电话、RSS 订阅等信息传递方式的指数式发 展；我们可以获得的历史数据越来越多；大量不一致、相互矛盾或根本就是错误的信息；信息群缺乏清晰的结构，这些信息群之间的关系线索不清；缺乏快速处理、比较和评估信息来源的简单程序；信息群缺乏清晰的结构，这些信息群之间的关系线索不清。这项研究指出了一些症状，这些症状可以被看作是信息超载的证据或征兆，此外，它还指出了信息超载对个人和组织集体造成的一些后果。在本文的最后，研究人员概述了如何应对信息超载的一些补救措施，因为信息超载已经成为一个长期存在的问题。},
  note = {關於解決方案，},
  file = {C:\Users\BlackCat\Zotero\storage\EQYHUCBG\Stanley - 2021 - Information Overload Causes, Symptoms, Consequenc.pdf}
}

@article{oleksandrpalaginOntoChatGPTInformationSystem2023,
  title = {{{OntoChatGPT Information System}}: {{Ontology-Driven Structured Prompts}} for {{ChatGPT Meta-Learning}}},
  shorttitle = {{{OntoChatGPT Information System}}},
  author = {{Oleksandr Palagin} and {Vladislav Kaverinskiy} and {Anna Litvin} and {Kyrylo Malakhov}},
  date = {2023-07-02},
  journaltitle = {International Journal of Computing},
  pages = {170--183},
  issn = {2312-5381},
  doi = {10.47839/ijc.22.2.3086},
  url = {https://computingonline.net/computing/article/view/3086},
  urldate = {2023-09-12},
  abstract = {This research presents a comprehensive methodology for utilizing an ontology-driven structured prompts system in interplay with ChatGPT, a widely used large language model (LLM). The study develops formal models, both information and functional, and establishes the methodological foundations for integrating ontology-driven prompts with ChatGPT’s meta-learning capabilities. The resulting productive triad comprises the methodological foundations, advanced information technology, and the OntoChatGPT system, which collectively enhance the effectiveness and performance of chatbot systems. The implementation of this technology is demonstrated using the Ukrainian language within the domain of rehabilitation. By applying the proposed methodology, the OntoChatGPT system effectively extracts entities from contexts, classifies them, and generates relevant responses. The study highlights the versatility of the methodology, emphasizing its applicability not only to ChatGPT but also to other chatbot systems based on LLMs, such as Google’s Bard utilizing the PaLM 2 LLM. The underlying principles of meta-learning, structured prompts, and ontology-driven information retrieval form the core of the proposed methodology, enabling their adaptation and utilization in various LLM-based systems. This versatile approach opens up new possibilities for NLP and dialogue systems, empowering developers to enhance the performance and functionality of chatbot systems across different domains and languages.},
  langid = {english},
  keywords = {ChatGPT,composite service,可解釋性,問答系統,已整理,框架,知識本體},
  annotation = {3 citations (Crossref) [2024-03-26]\\
abstractTranslation:  這項研究提出了一種綜合方法，用於利用本體驅動的結構化提示系統與 ChatGPT（一種廣泛使用的大語言模型 (LLM)）相互作用。該研究開發了信息和功能的正式模型，並為將本體驅動的提示與 ChatGPT 的元學習功能相集成奠定了方法基礎。由此產生的富有成效的三位一體包括方法基礎、先進信息技術和 OntoChatGPT 系統，它們共同提高了聊天機器人系統的有效性和性能。該技術的實施在康復領域使用烏克蘭語進行了演示。通過應用所提出的方法，OntoChatGPT 系統有效地從上下文中提取實體，對其進行分類並生成相關響應。該研究強調了該方法的多功能性，強調其不僅適用於 ChatGPT，還適用於其他基於 LLM 的聊天機器人系統，例如利用 PaLM 2 LLM 的 Google 的 Bard。元學習、結構化提示和本體驅動的信息檢索的基本原理構成了所提出方法的核心，使其能夠在各種基於法學碩士的系統中進行調整和利用。這種多功能方法為 NLP 和對話系統開闢了新的可能性，使開發人員能夠跨不同領域和語言增強聊天機器人系統的性能和功能。\\
titleTranslation: OntoChatGPT信息系統：本體驅動的ChatGPT元學習結構化提示},
  note = {建立一個框架，結合ChatGPT與python與Ontology，由Python在提問前將知識本體用json格式(應該)告訴ChatGPT，使其能夠基於Ontology回應。然而準確度仍然只有73\textasciitilde 82\%，有相當程度的幻覺問題。},
  file = {C:\Users\BlackCat\Zotero\storage\9NFFDRTK\Palagin 等。 - 2023 - OntoChatGPT Information System Ontology-Driven St.pdf}
}

@article{osmanOntologyBasedKnowledgeManagement2022,
  title = {Ontology-{{Based Knowledge Management Tools}} for {{Knowledge Sharing}} in {{Organization}}—{{A Review}}},
  author = {Osman, Mohamad Amin and Mohd Noah, Shahrul Azman and Saad, Saidah},
  date = {2022},
  journaltitle = {IEEE Access},
  volume = {10},
  pages = {43267--43283},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3163758},
  url = {https://ieeexplore.ieee.org/abstract/document/9745581},
  urldate = {2024-01-17},
  abstract = {Knowledge management (KM) comprises several processes, and one of the most important is the knowledge sharing activities. The ability of an organization to manage its organizational knowledge, specifically in the context of knowledge sharing, may enhance the organization’s overall performance. Various approaches and technologies have been introduced to assist the process in achieving that target. Ontology as one of the knowledge representation methods has been becoming popular to assist knowledge sharing in the organization. Previous reviews have mainly focused on general KM issues, with little emphasis on the use of ontology in knowledge sharing. Thus, this article reviews several ontology-based KM tools that can support knowledge-sharing activities to provide some insight into future research in this area. Thirteen ontology-based KM tools were reviewed using ten elements’ comparison criteria: the motivation, domain, source of knowledge, type of knowledge, knowledge extraction, knowledge input process, knowledge retrieval process, knowledge sharing technology, source of ontology component, and ontology methodology. The review found that several elements can be further studied to improve KM implementation in the organization, especially on the knowledge sharing dimension. This includes simplifying the knowledge extraction and retrieval process to explore various knowledge domains from implicit knowledge sources. The review’s outcome also includes proposed components and functions of an ideal ontology-based KM tool.},
  eventtitle = {{{IEEE Access}}},
  langid = {english},
  keywords = {未整理,知識圖譜},
  annotation = {9 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於本體的組織知識分享知識管理工具—綜述},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\AVJKLQ5T\\Osman 等。 - 2022 - Ontology-Based Knowledge Management Tools for Know.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\JDU9UD45\\9745581.html}
}

@article{p.ruchComparingGeneralMedical2001,
  title = {Comparing General and Medical Texts for Information Retrieval Based on Natural Language Processing: An Inquiry into Lexical Disambiguation},
  shorttitle = {Comparing General and Medical Texts for Information Retrieval Based on Natural Language Processing},
  author = {{P. Ruch} and {R. Baud} and {A. Geissbühler} and {A. M. Rassinoux}},
  date = {2001},
  journaltitle = {Studies in Health Technology and Informatics},
  shortjournal = {Stud Health Technol Inform},
  volume = {84},
  eprint = {11604745},
  eprinttype = {pmid},
  pages = {261--265},
  issn = {0926-9630},
  abstract = {In this paper we compare two types of corpus, focusing on the lexical ambiguity of each of them. The first corpus consists mainly of general newspaper articles and literature excerpts, while the second belongs to the medical domain. To conduct the study, we have used two different disambiguation tools. First, each tool was validated in its respective application area. We then use these systems in order to assess and compare both the general ambiguity rate and the particularities of each domain. Quantitative results show that medical documents are lexically less ambiguous than unrestricted documents. Our conclusions emphasize the importance of the application area in the design of NLP tools.},
  issue = {Pt 1},
  langid = {english},
  keywords = {Information Storage and Retrieval,Linguistics,Medical Records Systems Computerized,Natural Language Processing,Newspapers as Topic,Vocabulary Controlled},
  annotation = {abstractTranslation:  在本文中，我們比較了兩種類型的語料庫，重點關注每種語料庫的詞彙歧義。第一個語料庫主要由一般報紙文章和文獻摘錄組成，第二個語料庫屬於醫學領域。為了進行這項研究，我們使用了兩種不同的消歧工具。首先，每個工具都在其各自的應用領域進行了驗證。然後，我們使用這些系統來評估和比較一般模糊率和每個領域的特殊性。定量結果表明，醫療文檔在詞彙上比不受限制的文檔更不那麼模糊。我們的結論強調了 NLP 工具設計中應用領域的重要性。\\
titleTranslation: 基於自然語言處理的信息檢索的一般文本和醫學文本的比較：詞彙消歧的探究},
  note = {證明醫學文本中的的詞彙歧異較少，
\par
但是研究對象是英文?},
  file = {C:\Users\BlackCat\Zotero\storage\NQWESMDX\Ruch 等。 - 2001 - Comparing general and medical texts for informatio.pdf}
}

@inproceedings{paolaespinoza-ariasExtendingOntologyEngineering2022,
  title = {Extending {{Ontology Engineering Practices}} to~{{Facilitate Application Development}}},
  booktitle = {Knowledge {{Engineering}} and {{Knowledge Management}}: 23rd {{International Conference}}, {{EKAW}} 2022, {{Bolzano}}, {{Italy}}, {{September}} 26–29, 2022, {{Proceedings}}},
  author = {{Paola Espinoza-Arias} and {Daniel Garijo} and {Oscar Corcho}},
  year = {9 月 26, 2022},
  pages = {19--35},
  publisher = {Springer-Verlag},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-031-17105-5_2},
  url = {https://doi.org/10.1007/978-3-031-17105-5_2},
  urldate = {2023-09-26},
  abstract = {本體定義知識圖（KG）中的資料組織和意義。然而，在設計和產生應用程式介面（API）以允許開發人員以開發人員友好的方式使用知識圖譜資料時，通常沒有考慮本體。為了填補這一空白，本文提出了一種基於本體開發過程中產生的工件的 API 生成方法。該方法被描述為稱為本體開發的新階段的一部分，該階段可能包含在傳統本體開發方法的最後階段。此外，為了支援所提出方法的一些任務，我們開發了 OATAPI，這是一種從兩個本體工件（能力問題和本體序列化）產生 API 的工具。這項工作的結論反映出，在基於本體工件產生 API 的方法和工具層面上，現有技術中發現的限制已得到解決。最後，未來的工作提出了一些需要解決的挑戰，以便應用程式開發人員可以更輕鬆地利用知識圖譜和本體的潛力。},
  isbn = {978-3-031-17104-8},
  keywords = {API,Application Development,Application Programming Interface,Ontology Artefacts,Ontology Engineering,知識本體},
  annotation = {0 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\LELQHCZ5\Espinoza-Arias 等。 - 2022 - Extending Ontology Engineering Practices to Facili.pdf}
}

@inproceedings{paolavelardiUsingTextProcessing2001,
  title = {Using Text Processing Techniques to Automatically Enrich a Domain Ontology},
  booktitle = {Proceedings of the International Conference on {{Formal Ontology}} in {{Information Systems}} - {{Volume}} 2001},
  author = {{Paola Velardi} and {Paolo Fabriani} and {Michele Missikoff}},
  year = {10 月 17, 2001},
  series = {{{FOIS}} '01},
  pages = {270--284},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/505168.505194},
  url = {https://dl.acm.org/doi/10.1145/505168.505194},
  urldate = {2023-09-15},
  abstract = {Though the utility of domain Ontologies is now widely acknowledged in an increasing number of domains, several barriers must be overcome before Ontologies become practical and useful tools. A critical issue is the task of identifying, defining, and entering the concept definitions. In case of large and complex application domains this task can be lengthy, costly, and controversial (since different persons may have different points of view about the same concept). To reduce time, cost (and, sometimes, harsh discussions) it is highly advisable to refer, in constructing or updating an ontology, to the documents available in the field. In this paper we describe OntoLearn, a text-mining tool devised to improve human productivity during the process of ontology construction.},
  isbn = {978-1-58113-377-6},
  langid = {english},
  keywords = {/unread,已整理,建立本體,知識本體,重要},
  annotation = {80 citations (Crossref) [2024-03-26]\\
abstractTranslation:  儘管領域本體的實用性現在在越來越多的領域中得到廣泛認可，但在本體成為實用且有用的工具之前，必須克服一些障礙。一個關鍵問題是識別、定義和輸入概念定義的任務。對於大型且複雜的應用領域，這項任務可能會很漫長、成本高昂且有爭議（因為不同的人可能對同一概念有不同的觀點）。為了減少時間、成本（有時還減少激烈的討論），強烈建議在建置或更新本體時參考該領域可用的文件。在本文中，我們描述了 OntoLearn，一種文本挖掘工具，旨在提高本體建構過程中的人類生產力。\\
titleTranslation: 使用文字處理技術自動豐富領域本體},
  file = {C:\Users\BlackCat\Zotero\storage\C48LWWDE\Velardi 等。 - 2001 - Using text processing techniques to automatically .pdf}
}

@inproceedings{paulabrauerMovementVirtualTime2023,
  title = {Movement in {{Virtual Time}}: {{How Virtual Reality Can Support Long-Term Thinking}}},
  shorttitle = {Movement in {{Virtual Time}}},
  booktitle = {Mensch Und {{Computer}} 2023},
  author = {{Paula Bräuer} and {Margarita Berg} and {Athanasios Mazarakis} and {Isabella Peters}},
  year = {9 月 3, 2023},
  series = {{{MuC}} '23},
  pages = {477--481},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3603555.3608569},
  url = {https://doi.org/10.1145/3603555.3608569},
  urldate = {2023-08-23},
  abstract = {The immersive nature of virtual reality (VR) allows even complex information to be communicated in an engaging, interactive, and fast way. This is particularly useful in the case of long-term political processes, such as the siting, construction, and management of a repository for high-level radioactive waste (HLRW). In this paper, we present an approach to convey an understanding of such long-time horizons by converting the information into a motion-controlled VR application. To gain insight into how users experience a period of 500 years, a pilot study with 15 subjects was conducted. The first results were positive; the subjects were thrilled with the presentation format in VR and offered several recommendations for improvement of the time visualization. For inexperienced users, the interaction was possible without any assistance and has the potential to be adapted to other use cases.},
  isbn = {9798400707711},
  langid = {english},
  keywords = {radioactive waste,timeline,virtual reality,可視化,回收},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 虛擬時間中的運動：虛擬現實如何支持長期思考\\
abstractTranslation:  虛擬現實（VR）的沉浸式特性甚至允許以引人入勝、吸引且快速的方式傳送複雜的信息。這對於長期的政治進程特別有用，例如高放廢物（HLRW）廢棄庫的選址、建設和管理。在本文中，我們提出了一種通過將信息轉換為運動控制的VR 應用程序來表達對這種垂直視野的理解的方法。為了深入了解用戶在500 年期間的體驗，我們對15 人進行聽覺做了一個專題研究。初步結果是積極的；衷心對VR 中的練習格式感到興奮，並提出了一些改進時間可視化的建議。對於沒有經驗的用戶，},
  note = {用VR展現跨度時間較長的政策可能會有怎樣的影響。},
  file = {C:\Users\BlackCat\Zotero\storage\4BVN9NMD\Bräuer 等。 - 2023 - Movement in Virtual Time How Virtual Reality Can .pdf}
}

@inproceedings{paulosergiosantosjuniorFederatedOntologyDrivenData2023,
  title = {Towards {{Federated Ontology-Driven Data Integration}} in {{Continuous Software Engineering}}},
  booktitle = {Proceedings of the {{XXXVII Brazilian Symposium}} on {{Software Engineering}}},
  author = {{Paulo Sérgio Santos Júnior} and {João Paulo A. Almeida} and {Monalessa Barcellos}},
  date = {2023-09-25},
  series = {{{SBES}} '23},
  pages = {31--36},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3613372.3613380},
  url = {https://doi.org/10.1145/3613372.3613380},
  urldate = {2023-09-26},
  abstract = {Organizations have adopted Continuous Software Engineering (CSE) practices aiming at making software development faster, iterative, integrated, continuous, and aligned with the business. In this context, they often use different applications (e.g., project management tools, source repositories, and quality assessment tools) that store valuable data to support daily activities and decision-making. However, data items often remain spread in different applications that adopt different data and behavioral models, posing a barrier to integrated data usage. As a consequence, data-driven software development is uncommon, missing valuable opportunities for product and process improvement. In this paper, we explore an ontology network addressing CSE aspects to develop a data integration solution in which networked ontologies are the basis to build reusable and autonomous software components that work together in a system federation to provide meaningful integrated data. We achieve a comprehensive and flexible solution that can be used as a whole or partially, by extracting only the components related to the subdomains of interest.},
  isbn = {9798400707872},
  langid = {english},
  keywords = {Continuous Software Engineering,Data Integration,已整理,本體建立,知識分享,知識本體,軟體開發},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 在連續軟體工程中邁向聯合本體驅動的資料集成\\
abstractTranslation:  組織已採用持續軟體工程 (CSE) 實踐，旨在使軟體開發更快、迭代、整合、連續並與業務保持一致。在這種情況下，他們經常使用不同的應用程式（例如專案管理工具、來源儲存庫和品質評估工具）來儲存有價值的資料以支援日常活動和決策。然而，資料項通常分散在採用不同資料和行為模型的不同應用程式中，這對整合資料使用造成了障礙。因此，數據驅動的軟體開發並不常見，錯過了產品和流程改善的寶貴機會。在本文中，我們探索了解決CSE 方面的本體網絡，以開發數據集成解決方案，其中網絡本體是構建可重用和自主軟體組件的基礎，這些組件在系統聯合中協同工作以提供有意義的集成數據。透過僅提取與感興趣的子域相關的元件，我們實現了一個全面且靈活的解決方案，可以整體或部分使用。},
  note = {在持續軟體開發的過程中，因為需要使用多個工具結合。此外一般決策者都是以個人經驗做決策。本篇論文希望藉由提出一個本體來幫助決策者以數據導向來決策。
\par
和研究方向關係較小。},
  file = {D:\Paper\Santos Júnior et al. - 2023 - Towards Federated Ontology-Driven Data Integration.pdf}
}

@article{pegdwenden.sawadogoDLBenchBenchmarkQuantitative2023,
  title = {{{DLBench}}+: {{A}} Benchmark for Quantitative and Qualitative Data Lake Assessment},
  shorttitle = {{{DLBench}}+},
  author = {{Pegdwendé N. Sawadogo} and {Jérôme Darmont}},
  date = {2023-05-01},
  journaltitle = {Data \& Knowledge Engineering},
  shortjournal = {Data \& Knowledge Engineering},
  volume = {145},
  pages = {102154},
  issn = {0169-023X},
  doi = {10.1016/j.datak.2023.102154},
  url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000149},
  urldate = {2023-09-13},
  abstract = {In the last few years, the concept of data lake has become trendy for data storage and analysis. Thus, several approaches have been proposed to build data lake systems. However, such proposals are difficult to evaluate as there are no commonly shared criteria for comparing data lake systems. Thus, we introduce in this paper DLBench+, a benchmark to evaluate and compare data lake implementations that support textual and/or tabular contents. More concretely, we propose a data model made of both textual and CSV documents, a workload model composed of a set of various tasks, as well as a set of performance-based metrics, all relevant to the context of data lakes. Beyond a purely quantitative assessment, we also propose a methodology to qualitatively evaluate data lake systems through the assessment of user experience. As a proof of concept, we use DLBench+ to evaluate an open source data lake system we developed.},
  langid = {english},
  keywords = {Data lakes,Quality and metrics,Tabular data,Textual documents,使用者研究,已整理,框架,軟體測試},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: DLBench+：定量和定性資料湖評估的基準\\
abstractTranslation:  在過去的幾年裡，資料湖的概念已經成為資料儲存和分析的流行趨勢。因此，已經提出了幾種建構資料湖系統的方法。然而，此類提案很難評估，因為沒有比較資料湖系統的共同標準。因此，我們在本文中引入了 DLBench+，這是用於評估和比較支援文字和/或表格內容的資料湖實現的基準。更具體地說，我們提出了一個由文字和 CSV 文件組成的資料模型，一個由一組各種任務組成的工作負載模型，以及一組基於效能的指標，所有這些都與資料湖的上下文相關。除了純粹的定量評估之外，我們還提出了一種透過評估使用者體驗來定性評估資料湖系統的方法。作為概念證明，我們使用 DLBench+ 來評估我們開發的開源資料湖系統。},
  note = {開發一個測試框架用於測試數據湖的可用性。與研究較無關係，但評估可用性的部分可能用的上。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\6C96Z4D2\\Sawadogo 與 Darmont - 2023 - DLBench+ A benchmark for quantitative and qualita.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\UD56JDZC\\S0169023X23000149.html}
}

@article{peraltaFrameworkMultiSourceInformation2012,
  title = {A {{Framework}} for {{Multi-Source Information Systems Development}}},
  author = {Peralta, Veronika and Prism, Laboratoire},
  date = {2012-01-03},
  abstract = {A Multi-Source Information System (MSIS) is composed of a set of independent data sources and a set of views that define user requirements. Its differences with classical information systems introduced new design activities and motivated the development of new techniques. In this paper we present a general framework that allows an easy integration of design and maintenance tools through a common metadata platform that centralizes the inter-application data management and the integrity control routines. We study a particular case of MSIS: a Data Warehouse (DW) and propose a model to represent its metadata from two points of view: the schema representation and the inter-schema relationships that allow to calculate an object from another ones. Our work includes the development of a prototype that implements the DW metadata model as the core of the design platform.},
  langid = {english},
  keywords = {⛔ No DOI found,未整理},
  annotation = {titleTranslation: 多源資訊系統開發框架\\
abstractTranslation:  多源資訊系統（MSIS）由一組獨立的資料來源和一組定義使用者需求的視圖組成。它與經典資訊系統的差異引入了新的設計活動並推動了新技術的開發。在本文中，我們提出了一個通用框架，該框架允許透過集中應用程式間資料管理和完整性控制例程的通用元資料平台輕鬆整合設計和維護工具。我們研究了 MSIS 的一個特殊案例：資料倉儲 (DW)，並提出了一個模型來從兩個角度表示其元資料：模式表示和允許從另一個物件計算物件的模式間關係。我們的工作包括開發一個實現 DW 元資料模型作為設計平台核心的原型。},
  file = {C:\Users\BlackCat\Zotero\storage\M2QMPF3A\Peralta 與 Prism - 2012 - A Framework for Multi-Source Information Systems D.pdf}
}

@online{peterwestSymbolicKnowledgeDistillation2022,
  title = {Symbolic {{Knowledge Distillation}}: From {{General Language Models}} to {{Commonsense Models}}},
  shorttitle = {Symbolic {{Knowledge Distillation}}},
  author = {{Peter West} and {Chandra Bhagavatula} and {Jack Hessel} and {Jena D. Hwang} and {Liwei Jiang} and {Ronan Le Bras} and {Ximing Lu} and {Sean Welleck} and {Yejin Choi}},
  date = {2022-11-28},
  eprint = {2110.07178},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2110.07178},
  url = {http://arxiv.org/abs/2110.07178},
  urldate = {2023-08-24},
  abstract = {The common practice for training commonsense models has gone from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from-machine-to-corpus-to-machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically-as text-in addition to the neural model. We also distill only one aspect-the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and share our new symbolic knowledge graph and commonsense models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {ChatGPT,回收,未發表,機器學習,知識圖譜,知識本體,知識蒸餾},
  annotation = {titleTranslation: 符號知識蒸餾：從通用語言模型到常識模型\\
abstractTranslation:  訓練常識模型的常見做法已經從人類到語料庫再到機器：人類編寫常識知識圖來訓練常識模型。在這項工作中，我們研究了一種替代方案，從機器到語料庫到機器：通用語言模型編寫這些常識知識圖來訓練常識模型。我們的研究提出了一個新的框架：符號知識蒸餾。與知識蒸餾的現有技術一樣（Hinton 等人，2015），我們的方法使用較大的模型來教授較小的模型。一個關鍵的區別是，除了神經模型之外，我們還以符號方式提取知識（如文本）。我們也只提煉一個方面——一般語言模型老師的常識，讓學生成為另一種類型，常識模型。總而言之，我們表明，仔細的提示工程和單獨訓練的批評家模型使我們能夠有選擇地從通用語言模型 GPT-3 中提取高質量的因果常識。實證結果表明，人類編寫的常識知識圖譜首次在數量、質量和多樣性這三個標准上被我們自動提取的變體超越。此外，它產生的神經常識模型超越了教師模型的常識能力，儘管其尺寸小了 100 倍。我們將其應用於 ATOMIC 資源，並分享我們新的符號知識圖和常識模型。},
  note = {通過產生高品質的因果三元體，讓小十倍的模型能學會ChatGPT中的高品質因果資訊。和研究較無關係。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\JB7J938X\\West 等。 - 2022 - Symbolic Knowledge Distillation from General Lang.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\Q2K3WL97\\2110.html}
}

@inproceedings{phillipschneiderDecadeKnowledgeGraphs2022,
  title = {A {{Decade}} of {{Knowledge Graphs}} in {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {A {{Decade}} of {{Knowledge Graphs}} in {{Natural Language Processing}}},
  booktitle = {Proceedings of the 2nd {{Conference}} of the {{Asia-Pacific Chapter}} of the {{Association}} for {{Computational Linguistics}} and the 12th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {{Phillip Schneider} and {Tim Schopf} and {Juraj Vladika} and {Mikhail Galkin} and {Elena Simperl} and {Florian Matthes}},
  date = {2022-01},
  pages = {601--614},
  publisher = {Association for Computational Linguistics},
  location = {Online only},
  url = {https://aclanthology.org/2022.aacl-main.46},
  urldate = {2023-09-19},
  abstract = {In pace with developments in the research field of artificial intelligence, knowledge graphs (KGs) have attracted a surge of interest from both academia and industry. As a representation of semantic relations between entities, KGs have proven to be particularly relevant for natural language processing (NLP), experiencing a rapid spread and wide adoption within recent years. Given the increasing amount of research work in this area, several KG-related approaches have been surveyed in the NLP research community. However, a comprehensive study that categorizes established topics and reviews the maturity of individual research streams remains absent to this day. Contributing to closing this gap, we systematically analyzed 507 papers from the literature on KGs in NLP. Our survey encompasses a multifaceted review of tasks, research types, and contributions. As a result, we present a structured overview of the research landscape, provide a taxonomy of tasks, summarize our findings, and highlight directions for future work.},
  eventtitle = {{{AACL-IJCNLP}} 2022},
  langid = {english},
  keywords = {Survey,基礎理論,知識圖譜,重要},
  annotation = {titleTranslation: 自然語言處理知識圖的十年：調查\\
abstractTranslation:  隨著人工智慧研究領域的發展，知識圖譜（KG）引起了學術界和工業界的濃厚興趣。作為實體之間語義關係的表示，知識圖譜已被證明與自然語言處理（NLP）特別相關，近年來經歷了快速傳播和廣泛採用。鑑於該領域的研究工作量不斷增加，NLP 研究界已經調查了幾種與知識圖譜相關的方法。然而，迄今為止，仍然缺乏對既定主題進行分類並審查各個研究流的成熟度的綜合研究。為了縮小這一差距，我們系統性地分析了 NLP 知識圖譜文獻中的 507 篇論文。我們的調查包括對任務、研究類型和貢獻的多方面回顧。因此，我們對研究領域進行了結構化概述，提供了任務分類，總結了我們的發現，並強調了未來工作的方向。},
  file = {C:\Users\BlackCat\Zotero\storage\AILJ9Q4G\Schneider 等。 - 2022 - A Decade of Knowledge Graphs in Natural Language P.pdf}
}

@article{pierre-yvesvandenbusscheLinkedOpenVocabularies2016,
  title = {Linked {{Open Vocabularies}} ({{LOV}}): {{A}} Gateway to Reusable Semantic Vocabularies on the {{Web}}},
  shorttitle = {Linked {{Open Vocabularies}} ({{LOV}})},
  author = {{Pierre-Yves Vandenbussche} and {Ghislain A. Atemezing} and {María Poveda-Villalón} and {Bernard Vatant}},
  editor = {{Michel Dumontier}},
  date = {2016-12-06},
  journaltitle = {Semantic Web},
  shortjournal = {SW},
  volume = {8},
  number = {3},
  pages = {437--452},
  issn = {22104968, 15700844},
  doi = {10.3233/SW-160213},
  url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-160213},
  urldate = {2023-09-28},
  abstract = {One of the major barriers to the deployment of Linked Data is the difficulty that data publishers have in determining which vocabularies to use to describe the semantics of data. This system report describes the Linked Open Vocabularies (LOV), a high quality catalogue of reusable vocabularies for the description of data on the Web. The LOV initiative gathers and makes visible indicators that have not been previously been harvested such as interconnection between vocabularies, version history, maintenance policy, along with past and current referent (individual or organization). The LOV goes beyond existing Semantic Web search engines and takes into consideration the value’s property type, matched with a query, to improve terms scoring. By providing an extensive range of data access methods (SPARQL endpoint, API, data dump or UI), we try to facilitate the reuse of well-documented vocabularies in the linked data ecosystem. We conclude that the adoption in many applications and methods of the LOV shows the benefits of such a set of vocabularies and related features to aid the design and publication of data on the Web.},
  langid = {english},
  annotation = {179 citations (Crossref) [2024-03-26]\\
abstractTranslation:  部署關聯資料的主要障礙之一是資料發布者難以確定使用哪些詞彙表來描述資料語義。此系統報告描述了連結開放詞彙表 (LOV)，這是一個用於描述 Web 資料的可重複使用詞彙表的高品質目錄。 LOV 計劃收集並製作以前未收集到的可見指標，例如詞彙表、版本歷史、維護策略以及過去和目前所指物件（個人或組織）之間的互連。 LOV 超越了現有的語意 Web 搜尋引擎，並考慮與查詢相符的值的屬性類型，以提高術語評分。透過提供廣泛的資料存取方法（SPARQL 端點、API、資料轉儲或 UI），我們嘗試促進連結資料生態系統中記錄良好的詞彙表的重複使用。我們的結論是，LOV 在許多應用程式和方法中的採用表明了這樣一組詞彙表和相關功能在幫助網路上資料的設計和發布方面的好處。\\
titleTranslation: 連結開放詞彙表 (LOV)：通往 Web 上可重複使用語意詞彙表的門戶},
  file = {C:\Users\BlackCat\Zotero\storage\6B88GRH6\Vandenbussche et al. - 2016 - Linked Open Vocabularies (LOV) A gateway to reusa.pdf}
}

@article{pingwuStatusProspectInternational2021,
  title = {Status and Prospect of International Standardization of {{TCM}} Diagnosis},
  author = {{Ping Wu} and {Jing Li} and {Hai-xia Yan} and {Rui Guo} and {Yi Lv} and {Yi-qin Wang}},
  date = {2021-09-01},
  journaltitle = {Pharmacological Research},
  shortjournal = {Pharmacological Research},
  volume = {171},
  pages = {105746},
  issn = {1043-6618},
  doi = {10.1016/j.phrs.2021.105746},
  url = {https://www.sciencedirect.com/science/article/pii/S1043661821003303},
  urldate = {2023-07-28},
  abstract = {The present study aimed to review the current status and development of international standards in the domain of traditional Chinese medicine (TCM) diagnosis. Moreover, the roles and relevant work of different organizations in developing such standards were explored, and the difficulties and challenges encountered were analyzed. The study further elaborated on the approaches to establish a complete set of international standards on TCM diagnosis. It also provided a promising solution for the development of international standards on TCM diagnosis.},
  langid = {english},
  keywords = {International standardization,TCM diagnosis},
  annotation = {7 citations (Crossref) [2024-03-26]\\
2 citations (Semantic Scholar/DOI) [2023-08-11]\\
titleTranslation: 中醫診斷國際標準化現狀與展望\\
abstractTranslation:  本研究旨在回顧中醫診斷領域國際標準的現狀和發展。此外，還探討了不同組織在標準制定過程中的作用和相關工作，分析了標準制定過程中遇到的困難和挑戰。該研究進一步闡述了建立一套完整的中醫診斷國際標準的途徑。它還為中醫診斷國際標準的製定提供了一個有前景的解決方案。},
  file = {C:\Users\BlackCat\Zotero\storage\S4BTPUXD\S1043661821003303.html}
}

@inproceedings{pingxuPracticalApplicationSoftware2017,
  title = {The {{Practical Application}} of {{Software Engineering}} to {{Graduation Project}} ({{Thesis}})},
  booktitle = {Proceedings of the 2017 3rd {{Conference}} on {{Education}} and {{Teaching}} in {{Colleges}} and {{Universities}} ({{CETCU}} 2017)},
  author = {{Ping Xu} and {Deyun Yang}},
  date = {2017},
  publisher = {Atlantis Press},
  location = {Taian, China},
  doi = {10.2991/cetcu-17.2017.61},
  url = {http://www.atlantis-press.com/php/paper-details.php?id=25882089},
  urldate = {2023-10-23},
  abstract = {This paper introduces the application of software engineering to graduation project (thesis), and starts with the discussion about topic selection, thesis proposal, system design and testing, the compiling of software documentation, thesis writing and other aspects, so as to ameliorate the current problems existing in graduation project (thesis) and improve its quality. Keywords—Graduation project (thesis);Practical teaching;Software engineering;Software life cycle},
  eventtitle = {2017 3rd {{Conference}} on {{Education}} and {{Teaching}} in {{Colleges}} and {{Universities}} ({{CETCU}} 2017)},
  isbn = {978-94-6252-374-6},
  langid = {english},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 软件工程在毕业项目中的实际应用（论文）\\
abstractTranslation:  本文介绍了软件工程在毕业设计（论文）中的应用，从毕业设计（论文）选题、毕业设计（论文）方案、系统设计与测试、软件文档编写、毕业设计（论文）撰写等方面展开论述，以期改善目前毕业设计（论文）中存在的问题，提高毕业设计（论文）质量。关键词-毕业设计（论文）;实践教学;软件工程;软件生命周期},
  note = {[TLDR] This paper introduces the application of software engineering to graduation project (thesis), and starts with the discussion about topic selection, thesis proposal, system design and testing, the compiling of software documentation, thesis writing and other aspects, so as to ameliorate the current problems existing in graduation project(thesis) and improve its quality.},
  file = {C:\Users\BlackCat\Zotero\storage\DSUL9YQJ\Xu and Yang - 2017 - The Practical Application of Software Engineering .pdf}
}

@online{pircherPersonalKnowledgeManagement2009,
  type = {SSRN Scholarly Paper},
  title = {Personal {{Knowledge Management}} as {{Management}} of {{Metacognition}} and {{Self Reflexion}} ({{Persönliches Wissensmanagement}} Als {{Management}} Der {{Metakognition}} Und {{Selbstreflexion}}) ({{German}})},
  author = {Pircher, Richard},
  date = {2009-01-12},
  number = {1593776},
  location = {Rochester, NY},
  doi = {10.2139/ssrn.1593776},
  url = {https://papers.ssrn.com/abstract=1593776},
  urldate = {2024-04-16},
  abstract = {Im  Beitrag wird ein differenzierter Wissensbegriff als Ausgangspunkt herangezogen, der eine Verbindung zwischen Wissen und rationaler Kognition einerseits und Körper und Emotionandererseits nahe legt. Ausgewählte, zentrale Erkenntnisse  der aktuellen Neurowissenschaften werden skizziert. Das Gehirn als physiologisches Organ zur Entwicklung  und Speicherung des Wissens stellt sich auf dieser Basis als verbindendes Element zwischen  Kognition und Emotion dar. Persönliches Wissensmanagement kann damit als Zusammenführung von Metakognition und Selbstreflexion definiert werden. Es werden strategische Leitlinien für ein persönliches Wissensmanagement abgeleitet.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Personal Knowledge Management as Management of Metacognition and Self Reflexion (Persönliches Wissensmanagement als Management der Metakognition und Selbstreflexion) (German),Richard Pircher,SSRN,未整理},
  annotation = {titleTranslation: 個人知識管理作為後設認知和自我反思的管理 (Persönliches Wissensmanagement als Management der Metakognition und Selbstreflexion)（德語）\\
abstractTranslation:  我認為，不同的科學概念是不同的，它是知識的結合和理性的認知、知識和情感的結合。 Ausgewählte, zentrale Erkenntnisse der aktuellen Neurowissenschaften werdenskizziert. Das Gehirn als Physologisches Organ zur Entwicklung und Speicherung des Wissens stellt sich auf dieser Basis als verbindendes Element zwischen Kognition und Emotion dar.個人科學管理可以定義為「後設認知與自我反射」。這是個人智慧管理的策略方針。},
  file = {C:\Users\BlackCat\Zotero\storage\Q85K83WR\Pircher - 2009 - Personal Knowledge Management as Management of Met.pdf}
}

@inproceedings{po-shenlinImprovingFuzzySyndrome2020,
  title = {Improving {{Fuzzy Syndrome Differentiation}} for {{Deficiency Syndromes}} in {{Traditional Chinese Medicine}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Consumer Electronics}} - {{Taiwan}} ({{ICCE-Taiwan}})},
  author = {{Po-Shen Lin} and {Nai-Wei Lin} and {Ming-Hsien Yeh} and {Chia-Chou Yeh} and {Hung-Pin Chiu}},
  date = {2020-09},
  pages = {1--2},
  issn = {2575-8284},
  doi = {10.1109/ICCE-Taiwan49838.2020.9258001},
  abstract = {The diseases in traditional Chinese medicine are defined as syndromes. The diagnosis of syndromes in traditional Chinese medicine is called syndrome differentiation. Our previous work proposes a fuzzy classification approach to solve the syndrome differentiation for 20 deficiency syndromes. This paper proposes an improved version of the previous fuzzy classification approach. This paper also presents a preliminary evaluation of the improvement based on the baseline differentiation coefficient of these 20 deficiency syndromes and the Jaccard similarity coefficient of 40 case reports from traditional Chinese medicine journal papers.},
  eventtitle = {2020 {{IEEE International Conference}} on {{Consumer Electronics}} - {{Taiwan}} ({{ICCE-Taiwan}})},
  langid = {english},
  keywords = {Blood,Diseases,Heart,Inspection,Kidney,Liver,Tongue},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 提高中醫虛證模糊辨證水平\\
abstractTranslation:  中醫把疾病定義為證候。中醫對證候的診斷稱為辨證。我們前期的工作提出了一種模糊分類方法來解決20種虛證的辨證問題。本文提出了先前模糊分類方法的改進版本。本文也根據這 20 個虛證的基線區分係數和 40 個中醫期刊論文病例報告的杰卡德相似係數對改善情況進行了初步評估。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\E8AQ5X36\\Lin 等。 - 2020 - Improving Fuzzy Syndrome Differentiation for Defic.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\LDIBR3XI\\9258001.html}
}

@article{pomerantzLinguisticAnalysisQuestion2005,
  title = {A Linguistic Analysis of Question Taxonomies},
  author = {Pomerantz, Jeffrey},
  date = {2005-05},
  journaltitle = {Journal of the American Society for Information Science and Technology},
  shortjournal = {J. Am. Soc. Inf. Sci.},
  volume = {56},
  number = {7},
  pages = {715--728},
  issn = {1532-2882, 1532-2890},
  doi = {10.1002/asi.20162},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/asi.20162},
  urldate = {2024-05-02},
  abstract = {Abstract             Recent work in automatic question answering has called for question taxonomies as a critical component of the process of machine understanding of questions. There is a long tradition of classifying questions in library reference services, and digital reference services have a strong need for automation to support scalability. Digital reference and question answering systems have the potential to arrive at a highly fruitful symbiosis. To move towards this goal, an extensive review was conducted of bodies of literature from several fields that deal with questions, to identify question taxonomies that exist in these bodies of literature. In the course of this review, five question taxonomies were identified, at four levels of linguistic analysis.},
  langid = {english},
  keywords = {未整理,重要},
  note = {[TLDR] An extensive review was conducted of bodies of literature from several fields that deal with questions, to identify question taxonomies that exist in these bodies of books and at four levels of linguistic analysis.},
  file = {C:\Users\BlackCat\Zotero\storage\LL2ZQ9BP\Pomerantz - 2005 - A linguistic analysis of question taxonomies.pdf}
}

@inproceedings{prantikachakrabortyPersonalResearchKnowledge2022,
  title = {Personal {{Research Knowledge Graphs}}},
  booktitle = {Companion {{Proceedings}} of the {{Web Conference}} 2022},
  author = {{Prantika Chakraborty} and {Sudakshina Dutta} and {Debarshi Kumar Sanyal}},
  year = {8 月 16, 2022},
  series = {{{WWW}} '22},
  pages = {763--768},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3487553.3524654},
  url = {https://dl.acm.org/doi/10.1145/3487553.3524654},
  urldate = {2023-09-16},
  abstract = {Maintaining research-related information in an organized manner can be challenging for a researcher. In this paper, we envision personal research knowledge graphs (PRKGs) as a means to represent structured information about the research activities of a researcher. PRKGs can be used to power intelligent personal assistants, and personalize various applications. We explore what entities and relations could be potentially included in a PRKG, how to extract them from various sources, and how to share a PRKG within a research group.},
  isbn = {978-1-4503-9130-6},
  langid = {english},
  keywords = {entities and relations,knowledge representation,personal knowledge graphs,Personal research knowledge graphs,scholarly data,已整理,待讀,本體建立,知識圖譜,重要},
  annotation = {5 citations (Crossref) [2024-03-26]\\
titleTranslation: 個人研究知識圖譜\\
abstractTranslation:  對於研究人員來說，以有組織的方式維護與研究相關的信息可能具有挑戰性。在本文中，我們設想個人研究知識圖（PRKG）作為表示研究人員研究活動結構化信息的一種手段。 PRKG 可用於為智能個人助理提供支持，並個性化各種應用程序。我們探索 PRKG 中可能包含哪些實體和關係，如何從各種來源提取它們，以及如何在研究小組內共享 PRKG。},
  note = {一個關於個人研究知識圖譜的構想，包含應用及內容等，包含出版物、實驗室資源、成員、專業領域等。基本和構想類似。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\9BJIY9SH\\Personal Research Knowledge Graphs.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\EXEWCFVB\\Chakraborty 等。 - 2022 - Personal Research Knowledge Graphs.pdf}
}

@inproceedings{pratiksharajeshraoGeneratingQARulebased2022,
  title = {Generating QA from Rule-based Algorithms},
  booktitle = {2022 International Conference on Electronics and Renewable Systems (ICEARS)},
  author = {{Pratiksha Rajesh Rao} and {Tanay Navneet Jhawar} and {Yash Avinash Kachave} and {Vaishali Hirlekar}},
  date = {2022-03},
  pages = {1697--1703},
  doi = {10.1109/ICEARS53579.2022.9751723},
  url = {https://ieeexplore.ieee.org/document/9751723},
  urldate = {2023-11-23},
  abstract = {In the education industry answering questions is used as a common parameter to judge one’s understanding of a topic. Taking quizzes on a regular basis helps an individual feel confident and it also helps the professor assess the student’s understanding on a particular topic. Generating question and answer pairs is a time-consuming task. To solve this problem, this paper discusses methods to generate automatic Natural Language Processing models which creates diverse types of question-answer pairs. The model takes an input in the form of text in the English language and produces output as Complex Questions, Multiple Choice Questions with relevant distractors, and Fill in the Blanks type of questions. To generate Complex Questions a Rule-Based Algorithm is used. To generate Multiple Choice Questions and Fill in the Blanks type questions, a Vector Algorithm from the GLoVe Model is used along with Rule-Based Algorithms. This paper also includes a detailed explanation of the analysis of the pattern and rules that are observed in the question-making process. SQuAD dataset is used for this analysis and used the same dataset to train the model. The implementation process of this model focused on generating diverse questions with higher syntactic correctness than the existing models. The approach mentioned in this paper can be used in the fields of education, entertainment, generation of quizzes, virtual learning assistance and to get a deeper insight into any topic.},
  eventtitle = {2022 International Conference on Electronics and Renewable Systems (ICEARS)},
  langid = {zh\_CN},
  keywords = {問答生成,問答系統,回收,已整理},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 從基於規則的演算法產生 QA},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\588Y859E\\Rao et al. - 2022 - Generating QA from Rule-based Algorithms.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\KQI3PWUK\\9751723.html}
}

@article{qilangAFSGraphMultidimensional2022,
  title = {{{AFS Graph}}: {{Multidimensional Axiomatic Fuzzy Set Knowledge Graph}} for {{Open-Domain Question Answering}}},
  shorttitle = {{{AFS Graph}}},
  author = {{Qi Lang} and {Xiaodong Liu} and {Wenjuan Jia}},
  date = {2022},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--15},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2022.3171677},
  url = {https://ieeexplore.ieee.org/document/9772750},
  urldate = {2023-11-23},
  abstract = {Open-domain question answering (QA) tasks require a model to retrieve inference chains associated with the answer from massive documents. The core of a QA model is the information filtering ability and reasoning ability. This article proposes a semantic knowledge reasoning graph model based on the multidimensional axiomatic fuzzy set (AFS), which can generate the knowledge graph (KG) and build reasoning paths for reading comprehension tasks through unsupervised learning. Moreover, taking advantage of the interpretable AFS framework enables the proposed model to have the ability to learn and analyze the semantic relationships between candidate documents. Meanwhile, the utilization of the multidimensional AFS acquires semantic descriptions of candidate documents more concise and flexible. The similarity degree between paragraphs is calculated according to the AFS description to generate the graph. Interpretable chains of reasoning provided by the AFS knowledge graph (AFS Graph) will serve as the basis for the answer prediction. Compared with the previous methods, the AFS Graph model presented in this article improves interpretability and reasoning ability. Experimental results show that the proposed model can achieve the state-of-the-art performance on datasets of HotpotQA, SQuAD, and Natural Questions Open.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  langid = {english},
  keywords = {問答系統,回收,已整理,文檔分析,模糊理論,語意分析},
  annotation = {4 citations (Crossref) [2024-03-26]\\
titleTranslation: AFS 圖：用於開放域問答的多維公理模糊集知識圖},
  note = {本研究注重於如何從多個文件中抽取出知識並建立知識圖譜，和研究較無相關。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\XTPSXGGI\\Lang et al. - 2022 - AFS Graph Multidimensional Axiomatic Fuzzy Set Kn.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\D74U88LF\\9772750.html}
}

@article{qinanhuEndtoEndSyndromeDifferentiation2019,
  title = {End-to-{{End}} Syndrome Differentiation of {{Yin}} Deficiency and {{Yang}} Deficiency in Traditional {{Chinese}} Medicine},
  author = {{Qinan Hu} and {Tong Yu} and {Jinghua Li} and {Qi Yu} and {Ling Zhu} and {Yueguo Gu}},
  year = {6 月 1, 2019},
  journaltitle = {Computer Methods and Programs in Biomedicine},
  shortjournal = {Comput. Methods Prog. Biomed.},
  volume = {174},
  number = {C},
  pages = {9--15},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2018.10.011},
  url = {https://doi.org/10.1016/j.cmpb.2018.10.011},
  urldate = {2023-09-18},
  abstract = {• We model syndrome differentiation as a text classification task. Accordingly, unlike most previous studies on syndrome differentiation, which use structured datasets, this study takes unstructured texts in medical records as its inputs. • We experiment with two state-of-the-art end-to-end deep learning algorithms for text classification, i.e. a classic convolutional neural network and fastText. When evaluated on a data set with 7,326 modern medical records in TCM, it’s observed that these algorithms generally give rise to comparable performances. • These systems take n-grams of characters, terms, and words as their inputs. The best accuracy 92.55\% comes from the system taking inputs as raw as n-grams of characters. It is implied that one can at least get a moderate system for syndrome differentiation of yin deficiency and yang deficiency even if he has no glossary or tokenizer at hand. • Both models used in this study are open source, and follow end-to-end paradigm, equiring minimal efforts on preprocessing and entirely no efforts on feature engineering, making it easy to be conducted by physicians and other medical practitioners with limited computer skills for their own studies.8Table 26 Background and Objective.Yin and Yang, two concepts adapted from classical Chinese philosophy, play a diagnostic role in Traditional Chinese Medicine (TCM). The Yin and Yang in harmonious balance indicate health, whereas imbalances to either side indicate unhealthiness, which may result in diseases. Yin-yang disharmony is considered to be the cause of pathological changes. Syndrome differentiation of yin-yang is crucial to clinical diagnosis. It lays a foundation for subsequent medical judgments, including therapeutic methods, and formula, among many others. However, because of the complexities of the mechanisms and manifestations of disease, it is difficult to exactly point out which one, yin or yang, is disharmonious. There has been inadequate research conducted on syndrome differentiation of yin and yang from a computational perspective. In this study, we present a computational method, viz. an end-to-end syndrome differentiation of yin deficiency and yang deficiency. Methods.Unlike most previous studies on syndrome differentiation, which use structured datasets, this study takes unstructured texts in medical records as its inputs. It models syndrome differentiation as a task of text classification. This study experiments on two state-of-the-art end-to-end algorithms for text classification, i.e. a classic convolutional neural network (CNN) and fastText. These two systems take the n-grams of several types of tokens as their inputs, including characters, terms, and words. Results.When evaluated on a data set with 7326 modern medical records in TCM, it is observed that CNN and fastText generally give rise to comparable performances. The best accuracy rate of 92.55\% comes from the system taking inputs as raw as n-grams of characters. It implies that one can build at least a moderate system for the differentiation of yin deficiency and yang deficiency even if he has no glossary or tokenizer at hand. Conclusions.This study has demonstrated the feasibility of using end-to-end text classification algorithms to differentiate yin deficiency and yang deficiency on unstructured medical records.},
  langid = {english},
  keywords = {Convolutional neural networks,End-to-end,FastText,Syndrome differentiation,Text classification,Traditional Chinese medicine,Yang deficiency,Yin deficiency,中醫,已整理,機器學習,辨證,重要},
  annotation = {46 citations (Crossref) [2024-03-26]\\
titleTranslation: 中醫陰虛陽虛的全程辨證\\
abstractTranslation:  • 我們將辨證建模為文本分類任務。因此，與先前大多數使用結構化資料集的辨證研究不同，本研究以病歷中的非結構化文字作為輸入。 • 我們嘗試了兩種最先進的端到端深度學習演算法來進行文字分類，即經典的捲積神經網路和 fastText。當對包含 7,326 個現代中醫病歷的資料集進行評估時，我們發現這些演算法通常會產生類似的效能。 • 這些系統採用n-gram 字元、術語和單字作為輸入。 92.55\% 的最佳準確率來自於系統將輸入作為 n 元字元的原始輸入。這意味著，即使手邊沒有詞彙表或符號化工具，至少也能得到一套適度的陰陽虛證辨證體系。 • 本研究中使用的兩種模型都是開源的，並遵循端到端範式，只需最少的預處理工作，完全不需要進行特徵工程，使得計算機技能有限的醫生和其他醫療從業者可以輕鬆地進行8 表26 背景與目的。陰陽這兩個源自中國古典哲學的概念在中醫（TCM）中發揮診斷作用。陰陽和諧平衡則是健康，而任何一方失衡則不健康，從而可能導致疾病。陰陽失調被認為是病理變化的原因。陰陽辨證對於臨床診斷至關重要。它為後續的醫學判斷，包括治療方法、配方等奠定了基礎。然而，由於疾病的機制和表現的複雜性，很難準確指出陰和陽哪一種不和諧。從計算角度對陰陽辨證進行的研究還不夠。在這項研究中，我們提出了一種計算方法，即。陰虛陽虛的端對端辨證。方法：與先前大多數使用結構化資料集的辨證研究不同，本研究以病歷中的非結構化文字作為輸入。它將辨證建模為文本分類任務。本研究對兩種最先進的端到端文字分類演算法進行了實驗，即經典的捲積神經網路 (CNN) 和 fastText。這兩個系統將多種類型標記的 n 元語法作為輸入，包括字元、術語和單字。結果。當對包含 7326 個現代中醫病歷的資料集進行評估時，我們發現 CNN 和 fastText 通常會產生類似的效能。 92.55\% 的最佳準確率來自於系統將輸入作為 n 元字元的原始輸入。這意味著，即使手邊沒有詞彙表或標記器，也至少可以建立一個適度的陰陽虛辨體系。結論：本研究證明了使用端到端文本分類演算法來區分非結構化病歷上的陰虛和陽虛的可行性。}
}

@inproceedings{qinanhuSemanticRepresentationsTerms2019,
  title = {Semantic {{Representations}} of {{Terms}} in {{Traditional Chinese Medicine}}},
  booktitle = {Chinese {{Lexical Semantics}}: 20th {{Workshop}}, {{CLSW}} 2019, {{Beijing}}, {{China}}, {{June}} 28–30, 2019, {{Revised Selected Papers}}},
  author = {{Qinan Hu} and {Ling Zhu} and {Feng Yang} and {Jinghua Li} and {Qi Yu} and {Ye Tian} and {Tong Yu} and {Yueguo Gu}},
  year = {6 月 28, 2019},
  pages = {764--775},
  publisher = {Springer-Verlag},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-030-38189-9_77},
  url = {https://doi.org/10.1007/978-3-030-38189-9_77},
  urldate = {2023-09-17},
  abstract = {Word embeddings have been widely used in lexical semantics and neural networks in Natural Language Processing. This article investigates the semantic representations using word embedding technologies by verifying them on a human constructed domain ontology. The domain of Traditional Chinese Medicine (TCM) is used as a workbench in this study, because this domain is knowledge-rich and has a large-scale domain ontology with well-defined entity types and relation types. This article releases a dataset, named “TCMSem”, to capture TCM domain experts’ intuitions of semantic relatedness. This data set is designed to cover the medical entities and relations with as many semantic types as possible so as to initiate a diverse and comprehensive evaluation on word embeddings. Experimental results show that word embeddings have demonstrated higher proficiencies in the detection of synonyms and collocations than other types of semantic relations. Furthermore, the semantic relatedness of thousands of terms of major categories in TCM is visualized using the taxonomy defined in the ontology.},
  isbn = {978-3-030-38188-2},
  langid = {english},
  keywords = {Evaluation,Semantic representation,TCMSem,Word embeddings,中醫,嵌入,已整理,機器學習,重要},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 中醫術語的語意表示\\
abstractTranslation:  詞嵌入已廣泛應用於自然語言處理中的詞彙語義和神經網路。本文透過在人類建構的領域本體上進行驗證來研究使用詞嵌入技術的語義表示。本研究以中醫領域為工作平台，因為該領域知識豐富，擁有大規模的領域本體，具有明確的實體類型和關係類型。本文發布了一個名為「TCMSem」的資料集，以捕捉中醫領域專家對語意相關性的直覺。該資料集旨在涵蓋盡可能多的語義類型的醫療實體和關係，以便對詞嵌入進行多樣化和全面的評估。實驗結果表明，詞嵌入在同義詞和搭配檢測方面表現出比其他類型的語義關係更高的熟練程度。此外，使用本體中定義的分類法來視覺化中醫主要類別的數千個術語的語義相關性。}
}

@article{qiongwangStudyEntitylinkingMethods2020,
  title = {A Study of Entity-Linking Methods for Normalizing {{Chinese}} Diagnosis and Procedure Terms to {{ICD}} Codes},
  author = {{Qiong Wang} and {Zongcheng Ji} and {Jingqi Wang} and {Stephen Wu} and {Weiyan Lin} and {Wenzhen Li} and {Li Ke} and {Guohong Xiao} and {Qing Jiang} and {Hua Xu} and {Yi Zhou}},
  date = {2020-05},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {J Biomed Inform},
  volume = {105},
  eprint = {32298846},
  eprinttype = {pmid},
  pages = {103418},
  issn = {1532-0480},
  doi = {10.1016/j.jbi.2020.103418},
  abstract = {OBJECTIVE: This study aims to develop and evaluate effective methods that can normalize diagnosis and procedure terms written by physicians to standard concepts in International Classification of Diseases(ICD) in Chinese, with the goal to facilitate automated medical coding in China. METHODS: We applied the entity-linking framework to normalize Chinese diagnosis and procedure terms, which consists of two steps - candidate concept generation and candidate concept ranking. For candidate concept generation, we implemented both the traditional BM25 algorithm and an extended version that integrates a synonym knowledgebase. For candidate concept ranking, we investigated a number of different algorithms: (1) the BM25 algorithm, (2) ranking support vector machines (RankSVM), (3) a previously reported Convolutional Neural Network (CNN) approach, (4) 11 deep ranking-based methods from the MatchZoo toolkit, and (5) a new BERT (Bidirectional Encoder Representations from Transformers) based ranking method. Using two manually annotated datasets (8,547 diagnoses and 8,282 procedures) collected from a Tier 3A hospital in China, we evaluated above methods and reported their performance (i.e., accuracy) at different cutoffs. RESULTS: The coverage of candidate concept generation was greatly improved after integrating the synonym knowledgebase, achieving 97.9\% for diagnoses and 93.4\% for procedures respectively. Overall the new BERT-based ranking method achieved the best performance on both diagnosis and procedure normalization, with the best accuracy of 92.1\% for diagnosis and 80.1\% for procedure, when the top one concept and exact match criteria were used. CONCLUSIONS: This study developed and compared diverse entity-linking methods to normalize clinical terms in Chinese and our evaluation shows good performance on mapping disease terms to ICD codes, demonstrating the feasibility of automated encoding of clinical terms in Chinese.},
  langid = {english},
  keywords = {BERT,China,Clinical Coding,Computer assisted coding,Entity-linking,ICD,ICD encoding,International Classification of Diseases,Neural Networks Computer,Support Vector Machine,中文,實體連結,已整理,機器學習,醫療},
  annotation = {16 citations (Crossref) [2024-03-26]\\
22 citations (Semantic Scholar/DOI) [2023-08-11]\\
titleTranslation: 將中文診斷和程序術語規範化為 ICD 代碼的實體鏈接方法研究\\
abstractTranslation:  目的：本研究旨在開發和評估有效的方法，使醫生編寫的診斷和程序術語標準化為中文國際疾病分類（ICD）中的標準概念，以促進中國的自動化醫療編碼。方法：我們應用實體鏈接框架來標準化中文診斷和程序術語，該框架包括兩個步驟：候選概念生成和候選概念排序。對於候選概念生成，我們實現了傳統的 BM25 算法和集成同義詞知識庫的擴展版本。對於候選概念排名，我們研究了許多不同的算法：(1) BM25 算法，(2) 排名支持向量機 (RankSVM)，(3) 先前報導的捲積神經網絡 (CNN) 方法，(4) 11 深度MatchZoo 工具包中基於排名的方法，以及(5) 一種新的基於BERT（來自Transformers 的雙向編碼器表示）的排名方法。使用從中國三級醫院收集的兩個手動註釋的數據集（8,547 例診斷和 8,282 例手術），我們評估了上述方法並報告了它們在不同截止值下的性能（即準確性）。結果：整合同義詞知識庫後，候選概念生成的覆蓋率大大提高，診斷覆蓋率達到97.9\%，手術覆蓋率達到93.4\%。總體而言，當使用頂級概念和精確匹配標準時，基於BERT 的新排序方法在診斷和程序標準化方面均取得了最佳性能，診斷的最佳準確率為92.1\%，程序的最佳準確率為80.1 \%。結論：本研究開發並比較了多種實體鏈接方法來規範中文臨床術語，我們的評估顯示在將疾病術語映射到 ICD 代碼方面具有良好的性能，證明了中文臨床術語自動編碼的可行性。},
  file = {C:\Users\BlackCat\Zotero\storage\Z9I6S76U\Wang 等。 - 2020 - A study of entity-linking methods for normalizing .pdf}
}

@thesis{QiuYiJieBERTMoXingYuZhongWenYouErShiBiaoTiKeJieShiXingZhiYanJiu,
  title = {{{BERT}} 模型於中文誘餌式標題可解釋性之研究},
  author = {{邱奕傑}},
  institution = {國立中正大學},
  location = {嘉義縣},
  abstract = {自然語言處理領域 (Natural Language Processing, NLP) 隨著機器學習(Machine Learning, ML) 與深度學習 (Deep Learning, DL) 的迅速發展，在許多任務上都得到了卓越的改善。誘餌式標題任務 (Clickbait) 在NLP中被視為一項重要任務，定義為在網路上故意用較為誇大、聳動、不實的文章標題以吸引網友點擊觀看的文章，像是常見的網路標題：「看完後我驚呆了！」、「全世界百萬人瘋傳，大家都驚呆了」、「面對記者的瘋狂追問，他竟這麼說」，主要是網路新媒體業想吸引大眾目光，獲得曝光度與點閱量，透過點閱量賺取廣告收益常採用的方法。在中文誘餌式標題任務中，常採用的ML與DL方法非常依賴人工特徵以及循環神經網路記憶長期資訊，無法準確學習到任務的內部特徵，導致模型分類準確率並不高，且模型不具有可解釋性 (Explainable AI, XAI)。XAI關心模型下判斷的理由，當模型的判斷會對使用者的生活造成重大影響，使用者卻無法得知判斷理由與信任模型時，模型將會百無一用。本論文以改善中文誘餌式任務準確度與證實模型具有可解釋性為目標，使用預訓練BERT模型搭配線性分類器與改善BERT輸入限制等問題，提升模型準確率；利用可視化注意力工具BertViz觀察模型內部權重，針對模型的XAI進行研究，證實模型具有可解釋性。}
}

@inproceedings{quanluResearchBasicClinical2023,
  title = {Research on {{Basic Clinical Treatment Pattern Mining Based}} on {{Electronic Medical Record Big Data}}},
  booktitle = {2023 {{IEEE}} 8th {{International Conference}} on {{Big Data Analytics}} ({{ICBDA}})},
  author = {{Quan Lu} and {Xiaoying Zheng} and {Jing Chen}},
  date = {2023-03},
  pages = {57--61},
  doi = {10.1109/icbda57405.2023.10104953},
  abstract = {This paper analyzed the basic clinical treatment pattern by exploring the relationship between diseases, symptoms and drugs, which help non-medical people understand the basic clinical treatment pattern, so as to better carry out medical and health big data mining and eliminate public prejudice against symptomatic treatment. The FP growth algorithm was used to mine the association rules from EMR big data. Combined with intersection analysis, the basic clinical treatment pattern was summarized. 507 disease-drug rules and 2141 symptom-drug rules were obtained, indicating that both diseases and symptoms are strongly associated with drugs. Intersection analysis showed that 33.7\% of the disease-drug rules were symptom-independent, while 34.6\% of the symptom-drug rules were disease-independent. The basic clinical treatment pattern consists of three parts: (1) The combination of disease and symptomatic medication pattern. (2) Independent disease medication pattern. (3) Independent symptomatic medication pattern.},
  eventtitle = {2023 {{IEEE}} 8th {{International Conference}} on {{Big Data Analytics}} ({{ICBDA}})},
  langid = {english},
  keywords = {clinical treatmentpattern,Infectious diseases,Medical treatment,symptomatic treatment,已整理,數據挖掘,病歷分析,統計},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於電子病歷大數據的臨床基礎治療模式挖掘研究\\
abstractTranslation:  本文通過探究疾病、症狀與藥物之間的關係來分析臨床基本治療模式，幫助非醫學人士了解臨床基本治療模式，從而更好地進行醫療健康大數據挖掘，消除公眾對症狀的偏見。治療。採用FP增長算法從EMR大數據中挖掘關聯規則。結合交叉分析，總結臨床基本治療模式。獲得507條疾病-藥物規則和2141條症狀-藥物規則，表明疾病和症狀均與藥物強相關。交叉分析顯示，33.7\%的疾病-藥物規則與症狀無關，而34.6\%的症狀-藥物規則與疾病無關。臨床基本治療模式由三部分組成：（1）疾病與對症用藥相結合的模式。 (2)獨立的疾病用藥模式。 (3)獨立對症用藥模式。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\J37UTYV8\\Lu 等。 - 2023 - Research on Basic Clinical Treatment Pattern Minin.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\V2ESI7TL\\10104953.html}
}

@article{quanwangKnowledgeGraphEmbedding2017,
  title = {Knowledge {{Graph Embedding}}: {{A Survey}} of {{Approaches}} and {{Applications}}},
  shorttitle = {Knowledge {{Graph Embedding}}},
  author = {{Quan Wang} and {Zhendong Mao} and {Bin Wang} and {Li Guo}},
  date = {2017-02},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {29},
  number = {12},
  pages = {2724--2743},
  issn = {1558-2191},
  doi = {10.1109/tkde.2017.2754499},
  abstract = {Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  langid = {english},
  keywords = {Graphical models,Knowledge discovery,knowledge graph embedding,latent factor models,Market research,Matrix decomposition,Semantics,Statistical analysis,Statistical relational learning,Systematics,tensor/matrix factorization models,待讀,機器學習,知識圖,知識挖掘},
  annotation = {1373 citations (Crossref) [2024-03-26]\\
1533 citations (Semantic Scholar/DOI) [2023-05-18]\\
titleTranslation: 知識圖嵌入：方法和應用綜述\\
abstractTranslation:  知識圖譜（KG）嵌入是將知識圖譜的組成部分（包括實體和關係）嵌入到連續的向量空間中，從而在保留知識圖譜固有結構的同時簡化操作。它可以有益於KG補全、關係抽取等多種下游任務，因此迅速獲得了廣泛關注。在本文中，我們對現有技術進行了系統回顧，不僅包括最先進的技術，還包括最新趨勢的技術。特別是，我們根據嵌入任務中使用的信息類型進行審查。首先介紹僅使用知識圖譜中觀察到的事實進行嵌入的技術。我們描述了總體框架、具體模型設計、典型訓練程序以及此類技術的優缺點。之後，我們討論進一步整合除事實之外的附加信息的技術。我們特別關注實體類型、關係路徑、文本描述和邏輯規則的使用。最後，我們簡要介紹了知識圖譜嵌入如何應用於並有益於各種下游任務，例如知識圖譜補全、關係提取、問答等。},
  note = {說明何謂KG Embedding
\par
\href{https://zhuanlan.zhihu.com/p/102391664}{知识表示-KG Embedding - 知乎 (zhihu.com)}},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\54XSJWQQ\\Wang 等。 - 2017 - Knowledge Graph Embedding A Survey of Approaches .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\E9KPZU8E\\8047276.html}
}

@inproceedings{quarteroniPersonalizedInteractiveQuestion2008,
  title = {Personalized, {{Interactive Question Answering}} on the {{Web}}},
  booktitle = {Coling 2008: {{Proceedings}} of the Workshop on {{Knowledge}} and {{Reasoning}} for {{Answering Questions}}},
  author = {Quarteroni, Silvia},
  editor = {Moens, Marie-Francine and Saint-Dizier, Patrick},
  date = {2008-08},
  pages = {33--40},
  publisher = {Coling 2008 Organizing Committee},
  location = {Manchester, UK},
  url = {https://aclanthology.org/W08-1605},
  urldate = {2024-04-06},
  abstract = {Two of the current issues of Question Answering (QA) systems are the lack of personalization to the individual users’ needs, and the lack of interactivity by which at the end of each Q/A session the context of interaction is lost. We address these issues by designing and implementing a model of personalized, interactive QA based on a User Modelling component and on a conversational interface. Our evaluation with respect to a baseline QA system yields encouraging results in both personalization and interactivity.},
  langid = {english},
  keywords = {NLP,問答系統,已整理,機器學習,略讀},
  annotation = {titleTranslation: 網路上的個人化互動式問答\\
abstractTranslation:  問答 (QA) 系統目前存在的兩個問題是缺乏對個人使用者需求的個人化，以及缺乏互動性，導致每次問答會話結束時都會失去互動情境。我們透過設計和實現基於使用者建模元件和對話介面的個人化互動式 QA 模型來解決這些問題。我們對基線 QA 系統的評估在個人化和互動性方面都取得了令人鼓舞的結果。},
  note = {很久以前的研究，主要方法為建立一個使用者模型，來提供個人化的問答結果，包含年齡及閱讀程度。
\par
幾乎沒有研究用的上的地方。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\WBPVXACL\\Quarteroni - 2008 - Personalized, Interactive Question Answering on th.pdf;D\:\\Paper\\Personalized, Interactive Question Answering on the Web.pdf}
}

@article{QuChunYanZhongWenDianZiBingLiMingMingShiTiBiaoZhuYuLiaoKuGouJian2015,
  title = {中文电子病历命名实体标注语料库构建},
  author = {{曲春燕} and {关毅} and {杨锦锋} and {赵永杰} and {刘雅欣}},
  date = {2015},
  journaltitle = {高技术通讯},
  shortjournal = {Chinese High Technology Letters},
  volume = {25},
  number = {2},
  pages = {143--150},
  doi = {10.3772/j.issn.1002-0470.2015.02.005},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhBnanN0eDk4MjAxNTAyMDA1GghhOXd1eW5qNg%3D%3D},
  urldate = {2022-08-14},
  abstract = {针对中文电子病历命名实体语料标注空白的现状,研究了中文电子病历命名实体标注语料库的构建.参考2010年美国国家集成生物与临床信息学研究中心(I2B2)给出的电子病历命名实体类型及修饰类型的定义,在专业医生的指导下制定了详尽的中文电子病历标注规范;通过对大量中文电子病历的分析,提出了一套完整的中文电子病历命名实体标注方案,而且采用预标注和正式标注的方法,建立了一定规模的中文电子病历命名实体标注语料库,其标注语料的一致性达到了92％以上.该工作对中文电子病历的命名实体识别及信息抽取研究提供了可靠的数据支持,对医疗知识挖掘也有重要意义.},
  langid = {zh\_CN},
  keywords = {annotated corpus,annotation specification,Chinese electronic medical record(CEMR),Chinese High Technology Letters,inter-annotator agreement (IAA),named entity,中文電子病歷CEMR,命名实体,標註一致性(IAA),標註規範,標註語料庫},
  annotation = {哈尔滨工业大学计算机科学与技术学院 哈尔滨150001哈尔滨医科大学附属第四医院 哈尔滨150001哈尔滨医科大学附属第二医院 哈尔滨150001\\
国家自然科学基金\\
2015-07-31 （万方平台首次上网日期，不代表论文的发表时间）\\
abstractTranslation:  針對中文電子病歷命名實體語料嚴重空白的困境，研究了中文電子病歷命名實體標註語料庫的構建。參考2010年美國國家集成生物與臨床信息學研究中心(I2B2)提出了電子病歷命名實體類型及修飾類型的定義，在專業醫生的指導下制定了中文電子病歷標記規範；通過對大量中文電子病歷的分析，提出了一套完整的中文電子病歷命名實體標記方案，並採用預標記和正式標記的方法，建立了一定規模的中文電子病歷命名實體標記語料庫，其標記語料的一致性達到了92\%以上。該工作為中文電子病歷的命名實體識別及信息抽取研究提供了可靠的數據支持，為醫療挖掘知識也具有重要意義。\\
titleTranslation: 中文電子病歷命名實體標註語料庫構建},
  file = {C:\Users\BlackCat\Zotero\storage\NDQ3ELYD\曲 等。 - 2015 - 中文电子病历命名实体标注语料库构建.pdf}
}

@article{QuFengHanYuZiDongFenCiSuanFaZongShu2006,
  title = {汉语自动分词算法综述},
  author = {{瞿锋} and {陈纪元}},
  date = {2006},
  journaltitle = {福建电脑},
  number = {4},
  pages = {3},
  doi = {10.3969/j.issn.1673-2782.2006.04.015},
  url = {https://wenku.baidu.com/view/0925eed2da38376baf1fae66?fr=xueshu_top},
  urldate = {2022-08-25},
  abstract = {本文对目前已有的各种中文自动分词的算法,采用的模型,数据结构等方面进行了分析,比较,探讨了它们的优缺点,并指出了今后的研究方向.},
  keywords = {查全率,查准率,算法模型,自动分词}
}

@thesis{QuGuangXiaoJiYuZhiShiBenTiDeZhongYiYaoCaiPeiWuXiTong2016,
  title = {基於知識本體的中醫藥材配伍系統},
  author = {{屈光孝}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2016},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/6rh7wm},
  abstract = {「辨證論治」為中醫治療疾病所發展出來的診治方式。其中包含兩個階段，「辨證」為中醫師先透過望、聞、問、切四診方法，來收集病人的症狀及體徵資訊，再依據所收集來的症狀資訊，分析及辨識病人的病因、病位與病機。「論治」為中醫師根據不同的病因、病位與病機的辨證結果，對病患實施適合的治療方法與用藥。辨證與論治是治療疾病時，密不可分的兩個階段。 方劑學是依據治療方法，研究藥材配伍的學科。方劑是由一種或多種藥材所組成的藥方，組成須符合一定的原則，按照各藥材在方劑中所起的作用，共分成君藥、臣藥、佐藥、使藥四類，互相搭配，達成治療的功效。 方劑學的藥材配伍原則非常複雜，本論文應用知識本體及多目標最佳化技術，研製一個中醫藥材配伍系統。此系統包含一個中醫藥材知識本體，一個中醫藥材辭庫，及一個中醫藥材配伍模組。本論文進行中醫藥材的標準化，及中醫藥材功效的標準化，利用知識本體開發工具建構中醫藥材知識本體，並建構中醫藥材辭庫，方便查詢標準藥材及標準藥材功效。本論文應用多目標最佳化技術研製一個中醫藥材配伍模組，並針對28個常用脾氣虛及脾陽虛的方劑進行初步的系統評估，達到非常好的結果。這個評估結果也具體顯示中醫藥材配伍系統的可行性及重要性。},
  pagetotal = {101}
}

@article{rademacherChallengesDomainDrivenMicroservice2018,
  title = {Challenges of {{Domain-Driven Microservice Design}}: {{A Model-Driven Perspective}}},
  shorttitle = {Challenges of {{Domain-Driven Microservice Design}}},
  author = {Rademacher, Florian and Sorgalla, Jonas and Sachweh, Sabine},
  date = {2018-05},
  journaltitle = {IEEE Software},
  volume = {35},
  number = {3},
  pages = {36--43},
  issn = {1937-4194},
  doi = {10.1109/MS.2018.2141028},
  url = {https://ieeexplore.ieee.org/document/8354426},
  urldate = {2024-04-28},
  abstract = {Domain-driven design (DDD) is a model-driven methodology to capture relevant domain knowledge for software design. It provides the means to isolate domain concepts and identify concept relationships. This makes DDD particularly appropriate for designing microservice architectures, because functional microservices focus on realizing distinct business capabilities. This article explores the challenges of domain-driven microservice design and presents ways to cope with them based on model-driven development.},
  eventtitle = {{{IEEE Software}}},
  langid = {english},
  keywords = {Context modeling,DDD,domain-driven design,domain-specific architectures,Logic gates,MDD,microservice architecture,microservices,model-driven development,modeling of computer architecture,Service computing,service engineering,Software architecture,software development,software engineering,Software engineering,Unified modeling language,微服務,未整理},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\S8SZ32X7\\Rademacher 等。 - 2018 - Challenges of Domain-Driven Microservice Design A.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\VAP33E6M\\8354426.html}
}

@online{rainekauppinenSoftwareEngineeringEducation2022,
  type = {publication},
  title = {Software {{Engineering Education}} with {{Industry-Mentored Projects}}},
  author = {{Raine Kauppinen} and {Altti Lagstedt} and {Juha Lindstedt} and {Ohto Rainio}},
  date = {2022},
  publisher = {The Association for Information Systems},
  url = {http://www.theseus.fi/handle/10024/781024},
  urldate = {2023-10-13},
  abstract = {Many educational software engineering (SE) projects are done with little or no real outside involvement. Also, with an outside customer, the objective is typically to produce a demo, proof of concept, or minimum viable product. These lack real pressure from outside stakeholders because technological solutions are often agreed on based on a curriculum, and implemented features can be negotiated based on the students’ skills and the course schedule. Therefore, typical industry–education cooperation SE projects hardly resemble the real SE projects carried out in software companies. To overcome these limitations, we developed and tested a university–industry model where real-life development pressure coming from real customers and a software provider was generated to suit educational purposes. The model was beneficial for all parties and produced a more realistic learning experience compared with other project-based models. However, the model also required additional support, such as facilitation and mentoring for the students.},
  isbn = {9781958200018},
  langid = {english},
  organization = {67637},
  annotation = {Accepted: 2022-10-26T07:16:14Z\\
titleTranslation: 軟體工程教育與產業指導項目\\
abstractTranslation:  許多教育軟體工程 (SE) 專案是在很少或根本沒有真正的外部參與的情況下完成的。此外，對於外部客戶，目標通常是製作演示、概念驗證或最小可行產品。這些缺乏來自外部利害關係人的真正壓力，因為技術解決方案通常是根據課程達成一致的，並且可以根據學生的技能和課程安排來協商實施的功能。因此，典型的產教合作SE專案與真正在軟體公司進行的SE專案很難相似。為了克服這些限制，我們開發並測試了一種大學-行業模型，其中產生了來自真實客戶和軟體提供者的現實開發壓力，以滿足教育目的。與其他基於專案的模型相比，該模型對各方都有好處，並且產生了更真實的學習體驗。然而，該模型還需要額外的支持，例如對學生的促進和指導。},
  file = {C:\Users\BlackCat\Zotero\storage\HZ78TXZ5\Kauppinen et al. - 2022 - Software Engineering Education with Industry-Mento.pdf}
}

@inproceedings{rashmiprasadTrainingDialogueAct2002,
  title = {Training a {{Dialogue Act Tagger}} for Human-Human and Human-Computer Travel Dialogues},
  booktitle = {Proceedings of the 3rd {{SIGdial}} Workshop on {{Discourse}} and Dialogue - {{Volume}} 2},
  author = {{Rashmi Prasad} and {Marilyn Walker}},
  year = {7 月 11, 2002},
  series = {{{SIGDIAL}} '02},
  pages = {162--173},
  publisher = {Association for Computational Linguistics},
  location = {USA},
  doi = {10.3115/1118121.1118142},
  url = {https://dl.acm.org/doi/10.3115/1118121.1118142},
  urldate = {2023-09-05},
  abstract = {While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues, their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme. In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues. We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags. We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora, and the CMU human-human corpus in the travel planning domain. Our results show that we can achieve high accuracies on the human-computer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.},
  langid = {english},
  keywords = {機器學習,語言行為},
  annotation = {4 citations (Crossref) [2024-03-26]\\
abstractTranslation:  雖然對話行為為表徵人機和人與人對話中的對話行為提供了有用的模式，但它們的實用性受到使用對話行為標記方案手動標記對話所涉及的巨大努力的限制。在這項工作中，我們研究是否有可能完全自動化標記任務，目標是能夠快速創建語料庫來評估口語對話系統並將其與人與人的對話進行比較。我們報告訓練和測試自動分類器的結果，以使用 DATE（對話行為評估標記）對話行為標籤來標記信息提供者在人機口語和人機對話中的話語。我們在 DARPA Communicator 2000 年 6 月和 2001 年 10 月人機語料庫以及旅行規劃領域的 CMU 人機語料庫的各種組合上訓練和測試 DATE 標記器。我們的結果表明，我們可以在人機數據上實現高精度，並且令人驚訝的是，當只有少量人機訓練數據可用時，人機數據提高了人機數據的準確性。\\
titleTranslation: 訓練對話行為標記器進行人與人以及人與計算機的旅行對話},
  file = {C:\Users\BlackCat\Zotero\storage\KX64HEEH\Prasad 與 Walker - 2002 - Training a Dialogue Act Tagger for human-human and.pdf}
}

@inproceedings{raykekwaletsweActivityAnalysisKnowledge2011,
  title = {Activity Analysis of a Knowledge Management System: Adoption and Usage Case Study},
  shorttitle = {Activity Analysis of a Knowledge Management System},
  booktitle = {Proceedings of the {{South African Institute}} of {{Computer Scientists}} and {{Information Technologists Conference}} on {{Knowledge}}, {{Innovation}} and {{Leadership}} in a {{Diverse}}, {{Multidisciplinary Environment}}},
  author = {{Ray Kekwaletswe} and {Thuli Bobela}},
  year = {10 月 3, 2011},
  series = {{{SAICSIT}} '11},
  pages = {287--289},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2072221.2072259},
  url = {https://doi.org/10.1145/2072221.2072259},
  urldate = {2023-09-11},
  abstract = {The aim of this paper is to present a qualitative study whose aim was to investigate knowledge management system usage in a large organization through the lens of activity theory. The study analyzed actions and behavior of users, in order to identify the factors that determine the use of knowledge management systems. The empirical evidence was obtained through open-ended questionnaires, interviews and direct observations at the work environment. The study revealed that there was lack of employee participation and involvement during the implementation of the KMS, and this was identified, among other factors, as the contributing factor to user resistance and reluctance to use the system, thereof. The value and strategic intent of the knowledge management system was not clear to most employees; thus, instead of using the implemented system to share, create, store and transfer knowledge, employees instead reverted to using varied other mechanism to interact with colleagues.},
  isbn = {978-1-4503-0878-6},
  langid = {english},
  keywords = {activity theory,social and cultural factors,人機互動/使用者研究,知識系統},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 知識管理系統的活動分析：採用和使用案例研究\\
abstractTranslation:  本文的目的是提出一項定性研究，其目的是通過活動理論的視角來調查知識管理系統在大型組織中的使用情況。該研究分析了用戶的行動和行為，以確定決定知識管理系統使用的因素。經驗證據是通過開放式問卷、訪談和對工作環境的直接觀察獲得的。研究表明，在 KMS 實施過程中，員工缺乏參與和參與，這被認為是導致用戶抵制和不願使用該系統的因素之一。大多數員工並不清楚知識管理系統的價值和戰略意圖；因此，員工不再使用已實施的系統來共享、創建、存儲和轉移知識，而是轉而使用各種其他機制與同事互動。},
  file = {C:\Users\BlackCat\Zotero\storage\HCFEJ3M4\Kekwaletswe 與 Bobela - 2011 - Activity analysis of a knowledge management system.pdf}
}

@online{rehamomarChatGPTTraditionalQuestion2023,
  title = {{{ChatGPT}} versus {{Traditional Question Answering}} for {{Knowledge Graphs}}: {{Current Status}} and {{Future Directions Towards Knowledge Graph Chatbots}}},
  shorttitle = {{{ChatGPT}} versus {{Traditional Question Answering}} for {{Knowledge Graphs}}},
  author = {{Reham Omar} and {Omij Mangukiya} and {Panos Kalnis} and {Essam Mansour}},
  date = {2023-02-08},
  eprint = {2302.06466},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.06466},
  url = {http://arxiv.org/abs/2302.06466},
  urldate = {2023-10-25},
  abstract = {Conversational AI and Question-Answering systems (QASs) for knowledge graphs (KGs) are both emerging research areas: they empower users with natural language interfaces for extracting information easily and effectively. Conversational AI simulates conversations with humans; however, it is limited by the data captured in the training datasets. In contrast, QASs retrieve the most recent information from a KG by understanding and translating the natural language question into a formal query supported by the database engine. In this paper, we present a comprehensive study of the characteristics of the existing alternatives towards combining both worlds into novel KG chatbots. Our framework compares two representative conversational models, ChatGPT and Galactica, against KGQAN, the current state-of-the-art QAS. We conduct a thorough evaluation using four real KGs across various application domains to identify the current limitations of each category of systems. Based on our findings, we propose open research opportunities to empower QASs with chatbot capabilities for KGs. All benchmarks and all raw results are available1 for further analysis.},
  langid = {english},
  pubstate = {preprint},
  keywords = {ChatGPT,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,問答系統,已整理,知識圖譜},
  annotation = {titleTranslation: ChatGPT 與傳統知識圖問答：知識圖聊天機器人的現況與未來方向},
  note = {Comment: 9 pages},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\FSLMKEYZ\\Omar et al. - 2023 - ChatGPT versus Traditional Question Answering for .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\SHNGP72Z\\2302.html}
}

@inproceedings{rejimoanrComprehensiveReviewDeep2023,
  title = {A {{Comprehensive Review}} on {{Deep Learning Approaches}} for {{Question Answering}} and {{Machine Reading Comprehension}} in {{NLP}}},
  booktitle = {2023 2nd {{Edition}} of {{IEEE Delhi Section Flagship Conference}} ({{DELCON}})},
  author = {{Rejimoan R} and {Gnanapriya B} and {Jayasudha J S}},
  date = {2023-02},
  pages = {1--6},
  doi = {10.1109/DELCON57910.2023.10127327},
  url = {https://ieeexplore.ieee.org/document/10127327},
  urldate = {2023-11-23},
  abstract = {Natural Language Processing (NLP) deals with the development of methodologies capable of interacting with computer through human language. NLP improves machine’s comprehension of human language, allowing for human-computer communication based on linguistics. Recent years have seen phenomenal success of NLP models in language and grammatical tasks such as information extraction, translation, classification and reasoning. This accomplishment is mainly due to the influence of transformers, which inspired design ideas such as BERT, SQuAD 2.0 and others. These large-scale models produced unique results, despite the higher computational cost. As a result, current NLP systems use transfer learning, pruning, knowledge filtration and quantization to accomplish reasonable performance. Furthermore, Information Retrievers (IR) are created to extract precise data files from large datasets, addressing the large data assertion made by language models. Major contribution of this study is to understand the application of deep learning methods in NLP for automated question answering and obtaining a comprehension of essay text. Context-based NLP issues that are presented along with existing solutions. The challenges of using NLP in comprehension are examined, as well as research community methods for extracting answers from paragraphs. Further direction of this research is to develop novel deep learning models for QA and text comprehension that can overcome the demerits of existing approaches.},
  eventtitle = {2023 2nd {{Edition}} of {{IEEE Delhi Section Flagship Conference}} ({{DELCON}})},
  langid = {english},
  keywords = {問答系統,已整理,機器學習},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: NLP 中問答和機器閱讀理解深度學習方法的全面綜述},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\J9XJX9S3\\R et al. - 2023 - A Comprehensive Review on Deep Learning Approaches.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\Z82RF2IW\\10127327.html}
}

@inproceedings{reneewhiteCliniDigestCaseStudy2023,
  title = {{{CliniDigest}}: {{A Case Study}} in {{Large Language Model Based Large-Scale Summarization}} of {{Clinical Trial Descriptions}}},
  shorttitle = {{{CliniDigest}}},
  booktitle = {Proceedings of the 2023 {{ACM Conference}} on {{Information Technology}} for {{Social Good}}},
  author = {{Renee White} and {Tristan Peng} and {Pann Sripitak} and {Alexander Rosenberg Johansen} and {Michael Snyder}},
  year = {9 月 6, 2023},
  series = {{{GoodIT}} '23},
  pages = {396--402},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3582515.3609559},
  url = {https://doi.org/10.1145/3582515.3609559},
  urldate = {2023-08-23},
  abstract = {A clinical trial is a study that evaluates new biomedical interventions. To design new trials, researchers draw inspiration from those current and completed. In 2022, there were on average more than 100 clinical trials submitted to ClinicalTrials.gov every day, with each trial having a mean of approximately 1500 words [1]. This makes it nearly impossible to keep up to date. To mitigate this issue, we have created a batch clinical trial summarizer called CliniDigest using GPT-3.5. CliniDigest is, to our knowledge, the first tool able to provide real-time, truthful, and comprehensive summaries of clinical trials. CliniDigest can reduce up to 85 clinical trial descriptions (approximately 10,500 words) into a concise 200-word summary with references and limited hallucinations. We have tested CliniDigest on its ability to summarize 457 trials divided across 27 medical subdomains. For each field, CliniDigest generates summaries of μ = 153,\,σ = 69 words, each of which utilizes of the sources. A more comprehensive evaluation is planned and outlined in this paper.},
  isbn = {9798400701160},
  langid = {english},
  keywords = {ChatGPT,Prompt Engineering,回收,摘要,醫學},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: CliniDigest：基於大型語言模型的臨床試驗描述大規模總結案例研究\\
abstractTranslation:  臨床試驗正在評估新的生物醫學乾預措施的研究。為了設計新的試驗，研究人員從當前和已完成的試驗中汲取靈感。2022年，平均每天有超過100個臨床試驗提交到ClinicalTrials.gov，每個試驗的平均字數約為1500字[1]。這使得幾乎不可能保持最新狀態。為了解決這個問題，我們使用GPT-3.5 創建了一個名為CliniDigest 的批量臨床試驗總結器。據我們所知道， CliniDigest 是第一個能夠提供實時、真實、全面的臨床試驗總結的工具。CliniDigest 可以將多達85 個臨床試驗（約10,500 個單詞）縮減為200 個單詞的簡明摘要，其中包含參考文獻描述和有限的幻覺。我們測試了CliniDigest 總結了27 個醫學子領域的457 項試驗的能力。對於每個字段，CliniDigest 生成了μ = 153、σ = 69 個單詞的摘要，每個單詞都使用了來源論文。計劃並概述了更全面的評估。},
  note = {使用ChatGPT從醫療器材等醫學實驗結果產生摘要。超出研究範圍。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\JKY3WAEX\\White 等。 - 2023 - CliniDigest A Case Study in Large Language Model .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\VTDC3GK3\\White 等。 - 2023 - CliniDigest A Case Study in Large Language Model .pdf}
}

@misc{RenTianYitien-yijenYingYongZiDongYanZhengJiShuTiShengZiRanYuYanChengShiHuaWenDaXiTongKeKaoXingZhiZongHeYanJiu2023,
  title = {應用自動驗證技術提升自然語言程式化問答系統可靠性之綜合研究},
  author = {{任恬儀(Tien-Yi Jen)}},
  year = {1 月 1, 2023},
  abstract = {大型語言模型 (LLMs) 不僅革新了自然語言處理 (NLP) 領域，也為實際應用帶來了重大變革。儘管有這些進步，像程序生成這樣的領域仍然具有挑戰性。本論文專注於生成兩種類型的程序：數學程序和知識圖譜問答 (KGQA) 程序。    對於數學程序，我們的工作提出了一種新穎的回收數值數據擴增 (RNDA) 方法，該方法自動生成高質量的訓練實例與程序。實驗結果顯示，用擴增數據訓練的模型可以達到最先進的性能。    與此同時，在KGQA程序的領域，我們提出了一種反向生成的驗證方法以提高可靠性。實驗表明，這種方法也可以提高ChatGPT在此任務的性能。    總的來說，該研究通過引入新方法，描繪了程序生成的範式轉變，專注於改善數學和KGQA程序。這些發現為未來的研究提供了一個有前景的基礎，目標是充分利用大型語言模型。},
  langid = {chinese},
  organization = {國立台灣大學學位論文},
  keywords = {LLM,問答系統,已整理},
  annotation = {titleTranslation: 應用自動驗證技術提升自然語言方案化問答系統可靠度綜合研究\\
abstractTranslation:  大型語言模型（LLM）不僅革新了自然語言處理（NLP）領域，也為實際應用帶來了重大變革。儘管有這些進步，像程式生成這樣的領域仍然面臨著挑戰\hspace{0pt}\hspace{0pt}。論文重點在於生成兩大類本類型的程序：數學程序和知識圖譜問答（KGQA）程序。對於數學程序，我們的工作提出了一種新穎的恢復數值數據實驗補充（RNDA）方法，該方法自動生成高質量的實例與程序。結果顯示，以增量資料訓練的模型可以達到最先進的性能。同時，在KGQA程序的領域，我們提出了一種反向生成的驗證方法以提高可靠性。可以實驗表明，這種方法也可以總體而言，該研究透過引入新方法，繪製了程式生成的範式轉變，專注於改善數學和KGQA程式。這些發現為未來的研究提供了一個有前景的基礎，目標是充分利用大型語言模型。},
  file = {D:\Paper\應用自動驗證技術提升自然語言程式化問答系統可靠性之綜合研究.pdf}
}

@online{RetrievalAugmentedGenerationKnowledgeIntensive,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  url = {https://www.readthispaper.com/tw/paper/s2:218869575/abstract},
  urldate = {2024-03-13},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  langid = {english},
  organization = {Read This Paper},
  keywords = {RAG,問答系統,已整理,待讀,文獻,重要},
  annotation = {titleTranslation: 知識密集型 NLP 任務的檢索增強生成\\
abstractTranslation:  大型預訓練語言模型已被證明可以在其參數中儲存事實知識，並在下游 NLP 任務上進行微調時實現最先進的結果。然而，它們存取和精確操作知識的能力仍然有限，因此在知識密集型任務上，它們的性能落後於特定於任務的架構。此外，為他們的決策提供依據並更新他們的世界知識仍然是懸而未決的研究問題。具有對顯式非參數記憶體的可微存取機制的預訓練模型可以克服這個問題，但迄今為止僅針對提取性下游任務進行了研究。我們探索了一種用於檢索增強生成（RAG）的通用微調方法——結合了預先訓練的參數和非參數記憶來產生語言的模型。我們引入 RAG 模型，其中參數記憶體是預先訓練的 seq2seq 模型，非參數記憶體是維基百科的密集向量索引，可透過預先訓練的神經檢索器存取。我們比較了兩種 RAG 公式，一種以整個生成序列中相同的檢索段落為條件，另一種可以為每個標記使用不同的段落。我們在廣泛的知識密集型 NLP 任務上微調和評估我們的模型，並在三個開放域 QA 任務上設定最先進的技術，優於參數化 seq2seq 模型和特定於任務的檢索和提取架構。對於語言生成任務，我們發現 RAG 模型比最先進的純參數 seq2seq 基準生成更具體、更多樣化和更真實的語言。},
  file = {D\:\\Paper\\Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\7BJFP3ZT\\abstract.html}
}

@inproceedings{ribeiroAccuracyBehavioralTesting2020,
  title = {Beyond {{Accuracy}}: {{Behavioral Testing}} of {{NLP Models}} with {{CheckList}}},
  shorttitle = {Beyond {{Accuracy}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  date = {2020-07},
  pages = {4902--4912},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.acl-main.442},
  url = {https://aclanthology.org/2020.acl-main.442},
  urldate = {2024-05-04},
  abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
  eventtitle = {{{ACL}} 2020},
  langid = {english},
  keywords = {未整理,重要},
  file = {C:\Users\BlackCat\Zotero\storage\G69UZDHN\Ribeiro 等。 - 2020 - Beyond Accuracy Behavioral Testing of NLP Models .pdf}
}

@article{richarddavidevansNewParadigmVirtual2018,
  title = {A New Paradigm for Virtual Knowledge Sharing in Product Development Based on Emergent Social Software Platforms},
  author = {{Richard David Evans} and {James Xiaoyu Gao} and {Nick Martin} and {Clive Simmonds}},
  date = {2018-11},
  journaltitle = {PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART B-JOURNAL OF ENGINEERING MANUFACTURE},
  shortjournal = {Proc. Inst. Mech. Eng. Part B-J. Eng. Manuf.},
  volume = {232},
  number = {13},
  pages = {2297--2308},
  publisher = {SAGE Publications Ltd},
  location = {London},
  issn = {0954-4054, 2041-2975},
  doi = {10.1177/0954405417699018},
  url = {https://www.webofscience.com/api/gateway?GWVersion=2&SrcAuth=DOISource&SrcApp=WOS&KeyAID=10.1177%2F0954405417699018&DestApp=DOI&SrcAppSID=EUW1ED0B0Eh1M6RufxhQfBCxYjBK2&SrcJTitle=PROCEEDINGS+OF+THE+INSTITUTION+OF+MECHANICAL+ENGINEERS+PART+B-JOURNAL+OF+ENGINEERING+MANUFACTURE&DestDOIRegistrantName=SAGE+Publications},
  urldate = {2023-10-16},
  abstract = {The UK Government considers its Aerospace Industry a remarkable success story, enjoying a global market share of 17\% in 2015. The capture, management and sharing of employee knowledge is seen as vital if the industry is to remain highly innovative and retain its pre-eminent position internationally. Aerospace manufacturers, such as BAE Systems, often have to re-engineer business processes routinely to ensure their survival. Knowledge sharing in the industry is seen as challenging due to the dispersed nature of its operations and multi-tier supply chains. This article, through a 5-year participant-observation study at the World's second largest aerospace and defence organisation, BAE Systems, proposes a new paradigm for virtual knowledge sharing in dispersed aerospace product development based on emergent social software platforms such as Enterprise 2.0 technologies. The developed framework and methodologies are applied to the bespoke BAE Systems' engineering lifecycle process to validate its effectiveness with results indicating that Enterprise 2.0 technologies offer a more openly innovative environment in which employees may share and interact with knowledge more effectively and easily across geographical and functional boundaries, compared with conventional engineering information systems.},
  langid = {english},
  pagetotal = {12},
  keywords = {AEROSPACE,Aerospace manufacturing,COMPANY,DESIGN,dispersed product development,Enterprise 2.0,innovation and knowledge management,knowledge sharing,REQUIREMENTS,TECHNOLOGY,virtual teams,已整理,微讀,知識共享,知識管理,被引用},
  annotation = {14 citations (Crossref) [2024-03-26]\\
Web of Science ID: WOS:000447348300006\\
titleTranslation: 基於新興社群軟體平台的產品開發虛擬知識共享新範式},
  file = {C:\Users\BlackCat\Zotero\storage\BGE77K4L\Evans et al. - 2018 - A new paradigm for virtual knowledge sharing in pr.pdf}
}

@article{richardl.streetProviderInteractionElectronic2014,
  title = {Provider Interaction with the Electronic Health Record: {{The}} Effects on Patient-Centered Communication in Medical Encounters},
  shorttitle = {Provider Interaction with the Electronic Health Record},
  author = {{Richard L. Street} and {Lin Liu} and {Neil J. Farber} and {Yunan Chen} and {Alan Calvitti} and {Danielle Zuest} and {Mark T. Gabuzda} and {Kristin Bell} and {Barbara Gray} and {Steven Rick} and {Shazia Ashfaq} and {Zia Agha}},
  date = {2014-09-01},
  journaltitle = {Patient Education and Counseling},
  shortjournal = {Patient Education and Counseling},
  series = {Communication in {{Healthcare}}: {{Lessons}} from {{Diversity}}},
  volume = {96},
  number = {3},
  pages = {315--319},
  issn = {0738-3991},
  doi = {10.1016/j.pec.2014.05.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0738399114001840},
  urldate = {2022-12-15},
  abstract = {Objective The computer with the electronic health record (EHR) is an additional ‘interactant’ in the medical consultation, as clinicians must simultaneously or in alternation engage patient and computer to provide medical care. Few studies have examined how clinicians’ EHR workflow (e.g., gaze, keyboard activity, and silence) influences the quality of their communication, the patient's involvement in the encounter, and conversational control of the visit. Methods Twenty-three primary care providers (PCPs) from USA Veterans Administration (VA) primary care clinics participated in the study. Up to 6 patients per PCP were recruited. The proportion of time PCPs spent gazing at the computer was captured in real time via video-recording. Mouse click/scrolling activity was captured through Morae, a usability software that logs mouse clicks and scrolling activity. Conversational silence was coded as the proportion of time in the visit when PCP and patient were not talking. After the visit, patients completed patient satisfaction measures. Trained coders independently viewed videos of the interactions and rated the degree to which PCPs were patient-centered (informative, supportive, partnering) and patients were involved in the consultation. Conversational control was measured as the proportion of time the PCP held the floor compared to the patient. Results The final sample included 125 consultations. PCPs who spent more time in the consultation gazing at the computer and whose visits had more conversational silence were rated lower in patient-centeredness. PCPs controlled more of the talk time in the visits that also had longer periods of mutual silence. Conclusions PCPs were rated as having less effective communication when they spent more time looking at the computer and when there was more periods of silence in the consultation. Because PCPs increasingly are using the EHR in their consultations, more research is needed to determine effective ways that they can verbally engage patients while simultaneously managing data in the EHR. Practice implications EHR activity consumes an increasing proportion of clinicians’ time during consultations. To ensure effective communication with their patients, clinicians may benefit from using communication strategies that maintain the flow of conversation when working with the computer, as well as from learning EHR management skills that prevent extended periods of gaze at computer and long periods of silence. Next-generation EHR design must address better usability and clinical workflow integration, including facilitating patient-clinician communication.},
  langid = {english},
  keywords = {Adult,Aged,Aged 80 and over,Ambulatory Care Facilities,Communication,Electronic Health Records,Electronic medical records,Female,Humans,Male,Middle Aged,Patient centered communication,Patient Participation,Patient Satisfaction,Patient-Centered Care,Physician workflow,Physician-Patient Relations,Practice Patterns Physicians',Primary Health Care,Quality of Health Care,Referral and Consultation,United States,United States Department of Veterans Affairs,Videotape Recording},
  annotation = {109 citations (Crossref) [2024-03-26]\\
122 citations (Semantic Scholar/DOI) [2023-01-11]\\
abstractTranslation:  目的 帶有電子健康記錄 (EHR) 的計算機是醫療諮詢中的額外“交互者”，因為臨床醫生必須同時或交替使用患者和計算機來提供醫療護理。很少有研究探討臨床醫生的 EHR 工作流程（例如凝視、鍵盤活動和沈默）如何影響他們的溝通質量、患者對就診的參與以及就診的對話控制。方法 來自美國退伍軍人管理局 (VA) 初級保健診所的 23 名初級保健提供者 (PCP) 參與了本研究。每個 PCP 最多招募 6 名患者。通過視頻記錄實時記錄 PCP 凝視電腦的時間比例。鼠標點擊/滾動活動是通過 Morae 捕獲的，Morae 是一款記錄鼠標點擊和滾動活動的可用性軟件。對話沉默被編碼為就診期間 PCP 和患者不說話的時間比例。訪問結束後，患者完成了患者滿意度測量。訓練有素的編碼員獨立觀看互動視頻，並對 PCP 以患者為中心（提供信息、提供支持、合作）以及患者參與諮詢的程度進行評分。對話控制的衡量標準是 PCP 與患者保持發言的時間比例。結果 最終樣本包括 125 次諮詢。在諮詢中花更多時間盯著電腦以及就診時保持更多沉默的 PCP 在以患者為中心的方面評分較低。 PCP 在訪問中控制了更多的談話時間，也有更長的相互沉默時間。結論 當 PCP 花更多時間看電腦以及諮詢過程中出現更多沉默時，他們的溝通效率會降低。由於 PCP 在諮詢中越來越多地使用 EHR，因此需要進行更多研究來確定他們在管理 EHR 數據的同時與患者進行口頭交流的有效方法。實踐影響 EHR 活動在諮詢期間消耗了臨床醫生越來越多的時間。為了確保與患者的有效溝通，臨床醫生可能會受益於在使用計算機時使用保持對話流暢的溝通策略，以及學習電子病歷管理技能，以防止長時間注視計算機和長時間沉默。下一代 EHR 設計必須解決更好的可用性和臨床工作流程集成問題，包括促進患者與臨床醫生的溝通。\\
titleTranslation: 提供者與電子健康記錄的互動：對醫療遭遇中以患者為中心的溝通的影響}
}

@article{rihanhaiDataLakesSurvey2023,
  title = {Data {{Lakes}}: {{A Survey}} of {{Functions}} and {{Systems}}},
  shorttitle = {Data {{Lakes}}},
  author = {{Rihan Hai} and {Christos Koutras} and {Christoph Quix} and {Matthias Jarke}},
  date = {2023},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--20},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2023.3270101},
  url = {https://ieeexplore.ieee.org/document/10107808},
  urldate = {2023-10-12},
  abstract = {Data lakes are becoming increasingly prevalent for big data management and data analytics. In contrast to traditional ‘schema-on-write’ approaches such as data warehouses, data lakes are repositories storing raw data in its original formats and providing a common access interface. Despite the strong interest raised from both academia and industry, there is a large body of ambiguity regarding the definition, functions and available technologies for data lakes. A complete, coherent picture of data lake challenges and solutions is still missing. This survey reviews the development, architectures, and systems of data lakes. We provide a comprehensive overview of research questions for designing and building data lakes. We classify the existing approaches and systems based on their provided functions for data lakes, which makes this survey a useful technical reference for designing, implementing and deploying data lakes. We hope that the thorough comparison of existing solutions and the discussion of open research challenges in this survey will motivate the future development of data lake research and practice.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  annotation = {7 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\33T5GEEE\Hai 等。 - 2023 - Data Lakes A Survey of Functions and Systems.pdf}
}

@inproceedings{riloffRulebasedQuestionAnswering2000,
  title = {A Rule-Based Question Answering System for Reading Comprehension Tests},
  booktitle = {{{ANLP}}/{{NAACL}} 2000 {{Workshop}} on {{Reading}} Comprehension Tests as Evaluation for Computer-Based Language Understanding Sytems  -},
  author = {Riloff, Ellen and Thelen, Michael},
  date = {2000},
  volume = {6},
  pages = {13--19},
  publisher = {Association for Computational Linguistics},
  location = {Seattle, Washington},
  doi = {10.3115/1117595.1117598},
  url = {http://portal.acm.org/citation.cfm?doid=1117595.1117598},
  urldate = {2024-05-01},
  eventtitle = {{{ANLP}}/{{NAACL}} 2000 {{Workshop}}},
  langid = {english},
  keywords = {問答系統,文獻,未整理},
  file = {C:\Users\BlackCat\Zotero\storage\6WMXJ9EB\Riloff 與 Thelen - 2000 - A rule-based question answering system for reading.pdf}
}

@inproceedings{robbmitchellCyranicContraptionsUsing2011,
  title = {Cyranic Contraptions: Using Personality Surrogates to Explore Ontologically and Socially Dynamic Contexts},
  shorttitle = {Cyranic Contraptions},
  booktitle = {Procedings of the {{Second Conference}} on {{Creativity}} and {{Innovation}} in {{Design}}},
  author = {{Robb Mitchell} and {Alex Gillespie} and {Brian O'Neill}},
  year = {10 月 19, 2011},
  series = {{{DESIRE}} '11},
  pages = {199--210},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2079216.2079246},
  url = {https://dl.acm.org/doi/10.1145/2079216.2079246},
  urldate = {2023-09-16},
  abstract = {Understanding contexts is an important challenge that is made harder for designers by the increasing speed at which contexts change. To assist designers, three types of contextual dynamism are distinguished: physical, ontological and social. To inform understanding ontological dynamism and social dynamism, "social contraptions" - a form of socially interactive design experimentation is proposed. This paper focuses on cryanic social contraptions in which unseen users interact through a human surrogate that they guide via radio transmissions. Observations from initial trials are reported along with a discussion of themes arising for design and an appraisal of this approach's potential as a design tool.},
  isbn = {978-1-4503-0754-3},
  langid = {english},
  keywords = {/unread,contextual design,design experimentation,design research,interaction design,interpersonal,social interaction,回收},
  annotation = {10 citations (Crossref) [2024-03-26]\\
abstractTranslation:  理解環境是一項重要的挑戰，隨著環境變化速度的加快，對設計師來說變得更加困難。為了幫助設計師，區分了三種類型的情境活力：物理的、本體論的和社會的。為了幫助理解本體論活力和社會活力，提出了「社會裝置」——一種社會互動設計實驗的形式。本文重點關注瘋狂的社交裝置，其中看不見的用戶透過他們透過無線電傳輸引導的人類代理人進行互動。報告了初步試驗的觀察結果，並對設計中出現的主題進行了討論，並對這種方法作為設計工具的潛力進行了評估。\\
titleTranslation: Cyranic 裝置：使用人格代理人來探索本體論和社會動態背景},
  file = {C:\Users\BlackCat\Zotero\storage\23SFFAI7\Mitchell 等。 - 2011 - Cyranic contraptions using personality surrogates.pdf}
}

@inproceedings{robertfeldtGenericSkillsSoftware2009,
  title = {Generic {{Skills}} in {{Software Engineering Master Thesis Projects}}: {{Towards Rubric-Based Evaluation}}},
  shorttitle = {Generic {{Skills}} in {{Software Engineering Master Thesis Projects}}},
  booktitle = {2009 22nd {{Conference}} on {{Software Engineering Education}} and {{Training}}},
  author = {{Robert Feldt} and {Martin Höst} and {Frank Lüders}},
  date = {2009-02},
  pages = {12--15},
  issn = {2377-570X},
  doi = {10.1109/CSEET.2009.54},
  url = {https://ieeexplore.ieee.org/document/4812668},
  urldate = {2023-10-13},
  abstract = {There has been much recent interest in how to help students in higher education develop their generic skills, especially since this is a focus of the Bologna process that aims to standardize European higher education. However, even though the Master thesis is the final and often crucial part of a graduate degree and requires many generic skills very little research has directly focused on them. In particular, there is a lack of such knowledge for engineering education programs. In this paper we present results from a survey where we asked 23 students from three different Swedish universities about which generic skills are needed and developed in a Master thesis project in Software Engineering. One outcome of our analysis is that there is a lack of understanding on how to define, and thus examine, generic skills in software engineering thesis projects.},
  eventtitle = {2009 22nd {{Conference}} on {{Software Engineering Education}} and {{Training}}},
  annotation = {10 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\P42BKEW3\Feldt et al. - 2009 - Generic Skills in Software Engineering Master Thes.pdf}
}

@inproceedings{rongxuangaoKnowledgeQuestionAnsweringSystem2020,
  title = {Knowledge {{Question-Answering System Based}} on {{Knowledge Graph}} of {{Traditional Chinese Medicine}}},
  booktitle = {2020 {{IEEE}} 9th {{Joint International Information Technology}} and {{Artificial Intelligence Conference}} ({{ITAIC}})},
  author = {{Rongxuan Gao} and {Chen Li}},
  date = {2020-02},
  volume = {9},
  pages = {27--31},
  issn = {2693-2865},
  doi = {10.1109/ITAIC49862.2020.9339040},
  url = {https://ieeexplore.ieee.org/document/9339040},
  urldate = {2023-09-25},
  abstract = {With the change of health concept and medical model, traditional Chinese medicine (TCM) has shown more and more unique advantages. Users expect to query TCM knowledge efficiently through online. At the same time, intelligent question-answering system based on knowledge graphs is gradually becoming a new trend of natural interaction between humans and machines. It uses knowledge graph as the source of answers, which can more accurately understand the user's questions described in natural language, and return more precise answers according to the real intention of the user. This paper extracts the triples of TCM knowledge from the instructions of pharmacy database and the special database of TCM and then stores them in the Neo4j graph database. Based on this knowledge graph, the question-answering system of TCM is realized. For the natural sentences entered by users, the question-answering system first identifies the entities in the sentences, then combines TF-IDF and word vector to generate sentence vectors, matches the most similar question-answering template, searches the answers in the knowledge graph according to the semantics of the templates and the entities in the questions, and finally answers the questions according to the returned data. The system can accurately answer the main components and pharmaceutical enterprises of TCM, which diseases and symptoms can be treated by TCM and so on.},
  eventtitle = {2020 {{IEEE}} 9th {{Joint International Information Technology}} and {{Artificial Intelligence Conference}} ({{ITAIC}})},
  langid = {english},
  keywords = {中醫,問答系統,已整理,文獻,知識圖譜,重要},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於中醫知識圖譜的知識問答系統\\
abstractTranslation:  隨著健康觀念和醫療模式的轉變，中醫藥越來越展現出獨特的優勢。使用者希望透過線上方式高效查詢中醫知識。同時，基於知識圖譜的智慧問答系統正逐漸成為人機自然互動的新趨勢。它使用知識圖譜作為答案的來源，可以更準確地理解使用者用自然語言描述的問題，並根據使用者的真實意圖返回更精準的答案。本文從藥房資料庫和中醫專用資料庫的指令中提取中醫知識三元組，然後將其儲存到Neo4j圖資料庫中。基於這個知識圖譜，實現了中醫問答系統。對於使用者輸入的自然句子，問答系統首先識別句子中的實體，然後結合TF-IDF和詞向量生成句子向量，匹配最相似的問答模板，在知識圖譜中搜索答案根據模板的語義和問題中的實體，最終根據返回的數據回答問題。系統可以準確解答中藥的主要成分、製藥公司、中藥可以治療哪些疾病和症狀等。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\A9N36HSX\\Knowledge Question-Answering System Based on Knowledge Graph of Traditional Chinese Medicine.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\HPCQDL9L\\9339040.html}
}

@article{ruffInformationOverloadCauses2002,
  title = {Information Overload: {{Causes}}, Symptoms and Solutions},
  shorttitle = {Information Overload},
  author = {Ruff, Joseph},
  date = {2002},
  journaltitle = {Harvard Graduate School of Education},
  pages = {1--13},
  publisher = {Citeseer},
  url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=4350aca4bb76998cce0b6aedfbd44e535ef78e6c},
  urldate = {2024-04-30},
  langid = {english},
  keywords = {⛔ No DOI found,已整理,資訊超載},
  annotation = {titleTranslation: 資訊過載：原因、症狀和解決方案},
  file = {C:\Users\BlackCat\Zotero\storage\RS3Y26PY\Ruff - 2002 - Information overload Causes, symptoms and solutio.pdf}
}

@article{ruichaoxueTCMIDTraditionalChinese2013,
  title = {{{TCMID}}: Traditional {{Chinese}} Medicine Integrative Database for Herb Molecular Mechanism Analysis},
  shorttitle = {{{TCMID}}},
  author = {{Ruichao Xue} and {Zhao Fang} and {Meixia Zhang} and {Zhenghui Yi} and {Chengping Wen} and {Tieliu Shi}},
  date = {2013-01},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Res},
  volume = {41},
  eprint = {23203875},
  eprinttype = {pmid},
  pages = {D1089-D1095},
  issn = {0305-1048},
  doi = {10.1093/nar/gks1100},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3531123/},
  urldate = {2023-09-15},
  abstract = {As an alternative to modern western medicine, Traditional Chinese Medicine (TCM) is receiving increasingly attention worldwide. Great efforts have been paid to TCM’s modernization, which tries to bridge the gap between TCM and modern western medicine. As TCM and modern western medicine share a common aspect at molecular level that the compound(s) perturb human’s dysfunction network and restore human normal physiological condition, the relationship between compounds (in herb, refer to ingredients) and their targets (proteins) should be the key factor to connect TCM and modern medicine. Accordingly, we construct this Traditional Chinese Medicine Integrated Database (TCMID, http://www.megabionet.org/tcmid/), which records TCM-related information collected from different resources and through text-mining method. To enlarge the scope of the TCMID, the data have been linked to common drug and disease databases, including Drugbank, OMIM and PubChem. Currently, our TCMID contains ∼47 000 prescriptions, 8159 herbs, 25 210 compounds, 6828 drugs, 3791 diseases and 17 521 related targets, which is the largest data set for related field. Our web-based software displays a network for integrative relationships between herbs and their treated diseases, the active ingredients and their targets, which will facilitate the study of combination therapy and understanding of the underlying mechanisms for TCM at molecular level.},
  issue = {Database issue},
  langid = {english},
  pmcid = {PMC3531123},
  keywords = {中醫,已整理,數據挖掘,資料集},
  annotation = {370 citations (Crossref) [2024-03-26]\\
abstractTranslation:  作為現代西方醫學的替代療法，中醫藥越來越受到世界性的關注。大力推動中醫藥現代化，努力彌合中醫藥與現代西醫的差距。由於中醫和現代西醫在分子層面上有一個共同點，即化合物擾亂人體功能障礙網絡並恢復人體正常生理狀態，因此化合物（在草藥中指成分）與其靶標（蛋白質）之間的關係應該是連接中醫與現代醫學的關鍵因素。據此，我們建立了這個中醫藥綜合資料庫（TCMID，http://www.megabionet.org/tcmid/），該資料庫記錄了從不同資源收集並透過文字挖掘方法收集的中醫藥相關資訊。為了擴大 TCMID 的範圍，數據已連結到常見藥物和疾病資料庫，包括 Drugbank、OMIM 和 PubChem。目前，我們的TCMID包含約47 000個處方、8159種草藥、25 210種化合物、6828種藥物、3791種疾病和17 521個相關靶點，是相關領域最大的數據集。我們的網路為基礎的軟體顯示了草藥與其治療的疾病、活性成分及其標靶之間的綜合關係網絡，這將有助於聯合療法的研究以及在分子層面上理解中醫的潛在機制。\\
titleTranslation: TCMID：中藥分子機制分析綜合資料庫},
  note = {結合數據挖掘集許多其他的資料集合併成這個資料集。然而網址已無法存取。但其中提到的多個資料集都值得參考。},
  file = {C:\Users\BlackCat\Zotero\storage\KWUZFQAR\Xue 等。 - 2013 - TCMID traditional Chinese medicine integrative da.pdf}
}

@article{s.ibrihichReviewRecentResearch2022,
  title = {A {{Review}} on Recent Research in Information Retrieval},
  author = {{S. Ibrihich} and {A. Oussous} and {O. Ibrihich} and {M. Esghir}},
  date = {2022-01-01},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  series = {The 13th {{International Conference}} on {{Ambient Systems}}, {{Networks}} and {{Technologies}} ({{ANT}}) / {{The}} 5th {{International Conference}} on {{Emerging Data}} and {{Industry}} 4.0 ({{EDI40}})},
  volume = {201},
  pages = {777--782},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2022.03.106},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050922005191},
  urldate = {2023-10-20},
  abstract = {In this paper, we present a survey of modeling and simulation approaches to describe information retrieval basics. We investigate its methods, its challenges, its models, its components and its applications. Our contribution is twofold: on the one hand, reviewing the literature on discovery some search techniques that help to get pertinent results and reach an effective search, and on the other hand, discussing the different research perspectives for study and compare more techniques used in information retrieval. This paper will also shedding the light on some of the famous AI applications in the legal field.},
  langid = {english},
  keywords = {Data Mining,Information Retrieval,Intelligent Search,IR models,Natural Language Processing,Survey,已整理,重要},
  annotation = {8 citations (Crossref) [2024-03-26]\\
titleTranslation: 資訊檢索最新研究綜述\\
abstractTranslation:  在本文中，我們對描述資訊檢索基礎知識的建模和模擬方法進行了調查。我們研究它的方法、挑戰、模型、組件和應用。我們的貢獻是雙重的：一方面，回顧有關發現一些有助於獲得相關結果並實現有效搜索的搜索技術的文獻，另一方面，討論不同的研究視角以進行研究並比較信息中使用的更多技術恢復。本文也將介紹法律領域一些著名的人工智慧應用。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\K8SLI7GJ\\Ibrihich et al. - 2022 - A Review on recent research in information retriev.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\K5SDMMY4\\S1877050922005191.html}
}

@inproceedings{s.m.obrienEnduserKnowledgeManipulation1992,
  title = {End-User Knowledge Manipulation Systems: The Speech Knowledge Interface},
  shorttitle = {End-User Knowledge Manipulation Systems},
  booktitle = {Proceedings of the 1992 {{ACM}} Annual Conference on {{Communications}}},
  author = {{S. M. O'Brien} and {L. Candy} and {E. A. Edmonds} and {T. J. Foster} and {E. McDaid}},
  year = {4 月 1, 1992},
  series = {{{CSC}} '92},
  pages = {359--366},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/131214.131260},
  url = {https://dl.acm.org/doi/10.1145/131214.131260},
  urldate = {2023-08-23},
  abstract = {最終用戶知識操縱系統（EUKMS）使得非編程人員能夠直接操縱知識庫。圖形界面提供的功能包括與源數據的交互、知識編輯以及將規則自動轉換為可操作的代碼，從而促進複雜領域的知識獲取和細化。提出了對這種系統的研究。},
  isbn = {978-0-89791-472-7},
  langid = {english},
  keywords = {知識},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 最終用戶知識操縱系統：語音知識界面\\
abstractTranslation:  最終用戶知識整理系統(EUKMS)使得非編程人員能夠直接整理知識庫。圖形界面提供的功能包括與源數據的交換、知識編輯以及將規則自動轉換為可操作的代碼，從而促進複雜領域的知識獲取和細化。提出了對這種系統的研究。},
  note = {太舊，也沒什麼引用，丟冰箱},
  file = {C:\Users\BlackCat\Zotero\storage\7DMHL6IY\O'Brien 等。 - 1992 - End-user knowledge manipulation systems the speec.pdf}
}

@inproceedings{saierUnarXive2022All2023,
  title = {{{unarXive}} 2022: {{All arXiv Publications Pre-Processed}} for {{NLP}}, {{Including Structured Full-Text}} and {{Citation Network}}},
  shorttitle = {{{unarXive}} 2022},
  booktitle = {2023 {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Saier, Tarek and Krause, Johan and Färber, Michael},
  date = {2023-06},
  pages = {66--70},
  issn = {2575-8152},
  doi = {10.1109/JCDL57899.2023.00020},
  url = {https://ieeexplore.ieee.org/document/10266058},
  urldate = {2023-11-30},
  abstract = {Large-scale data sets on scholarly publications are the basis for a variety of bibliometric analyses and natural language processing (NLP) applications. Especially data sets derived from publication's full-text have recently gained attention. While several such data sets already exist, we see key shortcomings in terms of their domain and time coverage, citation network completeness, and representation of full-text content. To address these points, we propose a new version of the data set unarXive. We base our data processing pipeline and output format on two existing data sets, and improve on each of them. Our resulting data set comprises 1.9 \textbackslash mathrmM publications spanning multiple disciplines and 32 years. It furthermore has a more complete citation network than its predecessors and retains a richer representation of document structure as well as non-textual publication content such as mathematical notation. In addition to the data set, we provide ready-to-use training/test data for citation recommendation and IMRaD classification. All data and source code is publicly available at https://github.com/IlIDepence/unarXive.},
  eventtitle = {2023 {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  langid = {english},
  keywords = {待整理},
  annotation = {4 citations (Crossref) [2024-03-26]\\
titleTranslation: unarXive 2022：所有 arXiv 出版物均經過 NLP 預處理，包括結構化全文和引文網絡},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\38FE5WL7\\Saier et al. - 2023 - unarXive 2022 All arXiv Publications Pre-Processe.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\RUK6SLME\\10266058.html}
}

@inproceedings{saisharathjapaQuestionAnsweringKnowledge2022,
  title = {Question {{Answering}} over {{Knowledge Base}} with {{Variational Auto-Encoder}}},
  booktitle = {2022 {{IEEE Eighth International Conference}} on {{Multimedia Big Data}} ({{BigMM}})},
  author = {{Sai Sharath Japa} and {Sarah Green}},
  date = {2022-02},
  pages = {29--36},
  doi = {10.1109/BigMM55396.2022.00012},
  url = {https://ieeexplore.ieee.org/document/9999076},
  urldate = {2023-11-23},
  abstract = {Knowledge Base is a semantic data repos-itory made available over the web. Knowledge bases are multi-relational graphs consisting of entities and relations representing facts about the world. These facts follow a controlled vocabulary that drives the knowledge base, often called ontology. Entities are the graph nodes, while relations are the edges connecting the nodes. While knowledge base attention prolifer-ates, extracting information from these resources is challenging. One of the promising approaches is Question Answering systems. The Question Answering over Knowledge Base(KB-QA) system aims to provide a natural language interface to pose a query to the KB. QA systems use embeddings to map the posed Question to the facts stored in the knowledge bases. QA systems can be effectively improved using large transformer models. These transformer models are computationally expensive, which prevents their use in real-world applications. To solve this, we started our study with the use-case of Knowledge Base Question Answering systems. In KB-QA systems, representing the entities and relationships play a pivotal role in extracting the answer to the posed query. Traditional KB embedding techniques represent entities/relations as vectors, mapping them in different semantic spaces and ignoring the subtle cor-relation. A pre-training language model for encoding the KB facts will preserve the semantic and contextual correlation. While learning linguistic knowledge, these models also store relational knowledge in the training data. These Language Models are often trained on a massive corpus addressing the Out-Of- Vocabulary problem. In this paper, we incorporate a co-embedding model for knowledge base embedding, which learns low-dimensional representations of entities and relations in the same semantic space. We propose a Variational Auto-Encoder, an efficient transformer architecture that represents knowledge base representations as Gaussian distributions to address the issue of neglecting uncertainty. In addition, our method has high quality and interpretability advantages compared with previous methods. Our experimental results show the effectiveness and the superiority of the VAE approach for question-answering systems on knowledge bases over other well-known pre-trained embedding methods.},
  eventtitle = {2022 {{IEEE Eighth International Conference}} on {{Multimedia Big Data}} ({{BigMM}})},
  langid = {english},
  keywords = {BERT,問答系統,已整理,機器學習,知識圖譜},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用變分自動編碼器對知識庫進行問答},
  note = {本研究使用VAE將知識圖譜遷入到BERT中，以實現自然語言問答。
\par
方法和本研究較無相關，但在問題的定義上可以參考。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\TR366BFE\\Japa and Green - 2022 - Question Answering over Knowledge Base with Variat.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\GRDENRKI\\9999076.html}
}

@article{sajidaliExplainableArtificialIntelligence2023,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{What}} We Know and What Is Left to Attain {{Trustworthy Artificial Intelligence}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {{Sajid Ali} and {Tamer Abuhmed} and {Shaker El-Sappagh} and {Khan Muhammad} and {Jose M. Alonso-Moral} and {Roberto Confalonieri} and {Riccardo Guidotti} and {Javier Del Ser} and {Natalia Díaz-Rodríguez} and {Francisco Herrera}},
  date = {2023-11-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {99},
  pages = {101805},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2023.101805},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253523001148},
  urldate = {2023-10-25},
  abstract = {Artificial intelligence (AI) is currently being utilized in a wide range of sophisticated applications, but the outcomes of many AI models are challenging to comprehend and trust due to their black-box nature. Usually, it is essential to understand the reasoning behind an AI model’s decision-making. Thus, the need for eXplainable AI (XAI) methods for improving trust in AI models has arisen. XAI has become a popular research subject within the AI field in recent years. Existing survey papers have tackled the concepts of XAI, its general terms, and post-hoc explainability methods but there have not been any reviews that have looked at the assessment methods, available tools, XAI datasets, and other related aspects. Therefore, in this comprehensive study, we provide readers with an overview of the current research and trends in this rapidly emerging area with a case study example. The study starts by explaining the background of XAI, common definitions, and summarizing recently proposed techniques in XAI for supervised machine learning. The review divides XAI techniques into four axes using a hierarchical categorization system: (i) data explainability, (ii) model explainability, (iii) post-hoc explainability, and (iv) assessment of explanations. We also introduce available evaluation metrics as well as open-source packages and datasets with future research directions. Then, the significance of explainability in terms of legal demands, user viewpoints, and application orientation is outlined, termed as XAI concerns. This paper advocates for tailoring explanation content to specific user types. An examination of XAI techniques and evaluation was conducted by looking at 410 critical articles, published between January 2016 and October 2022, in reputed journals and using a wide range of research databases as a source of information. The article is aimed at XAI researchers who are interested in making their AI models more trustworthy, as well as towards researchers from other disciplines who are looking for effective XAI methods to complete tasks with confidence while communicating meaning from data.},
  langid = {english},
  keywords = {AI principles,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Interpretable machine learning,Post-hoc explainability,Trustworthy AI,XAI assessment,可解釋性,已整理},
  annotation = {84 citations (Crossref) [2024-03-26]\\
titleTranslation: 可解釋的人工智慧（XAI）：我們所知道的以及要實現可信賴的人工智慧還需要做什麼\\
abstractTranslation:  人工智慧 (AI) 目前被廣泛應用於各種複雜的應用中，但許多人工智慧模型的結果由於其黑盒性質而難以理解和信任。通常，了解人工智慧模型決策背後的推理至關重要。因此，需要可解釋的人工智慧（XAI）方法來提高對人工智慧模型的信任。近年來，XAI已成為人工智慧領域的熱門研究主題。現有的調查論文已經討論了 XAI 的概念、其一般術語和事後可解釋性方法，但還沒有任何評論關注評估方法、可用工具、XAI 數據集和其他相關方面。因此，在這項綜合研究中，我們透過案例研究範例為讀者提供了這個快速新興領域的當前研究和趨勢的概述。研究首先解釋 XAI 的背景、常見定義，並總結最近提出的監督機器學習的 XAI 技術。本綜述使用分層分類系統將 XAI 技術分為四個軸：（i）資料可解釋性，（ii）模型可解釋性，（iii）事後可解釋性，以及（iv）解釋評估。我們還介紹了可用的評估指標以及具有未來研究方向的開源套件和資料集。然後，概述了可解釋性在法律要求、使用者觀點和應用方向方面的重要性，稱為 XAI 關注點。本文主張針對特定使用者類型客製化解釋內容。透過查看 2016 年 1 月至 2022 年 10 月期間在知名期刊上發表的 410 篇批評文章，並使用廣泛的研究資料庫作為資訊來源，對 XAI 技術和評估進行了檢查。本文針對的是那些有興趣讓自己的 AI 模型更值得信賴的 XAI 研究人員，以及來自其他學科的研究人員，他們正在尋找有效的 XAI 方法來自信地完成任務，同時傳達數據的含義。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\6R2TPUJP\\Sajid Ali 等。 - 2023 - Explainable Artificial Intelligence (XAI) What we.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\V54WUF6C\\S1566253523001148.html}
}

@online{salatinoOntologyExtractionUsage2020,
  title = {Ontology {{Extraction}} and {{Usage}} in the {{Scholarly Knowledge Domain}}},
  author = {Salatino, Angelo A. and Osborne, Francesco and Motta, Enrico},
  date = {2020-08-04},
  eprint = {2003.12611},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.12611},
  url = {http://arxiv.org/abs/2003.12611},
  urldate = {2024-02-24},
  abstract = {Ontologies of research areas have been proven to be useful in many application for analysing and making sense of scholarly data. In this chapter, we present the Computer Science Ontology (CSO), which is the largest ontology of research areas in the field of Computer Science, and discuss a number of applications that build on CSO, to support high-level tasks, such as topic classification, metadata extraction, and recommendation of books.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Digital Libraries,Computer Science - Information Retrieval,待整理,知識本體},
  annotation = {titleTranslation: 學術知識領域本體的提取與使用},
  note = {介紹電腦領域學術知識本體CSO},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\4Z73R4WG\\Salatino 等。 - 2020 - Ontology Extraction and Usage in the Scholarly Kno.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\JA8SYSN5\\2003.html}
}

@inproceedings{salvatoreflaviopileggiOntologicalModellingSocial2023,
  title = {Ontological {{Modelling}} and~{{Social Networks}}: {{From Expert Validation}} to~{{Consolidated Domains}}},
  shorttitle = {Ontological {{Modelling}} and~{{Social Networks}}},
  booktitle = {Computational {{Science}} – {{ICCS}} 2023: 23rd {{International Conference}}, {{Prague}}, {{Czech Republic}}, {{July}} 3–5, 2023, {{Proceedings}}, {{Part V}}},
  author = {{Salvatore Flavio Pileggi}},
  year = {7 月 3, 2023},
  pages = {672--687},
  publisher = {Springer-Verlag},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-031-36030-5_53},
  url = {https://doi.org/10.1007/978-3-031-36030-5_53},
  urldate = {2023-09-26},
  abstract = {Data from Social Networks is a valuable asset within both a scientific and a business world. In the context of this work, ontological modelling from Social Networks is understood as a knowledge building process to generate a shared domain model. Such a technique relies on a balanced co-existence of human intuition/creativity and technological support, referred to as Hybrid Intelligence. Additionally, it assumes collaborative modelling and collective/social intelligence. The method implies a certain degree of uncertainty that is, in principle, inversely proportional to the achieved consensus. There are two clear different convergence points between the proposed process and collective/social intelligence: (i) at a data level, because of the nature of the input which is generated by different individuals, communities, stakeholders and actors; and (ii) at a modelling level, where human and automatically generated inputs, design decisions and validations are expected to involve several contributors, experts, modellers or analysts. Although looking holistically at the modelling process, this paper concisely focuses mostly on the ontological structure and the associated uncertainty, while resulting systems and studies are object of future work.},
  isbn = {978-3-031-36029-9},
  keywords = {Collaborative Modelling,Hybrid Intelligence,Knowledge Engineering,Ontological Modelling,Ontology,Social Networks,Uncertainty,未整理},
  annotation = {0 citations (Crossref) [2024-03-26]}
}

@article{samuelCollaborativeSemanticbasedProvenance2022,
  title = {A Collaborative Semantic-Based Provenance Management Platform for Reproducibility},
  author = {Samuel, Sheeba and König-Ries, Birgitta},
  date = {2022-03-10},
  journaltitle = {PeerJ Computer Science},
  shortjournal = {PeerJ Comput Sci},
  volume = {8},
  eprint = {35494870},
  eprinttype = {pmid},
  pages = {e921},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.921},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9044346/},
  urldate = {2024-01-17},
  abstract = {Scientific data management plays a key role in the reproducibility of scientific results. To reproduce results, not only the results but also the data and steps of scientific experiments must be made findable, accessible, interoperable, and reusable. Tracking, managing, describing, and visualizing provenance helps in the understandability, reproducibility, and reuse of experiments for the scientific community. Current systems lack a link between the data, steps, and results from the computational and non-computational processes of an experiment. Such a link, however, is vital for the reproducibility of results. We present a novel solution for the end-to-end provenance management of scientific experiments. We provide a framework, CAESAR (CollAborative Environment for Scientific Analysis with Reproducibility), which allows scientists to capture, manage, query and visualize the complete path of a scientific experiment consisting of computational and non-computational data and steps in an interoperable way. CAESAR integrates the REPRODUCE-ME provenance model, extended from existing semantic web standards, to represent the whole picture of an experiment describing the path it took from its design to its result. ProvBook, an extension for Jupyter Notebooks, is developed and integrated into CAESAR to support computational reproducibility. We have applied and evaluated our contributions to a set of scientific experiments in microscopy research projects.},
  langid = {english},
  pmcid = {PMC9044346},
  keywords = {可再現性,待讀,未整理,重要},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 一個基於語意的協作來源管理平台，可實現可重複性\\
abstractTranslation:  科學數據管理對於科學結果的可重複性起著關鍵作用。為了重現結果，科學實驗的結果和數據和步驟都必須可找到、可存取、可互通和可重複使用。追蹤、管理、描述和視覺化出處有助於科學界提高實驗的可理解性、可重複性和重用性。目前的系統缺乏實驗的計算和非計算過程的數據、步驟和結果之間的連結。然而，這種聯繫對於結果的可重複性至關重要。我們提出了一種用於科學實驗端到端來源管理的新穎解決方案。我們提供了一個框架，CAESAR（具有重現性的科學分析協作環境），它允許科學家以可互通的方式捕獲、管理、查詢和可視化由計算和非計算數據和步驟組成的科學實驗的完整路徑。 CAESAR 整合了從現有語意網路標準擴展而來的 REPRODUCE-ME 來源模型，以表示實驗的全貌，描述從設計到結果的路徑。 ProvBook 是 Jupyter Notebooks 的擴展，已開發並整合到 CAESAR 中以支援計算再現性。我們已經應用並評估了我們對顯微鏡研究項目中的一系列科學實驗的貢獻。},
  file = {C:\Users\BlackCat\Zotero\storage\ABSN7T4D\Samuel and König-Ries - 2022 - A collaborative semantic-based provenance manageme.pdf}
}

@article{samuelgonzalez-lopezModelIdentifyingSteps2019,
  title = {A {{Model}} for {{Identifying Steps}} in {{Undergraduate Thesis Methodology}}},
  author = {{Samuel González-López} and {Aurelio López-López} and {Steven Bethard} and {Jesús Miguel García-Gorrostieta}},
  date = {2019-12-31},
  journaltitle = {Research in Computing Science},
  shortjournal = {RCS},
  volume = {148},
  number = {5},
  pages = {17--24},
  issn = {1870-4069},
  doi = {10.13053/rcs-148-5-2},
  url = {http://rcs.cic.ipn.mx/2019_148_5/A%20Model%20for%20Identifying%20Steps%20in%20Undergraduate%20Thesis%20Methodology.pdf},
  urldate = {2023-10-23},
  abstract = {. Knowledge generation is an important asset of great economic powers, and knowledge societies are a fundamental part in the development of countries. Mexico is a country that is in the process of development and improvement of its education system, according to the Educational Reform promoted since 2012 by the Federal Gov-ernment. We identified an area of opportunity at the undergraduate level to help improve the writing of students, specifically in draft theses and research proposals. This work focuses its efforts on analyzing with natural language processing techniques the ”Methodology” section, an important element for the development of a thesis, that helps the reader to understand if the techniques and data used are appropriate in an investigation. This paper proposes a Model to identify a series of steps in such a section. In addition, preliminary results of a basic exploration of a collected corpus are presented, pre-processing the text to generate a representation according to Language Models. The corpus contains documents of graduate and undergraduate levels in the computer science and information technologies domain. The preliminary results showed that the information extracted from the corpus serves to adequately differentiate the methodologies of both levels.},
  keywords = {研究步驟,研究流程},
  annotation = {0 citations (Crossref) [2024-03-26]},
  note = {[TLDR] A Model is proposed to identify a series of steps in the ”Methodology” section, an important element for the development of a thesis, that helps the reader to understand if the techniques and data used are appropriate in an investigation.},
  file = {C:\Users\BlackCat\Zotero\storage\RPXDS6MY\González-López et al. - 2019 - A Model for Identifying Steps in Undergraduate The.pdf}
}

@article{samuelUnderstandingExperimentsResearch2021,
  title = {Understanding Experiments and Research Practices for Reproducibility: An Exploratory Study},
  shorttitle = {Understanding Experiments and Research Practices for Reproducibility},
  author = {Samuel, Sheeba and König-Ries, Birgitta},
  date = {2021-04-21},
  journaltitle = {PeerJ},
  shortjournal = {PeerJ},
  volume = {9},
  pages = {e11140},
  publisher = {PeerJ Inc.},
  issn = {2167-8359},
  doi = {10.7717/peerj.11140},
  url = {https://peerj.com/articles/11140},
  urldate = {2024-03-22},
  abstract = {Scientific experiments and research practices vary across disciplines. The research practices followed by scientists in each domain play an essential role in the understandability and reproducibility of results. The “Reproducibility Crisis”, where researchers find difficulty in reproducing published results, is currently faced by several disciplines. To understand the underlying problem in the context of the reproducibility crisis, it is important to first know the different research practices followed in their domain and the factors that hinder reproducibility. We performed an exploratory study by conducting a survey addressed to researchers representing a range of disciplines to understand scientific experiments and research practices for reproducibility. The survey findings identify a reproducibility crisis and a strong need for sharing data, code, methods, steps, and negative and positive results. Insufficient metadata, lack of publicly available data, and incomplete information in study methods are considered to be the main reasons for poor reproducibility. The survey results also address a wide number of research questions on the reproducibility of scientific results. Based on the results of our explorative study and supported by the existing published literature, we offer general recommendations that could help the scientific community to understand, reproduce, and reuse experimental data and results in the research data lifecycle.},
  langid = {english},
  keywords = {未整理},
  annotation = {15 citations (Crossref) [2024-03-26]\\
titleTranslation: 了解實驗和研究實踐的可重複性：一項探索性研究},
  file = {C:\Users\BlackCat\Zotero\storage\4USH3HDG\Samuel and König-Ries - 2021 - Understanding experiments and research practices f.pdf}
}

@article{sandrageislerKnowledgeDrivenDataEcosystems2021,
  title = {Knowledge-{{Driven Data Ecosystems Toward Data Transparency}}},
  author = {{Sandra Geisler} and {Maria-Esther Vidal} and {Cinzia Cappiello} and {Bernadette Farias Lóscio} and {Avigdor Gal} and {Matthias Jarke} and {Maurizio Lenzerini} and {Paolo Missier} and {Boris Otto} and {Elda Paja} and {Barbara Pernici} and {Jakob Rehof}},
  year = {12 月 23, 2021},
  journaltitle = {Journal of Data and Information Quality},
  shortjournal = {J. Data and Information Quality},
  volume = {14},
  number = {1},
  pages = {3:1--3:12},
  issn = {1936-1955},
  doi = {10.1145/3467022},
  url = {https://dl.acm.org/doi/10.1145/3467022},
  urldate = {2023-08-24},
  abstract = {A data ecosystem (DE) offers a keystone-player or alliance-driven infrastructure that enables the interaction of different stakeholders and the resolution of interoperability issues among shared data. However, despite years of research in data governance and management, trustability is still affected by the absence of transparent and traceable data-driven pipelines. In this work, we focus on requirements and challenges that DEs face when ensuring data transparency. Requirements are derived from the data and organizational management, as well as from broader legal and ethical considerations. We propose a novel knowledge-driven DE architecture, providing the pillars for satisfying the analyzed requirements. We illustrate the potential of our proposal in a real-world scenario. Last, we discuss and rate the potential of the proposed architecture in the fulfillmentof these requirements.},
  langid = {english},
  keywords = {data ecosystems,data quality,Data transparency,trustability},
  annotation = {11 citations (Crossref) [2024-03-26]\\
titleTranslation: 知識驅動的數據生態系統實現數據透明度\\
abstractTranslation:  數據生態系統（DE）提供了一個關鍵參與者或聯盟驅動的基礎設施，使不同利益相關者能夠進行交互並解決共享數據之間的互操作性問題。然而，儘管在數據治理和管理方面進行了多年的研究，但由於缺乏透明和可追溯的數據驅動管道，可信度仍然受到影響。在這項工作中，我們重點關注數據中心在確保數據透明度時面臨的要求和挑戰。要求源自數據和組織管理，以及更廣泛的法律和道德考慮。我們提出了一種新穎的知識驅動的 DE 架構，為滿足分析的需求提供了支柱。我們展示了我們的提案在現實場景中的潛力。最後，我們討論並評估所提出的架構在滿足這些要求方面的潛力。},
  note = {在這項工作中，我們重點關注數據中心在確保數據透明度時面臨的要求和挑戰。要求源自數據和組織管理，以及更廣泛的法律和道德考慮。我們提出了一種新穎的知識驅動的 DE 架構，為滿足分析的需求提供了支柱。我們展示了我們的提案在現實場景中的潛力。},
  file = {C:\Users\BlackCat\Zotero\storage\GAXL7TVV\Geisler 等。 - 2021 - Knowledge-Driven Data Ecosystems Toward Data Trans.pdf}
}

@inproceedings{santhanamColBERTv2EffectiveEfficient2022,
  title = {{{ColBERTv2}}: {{Effective}} and {{Efficient Retrieval}} via {{Lightweight Late Interaction}}},
  shorttitle = {{{ColBERTv2}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and Potts, Christopher and Zaharia, Matei},
  editor = {Carpuat, Marine and family=Marneffe, given=Marie-Catherine, prefix=de, useprefix=true and Meza Ruiz, Ivan Vladimir},
  date = {2022-07},
  pages = {3715--3734},
  publisher = {Association for Computational Linguistics},
  location = {Seattle, United States},
  doi = {10.18653/v1/2022.naacl-main.272},
  url = {https://aclanthology.org/2022.naacl-main.272},
  urldate = {2024-03-30},
  abstract = {Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6–10x.},
  eventtitle = {{{NAACL-HLT}} 2022},
  langid = {english},
  keywords = {問答系統,未整理,檢索系統,資料集},
  annotation = {titleTranslation: ColBERTv2：透過輕量級後期互動進行有效且有效率的檢索\\
abstractTranslation:  神經資訊檢索（IR）極大地推進了搜尋和其他知識密集型語言任務。雖然許多神經 IR 方法將查詢和文件編碼為單向量表示，但後期交互模型在每個標記的粒度上產生多向量表示，並將相關性建模分解為可擴展的標記級計算。這種分解已被證明可以使後期交互作用更加有效，但它使這些模型的空間足跡擴大了一個數量級。在這項工作中，我們引入了 ColBERTv2，這是一種檢索器，它將積極的殘差壓縮機制與去噪監督策略結合起來，以同時提高後期互動的品質和空間佔用。我們透過廣泛的基準評估 ColBERTv2，在訓練領域內外建立最先進的質量，同時將後期互動模型的空間佔用減少 6-10 倍。},
  note = {看不懂},
  file = {C:\Users\BlackCat\Zotero\storage\UHYSTTBR\Santhanam 等。 - 2022 - ColBERTv2 Effective and Efficient Retrieval via L.pdf}
}

@article{sarveshsoniQuEHRyQuestionAnswering2023,
  title = {{{quEHRy}}: A Question Answering System to Query Electronic Health Records},
  shorttitle = {{{quEHRy}}},
  author = {{Sarvesh Soni} and {Surabhi Datta} and {Kirk Roberts}},
  date = {2023-06-01},
  journaltitle = {Journal of the American Medical Informatics Association},
  shortjournal = {Journal of the American Medical Informatics Association},
  volume = {30},
  number = {6},
  pages = {1091--1102},
  issn = {1527-974X},
  doi = {10.1093/jamia/ocad050},
  url = {https://doi.org/10.1093/jamia/ocad050},
  urldate = {2023-10-12},
  abstract = {We propose a system, quEHRy, to retrieve precise, interpretable answers to natural language questions from structured data in electronic health records (EHRs).We develop/synthesize the main components of quEHRy: concept normalization (MetaMap), time frame classification (new), semantic parsing (existing), visualization with question understanding (new), and query module for FHIR mapping/processing (new). We evaluate quEHRy on 2 clinical question answering (QA) datasets. We evaluate each component separately as well as holistically to gain deeper insights. We also conduct a thorough error analysis for a crucial subcomponent, medical concept normalization.Using gold concepts, the precision of quEHRy is 98.33\% and 90.91\% for the 2 datasets, while the overall accuracy was 97.41\% and 87.75\%. Precision was 94.03\% and 87.79\% even after employing an automated medical concept extraction system (MetaMap). Most incorrectly predicted medical concepts were broader in nature than gold-annotated concepts (representative of the ones present in EHRs), eg, Diabetes versus Diabetes Mellitus, Non-Insulin-Dependent.The primary performance barrier to deployment of the system is due to errors in medical concept extraction (a component not studied in this article), which affects the downstream generation of correct logical structures. This indicates the need to build QA-specific clinical concept normalizers that understand EHR context to extract the “relevant” medical concepts from questions.We present an end-to-end QA system that allows information access from EHRs using natural language and returns an exact, verifiable answer. Our proposed system is high-precision and interpretable, checking off the requirements for clinical use.},
  keywords = {問答系統,已整理,機器學習,病歷分析,醫學,重要},
  annotation = {0 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\6YP4V3F9\7136720.html}
}

@article{saryekawahyuniProblemsGeneratingIdeas2020,
  title = {The {{Problems}} of {{Generating Ideas Faced}} by {{English Language Students}} in {{Research Proposal Writing}}},
  author = {{Sary Eka Wahyuni} and {Nina Inayati}},
  date = {2020-12-31},
  journaltitle = {Pioneer: Journal of Language and Literature},
  volume = {12},
  number = {2},
  pages = {88--102},
  issn = {2655-8718},
  doi = {10.36841/pioneer.v12i2.633},
  url = {https://unars.ac.id/ojs/index.php/pioneer/article/view/633},
  urldate = {2023-10-20},
  abstract = {Generating ideas plays the key role in the initial phase of writing process, but not many empirical research is available in the literature that address the problems that students face in generating ideas, especially during research proposal writing. The current study aims to find out students’ problems in generating ideas in writing the research proposal. This descriptive study mainly collected the data through survey and interview involving seventh semester students who were in their initial phase of drafting research proposal for their final projects. The data were then analyzed and presented using descriptive quantitative method. The findings indicate that most of the students’ reported facing problems when generating ideas during the initial phase of research writing. Data analysis show that the problems range from the topic development, theoretical frameworks identification, relevant theory search, trusted sources evaluation, research ideas and relevant theory connection, as well as problems and theory assessment. Some pedagogical implications in light of the findings are discussed following the presentation of the results of the study, such as the importance to provide constructive feedback in a timely manner, and discussing ways to evaluate quality resources.},
  issue = {2},
  langid = {english},
  keywords = {待讀,研究流程,重要},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 英语语言专业学生在研究计划书写作中面临的构思问题\\
abstractTranslation:  构思在写作过程的初始阶段起着关键作用，但针对学生在构思过程中，尤其是在研究计划书写作过程中面临的问题的实证研究并不多。本研究旨在了解学生在撰写研究计划书时产生想法的问题。这项描述性研究主要通过调查和访谈的方式收集数据，受访者为第七学期的学生，他们正处于起草毕业项目研究计划书的初始阶段。然后采用描述性定量方法对数据进行分析和展示。研究结果表明，大多数学生表示在研究报告撰写的初始阶段，在产生想法时遇到了问题。数据分析显示，这些问题涉及课题开发、理论框架识别、相关理论搜索、可信来源评估、研究思路与相关理论联系以及问题与理论评估等多个方面。在介绍研究结果后，还讨论了研究结果对教学的一些启示，如及时提供建设性反馈的重要性，以及讨论评估优质资源的方法。},
  note = {調查數位撰寫論文的學生，看他們如何解決撰寫論文過程中長期記憶的問題。},
  file = {C:\Users\BlackCat\Zotero\storage\SDXDJNUQ\Sary Eka Wahyuni 與 Nina Inayati - 2020 - The Problems of Generating Ideas Faced by English .pdf}
}

@article{schroderStructurebasedKnowledgeAcquisition2022,
  title = {Structure-Based Knowledge Acquisition from Electronic Lab Notebooks for Research Data Provenance Documentation},
  author = {Schröder, Max and Staehlke, Susanne and Groth, Paul and Nebe, J. Barbara and Spors, Sascha and Krüger, Frank},
  date = {2022-01-31},
  journaltitle = {Journal of Biomedical Semantics},
  shortjournal = {J Biomed Semantics},
  volume = {13},
  eprint = {35101121},
  eprinttype = {pmid},
  pages = {4},
  issn = {2041-1480},
  doi = {10.1186/s13326-021-00257-x},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8802522/},
  urldate = {2024-01-17},
  abstract = {Background Electronic Laboratory Notebooks (ELNs) are used to document experiments and investigations in the wet-lab. Protocols in ELNs contain a detailed description of the conducted steps including the necessary information to understand the procedure and the raised research data as well as to reproduce the research investigation. The purpose of this study is to investigate whether such ELN protocols can be used to create semantic documentation of the provenance of research data by the use of ontologies and linked data methodologies. Methods Based on an ELN protocol of a biomedical wet-lab experiment, a retrospective provenance model of the raised research data describing the details of the experiment in a machine-interpretable way is manually engineered. Furthermore, an automated approach for knowledge acquisition from ELN protocols is derived from these results. This structure-based approach exploits the structure in the experiment’s description such as headings, tables, and links, to translate the ELN protocol into a semantic knowledge representation. To satisfy the Findable, Accessible, Interoperable, and Reuseable (FAIR) guiding principles, a ready-to-publish bundle is created that contains the research data together with their semantic documentation. Results While the manual modelling efforts serve as proof of concept by employing one protocol, the automated structure-based approach demonstrates the potential generalisation with seven ELN protocols. For each of those protocols, a ready-to-publish bundle is created and, by employing the SPARQL query language, it is illustrated that questions about the processes and the obtained research data can be answered. Conclusions The semantic documentation of research data obtained from the ELN protocols allows for the representation of the retrospective provenance of research data in a machine-interpretable way. Research Object Crate (RO-Crate) bundles including these models enable researchers to easily share the research data including the corresponding documentation, but also to search and relate the experiment to each other.},
  langid = {english},
  pmcid = {PMC8802522},
  keywords = {ELN,實驗流程,略讀,重要},
  annotation = {12 citations (Crossref) [2024-03-26]\\
titleTranslation: 從電子實驗室筆記本中獲取基於結構的知識，用於研究資料來源記錄\\
abstractTranslation:  背景電子實驗室筆記本 (ELN) 用於記錄濕實驗室中的實驗和研究。 ELN 中的協議包含所執行步驟的詳細描述，包括理解程序和所提出的研究數據以及重現研究調查的必要資訊。本研究的目的是調查此類 ELN 協議是否可用於透過使用本體和連結資料方法來建立研究資料來源的語義文件。方法 基於生物醫學濕實驗室實驗的 ELN 協議，手動設計所提出的研究數據的回顧性來源模型，以機器可解釋的方式描述實驗的細節。此外，從這些結果中得出了一種從 ELN 協議獲取知識的自動化方法。這種基於結構的方法利用實驗描述中的結構（例如標題、表格和連結）將 ELN 協議轉換為語義知識表示。為了滿足可查找、可存取、可互通和可重複使用 (FAIR) 指導原則，創建了一個可立即發布的捆綁包，其中包含研究資料及其語義文件。結果雖然手動建模工作透過採用一種協議來作為概念證明，但基於自動化結構的方法展示了七個 ELN 協議的潛在泛化能力。對於每個協議，都會建立一個準備發布的包，並透過使用 SPARQL 查詢語言來說明可以回答有關流程和所獲得的研究資料的問題。結論 從 ELN 協議獲得的研究數據的語義文件允許以機器可解釋的方式表示研究數據的回顧性來源。包含這些模型的研究對象箱 (RO-Crate) 捆綁包使研究人員能夠輕鬆共享包括相應文件在內的研究數據，還可以搜尋實驗並將其相互關聯。},
  note = {主要著重在如何將實驗資料轉為本體，大部分與研究較無相關。
\par
但相關研究中有提到類似資工筆記的相關研究，因此將其中提到的論文都拿進資料庫中。},
  file = {C:\Users\BlackCat\Zotero\storage\8N5LVXVE\Schröder et al. - 2022 - Structure-based knowledge acquisition from electro.pdf}
}

@inproceedings{sebastianmeierClassifyInterpretBuilding2023,
  title = {To {{Classify}} Is to {{Interpret}}: {{Building Taxonomies}} from {{Heterogeneous Data}} through {{Human-AI Collaboration}}},
  shorttitle = {To {{Classify}} Is to {{Interpret}}},
  booktitle = {Mensch Und {{Computer}} 2023},
  author = {{Sebastian Meier} and {Katrin Glinka}},
  year = {9 月 3, 2023},
  series = {{{MuC}} '23},
  pages = {395--401},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3603555.3608532},
  url = {https://doi.org/10.1145/3603555.3608532},
  urldate = {2023-08-23},
  abstract = {Taxonomy building is a task that requires interpreting and classifying data within a given frame of reference, which comes to play in many areas of application that deal with knowledge and information organization. In this paper, we explore how taxonomy building can be supported with systems that integrate machine learning (ML). However, relying only on black-boxed ML-based systems to automate taxonomy building would sideline the users’ expertise. We propose an approach that allows the user to iteratively take into account multiple model’s outputs as part of their sensemaking process. We implemented our approach in two real-world use cases. The work is positioned in the context of HCI research that investigates the design of ML-based systems with an emphasis on enabling human-AI collaboration.},
  isbn = {9798400707711},
  langid = {english},
  keywords = {Human-AI Collaboration,人機互動,可視化,完整公開,嵌入,已整理,機器學習,知識分類},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 分類就是解釋：通過人機協作從異構數據構建分類法\\
abstractTranslation:  分類法構建是一個需要在給定參考框架內對數據進行解釋和分類的任務，它在處理知識和信息組織的許多應用領域中發揮作用。在本文中，我們探討瞭如何通過集成機器學習（ ML ）的系統來支持分類法構建。然而，僅依靠基於機器學習的黑盒系統來自動構建分類法會消耗用戶的專業知識。我們提出了一種方法，允許用戶迭代地考慮多個模型的輸出作為其意義在於今天過程的一部分。我們在兩個現實場景中實現了我們的方法。這項工作定位於人機交互研究的背景下，研究基於機器學習的系統的設計，重點是實現人類與人工智能的協作。},
  note = {設計一個可互動且清楚的AI分類介面，可以清楚的比對不同模型算出來的結果有什麼不同。後續可以比較好的調整模型?目前看起來互動的結果不能直接影響模型，所謂的互動比較像是單方面的檢查。},
  file = {C:\Users\BlackCat\Zotero\storage\T3YVPHNT\Meier 與 Glinka - 2023 - To Classify is to Interpret Building Taxonomies f.pdf}
}

@inproceedings{sebastienkoniecznyRationalInferenceRelations2019,
  title = {Rational Inference Relations from Maximal Consistent Subsets Selection},
  booktitle = {Proceedings of the 28th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {{Sébastien Konieczny} and {Pierre Marquis} and {Srdjan Vesic}},
  year = {8 月 10, 2019},
  series = {{{IJCAI}}'19},
  pages = {1749--1755},
  publisher = {AAAI Press},
  location = {Macao, China},
  abstract = {When one wants to draw non-trivial inferences from an inconsistent belief base, a very natural approach is to take advantage of the maximal consistent subsets of the base. But few inference relations from maximal consistent subsets exist. In this paper we point out new inference relations based on selection of some maximal consistent subsets, leading thus to inference relations with a stronger inferential power. The selection process must obey some principles to ensure that it leads to an inference relation which is rational. We define a general class of monotonic selection relations for comparing maximal consistent subsets and show that it corresponds to the class of rational inference relations.},
  isbn = {978-0-9992411-4-1},
  keywords = {已整理,知識推理,邏輯}
}

@inproceedings{sergeinirenburgOntologicalSemanticsFormal2001,
  title = {Ontological Semantics, Formal Ontology, and Ambiguity},
  booktitle = {Proceedings of the International Conference on {{Formal Ontology}} in {{Information Systems}} - {{Volume}} 2001},
  author = {{Sergei Nirenburg} and {Victor Raskin}},
  year = {10 月 17, 2001},
  series = {{{FOIS}} '01},
  pages = {151--161},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/505168.505183},
  url = {https://dl.acm.org/doi/10.1145/505168.505183},
  urldate = {2023-09-15},
  abstract = {Ontological semantics is a theory of meaning in natural language and an approach to natural language processing (NLP) which uses an ontology as the central resource for extracting and representing meaning of natural language texts, reasoning about knowledge derived from texts as well as generating natural language texts based on representations of their meaning. Ontological semantics directly supports such applications as machine translation of natural languages, information extraction, text summarization, question answering, advice giving, collaborative work of networks of human and software agents, etc. Ontological semantics pays serious attention to its theoretical foundations by explicating its premises; therefore, formal ontology and its relations with ontological semantics are important. Besides a general brief discussion of these relations, the paper focuses on the important theoretical and practical issue of the distinction between ontology and natural language. It is argued that this crucial distinction lies not in the (inaccurately) presumed nonambiguity of the one and the well-established ambiguity of the other but rather in the constructed and overtly defined nature of ontological concepts and labels on which no human background knowledge can operate unintentionally to introduce ambiguity, as opposed to pervasive uncontrolled and uncontrollable ambiguity in natural language. The emphasis on this distinction, we argue, will provide better theoretical support for the central tenets of formal ontology by freeing it from the Wittgensteinian and Rortyan retreats from the analytical paradigm; it also reinforces the methodology of NLP by maintaining a productive demarcation between the language-independent nature of ontology and language-specific nature of the lexicons, a demarcation that has paid off well in consecutive implementations of ontological semantics and their applications in practical computer systems.},
  isbn = {978-1-58113-377-6},
  langid = {english},
  keywords = {/unread,基礎理論,已整理,知識本體,語意分析},
  annotation = {21 citations (Crossref) [2024-03-26]\\
abstractTranslation:  本體語義學是一種自然語言意義理論，是一種自然語言處理（NLP）方法，它以本體為中心資源，提取和表示自然語言文本的意義，推理文本中的知識並生成自然語言文本基於其含義的表示。本體語義直接支援自然語言的機器翻譯、資訊提取、文本摘要、問答、建議、人類和軟體代理網絡的協作工作等應用。本體語義透過闡明其前提來認真關注其理論基礎。 ;因此，形式本體論及其與本體語義學的關係非常重要。除了對這些關係進行一般性簡要討論外，本文還重點討論了本體論與自然語言之間的區別這一重要的理論和實踐問題。有人認為，這種關鍵區別不在於一個（不準確地）假定的非歧義性和另一個的既定的歧義性，而在於構建的和公開定義的本體概念和標籤的本質，人類背景知識無法對其進行操作無意中引入歧義，而不是自然語言中普遍存在的不受控制和無法控制的歧義。我們認為，對這種差異的強調將為形式本體論的中心原則提供更好的理論支持，使形式本體論擺脫維根斯坦和羅蒂安對分析範式的退縮。它還透過在本體的獨立於語言的性質和詞典的語言特定的性質之間保持有效的劃分來強化NLP 的方法論，這種劃分在本體語義的連續實現及其在實際計算機系統中的應用中得到了很好的回報。\\
titleTranslation: 本體語意、形式本體與歧義},
  file = {C:\Users\BlackCat\Zotero\storage\VSU33VTR\Nirenburg 與 Raskin - 2001 - Ontological semantics, formal ontology, and ambigu.pdf}
}

@article{shahzadRelationshipITSelfEfficacy2023,
  title = {Relationship between {{IT Self-Efficacy}} and {{Personal Knowledge}} and {{Information Management}} for {{Sustainable Lifelong Learning}} and {{Organizational Performance}}: {{A Systematic Review}} from 2000 to 2022},
  shorttitle = {Relationship between {{IT Self-Efficacy}} and {{Personal Knowledge}} and {{Information Management}} for {{Sustainable Lifelong Learning}} and {{Organizational Performance}}},
  author = {Shahzad, Khurram and Javed, Yasir and Khan, Shakeel Ahmad and Iqbal, Abid and Hussain, Imran and Jaweed, M. Vaseem},
  date = {2023-01},
  journaltitle = {Sustainability},
  volume = {15},
  number = {1},
  pages = {5},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2071-1050},
  doi = {10.3390/su15010005},
  url = {https://www.mdpi.com/2071-1050/15/1/5},
  urldate = {2024-03-13},
  abstract = {This study aims to identify the relationship between IT self-efficacy and personal knowledge and information management (PKIM) practices. It also intends to investigate trending tools and approaches being applied for PKIM for sustainable lifelong learning and organizational performance. It also reveals challenges for the development of an effective PKIM system. To meet the study’s objectives, a systematic literature review was carried out. Fifty research papers published in peer-reviewed journals were included to conduct a comprehensive systematic review. The findings of the study revealed that a significant positive relationship exists between IT self-efficacy and personal knowledge and information management for sustainable lifelong learning and innovative organizational performance. Social media tools, the adoption of emerging technologies, and artificial intelligence were trending techniques for the successful implementation of PKIM practices in academia and the field. This research has significant theoretical, practical, social, academic, and managerial implications.},
  issue = {1},
  langid = {english},
  keywords = {higher education,information behavior,IT self-efficacy and PKIM,job professionals,Review,students,sustainable professional development,已整理,知識管理,研究流程,重要},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: IT 自我效能與永續終身學習和組織績效的個人知識和資訊管理之間的關係：2000 年至 2022 年的系統性回顧},
  file = {C:\Users\BlackCat\Zotero\storage\XWV6STZ2\Shahzad 等。 - 2023 - Relationship between IT Self-Efficacy and Personal.pdf}
}

@inproceedings{shainarazaQuestionAnsweringSystemCOVID192022,
  title = {A {{Question-Answering System}} on {{COVID-19 Scientific Literature}}},
  booktitle = {2022 {{IEEE}} 46th {{Annual Computers}}, {{Software}}, and {{Applications Conference}} ({{COMPSAC}})},
  author = {{Shaina Raza} and {Brian Schwartz} and {Nancy Ondrusek}},
  date = {2022-06},
  pages = {1331--1336},
  issn = {0730-3157},
  doi = {10.1109/COMPSAC54236.2022.00210},
  url = {https://ieeexplore.ieee.org/document/9842573},
  urldate = {2023-11-23},
  abstract = {The vast amount of COVID-19 research literature has made it difficult for medical experts, clinical scientists, and researchers to keep up with the latest research findings. We present two datasets for COVID-19 in this work: (1) first, we create a dataset from the up-to-date scientific publications on COVID-19, and (2) second, we build a gold-standard dataset of question-answering pairs annotated by volunteer biomedical experts on COVID-19 related scientific articles. We develop a question-answering (QA) pipeline that uses the first dataset to provide answers related to COVID-19 questions; we fine-tune MPNet (a Transformer model) on our gold-standard dataset and use it in the QA pipeline to enhance its reading capability. We also use this gold-standard dataset to evaluate the QA pipeline. The proposed MPNet version on the gold-standard dataset outperformed previous datasets and models, achieving an Exact Match/Fl score of 69.72/78.50 \%, respectively},
  eventtitle = {2022 {{IEEE}} 46th {{Annual Computers}}, {{Software}}, and {{Applications Conference}} ({{COMPSAC}})},
  langid = {english},
  keywords = {問答系統,已整理,文獻分析},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: COVID-19 科學文獻問答系統},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\YUYTGLNR\\Raza et al. - 2022 - A Question-Answering System on COVID-19 Scientific.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\X5ENZYTF\\9842573.html}
}

@online{shanchenEvaluationChatGPTFamily2023,
  title = {Evaluation of {{ChatGPT Family}} of {{Models}} for {{Biomedical Reasoning}} and {{Classification}}},
  author = {{Shan Chen} and {Yingya Li} and {Sheng Lu} and {Hoang Van} and {Hugo JWL Aerts} and {Guergana K. Savova} and {Danielle S. Bitterman}},
  date = {2023-04-05},
  eprint = {2304.02496},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.02496},
  url = {http://arxiv.org/abs/2304.02496},
  urldate = {2023-04-19},
  abstract = {Recent advances in large language models (LLMs) have shown impressive ability in biomedical question-answering, but have not been adequately investigated for more specific biomedical applications. This study investigates the performance of LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical tasks beyond question-answering. Because no patient data can be passed to the OpenAI API public interface, we evaluated model performance with over 10000 samples as proxies for two fundamental tasks in the clinical domain - classification and reasoning. The first task is classifying whether statements of clinical and policy recommendations in scientific literature constitute health advice. The second task is causal relation detection from the biomedical literature. We compared LLMs with simpler models, such as bag-of-words (BoW) with logistic regression, and fine-tuned BioBERT models. Despite the excitement around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks remained the best strategy. The simple BoW model performed on par with the most complex LLM prompting. Prompt engineering required significant investment.},
  langid = {english},
  pubstate = {preprint},
  keywords = {ChatGPT,LLM,已整理,微調,未發表,機器學習,語言模型,醫學},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-04-20]\\
0 citations (Semantic Scholar/DOI) [2023-04-20]\\
titleTranslation: ChatGPT 生物醫學推理和分類模型系列的評估\\
abstractTranslation:  大語言模型（LLM）的最新進展在生物醫學問答方面表現出了令人印象深刻的能力，但尚未針對更具體的生物醫學應用進行充分的研究。本研究調查了法學碩士（例如 ChatGPT 模型系列（GPT-3.5s、GPT-4））在問答以外的生物醫學任務中的表現。由於無法將患者數據傳遞到 OpenAI API 公共接口，因此我們使用超過 10000 個樣本作為臨床領域兩項基本任務（分類和推理）的代理來評估模型性能。第一項任務是對科學文獻中的臨床和政策建議的陳述是否構成健康建議進行分類。第二個任務是從生物醫學文獻中檢測因果關係。我們將法學碩士與更簡單的模型進行了比較，例如帶有邏輯回歸的詞袋 (BoW) 以及微調的 BioBERT 模型。儘管病毒式傳播的 ChatGPT 令人興奮，但我們發現對兩項基本 NLP 任務進行微調仍然是最佳策略。簡單的 BoW 模型的表現與最複雜的 LLM 提示相當。快速工程需要大量投資。},
  note = {在除了問答系統外的領域上，例如分類或推理，微調仍然要比使用LLM要有效，ChatGPT的表現和BOW相當。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\AXRS8E2U\\Chen 等。 - 2023 - Evaluation of ChatGPT Family of Models for Biomedi.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\DAJ9FG5N\\2304.html}
}

@online{shaojindingPersonalVADOptimizing2022,
  title = {Personal {{VAD}} 2.0: {{Optimizing Personal Voice Activity Detection}} for {{On-Device Speech Recognition}}},
  shorttitle = {Personal {{VAD}} 2.0},
  author = {{Shaojin Ding} and {Rajeev Rikhye} and {Qiao Liang} and {Yanzhang He} and {Quan Wang} and {Arun Narayanan} and {Tom O'Malley} and {Ian McGraw}},
  date = {2022-06-24},
  eprint = {2204.03793},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2204.03793},
  url = {http://arxiv.org/abs/2204.03793},
  urldate = {2023-11-02},
  abstract = {Personalization of on-device speech recognition (ASR) has seen explosive growth in recent years, largely due to the increasing popularity of personal assistant features on mobile devices and smart home speakers. In this work, we present Personal VAD 2.0, a personalized voice activity detector that detects the voice activity of a target speaker, as part of a streaming on-device ASR system. Although previous proof-of-concept studies have validated the effectiveness of Personal VAD, there are still several critical challenges to address before this model can be used in production: first, the quality must be satisfactory in both enrollment and enrollment-less scenarios; second, it should operate in a streaming fashion; and finally, the model size should be small enough to fit a limited latency and CPU/Memory budget. To meet the multi-faceted requirements, we propose a series of novel designs: 1) advanced speaker embedding modulation methods; 2) a new training paradigm to generalize to enrollment-less conditions; 3) architecture and runtime optimizations for latency and resource restrictions. Extensive experiments on a realistic speech recognition system demonstrated the state-of-the-art performance of our proposed method.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  annotation = {titleTranslation: 個人VAD 2.0：優化裝置上語音辨識的個人語音活動偵測},
  note = {Comment: Accepted by INTERSPEECH 2022},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\6DXRNHZN\\Ding 等。 - 2022 - Personal VAD 2.0 Optimizing Personal Voice Activi.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\F8GEM6XA\\2204.html}
}

@online{shauryarohatgiACLOCLCorpus2023,
  title = {The {{ACL OCL Corpus}}: Advancing {{Open}} Science in {{Computational Linguistics}}},
  shorttitle = {The {{ACL OCL Corpus}}},
  author = {{Shaurya Rohatgi} and {Yanxia Qin} and {Benjamin Aw} and {Niranjana Unnithan} and {Min-Yen Kan}},
  date = {2023-05-24},
  eprint = {2305.14996},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.14996},
  url = {http://arxiv.org/abs/2305.14996},
  urldate = {2023-09-19},
  abstract = {We present a scholarly corpus from the ACL Anthology to assist Open scientific research in the Computational Linguistics domain, named as ACL OCL. Compared with previous ARC and AAN versions, ACL OCL includes structured full-texts with logical sections, references to figures, and links to a large knowledge resource (semantic scholar). ACL OCL contains 74k scientific papers, together with 210k figures extracted up to September 2022. To observe the development in the computational linguistics domain, we detect the topics of all OCL papers with a supervised neural model. We observe ''Syntax: Tagging, Chunking and Parsing'' topic is significantly shrinking and ''Natural Language Generation'' is resurging. Our dataset is open and available to download from HuggingFace in https://huggingface.co/datasets/ACL-OCL/ACL-OCL-Corpus.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Digital Libraries,已整理,影像識別,資料集},
  annotation = {titleTranslation: ACL OCL 語料庫：推進計算語言學的開放科學\\
abstractTranslation:  我們提出了 ACL Anthology 的學術語料庫，以協助計算語言學領域的開放科學研究，命名為 ACL OCL。與先前的 ARC 和 AAN 版本相比，ACL OCL 包括帶有邏輯部分的結構化全文、對圖形的引用以及對大型知識資源（語義學者）的連結。 ACL OCL 包含 7.4 萬篇科學論文，以及截至 2022 年 9 月提取的 21 萬張圖表。為了觀察計算語言學領域的發展，我們使用監督神經模型來檢測所有 OCL 論文的主題。我們觀察到「文法：標記、分塊和解析」主題正在顯著萎縮，而「自然語言生成」正在重新興起。我們的資料集是開放的，可以從 HuggingFace 下載：https://huggingface.co/datasets/ACL-OCL/ACL-OCL-Corpus。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\4JIPGW8D\\Rohatgi 等。 - 2023 - The ACL OCL Corpus advancing Open science in Comp.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\LTYKV7Y7\\2305.html}
}

@article{ShengHuiZhongYiDianZiBingLiDaShuJuFuWuPingTaiSheJiYuYanJiu2017,
  title = {中医电子病历大数据服务平台设计与研究},
  author = {{生慧} and {王振国}},
  date = {2017},
  journaltitle = {山东中医药大学学报},
  volume = {41},
  number = {5},
  pages = {4},
  url = {https://www.zhangqiaokeyan.com/academic-journal-cn_detail_thesis/0201296584216.html},
  urldate = {2022-10-23},
  abstract = {通过对中医电子病历大数据服务平台的功能,技术和安全需求进行分析,设计出多层次的中医电子病历大数据服务平台,该平台具有数据采集,数据共享存储,数据分析,保证数据安全的功能,能够为个体化医疗与精准医疗,方药知识发现,治未病,个体化疗效评价等方面提供便利,在保证中医特色和数据安全的前提下,实现中医电子病历核心数据的开放,共享,分析与应用.},
  keywords = {MongoDB数据挖掘,No DOI found,中医电子病历,大数据,数据采集},
  file = {C:\Users\BlackCat\Zotero\storage\6DVXCN5V\生 與 王 - 2017 - 中医电子病历大数据服务平台设计与研究.pdf}
}

@inproceedings{shenKnowledgeawareAttentiveNeural2018,
  title = {Knowledge-Aware {{Attentive Neural Network}} for {{Ranking Question Answer Pairs}}},
  booktitle = {The 41st {{International ACM SIGIR Conference}} on {{Research}} \& {{Development}} in {{Information Retrieval}}},
  author = {Shen, Ying and Deng, Yang and Yang, Min and Li, Yaliang and Du, Nan and Fan, Wei and Lei, Kai},
  year = {6 月 27, 2018},
  series = {{{SIGIR}} '18},
  pages = {901--904},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3209978.3210081},
  url = {https://dl.acm.org/doi/10.1145/3209978.3210081},
  urldate = {2024-03-28},
  abstract = {Ranking question answer pairs has attracted increasing attention recently due to its broad applications such as information retrieval and question answering (QA). Significant progresses have been made by deep neural networks. However, background information and hidden relations beyond the context, which play crucial roles in human text comprehension, have received little attention in recent deep neural networks that achieve the state of the art in ranking QA pairs. In the paper, we propose KABLSTM, a Knowledge-aware Attentive Bidirectional Long Short-Term Memory, which leverages external knowledge from knowledge graphs (KG) to enrich the representational learning of QA sentences. Specifically, we develop a context-knowledge interactive learning architecture, in which a context-guided attentive convolutional neural network (CNN) is designed to integrate knowledge embeddings into sentence representations. Besides, a knowledge-aware attention mechanism is presented to attend interrelations between each segments of QA pairs. KABLSTM is evaluated on two widely-used benchmark QA datasets: WikiQA and TREC QA. Experiment results demonstrate that KABLSTM has robust superiority over competitors and sets state-of-the-art.},
  isbn = {978-1-4503-5657-2},
  langid = {english},
  keywords = {attention mechanism,question answering,嵌入,已整理,微讀,數學,機器學習,知識圖譜},
  annotation = {titleTranslation: 用於對問答對進行排名的知識感知注意力神經網絡\\
abstractTranslation:  由於其在資訊檢索和問答（QA）等領域的廣泛應用，對問題答案對進行排名最近引起了越來越多的關注。深度神經網路已經取得了重大進展。然而，背景資訊和超越上下文的隱藏關係在人類文本理解中發揮著至關重要的作用，但在最近的深度神經網路中卻很少受到關注，這些神經網路在對問答對進行排序方面達到了最先進的水平。在本文中，我們提出了 KABLSTM，一種知識感知雙向長短期記憶，它利用知識圖（KG）中的外部知識來豐富 QA 句子的表徵學習。具體來說，我們開發了一種上下文知識互動式學習架構，其中上下文引導的注意力卷積神經網路（CNN）旨在將知識嵌入整合到句子表示中。此外，也提出了一種知識感知注意機制來關注問答對各部分之間的相互關係。 KABLSTM 在兩個廣泛使用的基準 QA 資料集上進行評估：WikiQA 和 TREC QA。實驗結果表明，KABLSTM 相對於競爭對手具有強大的優勢，並達到了最先進的水平。},
  note = {本研究通過知識圖譜來提升QA對 Ranking任務的表現。
\par
使用BiLSTM分別生成嵌入結果Qinit和Ainit，結合知識圖譜生成的嵌入結果Qknow和Aknow，來完成QA對 Ranking。
\par
相當多數學及模型的設計，與研究較無相關。},
  file = {C:\Users\BlackCat\Zotero\storage\NIMJLTZB\Shen et al. - 2018 - Knowledge-aware Attentive Neural Network for Ranki.pdf}
}

@article{ShenNingQiaoLunZhongYiDianZiBingLiDeZhongYiFangXiang2011,
  title = {论中医电子病历的中医方向},
  author = {{沈宁乔} and {张伟威} and {余春兰} and {孙赟n} and {严正仲} and {罗俊}},
  date = {2011},
  journaltitle = {江苏中医药},
  volume = {43},
  number = {6},
  pages = {2},
  doi = {10.3969/j.issn.1672-397x.2011.06.055},
  abstract = {中医电子病历系统是在遵循中医辨证论治核心理论的基础上,运用现代信息科学和电子科学技术的成果,建立的结构化,标准化,规范化的系统,能引导和帮助临床医师实施辨证施治,并以此为基础,运用数据挖掘工具,探索中医科学的内在规律,推进中医的现代化.},
  keywords = {review,中醫,症狀標準化,辨證,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\EK98Y99X\沈宁乔 等。 - 2011 - 论中医电子病历的中医方向.pdf}
}

@inproceedings{ShenZhengPingMianXiangDuiXiangDeJieGouHuaBingLiMoXing2008,
  title = {面向对象的结构化病历模型},
  booktitle = {中华医院信息网络大会暨第四届中美医院信息化论坛},
  author = {{沈正平}},
  date = {2008-09-26},
  keywords = {回收,無法取得}
}

@online{ShiJianPianWuKBQA,
  title = {实践篇（五）：KBQA Demo},
  url = {https://zhuanlan.zhihu.com/p/33363861},
  urldate = {2023-09-28},
  abstract = {作为实践篇的最后一篇，我们将介绍如何用Python完成一个简易的问答程序。下图是demo的展示效果： 查询结果为空，回答“I don\&\#39;t know.”；不能理解问句，回答“I can\&\#39;t understand.”。本实现参考了王昊奋…},
  langid = {chinese},
  organization = {知乎专栏},
  keywords = {問答系統,知識圖譜,知識本體},
  annotation = {abstractTranslation:  作為實作篇的最後一篇，我們將介紹如何用Python完成一個簡單的問答程序。下圖是demo的展示效果：查詢結果為空，回答「我不知道。」；不能理解問句，回答「我聽不懂。」。本實現參考了王昊奮...\\
titleTranslation: 實作篇(五):KBQA Demo},
  file = {C:\Users\BlackCat\Zotero\storage\UXPDMQDW\33363861.html}
}

@inproceedings{shiladsenQuestQualityTags2007,
  title = {The Quest for Quality Tags},
  booktitle = {Proceedings of the 2007 {{ACM International Conference}} on {{Supporting Group Work}}},
  author = {{Shilad Sen} and {F. Maxwell Harper} and {Adam LaPitz} and {John Riedl}},
  year = {11 月 4, 2007},
  series = {{{GROUP}} '07},
  pages = {361--370},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1316624.1316678},
  url = {https://doi.org/10.1145/1316624.1316678},
  urldate = {2023-09-11},
  abstract = {Many online communities use tags - community selected words or phrases - to help people find what they desire. The quality of tags varies widely, from tags that capture akey dimension of an entity to those that are profane, useless, or unintelligible. Tagging systems must often select a subset of available tags to display to users due to limited screen space. Because users often spread tags they have seen, selecting good tags not only improves an individual's view of tags, it also encourages them to create better tags in the future. We explore implicit (behavioral) and explicit (rating) mechanisms for determining tag quality. Based on 102,056 tag ratings and survey responses collected from 1,039 users over 100 days, we offer simple suggestions to designers of online communities to improve the quality of tags seen by their users.},
  isbn = {978-1-59593-845-9},
  langid = {english},
  keywords = {moderation,tagging,user interfaces,分類標籤},
  annotation = {49 citations (Crossref) [2024-03-26]\\
titleTranslation: 對品質標籤的追求\\
abstractTranslation:  許多在線社區使用標籤（社區選擇的單詞或短語）來幫助人們找到他們想要的東西。標籤的質量差異很大，從捕捉實體關鍵維度的標籤到褻瀆、無用或難以理解的標籤。由於屏幕空間有限，標籤系統通常必須選擇可用標籤的子集來向用戶顯示。由於用戶經常傳播他們看到的標籤，選擇好的標籤不僅可以提高個人對標籤的看法，還可以鼓勵他們將來創建更好的標籤。我們探索用於確定標籤質量的隱式（行為）和顯式（評級）機制。根據 100 天內從 1,039 位用戶收集的 102,056 個標籤評級和調查回复，我們為在線社區設計者提供簡單的建議，以提高用戶看到的標籤質量。},
  file = {C:\Users\BlackCat\Zotero\storage\7RSFXV4Q\Sen 等。 - 2007 - The quest for quality tags.pdf}
}

@inproceedings{shiPerspectivesCognitiveInformatics2003,
  title = {Perspectives on Cognitive Informatics},
  booktitle = {The {{Second IEEE International Conference}} on {{Cognitive Informatics}}, 2003. {{Proceedings}}.},
  author = {Shi, Zhongzhi and Shi, Jun},
  date = {2003-08},
  pages = {129--133},
  doi = {10.1109/COGINF.2003.1225970},
  url = {https://ieeexplore.ieee.org/document/1225970},
  urldate = {2024-04-16},
  abstract = {From a scientific perspective explaining how the brain thinks is a big goal. Cognitive informatics studies intelligent behavior from a computational point of view in terms of updated research efforts and processes of brain science and neuroscience. Cognitive informatics is the interdisciplinary study of cognition. Cognition includes mental states and processes, such as thinking, reasoning, remembering, language understanding and generation, visual and auditory perception, learning, consciousness, emotions, etc. In this paper we will point out basic research topics of learning, memory, thought, language, and neural computing which are active fields related to cognitive informatics.},
  eventtitle = {The {{Second IEEE International Conference}} on {{Cognitive Informatics}}, 2003. {{Proceedings}}.},
  langid = {english},
  keywords = {Cognition,Cognitive informatics,Cognitive science,Data structures,Humans,Information processing,Intelligent sensors,Learning,Neurons,Neuroscience},
  annotation = {titleTranslation: 認知資訊學的觀點\\
abstractTranslation:  從科學的角度解釋大腦如何思考是一個很大的目標。認知資訊學從腦科學和神經科學的最新研究成果和過程的計算角度研究智慧行為。認知資訊學是認知的跨領域研究。認知包括思考、推理、記憶、語言理解和生成、視覺和聽覺感知、學習、意識、情緒等心理狀態和過程。的活躍領域。},
  file = {C:\Users\BlackCat\Zotero\storage\HLZ7F9UL\Shi 與 Shi - 2003 - Perspectives on cognitive informatics.pdf}
}

@online{shiruipanUnifyingLargeLanguage2023,
  title = {Unifying {{Large Language Models}} and {{Knowledge Graphs}}: {{A Roadmap}}},
  shorttitle = {Unifying {{Large Language Models}} and {{Knowledge Graphs}}},
  author = {{Shirui Pan} and {Linhao Luo} and {Yufei Wang} and {Chen Chen} and {Jiapu Wang} and {Xindong Wu}},
  date = {2023-06-20},
  eprint = {2306.08302},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.08302},
  url = {http://arxiv.org/abs/2306.08302},
  urldate = {2023-07-27},
  abstract = {Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,待讀,未整理,重要},
  annotation = {2 citations (Semantic Scholar/arXiv) [2023-07-27]\\
2 citations (Semantic Scholar/DOI) [2023-07-27]\\
titleTranslation: 統一大型語言模型和知識圖：路線圖\\
abstractTranslation:  ChatGPT 和 GPT4 等大型語言模型 (LLM) 憑藉其新興能力和泛化性，正在自然語言處理和人工智能領域掀起新的浪潮。然而，法學碩士是黑盒模型，通常無法捕獲和獲取事實知識。相比之下，知識圖譜（KG）、維基百科和花譜等都是結構化知識模型，顯式存儲豐富的事實知識。知識圖譜可以通過提供用於推理和可解釋性的外部知識來增強法學碩士。同時，知識圖譜本質上難以構建和進化，這對知識圖譜中現有的生成新事實和表示未見知識的方法提出了挑戰。因此，將LLM和KG結合起來，同時發揮各自的優勢，是互補的。在本文中，我們提出了 LLM 和 KG 統一的前瞻性路線圖。我們的路線圖由三個總體框架組成，即1）知識圖譜增強型法學碩士，在法學碩士的預訓練和推理階段納入知識圖譜，或者為了增強對法學碩士所學知識的理解； 2）LLM增強知識圖譜，利用LLM來完成不同的知識圖譜任務，例如嵌入、完成、構造、圖文生成和問答； 3）協同LLM+KG，其中LLM和KG發揮平等作用，以互惠互利的方式工作，以增強LLM和KG的數據和知識驅動的雙向推理能力。我們在路線圖中回顧和總結了這三個框架內的現有工作，並確定了它們未來的研究方向。},
  note = {Comment: 29 pages, 25 figures},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\NAKQZZVQ\\Pan 等。 - 2023 - Unifying Large Language Models and Knowledge Graph.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\C3B4SJZ8\\2306.html}
}

@article{shorishDataInformationLiteracy2015,
  title = {Data {{Information Literacy}} and {{Undergraduates}}: {{A Critical Competency}}},
  shorttitle = {Data {{Information Literacy}} and {{Undergraduates}}},
  author = {Shorish, Yasmeen},
  date = {2015-01-02},
  journaltitle = {College \& Undergraduate Libraries},
  volume = {22},
  number = {1},
  pages = {97--106},
  publisher = {Routledge},
  issn = {1069-1316},
  doi = {10.1080/10691316.2015.1001246},
  url = {https://doi.org/10.1080/10691316.2015.1001246},
  urldate = {2024-03-22},
  abstract = {As a primer on data information literacy (DIL), this column will cover the background of the field and why it is relevant to college and university libraries serving undergraduate populations. This article includes how data information literacy relates to information literacy, competencies associated with DIL, the relevance of DIL to undergraduates, DIL in library instruction, and the reasons for library engagement with DIL. Examining DIL within the larger framework of information literacy can help outreach and instruction librarians engage with a format that may be unfamiliar to them but whose underlying foundation is well-established.},
  langid = {english},
  keywords = {Academic libraries,data curation,data information literacy,data management,higher education,information literacy,instruction,outreach,未整理},
  annotation = {25 citations (Crossref) [2024-03-26]\\
titleTranslation: 數據資訊素養與大學部學生：一項關鍵能力},
  file = {C:\Users\BlackCat\Zotero\storage\IM69VWLB\Shorish - 2015 - Data Information Literacy and Undergraduates A Cr.pdf}
}

@inproceedings{shreyasaxenaLargeScaleKnowledgeSynthesis2022,
  title = {Large-{{Scale Knowledge Synthesis}} and {{Complex Information Retrieval}} from {{Biomedical Documents}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {{Shreya Saxena} and {Raj Sangani} and {Siva Prasad} and {Shubham Kumar} and {Mihir Athale} and {Rohan Awhad} and {Vishal Vaddina}},
  date = {2022-02},
  pages = {2364--2369},
  doi = {10.1109/BigData55660.2022.10020725},
  url = {https://ieeexplore.ieee.org/document/10020725},
  urldate = {2023-11-23},
  abstract = {Recent advances in the healthcare industry have led to an abundance of unstructured data, making it challenging to perform tasks such as efficient and accurate information retrieval at scale. Our work offers an all-in-one scalable solution for extracting and exploring complex information from large-scale research documents, which would otherwise be tedious. First, we briefly explain our knowledge synthesis process to extract helpful information from unstructured text data of research documents. Then, on top of the knowledge extracted from the documents, we perform complex information retrieval using three major components- Paragraph Retrieval, Triplet Retrieval from Knowledge Graphs, and Complex Question Answering (QA). These components combine lexical and semantic-based methods to retrieve paragraphs and triplets and perform faceted refinement for filtering these search results. The complexity of biomedical queries and documents necessitates using a QA system capable of handling queries more complex than factoid queries, which we evaluate qualitatively on the COVID-19 Open Research Dataset (CORD-19) to demonstrate the effectiveness and value-add.},
  eventtitle = {2022 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  langid = {english},
  keywords = {問答系統,已整理,待讀,文獻分析,知識圖譜,重要},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 生物醫學文獻中的大規模知識合成與複雜資訊檢索},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\TQTNRY2K\\Saxena et al. - 2022 - Large-Scale Knowledge Synthesis and Complex Inform.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\SFSQLRM7\\10020725.html}
}

@online{shu-wenyangSUPERBSpeechProcessing2021,
  title = {{{SUPERB}}: {{Speech}} Processing {{Universal PERformance Benchmark}}},
  shorttitle = {{{SUPERB}}},
  author = {{Shu-wen Yang} and {Po-Han Chi} and {Yung-Sung Chuang} and {Cheng-I. Jeff Lai} and {Kushal Lakhotia} and {Yist Y. Lin} and {Andy T. Liu} and {Jiatong Shi} and {Xuankai Chang} and {Guan-Ting Lin} and {Tzu-Hsien Huang} and {Wei-Cheng Tseng} and {Ko-tik Lee} and {Da-Rong Liu} and {Zili Huang} and {Shuyan Dong} and {Shang-Wen Li} and {Shinji Watanabe} and {Abdelrahman Mohamed} and {Hung-yi Lee}},
  date = {2021-10-15},
  eprint = {2105.01051},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2105.01051},
  url = {http://arxiv.org/abs/2105.01051},
  urldate = {2023-09-17},
  abstract = {Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,已整理,自監督式,語音辨識,資料集},
  annotation = {titleTranslation: SUPERB：語音處理通用效能基準\\
abstractTranslation:  事實證明，自我監督學習 (SSL) 對於推進自然語言處理 (NLP) 和電腦視覺 (CV) 的研究至關重要。該範例在大量未標記資料上預先訓練共享模型，並以最小的適應實現了各種任務的最先進（SOTA）。然而，語音處理社群缺乏類似的設定來系統地探索該範式。為了彌補這一差距，我們引入了語音處理通用效能基準 (SUPERB)。 SUPERB 是一個排行榜，用於對各種語音處理任務中的共享模型的效能進行基準測試，只需最少的架構更改和標記資料。在共享模型的多種用途中，我們特別關注提取從 SSL 學習到的表示，因為它具有更好的可重複使用性。我們提出了一個簡單的框架，透過在凍結共享模型之上學習任務專用的輕量級預測頭來解決 SUPERB 任務。我們的結果表明，該框架很有前途，因為 SSL 表示在 SUPERB 任務中顯示出有競爭力的通用性和可訪問性。我們發布 SUPERB 作為一項挑戰，其中包含排行榜和基準工具包，以推動表徵學習和通用語音處理的研究。},
  note = {Comment: To appear in Interspeech 2021},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\87CUTB3C\\Yang 等。 - 2021 - SUPERB Speech processing Universal PERformance Be.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\ZNC47PQB\\2105.html}
}

@article{ShuChenJiJiYuSuiJiSenLinSuanFaDeZhongYiHanZhengHeReZhengZhenDuanMoXingYanJiu2021,
  title = {基于随机森林算法的中医寒证和热证诊断模型研究},
  author = {{舒琛洁} and {梁浩} and {王耘}},
  date = {2021},
  journaltitle = {北京中医药大学学报},
  volume = {44},
  number = {6},
  pages = {6},
  doi = {10.3969/j.issn.1006-2157.2021.06.008},
  abstract = {目的 从症状体征的角度,构建中医寒证和热证的诊断模型,为寒热辨证标准化提供依据.方法 从《证候规范与辨证方法体系的研究》构建的证候要素-症状数据表中分别筛选与"寒""热"有关的症状,基于随机森林算法特征筛选出排序前15的症状,随机划分为10份,按照7:3作为训练集和测试集,重新采样后以最佳参数分别构建寒证和热证的随机森林模型,以受试者工作特征(ROC)曲线下面积(AUC),敏感度和特异度作为模型评价指标.结果 寒证的关键特征变量包括脉浮紧,恶寒,无汗,苔白,得温痛减,冷痛,舌淡,恶寒发热,口不渴,身痛,头痛,苔腻,食欲不振,便溏,肢冷,诊断模型AUC值为0.912,特异度和敏感度分别为0.89和0.80.热证的关键特征变量包括苔黄,口渴,脉滑数,发热,壮热,脉数,小便赤,舌红,脉弦数,口苦,苔腻,舌红绛,尿黄,心烦,头痛,诊断模型AUC值为0.891,特异度和敏感度分别为0.85和0.86.结论 基于变量筛选及随机森林算法,有效建立了寒热的辨证模型,显示出较好的分类效果,可以为标准化辨证提供方法学参考.},
  file = {C:\Users\BlackCat\Zotero\storage\8DXSQNYU\舒琛洁 等。 - 2021 - 基于随机森林算法的中医寒证和热证诊断模型研究.pdf}
}

@inproceedings{shumindengConstructionApplicationsBillionScale2023,
  title = {Construction and {{Applications}} of {{Billion-Scale Pre-Trained Multimodal Business Knowledge Graph}}},
  booktitle = {2023 {{IEEE}} 39th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {{Shumin Deng} and {Chengming Wang} and {Zhoubo Li} and {Ningyu Zhang} and {Zelin Dai} and {Hehong Chen} and {Feiyu Xiong} and {Ming Yan} and {Qiang Chen} and {Mosha Chen} and {Jiaoyan Chen} and {Jeff Z. Pan} and {Bryan Hooi} and {Huajun Chen}},
  date = {2023-04},
  pages = {2988--3002},
  issn = {2375-026X},
  doi = {10.1109/ICDE55515.2023.00229},
  abstract = {Business Knowledge Graphs (KGs) are important to many enterprises today, providing factual knowledge and structured data that steer many products and make them more intelligent. Despite their promising benefits, building business KG necessitates solving prohibitive issues of deficient structure and multiple modalities. In this paper, we advance the understanding of the practical challenges related to building KG in non-trivial real-world systems. We introduce the process of building an open business knowledge graph (OpenBG) derived from a well-known enterprise, Alibaba Group. Specifically, we define a core ontology to cover various abstract products and consumption demands, with fine-grained taxonomy and multimodal facts in deployed applications. OpenBG is an open business KG of unprecedented scale: 2.6 billion triples with more than 88 million entities covering over 1 million core classes/concepts and 2,681 types of relations. We release all the open resources (OpenBG benchmarks) derived from it for the community and report experimental results of KG-centric tasks. We also run up an online competition based on OpenBG benchmarks, and has attracted thousands of teams. We further pre-train OpenBG and apply it to many KG-enhanced downstream tasks in business scenarios, demonstrating the effectiveness of billion-scale multimodal knowledge for e-commerce. All the resources with codes have been released at https://github.com/OpenBGBenchmark/OpenBG.},
  eventtitle = {2023 {{IEEE}} 39th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  langid = {english},
  keywords = {Applications,Benchmark testing,Billion-scale,Buildings,Business,Construction,Knowledge engineering,Knowledge Graph,Knowledge graphs,Ontologies,Sociology,Taxonomy,已整理,待讀,本體建立,知識圖譜,重要},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 億級預訓練多模態業務知識圖譜建置及應用\\
abstractTranslation:  商業知識圖 (KG) 對當今的許多企業都很重要，它提供事實知識和結構化資料來引導許多產品並使它們更加智慧。儘管其好處頗多，但建立商業知識圖譜需要解決結構缺陷和多種模式等令人望而卻步的問題。在本文中，我們加深了對在非平凡的現實系統中建構知識圖譜相關的實際挑戰的理解。我們介紹源自知名企業阿里巴巴集團的開放業務知識圖譜（OpenBG）的建置過程。具體來說，我們定義了一個核心本體來涵蓋各種抽象產品和消費需求，並在已部署的應用程式中提供細粒度的分類和多模式事實。 OpenBG 是一個規模空前的開放商業知識庫：26 億個三元組，超過 8,800 萬個實體，涵蓋超過 100 萬個核心類/概念和 2,681 種關係。我們向社群發布由此衍生的所有開放資源（OpenBG benchmarks），並報告以KG為中心的任務的實驗結果。我們也舉辦了基於 OpenBG 基準的線上競賽，吸引了數千支隊伍。我們進一步對OpenBG進行預訓練，並將其應用到業務場景中許多KG增強的下游任務中，展示了億級多模態知識對於電子商務的有效性。所有帶有程式碼的資源已發佈在https://github.com/OpenBGBenchmark/OpenBG。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\ENXAYD2G\\Deng 等。 - 2023 - Construction and Applications of Billion-Scale Pre.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\LXNXA8MJ\\10184871.html}
}

@article{simonepaoloponzettoTaxonomyInductionBased2011,
  title = {Taxonomy Induction Based on a Collaboratively Built Knowledge Repository},
  author = {{Simone Paolo Ponzetto} and {Michael Strube}},
  date = {2011-06-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {175},
  number = {9},
  pages = {1737--1756},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2011.01.003},
  url = {https://www.sciencedirect.com/science/article/pii/S000437021100004X},
  urldate = {2023-09-19},
  abstract = {The category system in Wikipedia can be taken as a conceptual network. We label the semantic relations between categories using methods based on connectivity in the network and lexico-syntactic matching. The result is a large scale taxonomy. For evaluation we propose a method which (1) manually determines the quality of our taxonomy, and (2) automatically compares its coverage with ResearchCyc, one of the largest manually created ontologies, and the lexical database WordNet. Additionally, we perform an extrinsic evaluation by computing semantic similarity between words in benchmarking datasets. The results show that the taxonomy compares favorably in quality and coverage with broad-coverage manually created resources.},
  langid = {english},
  keywords = {Knowledge acquisition,Lexical semantics,Natural language processing,NLP,未整理,知識本體},
  annotation = {57 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於協作建構的知識庫的分類歸納\\
abstractTranslation:  維基百科中的類別系統可以看作是一個概念網絡。我們使用基於網路連結和詞彙句法匹配的方法來標記類別之間的語義關係。結果是一個大規模的分類。為了進行評估，我們提出了一種方法，該方法（1）手動確定分類法的質量，（2）自動將其覆蓋範圍與 ResearchCyc（最大的手動創建的本體之一）和詞彙資料庫 WordNet 進行比較。此外，我們透過計算基準資料集中單字之間的語義相似性來執行外在評估。結果表明，該分類法在品質和覆蓋範圍方面優於手動創建的廣泛覆蓋範圍的資源。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\F54HHCXK\\Ponzetto 與 Strube - 2011 - Taxonomy induction based on a collaboratively buil.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\9HNJUSY5\\S000437021100004X.html}
}

@article{simonl.jonesDealingInformationOverload2018,
  title = {Dealing {{With Information Overload}} in {{Multifaceted Personal Informatics Systems}}},
  author = {{Simon L. Jones} and {Ryan Kelly}},
  date = {2018-01-02},
  journaltitle = {Human–Computer Interaction},
  shortjournal = {Human–Computer Interaction},
  volume = {33},
  number = {1},
  pages = {1--48},
  issn = {0737-0024, 1532-7051},
  doi = {10.1080/07370024.2017.1302334},
  url = {https://www.tandfonline.com/doi/full/10.1080/07370024.2017.1302334},
  urldate = {2023-10-23},
  abstract = {Personal informatics systems are tools that capture, aggregate, and analyze data from distinct facets of their users’ lives. This article adopts a mixed-methods approach to understand the problem of information overload in personal informatics systems. We report findings from a 3-month study in which 20 participants collected multifaceted personal tracking data and used a system called Exist to reveal statistical correlations within their data. We explore the challenges that participants faced in reviewing the information presented by Exist, and we identify characteristics that exemplify “interesting” correlations. Based on these findings, we develop automated filtering mechanisms that aim to prevent information overload and support users in extracting interesting insights. Our approach deals with information overload by reducing the number of correlations shown to users by about 55\% on average and increases the percentage of displayed correlations rated as interesting to about 81\%, representing a 34 percentage point improvement over filters that only consider statistical significance at p {$<$} .05. We demonstrate how this curation can be achieved using objective data harvested by the system, including the use of Google Trends data as a proxy for subjective user interest.},
  langid = {english},
  keywords = {已整理,微讀,文獻,研究流程,資訊超載,重要},
  annotation = {19 citations (Crossref) [2024-03-26]\\
titleTranslation: 处理多方面个人信息系统中的信息超载问题\\
abstractTranslation:  个人信息系统是一种从用户生活的不同方面获取、汇总和分析数据的工具。本文采用混合方法来了解个人信息系统中的信息超载问题。我们报告了一项为期 3 个月的研究的结果，在这项研究中，20 名参与者收集了多方面的个人跟踪数据，并使用一个名为 Exist 的系统来揭示数据中的统计相关性。我们探讨了参与者在查看 Exist 提供的信息时所面临的挑战，并确定了 "有趣 "相关性的特征。基于这些发现，我们开发了自动过滤机制，旨在防止信息过载并支持用户提取有趣的见解。我们的方法可以解决信息过载问题，将显示给用户的相关性数量平均减少约 55\%，并将显示的相关性中被评为有趣的比例提高到约 81\%，与只考虑 p {$<$} .05 统计显著性的过滤器相比，提高了 34 个百分点。我们演示了如何利用系统采集的客观数据（包括使用谷歌趋势数据作为用户主观兴趣的代理）来实现这种筛选。},
  note = {提出一個關於過濾器的方法，試圖解決資訊中毒。
\par
[TLDR] This article adopts a mixed-methods approach to understand the problem of information overload in personal informatics systems and develops automated filtering mechanisms that aim to prevent information overload and support users in extracting interesting insights.},
  file = {C:\Users\BlackCat\Zotero\storage\PRBJMIWS\Jones and Kelly - 2018 - Dealing With Information Overload in Multifaceted .pdf}
}

@misc{SongFengYufeng-yusungJiYuJiYiKuWenDaDeJiYiFuZhuJiQiRen2023,
  title = {基於記憶庫問答的記憶輔助機器人},
  author = {{宋豐裕(Feng-Yu Sung)}},
  year = {1 月 1, 2023},
  abstract = {在日常生活，人們需要記憶各式各樣的事情，也時常需要作筆記，避免日後遺忘。因此，本論文提出一個基於記憶庫問答的記憶輔助機器人，主要以語音作為輸入，幫助使用者儲存記憶和回憶。  我們所提出系統有多個模組，包含記憶儲存模組、記憶庫問答模組和記憶日曆模組。記憶儲存模組能讓使用者透過語音來儲存記憶，我們的系統會通過語音辨識來得到逐字稿，接著使用多種方法抽取特徵，建立記憶並存儲在記憶庫中。而記憶庫問答模組，目的是讓使用者透過問答的方式回憶。根據問題，我們基於開放領域問答的架構來進行檢索與預測，接著使用一個答案排序的方法來得到最終的答案。除此之外，為了幫助使用者瀏覽記憶，我們提出一個記憶日曆模組，從逐字稿中提取日常事件，並顯示在日曆上。實驗部分，我們使用兩個實驗進行評估，第一個實驗為開放領域問答的實驗，而第二個實驗則是人機互動實驗。},
  langid = {chinese},
  organization = {國立台灣大學學位論文},
  keywords = {問答,已整理,私人,記憶},
  annotation = {titleTranslation: 基於記憶庫問答的記憶輔助機器人}
}

@inproceedings{sorensenInformationtheoreticApproachPrompt2022,
  title = {An {{Information-theoretic Approach}} to {{Prompt Engineering Without Ground Truth Labels}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Sorensen, Taylor and Robinson, Joshua and Rytting, Christopher and Shaw, Alexander and Rogers, Kyle and Delorey, Alexia and Khalil, Mahmoud and Fulda, Nancy and Wingate, David},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  date = {2022-05},
  pages = {819--862},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.60},
  url = {https://aclanthology.org/2022.acl-long.60},
  urldate = {2023-12-06},
  abstract = {Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates without labeled examples and without direct access to the model. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90\% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.},
  eventtitle = {{{ACL}} 2022},
  langid = {english},
  keywords = {LLM,prompt,已整理},
  annotation = {17 citations (Crossref) [2024-04-27]\\
titleTranslation: 一種無需地面真實標籤即可促進工程的資訊理論方法\\
abstractTranslation:  預先訓練的語言模型從訓練它們的大量語料庫中獲取大量的語言和事實知識，而即時工程則力求使這些模型與特定任務保持一致。不幸的是，現有的即時工程方法需要大量的標記資料、存取模型參數或兩者兼而有之。我們引入了一種新方法，用於選擇提示模板，無需標記範例，也無需直接存取模型。具體來說，在一組候選模板上，我們選擇最大化輸入和相應模型輸出之間的互資訊的模板。在代表 7 個不同 NLP 任務的 8 個資料集中，我們表明，當範本具有較高的互資訊時，它在任務上也具有較高的準確性。在最大的模型上，使用我們的方法選擇提示可以獲得從平均提示準確度到最佳提示準確度的 90\%，並且不需要真實標籤。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\8MFQZS3W\\Sorensen et al. - 2022 - An Information-theoretic Approach to Prompt Engine.pdf;D\:\\Paper\\An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels.pdf}
}

@inproceedings{srijonimajumdarEffectiveLowDimensionalSoftware2022,
  title = {An {{Effective Low-Dimensional Software Code Representation}} Using {{BERT}} and {{ELMo}}},
  booktitle = {2022 {{IEEE}} 22nd {{International Conference}} on {{Software Quality}}, {{Reliability}} and {{Security}} ({{QRS}})},
  author = {{Srijoni Majumdar} and {Ashutosh Varshney} and {Partha Pratim Das} and {Paul D Clough} and {Samiran Chattopadhyay}},
  date = {2022-02},
  pages = {763--774},
  issn = {2693-9177},
  doi = {10.1109/QRS57517.2022.00082},
  abstract = {Contextualised word representations (e.g., ELMo and BERT) have been shown to outperform static representations (e.g., Word2vec, Fasttext, and GloVe) for many NLP tasks. In this paper, we investigate the use of contextualised embeddings for code search and classification, an area receiving less attention. We construct CodeELMo by training ELMo from scratch and fine tuning CodeBERT embeddings using masked language modeling based on natural language (NL) texts related to software development concepts and programming language (PL) texts consisting of method comment pairs from open source code bases. The dimensionality of the Finetuned Code BERT embeddings is reduced using linear transformations and augmented with a CodeELMo representation to develop CodeELBE – a lowdimensional contextualised software code representation. Results for binary classification and retrieval tasks show that CodeELBE1 considerably improves retrieval performance on standard deep code search datasets compared to CodeBERT and baseline BERT models.},
  eventtitle = {2022 {{IEEE}} 22nd {{International Conference}} on {{Software Quality}}, {{Reliability}} and {{Security}} ({{QRS}})},
  langid = {english},
  keywords = {Bit error rate,Code Search,CodeBERT,Codes,Contextualised Word Embeddings,ELMo,Security,Software quality,Software reliability,Source coding,Training,已整理,文字處理,機器學習},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用 BERT 和 ELMo 的有效低維軟體程式碼表示\\
abstractTranslation:  對於許多 NLP 任務，上下文化的單字表示（例如 ELMo 和 BERT）已被證明優於靜態表示（例如 Word2vec、Fasttext 和 GloVe）。在本文中，我們研究了上下文嵌入在程式碼搜尋和分類中的使用，這是一個較少受到關注的領域。我們透過從頭開始訓練ELMo 來建構CodeELMo，並使用基於與軟體開發概念相關的自然語言(NL) 文字和由來自開源程式碼庫的方法註解對組成的程式語言(PL) 文字的掩碼語言模型來微調CodeBERT 嵌入。使用線性變換降低了微調程式碼 BERT 嵌入的維數，並使用 CodeELMo 表示進行增強，以開發 CodeELBE——一種低維上下文化軟體程式碼表示。二進位分類和檢索任務的結果表明，與 CodeBERT 和基線 BERT 模型相比，CodeELBE1 顯著提高了標準深度程式碼搜尋資料集的檢索效能。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\EDG5YUS2\\Majumdar 等。 - 2022 - An Effective Low-Dimensional Software Code Represe.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\J7W9XICL\\10062388.html}
}

@book{srijonimajumdarSmartKnowledgeTransfer2023,
  title = {Smart {{Knowledge Transfer}} Using {{Google-like Search}}},
  author = {{Srijoni Majumdar} and {Parthapratim Das}},
  date = {2023-08-12},
  abstract = {To address the issue of rising software maintenance cost due to program comprehension challenges, we propose SMARTKT (Smart Knowledge Transfer), a search framework, which extracts and integrates knowledge related to various aspects of an application in form of a semantic graph. This graph supports syntax and semantic queries and converts the process of program comprehension into a \{\textbackslash em google-like\} search problem.},
  langid = {english},
  keywords = {人機互動,已整理,待讀,知識圖譜},
  annotation = {titleTranslation: 使用類似 Google 的搜尋進行智慧知識轉移\\
abstractTranslation:  為了解決由於程式理解挑戰而導致軟體維護成本上升的問題，我們提出了 SMARTKT（智慧知識轉移），這是一種搜尋框架，它以語義圖的形式提取和整合與應用程式各個方面相關的知識。該圖支援語法和語義查詢，並將程式理解的過程轉換為類似谷歌的搜尋問題。},
  file = {C:\Users\BlackCat\Zotero\storage\MXLDU4A5\Majumdar 與 Das - 2023 - Smart Knowledge Transfer using Google-like Search.pdf}
}

@inproceedings{srijonimajumdarSMARTKTSearchFramework2019,
  title = {{{SMARTKT}}: {{A Search Framework}} to {{Assist Program Comprehension}} Using {{Smart Knowledge Transfer}}},
  shorttitle = {{{SMARTKT}}},
  booktitle = {2019 {{IEEE}} 19th {{International Conference}} on {{Software Quality}}, {{Reliability}} and {{Security}} ({{QRS}})},
  author = {{Srijoni Majumdar} and {Shakti Papdeja} and {Partha Pratim Das} and {Soumya Kanti Ghosh}},
  date = {2019-07},
  pages = {97--108},
  doi = {10.1109/QRS.2019.00026},
  abstract = {Regardless of attempts to extract knowledge from code bases to aid in program comprehension, there is an absence of a framework to extract and integrate knowledge to provide a near-complete multifaceted understanding of a program. To bridge this gap, we propose SMARTKT (Smart Knowledge Transfer) to extract and transfer knowledge related to software development and application-specific characteristics and their interrelationships in form of a knowledge graph. For an application, the knowledge graph provides an overall understanding of the design and implementation and can be used by an intelligent natural language query system to convert the process of knowledge transfer into a developer-friendly Google-like search. For validation, we develop an analyzer to discover concurrency-related design aspects from runtime traces in a machine learning framework and obtain a precision and recall of around 97\% and 95\% respectively. We extract application-specific knowledge from code comments and obtain 72\% match against human-annotated ground truth.},
  eventtitle = {2019 {{IEEE}} 19th {{International Conference}} on {{Software Quality}}, {{Reliability}} and {{Security}} ({{QRS}})},
  langid = {english},
  keywords = {Computer bugs,Data mining,Instruments,Knowledge transfer,Knowledge Transfer Program Comprehension Knowledge Graph Machine Learning Natural Language Processing,Reliability,Runtime,Software,人機互動,已整理,機器學習,知識圖譜,程式碼分析},
  annotation = {7 citations (Crossref) [2024-03-26]\\
titleTranslation: SMARTKT：使用智慧知識轉移輔助程式理解的搜尋框架\\
abstractTranslation:  儘管嘗試從程式碼庫中提取知識來幫助理解程序，但仍缺乏提取和整合知識以提供對程式近乎完整的多方面理解的框架。為了彌補這一差距，我們提出了 SMARTKT（智慧知識轉移），以知識圖的形式提取和轉移與軟體開發和應用程式特定特徵及其相互關係相關的知識。對於應用程式來說，知識圖譜提供了對設計和實現的整體理解，並且可以被智慧自然語言查詢系統使用，將知識轉移的過程轉換為開發人員友好的類似谷歌的搜尋。為了進行驗證，我們開發了一個分析器，用於從機器學習框架中的運行時追蹤中發現與並發相關的設計方面，並分別獲得約 97\% 和 95\% 的精確度和召回率。我們從程式碼註釋中提取特定於應用程式的知識，並獲得與人工註釋的基本事實 72\% 的匹配度。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\CAIMMSTD\\Majumdar 等。 - 2019 - SMARTKT A Search Framework to Assist Program Comp.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\K8ACLZ9J\\8854703.html}
}

@inproceedings{stefanofaralliContrastMediumAlgorithmTaxonomy2017,
  title = {The {{ContrastMedium Algorithm}}: {{Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links}}},
  shorttitle = {The {{ContrastMedium Algorithm}}},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Volume}} 1, {{Long Papers}}},
  author = {{Stefano Faralli} and {Alexander Panchenko} and {Chris Biemann} and {Simone Paolo Ponzetto}},
  date = {2017-04},
  pages = {590--600},
  publisher = {Association for Computational Linguistics},
  location = {Valencia, Spain},
  url = {https://aclanthology.org/E17-1056},
  urldate = {2023-09-19},
  abstract = {In this paper, we present ContrastMedium, an algorithm that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as, for instance, a set of manually selected input root and leaf concepts. This is achieved by leveraging structural information from a companion reference taxonomy, to which the input knowledge graph is linked (either automatically or manually). When used in conjunction with methods for hypernym acquisition and knowledge base linking, our methodology provides a complete solution for end-to-end taxonomy induction. We conduct experiments using automatically acquired knowledge graphs, as well as a SemEval benchmark, and show that our method is able to achieve high performance on the task of taxonomy induction.},
  eventtitle = {{{EACL}} 2017},
  langid = {english},
  keywords = {已整理,機器學習,知識分類,知識圖譜},
  annotation = {titleTranslation: ContrastMedium 演算法：僅用幾個連結從吵雜的知識圖進行分類歸納\\
abstractTranslation:  在本文中，我們提出了 ContrastMedium，一種將嘈雜的語意網路轉換為成熟、乾淨的分類法的演算法。 ContrastMedium 能夠從嘈雜的知識圖中識別嵌入的分類結構，而無需明確的人工監督，例如一組手動選擇的輸入根和葉概念。這是透過利用來自伴隨參考分類的結構資訊來實現的，輸入知識圖連結到該分類（自動或手動）。當與上位詞獲取和知識庫連結方法結合使用時，我們的方法為端到端分類歸納提供了完整的解決方案。我們使用自動取得的知識圖以及 SemEval 基準進行實驗，並表明我們的方法能夠在分類歸納任務上實現高效能。},
  file = {C:\Users\BlackCat\Zotero\storage\CVLLAKU9\Faralli 等。 - 2017 - The ContrastMedium Algorithm Taxonomy Induction F.pdf}
}

@inproceedings{steinl.tomassenOntologydrivenApproachWeb2009,
  title = {An Ontology-Driven Approach to Web Search: Analysis of Its Sensitivity to Ontology Quality and Search Tasks},
  shorttitle = {An Ontology-Driven Approach to Web Search},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Information Integration}} and {{Web-based Applications}} \& {{Services}}},
  author = {{Stein L. Tomassen} and {Darijus Strasunskas}},
  year = {12 月 14, 2009},
  series = {{{iiWAS}} '09},
  pages = {130--138},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1806338.1806368},
  url = {https://dl.acm.org/doi/10.1145/1806338.1806368},
  urldate = {2023-09-15},
  abstract = {An increasing number of recent information retrieval systems makes use of ontologies to help the users to detail queries and to come up with semantic representations of documents. A particular concern here is user-friendliness (usability) and scalability of those approaches for Web search purposes. In this paper, we present an approach where entities in an ontology are associated with domain terminology by feature vectors (FV). A FV reflects the semantic and linguistic neighbourhoods of a particular entity. The semantic neighbourhood is derived from an ontology and is based on related entities and specified properties, while linguistic neighbourhood is based on co-location of terms in a text corpus. Later, during the search process the FVs are used to filter and rerank the search results of the underlying search engine and thereby increasing the precision of the result. We elaborate on the approach and describe how the FVs are constructed. Then we report on a conducted evaluation where we analyse the sensitivity of the approach w.r.t. ontology quality and search tasks. Results indicate that the proposed approach and implemented prototype are able to improve the search results of a standard Web search engine. Furthermore, the analysis of the experiment data shows that the level of ontology specification is important for the quality of the FVs.},
  isbn = {978-1-60558-660-1},
  langid = {english},
  keywords = {/unread,回收,嵌入,知識本體},
  annotation = {1 citations (Crossref) [2024-03-26]\\
abstractTranslation:  最近越來越多的資訊檢索系統利用本體來幫助使用者詳細查詢並提出文件的語義表示。這裡特別值得關注的是這些用於網路搜尋目的的方法的使用者友善性（可用性）和可擴展性。在本文中，我們提出了一種方法，其中本體中的實體透過特徵向量（FV）與領域術語相關聯。 FV 反映了特定實體的語意和語言鄰域。語意鄰域源自本體，並且基於相關實體和指定屬性，而語言鄰域則是基於文本語料庫中術語的共置。隨後，在搜尋過程中，FV 用於對底層搜尋引擎的搜尋結果進行過濾和重新排序，從而提高結果的精確度。我們詳細闡述了該方法並描述了 FV 的建構方式。然後我們報告進行的評估，分析該方法的敏感性。本體品質和搜尋任務。結果表明，所提出的方法和實現的原型能夠改進標準網路搜尋引擎的搜尋結果。此外，對實驗數據的分析表明，本體規範的水平對於FV的品質非常重要。\\
titleTranslation: 本體驅動的網路搜尋方法：分析其對本體品質和搜尋任務的敏感度},
  note = {將知識本體中的詞向量應用於網路搜尋中，並證明了詞向量的重要性。},
  file = {C:\Users\BlackCat\Zotero\storage\V6P98J4L\Tomassen 與 Strasunskas - 2009 - An ontology-driven approach to web search analysi.pdf}
}

@inproceedings{su-shingchenFusionMultimediaInformation2023,
  title = {Fusion of {{Multimedia Information}} in {{Biomedicine}}},
  booktitle = {Advances in {{Multimedia Information Processing}} – {{PCM}} 2007: 8th {{Pacific Rim Conference}} on {{Multimedia}}, {{Hong Kong}}, {{China}}, {{December}} 11-14, 2007. {{Proceedings}}},
  author = {{Su-Shing Chen}},
  year = {3 月 15, 2023},
  pages = {494--500},
  publisher = {Springer-Verlag},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-77255-2_64},
  url = {https://doi.org/10.1007/978-3-540-77255-2_64},
  urldate = {2023-09-26},
  abstract = {Biomedicine is a very rich field of multimedia information. It is also a fruitful ground for information fusion and integration about scientific research data as well as clinical records of digital medical systems. In this paper, we present a global overview of these ideas, which have not been realized so far and could be interesting to the multimedia research community. We exemplify the complex information resources in terms of Gene Ontology (GO), Clinical Bioinformatics Ontology (CBO) and the Foundational Model of Anatomy (FMA). GO is a biomedical scientific research system used to describe genes and gene products, but no cellular components. CBO is a clinical oriented ontology of information, which potentially include many multimedia images: X-Ray, ultrasound and magnetic resonance images. FMA is a foundational clinical source used to describe the anatomy of the human body as well as cellular components. While scientists in each sector may use these systems to help develop their own information, it is very difficult for a layman or broad-spectrum researcher to integrate the two different languages into one interface. We will attempt to address these issues to describe how information fusion can be achieved.},
  isbn = {978-3-540-77254-5},
  keywords = {CBO,FMA,GO,information fusion,Ontology,已整理,知識本體,知識融合,醫學},
  annotation = {0 citations (Crossref) [2024-03-26]}
}

@article{sudhakarangajendranExtractionKnowledgeGraph2023,
  title = {Extraction of Knowledge Graph of {{Covid-19}} through Mining of Unstructured Biomedical Corpora},
  author = {{Sudhakaran Gajendran} and {D. Manjula} and {Vijayan Sugumaran} and {R. Hema}},
  year = {2 月 1, 2023},
  journaltitle = {Computational Biology and Chemistry},
  shortjournal = {Comput. Biol. Chem.},
  volume = {102},
  number = {C},
  issn = {1476-9271},
  doi = {10.1016/j.compbiolchem.2022.107808},
  url = {https://doi.org/10.1016/j.compbiolchem.2022.107808},
  urldate = {2023-09-16},
  abstract = {The number of biomedical articles published is increasing rapidly over the years. Currently there are about 30 million articles in PubMed and over 25 million mentions in Medline. Among these fundamentals, Biomedical Named Entity Recognition (BioNER) and Biomedical Relation Extraction (BioRE) are the most essential in analysing the literature. In the biomedical domain, Knowledge Graph is used to visualize the relationships between various entities such as proteins, chemicals and diseases. Scientific publications have increased dramatically as a result of the search for treatments and potential cures for the new Coronavirus, but efficiently analysing, integrating, and utilising related sources of information remains a difficulty. In order to effectively combat the disease during pandemics like COVID-19, literature must be used quickly and effectively. In this paper, we introduced a fully automated framework consists of BERT-BiLSTM, Knowledge graph, and Representation Learning model to extract the top diseases, chemicals, and proteins related to COVID-19 from the literature. The proposed framework uses Named Entity Recognition models for disease recognition, chemical recognition, and protein recognition. Then the system uses the Chemical - Disease Relation Extraction and Chemical - Protein Relation Extraction models. And the system extracts the entities and relations from the CORD-19 dataset using the models. The system then creates a Knowledge Graph for the extracted relations and entities. The system performs Representation Learning on this KG to get the embeddings of all entities and get the top related diseases, chemicals, and proteins with respect to COVID-19. Display Omitted • A BERT-BiLSTM-CRF model to identify the biomedical named entities. • A SciBERT model to extract the relations between the biomedical entities. • Creating a Knowledge Graph for the COVID – 19 with entities as nodes and relations as edges. • Use of Representation Learning to identify top related drugs, disease and proteins for COVID – 19.},
  langid = {english},
  keywords = {/unread,BERT,BiLSTM,Biomedical Named Entity Recognition (BioNER),Knowledge graph,Relation Extraction (RE),Representation learning,已整理,數據挖掘,知識本體,醫療},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 透過非結構化生物醫學語料挖掘提取Covid-19知識圖譜\\
abstractTranslation:  多年來發表的生物醫學文章數量迅速增加。目前，PubMed 上約有 3,000 萬篇文章，Medline 上有超過 2,500 萬次提及。在這些基礎知識中，生物醫學命名實體辨識（BioNER）和生物醫學關係提取（BioRE）是分析文獻中最重要的。在生物醫學領域，知識圖譜用於可視化蛋白質、化學物質和疾病等各種實體之間的關係。由於尋找新型冠狀病毒的治療方法和潛在療法，科學出版物急劇增加，但有效分析、整合和利用相關資訊來源仍然是一個困難。為了在像 COVID-19 這樣的大流行期間有效地對抗這種疾病，必須快速有效地利用文獻。在本文中，我們介紹了一個由 BERT-BiLSTM、知識圖和表示學習模型組成的全自動框架，用於從文獻中提取與 COVID-19 相關的熱門疾病、化學物質和蛋白質。所提出的框架使用命名實體識別模型進行疾病識別、化學識別和蛋白質識別。然後系統使用化學-疾病關係提取和化學-蛋白質關係提取模型。系統使用模型從 CORD-19 資料集中提取實體和關係。然後系統為提取的關係和實體創建知識圖。該系統在此知識圖譜上執行表示學習，以獲取所有實體的嵌入，並獲取與 COVID-19 相關的最重要的疾病、化學物質和蛋白質。省略顯示 • 用於識別生物醫學命名實體的 BERT-BiLSTM-CRF 模型。 • 用於擷取生物醫學實體之間關係的SciBERT 模型。 • 以實體為節點、以關係為邊建立 COVID-19 知識圖譜。 • 使用表徵學習來識別與 COVID-19 最相關的藥物、疾病和蛋白質。},
  file = {C:\Users\BlackCat\Zotero\storage\TWH443B9\Gajendran 等。 - 2023 - Extraction of knowledge graph of Covid-19 through .pdf}
}

@article{SunChaoZhongYiBingLiShuYuShiBieFangFaTanTao2020,
  title = {中医病历术语识别方法探讨},
  author = {{孙超} and {谢晴宇}},
  date = {2020},
  journaltitle = {中国中医药图书情报杂志},
  shortjournal = {Chinese Journal of Library and Information Science for Traditional Chinese Medicine},
  volume = {44},
  number = {2},
  pages = {1--5},
  doi = {10.3969/j.issn.2095-5707.2020.02.001},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhR6Z3p5eXRzcWJ6ejIwMjAwMjAwMRoIbW0ydzQ5ZDc%3D},
  urldate = {2022-08-14},
  abstract = {目的 探索中医领域利用少量标注语料进行电子病历中医学实体信息的命名实体识别(NER)研究工作,为更复杂的中医电子病历信息处理及深度学习方法在中医领域内的运用提供参考.方法 分析中医电子病历词汇术语与一般的NER任务相比较的特殊性,对比了目前3种NER技术的优缺点,找寻适合中医电子病历医学术语的NER技术.结果 长短时记忆神经网络(LSTM)是一种无监督学习模型,能有效利用序列数据中长距离依赖信息,特别适合处理文本序列数据;还可以和条件随机场(CRF)模型相结合,解决中医NER的难点.长短时记忆神经网络联合条件随机场模型(LSTM-CRF)可以在未标记的病历文本语料上无监督学习词语特征,不依赖于},
  langid = {zh\_CN},
  keywords = {Chinese Journal of Library and Information Science for Traditional Chinese Medicine,中医电子病历,中国中医药图书情报杂志,命名实体识别,条件随机场,谢晴宇,长短时记忆神经网络},
  annotation = {首都医科大学中医药学院,北京 100069中国中医科学院中医临床基础医学研究所,北京 100700\\
北京中医药薪火传承3+3工程崔锡章中医文化传承工作室\\
2020-04-24 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 中醫病歷術語識別方法探討\\
abstractTranslation:  目的探索中醫領域利用少量標註語料進行電子病歷中醫學實體信息的命名實體識別(NER)研究工作，為更複雜的中醫電子病歷信息處理及深度學習方法在中醫領域內的運用提供參考。電子病歷詞彙術語與一般的NER任務相比較的特殊性，對比了目前3種NER技術的優缺點，尋找適合中醫電子病歷醫學術語的NER技術。結果長短時記憶神經網絡（LSTM）是一種無學習模型，能有效利用序列數據中長距離監督信息，特別適合處理文本序列數據；還可以和條件隨機場（CRF）模型相結合，解決中醫NER的難點。長短時記憶神經網絡聯合隨機條件場模型（LSTM- CRF）可以在未標記的病歷文本語言上無監督學習詞彙特徵，不依賴於},
  file = {C:\Users\BlackCat\Zotero\storage\G5BXWZSI\孙 與 谢 - 2020 - 中医病历术语识别方法探讨.pdf}
}

@article{SunJianZhongWenDianZiBingLiWenBenZhongDeShiJianShiBieSuanFaYanJiu2018,
  title = {中文电子病历文本中的时间识别算法研究},
  author = {{孙健} and {高大启} and {刘珉} and {高炬} and {阮彤}},
  date = {2018},
  journaltitle = {山西大学学报（自然科学版）},
  shortjournal = {Journal of Shanxi University(Natural Science Edition)},
  volume = {41},
  number = {1},
  pages = {15--22},
  doi = {10.13451/j.cnki.shanxi.univ(nat.sci.).2018.01.002},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5Eg9zeGR4eGIyMDE4MDEwMDIaCDh2OW9ibHoy},
  urldate = {2022-08-14},
  abstract = {时间作为电子病历中的一类重要实体,对于标识患者从入院到出院期间不同阶段的病情变化,有着不可替代的作用.电子病历文本中的时间可分为独立时间和基于事件的时间,针对这两类时间分别提出了基于boot-strapping的识别算法和基于条件随机场的识别算法.其中,为了解决基于事件的时间短语太长而不能准确定位其边界的问题,引入了中文症状知识库作为词典特征,有效地提高了条件随机场识别结果的准确率、召回率和F1值.实验结果表明,该方法在独立时间和基于事件的时间识别上的F1值分别达到了92.57％和93.98％.},
  langid = {zh\_CN},
  keywords = {bootstrapping algorithm,bootstrapping算法,conditional random field,event-based time,independent time,Journal of Shanxi University(Natural Science Edition),named entity recognition,命名实体识别,基于事件的时间,山西大学学报（自然科学版）,条件随机场,独立时间,高炬},
  annotation = {华东理工大学信息科学与工程学院,上海,200237上海曙光医院,上海,200021\\
国家科技支撑计划 国家高技术研究发展计划（863计划）\\
2018-05-17 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 中文電子病歷文本中的時間識別算法研究\\
abstractTranslation:  時間作為電子病歷中的一類重要實體，對於症狀從入院到出院期間不同階段的病情變化，有著不可替代的作用。電子病歷文本中的時間可劃分獨立時間和基於事件的時間，針對此兩類時間分別提出了基於自舉的識別算法和基於條件隨機場的識別算法。其中，為了解決基於事件的時間片段太長而不能準確定位其邊界的問題，引入了中文症狀知識庫作為詞典特徵,有效地提高了條件隨機場識別結果的準確率、識別率和F1值。實驗結果表明,該方法在獨立時間和基於事件的時間識別上的F1值分別達到了92.57\%和93.98\%。},
  file = {C:\Users\BlackCat\Zotero\storage\ZDYVJWC6\孙 等。 - 2018 - 中文电子病历文本中的时间识别算法研究.pdf}
}

@article{SunYunTiXianBianZhengLunZhiSiXiangDeZhongYiDianZiBingLiXiTongSheJi2019,
  title = {体现辨证论治思想的中医电子病历系统设计},
  author = {{孙赟} and {沈宁乔} and {顾明辰}},
  date = {2019},
  journaltitle = {中国数字医学},
  shortjournal = {China Digital Medicine},
  volume = {14},
  number = {10},
  pages = {47--49},
  doi = {10.3969/j.issn.1673-7571.2019.10.015},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5Eg96Z3N6eXgyMDE5MTAwMTYaCG1tMnc0OWQ3},
  urldate = {2022-08-14},
  abstract = {在中医电子病历系统的设计中应充分体现中医辨证论治的核心思想,运用结构化诊断、临床路径、处方模板、协定处方等手段,构建了具有中医特色的电子病历系统,从而帮助医生在辨证论治的基础上记录和分析患者病情,积累临床经验和提高业务水平.},
  langid = {zh\_CN},
  keywords = {中醫,辨證,電子病歷},
  annotation = {南京中医药大学附属医院,210029,江苏省南京市秦淮区汉中路155号南京中医药大学附属医院,210029,江苏省南京市秦淮区汉中路155号南京中医药大学附属医院,210029,江苏省南京市秦淮区汉中路155号\\
2019-12-16 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 體現辨證論治思想的中醫電子病歷系統設計\\
abstractTranslation:  在中醫電子病歷系統的設計中應充分體現中醫辨證論治的核心思想，運用制定診斷、臨床路徑、處方模板、處方處方等手段，構建了具有中醫特色的電子病歷系統，從而幫助醫生在辨證論治的基礎上上記錄和分析患者病情，積累臨床經驗並提高業務水平。},
  file = {C:\Users\BlackCat\Zotero\storage\WYDFPNJE\孙 等。 - 2019 - 体现辨证论治思想的中医电子病历系统设计.pdf}
}

@online{suOneEmbedderAny2023,
  title = {One {{Embedder}}, {{Any Task}}: {{Instruction-Finetuned Text Embeddings}}},
  shorttitle = {One {{Embedder}}, {{Any Task}}},
  author = {Su, Hongjin and Shi, Weijia and Kasai, Jungo and Wang, Yizhong and Hu, Yushi and Ostendorf, Mari and Yih, Wen-tau and Smith, Noah A. and Zettlemoyer, Luke and Yu, Tao},
  date = {2023-05-30},
  eprint = {2212.09741},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.09741},
  url = {http://arxiv.org/abs/2212.09741},
  urldate = {2024-03-19},
  abstract = {We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are unseen during training), ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than the previous best model, achieves state-of-the-art performance, with an average improvement of 3.4\% compared to the previous best results on the 70 diverse datasets. Our analysis suggests that INSTRUCTOR is robust to changes in instructions, and that instruction finetuning mitigates the challenge of training a single model on diverse datasets. Our model, code, and data are available at https://instructor-embedding.github.io.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,嵌入,機器學習},
  annotation = {titleTranslation: 一個嵌入器，任何任務：指令微調文字嵌入\\
abstractTranslation:  我們引入了 INSTRUCTOR，一種計算給定任務指令的文字嵌入的新方法：每個文字輸入都與解釋用例的指令（例如任務和領域描述）嵌入在一起。與先前工作中更專業的編碼器不同，INSTRUCTOR 是一個單一的嵌入器，可以產生針對不同下游任務和領域的文本嵌入，無需任何進一步的培訓。我們首先對 330 個不同任務的指令進行註釋，並在這種多任務混合上使用對比損失來訓練 INSTRUCTOR。我們在 70 個嵌入評估任務（其中 66 個在訓練過程中看不見）上評估 INSTRUCTOR，範圍從分類和資訊檢索到語義文本相似性和文本生成評估。 INSTRUCTOR 雖然參數比之前的最佳模型少了一個數量級，但卻實現了最先進的性能，與之前在 70 個不同數據集上的最佳結果相比，平均提高了 3.4\%。我們的分析表明，INSTRUCTOR 對指令的變化具有穩健性，且指令微調減輕了在不同資料集上訓練單一模型的挑戰。我們的模型、程式碼和資料可在 https://instructor-embedding.github.io 上取得。},
  note = {Comment: Accepted in ACL2023 Findings},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\T38YUNYU\\Su 等。 - 2023 - One Embedder, Any Task Instruction-Finetuned Text.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\P7T754U2\\2212.html}
}

@inproceedings{susangauchUserProfilesPersonalized2007,
  title = {User {{Profiles}} for {{Personalized Information Access}}},
  booktitle = {The {{Adaptive Web}}},
  author = {{Susan Gauch} and {Mirco Speretta} and {Aravind Chandramouli} and {Alessandro Micarelli}},
  editor = {{Peter Brusilovsky} and {Alfred Kobsa} and {Wolfgang Nejdl}},
  date = {2007},
  volume = {4321},
  pages = {54--89},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-72079-9_2},
  url = {http://link.springer.com/10.1007/978-3-540-72079-9_2},
  urldate = {2023-10-23},
  abstract = {The amount of information available online is increasing exponentially. While this information is a valuable resource, its sheer volume limits its value. Many research projects and companies are exploring the use of personalized applications that manage this deluge by tailoring the information presented to individual users. These applications all need to gather, and exploit, some information about individuals in order to be effective. This area is broadly called user profiling. This chapter surveys some of the most popular techniques for collecting information about users, representing, and building user profiles. In particular, explicit information techniques are contrasted with implicitly collected user information using browser caches, proxy servers, browser agents, desktop agents, and search logs. We discuss in detail user profiles represented as weighted keywords, semantic networks, and weighted concepts. We review how each of these profiles is constructed and give examples of projects that employ each of these techniques. Finally, a brief discussion of the importance of privacy protection in profiling is presented.},
  isbn = {978-3-540-72078-2},
  langid = {english},
  annotation = {259 citations (Crossref) [2024-03-26]\\
titleTranslation: 个性化信息访问的用户配置文件\\
abstractTranslation:  在线信息量呈指数级增长。虽然这些信息是宝贵的资源，但其数量限制了其价值。许多研究项目和公司都在探索使用个性化应用程序，通过为个人用户提供量身定制的信息来管理这些海量信息。这些应用程序都需要收集和利用一些有关个人的信息，以便发挥效用。这一领域被广泛称为用户特征分析。本章将介绍一些最常用的收集用户信息、代表用户和建立用户档案的技术。其中，显式信息技术与使用浏览器缓存、代理服务器、浏览器代理、桌面代理和搜索日志等隐式收集用户信息的技术形成了鲜明对比。我们详细讨论了以加权关键词、语义网络和加权概念表示的用户配置文件。我们回顾了每种配置文件的构建方法，并列举了采用每种技术的项目实例。最后，我们将简要讨论隐私保护在剖析中的重要性。},
  note = {[TLDR] This chapter surveys some of the most popular techniques for collecting information about users, representing, and building user profiles and discusses in detail user profiles represented as weighted keywords, semantic networks, and weighted concepts.
\par
[TLDR] This chapter surveys some of the most popular techniques for collecting information about users, representing, and building user profiles and discusses in detail user profiles represented as weighted keywords, semantic networks, and weighted concepts.}
}

@article{suyamaMachineLearningTCM2023,
  title = {Machine Learning in {{TCM}} with Natural Products and Molecules: Current Status and Future Perspectives},
  shorttitle = {Machine Learning in {{TCM}} with Natural Products and Molecules},
  author = {{Suya Ma} and {Jinlei Liu} and {Wenhua Li} and {Yongmei Liu} and {Xiaoshan Hui} and {Peirong Qu} and {Zhilin Jiang} and {Jun Li} and {Jie Wang}},
  date = {2023-04-20},
  journaltitle = {Chinese Medicine},
  shortjournal = {Chinese Medicine},
  volume = {18},
  number = {1},
  pages = {43},
  issn = {1749-8546},
  doi = {10.1186/s13020-023-00741-9},
  url = {https://doi.org/10.1186/s13020-023-00741-9},
  urldate = {2023-09-15},
  abstract = {Traditional Chinese medicine (TCM) has been practiced for thousands of years with clinical efficacy. Natural products and their effective agents such as artemisinin and paclitaxel have saved millions of lives worldwide. Artificial intelligence is being increasingly deployed in TCM. By summarizing the principles and processes of deep learning and traditional machine learning algorithms, analyzing the application of machine learning in TCM, reviewing the results of previous studies, this study proposed a promising future perspective based on the combination of machine learning, TCM theory, chemical compositions of natural products, and computational simulations based on molecules and chemical compositions. In the first place, machine learning will be utilized in the effective chemical components of natural products to target the pathological molecules of the disease which could achieve the purpose of screening the natural products on the basis of the pathological mechanisms they target. In this approach, computational simulations will be used for processing the data for effective chemical components, generating datasets for analyzing features. In the next step, machine learning will be used to analyze the datasets on the basis of TCM theories such as the superposition of syndrome elements. Finally, interdisciplinary natural product-syndrome research will be established by unifying the results of the two steps outlined above, potentially realizing an intelligent artificial intelligence diagnosis and treatment model based on the effective chemical components of natural products under the guidance of TCM theory. This perspective outlines an innovative application of machine learning in the clinical practice of TCM based on the investigation of chemical molecules under the guidance of TCM theory.},
  langid = {english},
  keywords = {Chemical components,Multidisciplinary intersection,Natural products,Survey,中醫,已整理,機器學習},
  annotation = {4 citations (Crossref) [2024-03-26]\\
titleTranslation: 天然產物和分子的中醫機器學習：現況與未來前景\\
abstractTranslation:  中醫藥已有數千年的實務歷史，具有臨床療效。天然產物及其有效藥物（如青蒿素和紫杉醇）拯救了全世界數百萬人的生命。人工智慧在中醫領域的應用越來越廣泛。本研究透過總結深度學習和傳統機器學習演算法的原理和流程，分析機器學習在中醫的應用，回顧前人的研究成果，提出了基於機器學習、中醫理論、化學等相結合的未來前景。產物的成分以及基於分子和化學成分的計算模擬。首先，利用機器學習在天然產物的有效化學成分中，針對疾病的病理分子，達到根據其針對的病理機制篩選天然產物的目的。在這種方法中，計算模擬將用於處理有效化學成分的數據，產生用於分析特徵的數據集。下一步，將根據證候疊加等中醫理論，利用機器學習對資料集進行分析。最後，將上述兩個步驟的成果結合起來，建立跨學科的天然產物證候研究，可望實現在中醫理論指導下，基於天然產物有效化學成分的智慧人工智慧診療模式。該觀點概述了在中醫理論指導下基於化學分子研究的機器學習在中醫臨床實踐中的創新應用。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\XDXGYUID\\Ma 等。 - 2023 - Machine learning in TCM with natural products and .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\PMBL4BBN\\s13020-023-00741-9.html}
}

@inproceedings{ta-fuchenQuestionAnsweringSystem2023,
  title = {Question {{Answering System Based}} on {{Pre-Training Model}} and {{Retrieval Reranking}} for {{Industry}} 4.0},
  booktitle = {2023 {{Asia Pacific Signal}} and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA ASC}})},
  author = {{Ta-Fu Chen} and {Yi-Xing Lin} and {Ming-Hsiang Su} and {Po-Kai Chen} and {Tzu-Chiang Tai} and {Jia-Ching Wang}},
  date = {2023-10},
  pages = {2178--2181},
  issn = {2640-0103},
  doi = {10.1109/APSIPAASC58517.2023.10317470},
  url = {https://ieeexplore.ieee.org/document/10317470},
  urldate = {2023-11-23},
  abstract = {By providing enhanced knowledge retrieval capabilities, real-time decision support, and efficient information exchange, question answering (QA) systems play a crucial role in driving productivity, efficiency, and innovation in Industry 4.0. Today's most reliable knowledge-based QA systems require a large knowledge base, which tends to consume more reasoning time. In order to improve the inference speed and response accuracy of the system, this paper adds a Reranker between the Retriever and Reader of the traditional two-stage mechanism. This study uses pretrained Roberta to perform system retrieval and improve data processing and training methods. Experiments on Chinese Wikipedia show that the proposed system significantly reduces the system response time and improves the accuracy and scope of the response.},
  eventtitle = {2023 {{Asia Pacific Signal}} and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA ASC}})},
  langid = {english},
  keywords = {Fourth Industrial Revolution,Knowledge based systems,Pre-training model,QA system,Question answering (information retrieval),Reranking,Semantics,Technological innovation,Training,Training data,中文,問答系統,問題分析,已整理,機器學習,重要},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於預訓練模型與檢索重排序的工業4.0問答系統},
  note = {本研究參考他人做法，強化了問題分析訓練時的資料，僅有些微的提升。
\par
但是在該他人做法方面可能對研究有幫助。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\DG6PGTLV\\Chen et al. - 2023 - Question Answering System Based on Pre-Training Mo.pdf;D\:\\Paper\\Question_Answering_System_Based_on_Pre-Training_Model_and_Retrieval_Reranking_for_Industry_4.0.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\TD3T76I5\\10317470.html}
}

@article{tadashinomotoKeywordExtractionModern2022,
  title = {Keyword {{Extraction}}: {{A Modern Perspective}}},
  shorttitle = {Keyword {{Extraction}}},
  author = {{Tadashi Nomoto}},
  date = {2022-12-15},
  journaltitle = {SN Computer Science},
  shortjournal = {SN COMPUT. SCI.},
  volume = {4},
  number = {1},
  pages = {92},
  issn = {2661-8907},
  doi = {10.1007/s42979-022-01481-7},
  url = {https://link.springer.com/10.1007/s42979-022-01481-7},
  urldate = {2023-10-23},
  abstract = {Abstract             The goal of keyword extraction is to extract from a text, words, or phrases indicative of what it is talking about. In this work, we look at keyword extraction from a number of different perspectives: Statistics, Automatic Term Indexing, Information Retrieval (IR), Natural Language Processing (NLP), and the emerging Neural paradigm. The 1990s have seen some early attempts to tackle the issue primarily based on text statistics [13, 17]. Meanwhile, in IR, efforts were largely led by DARPA’s Topic Detection and Tracking (TDT) project [2]. In this contribution, we discuss how past innovations paved a way for more recent developments, such as LDA, PageRank, and Neural Networks. We walk through the history of keyword extraction over the last 50 years, noting differences and similarities among methods that emerged during the time. We conduct a large meta-analysis of the past literature using datasets from news media, science, and medicine to business and bureaucracy, to draw a general picture of what a successful approach would look like.},
  langid = {english},
  keywords = {關鍵字},
  annotation = {4 citations (Crossref) [2024-03-26]\\
titleTranslation: 关键词提取：现代视角\\
abstractTranslation:  摘要 关键字提取的目的是从文本中提取能表明文本内容的单词或短语。在这项工作中，我们从多个不同的角度来研究关键词提取：统计、自动术语索引、信息检索 (IR)、自然语言处理 (NLP) 以及新兴的神经范式。20 世纪 90 年代，一些早期尝试主要基于文本统计来解决这一问题[13, 17]。同时，在 IR 方面，主要由 DARPA 的主题检测和跟踪 (TDT) 项目 [2] 领导了相关工作。在本文中，我们将讨论过去的创新是如何为 LDA、PageRank 和神经网络等最新发展铺平道路的。我们回顾了过去 50 年关键词提取的历史，并指出了在此期间出现的各种方法之间的异同。我们利用从新闻媒体、科学、医学到商业和官僚机构的数据集，对过去的文献进行了大量的元分析，从而勾勒出成功方法的大致轮廓。},
  note = {[TLDR] In this work, a large meta-analysis of the past literature is conducted using datasets from news media, science, and medicine to business and bureaucracy, to draw a general picture of what a successful approach would look like.},
  file = {C:\Users\BlackCat\Zotero\storage\3GUUWG42\Nomoto - 2022 - Keyword Extraction A Modern Perspective.pdf}
}

@online{taffaLeveragingLLMsScholarly2023,
  title = {Leveraging {{LLMs}} in {{Scholarly Knowledge Graph Question Answering}}},
  author = {Taffa, Tilahun Abedissa and Usbeck, Ricardo},
  date = {2023-11-16},
  eprint = {2311.09841},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.09841},
  url = {http://arxiv.org/abs/2311.09841},
  urldate = {2023-11-28},
  abstract = {This paper presents a scholarly Knowledge Graph Question Answering (KGQA) that answers bibliographic natural language questions by leveraging a large language model (LLM) in a few-shot manner. The model initially identifies the top-n similar training questions related to a given test question via a BERT-based sentence encoder and retrieves their corresponding SPARQL. Using the top-n similar question-SPARQL pairs as an example and the test question creates a prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs the SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and returns an answer. Our system achieves an F1 score of 99.0\%, on SciQA - one of the Scholarly-QALD-23 challenge benchmarks.},
  langid = {english},
  pubstate = {preprint},
  keywords = {BERT,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Machine Learning,LLM,ORKG,SPARQL,問答系統,已整理,待讀,機器學習,知識圖譜,重要},
  annotation = {titleTranslation: 利用法學碩士進行學術知識圖問答\\
abstractTranslation:  本文提出了一種學術知識圖問答（KGQA），它透過利用大型語言模型（LLM）以少量的方式回答書目自然語言問題。該模型最初透過基於 BERT 的句子編碼器識別與給定測試問題相關的前 n 個相似訓練問題，並檢索其相應的 SPARQL。先前 n 個相似問題-SPARQL 對為例，測試問題會建立一個提示。然後將提示傳遞給 LLM 並產生 SPARQL。最後，針對底層 KG - ORKG (Open Research KG) 端點執行 SPARQL 並回傳答案。我們的系統在 SciQA（Scholarly-QALD-23 挑戰基準之一）上獲得了 99.0\% 的 F1 分數。},
  note = {用BERT挑選最接近的問題模版，用few-shot交給Vicuna-13B來生成自然問題對應的SPARQL。仍有語法、關鍵字匹配、複雜問題等問題需要解決。
\par
但仍然在SciQA資料集上取得0.99F1的第二名成績。
\par
可以做為方法的主要參考。},
  file = {/home/domaj/paper/Leveraging LLMs in Scholarly Knowledge Graph Question Answering.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\M4NUN2VK\\Leveraging LLMs in Scholarly Knowledge Graph Question Answering.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\TY5WUE5L\\2311.html}
}

@inproceedings{tanCanChatGPTReplace2023,
  title = {Can {{ChatGPT Replace Traditional KBQA Models}}? {{An~In-Depth Analysis}} of~the~{{Question Answering Performance}} of~the~{{GPT LLM Family}}},
  shorttitle = {Can {{ChatGPT Replace Traditional KBQA Models}}?},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2023},
  author = {Tan, Yiming and Min, Dehai and Li, Yu and Li, Wenbo and Hu, Nan and Chen, Yongrui and Qi, Guilin},
  editor = {Payne, Terry R. and Presutti, Valentina and Qi, Guilin and Poveda-Villalón, María and Stoilos, Giorgos and Hollink, Laura and Kaoudi, Zoi and Cheng, Gong and Li, Juanzi},
  date = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {348--367},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-47240-4_19},
  abstract = {ChatGPT is a powerful large language model (LLM) that covers knowledge resources such as Wikipedia and supports natural language question answering using its own knowledge. Therefore, there is growing interest in exploring whether ChatGPT can replace traditional knowledge-based question answering (KBQA) models. Although there have been some works analyzing the question answering performance of ChatGPT, there is still a lack of large-scale, comprehensive testing of various types of complex questions to analyze the limitations of the model. In this paper, we present a framework that follows the black-box testing specifications of CheckList proposed by [38]. We evaluate ChatGPT and its family of LLMs on eight real-world KB-based complex question answering datasets, which include six English datasets and two multilingual datasets. The total number of test cases is approximately 190,000. In addition to the GPT family of LLMs, we also evaluate the well-known FLAN-T5 to identify commonalities between the GPT family and other LLMs. The dataset and code are available at https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git.},
  isbn = {978-3-031-47240-4},
  langid = {english},
  keywords = {Black-box testing,ChatGPT,Complex question answering,Evaluation,Knowledge base,Large language model,LLM,問答系統,已整理,待讀,機器學習,知識圖譜,重要},
  annotation = {5 citations (Crossref) [2024-04-27]\\
titleTranslation: ChatGPT 可以取代傳統的 KBQA 模型嗎？ GPT LLM家族答疑表現深度剖析\\
abstractTranslation:  ChatGPT是一個強大的大語言模型（LLM），涵蓋了維基百科等知識資源，並支援利用自身知識進行自然語言問答。因此，人們越來越有興趣探索 ChatGPT 是否可以取代傳統的知識為基礎的問答（KBQA）模型。儘管已經有一些工作分析ChatGPT的問答性能，但仍缺乏對各類複雜問題的大規模、全面的測試來分析模型的局限性。在本文中，我們提出了一個遵循[38]提出的 CheckList 黑盒測試規範的架構。我們在八個現實世界中基於知識庫的複雜問答資料集（包括六個英語資料集和兩個多語言資料集）上評估 ChatGPT 及其法學碩士系列。測試用例總數約19萬個。除了 GPT 法學碩士家族之外，我們還評估了著名的 FLAN-T5，以確定 GPT 家族與其他法學碩士之間的共通點。資料集和程式碼可在 https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git 取得。},
  note = {比對不同LLM在多個KG問答測試集上的表現，並運用COT來增強LLM的搜尋效果。},
  file = {C:\Users\BlackCat\Zotero\storage\CVEL22GP\Tan 等。 - 2023 - Can ChatGPT Replace Traditional KBQA Models An In.pdf}
}

@article{TangLuJieHeDianZiBingLiZhiDaoGuiPeiYiShiZhongYiNeiKeBingLiShuXieDeTiHui2018,
  title = {结合电子病历指导规培医师中医内科病历书写的体会},
  author = {{唐璐} and {高颖}},
  date = {2018},
  journaltitle = {中国中医药现代远程教育},
  volume = {16},
  number = {5},
  pages = {4},
  abstract = {住院醫師規范化培訓是醫學生畢業后教育的重要組成部分,占據了醫學終生教育承前啟后的重要地位.病歷書寫是規培醫師的日常工作之一,一份病歷質量的高低,不僅體現規培醫師的醫學基礎理論水平,更能反映出其對專業知識的認識和應用能力 加強對規培醫師的書寫指導,既可以加深對疾病的認識和理解,又可以培養臨床思維能力,從而提高住院醫師范化培訓的效果.我們在臨床帶教過程中,應及時發現他們在病歷書寫過程中存在的問題并給予詳細指導,不斷提高其書寫能力,從而促進其更好的掌握專科知識.},
  keywords = {中醫,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\EP8RM9Z6\唐璐 與 高颖 - 2018 - 结合电子病历指导规培医师中医内科病历书写的体会.pdf}
}

@inproceedings{tarasafaviRelationalWorldKnowledge2021,
  title = {Relational {{World Knowledge Representation}} in {{Contextual Language Models}}: {{A Review}}},
  shorttitle = {Relational {{World Knowledge Representation}} in {{Contextual Language Models}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {{Tara Safavi} and {Danai Koutra}},
  date = {2021-01},
  pages = {1053--1067},
  publisher = {Association for Computational Linguistics},
  location = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.81},
  url = {https://aclanthology.org/2021.emnlp-main.81},
  urldate = {2023-09-19},
  abstract = {Relational knowledge bases (KBs) are commonly used to represent world knowledge in machines. However, while advantageous for their high degree of precision and interpretability, KBs are usually organized according to manually-defined schemas, which limit their expressiveness and require significant human efforts to engineer and maintain. In this review, we take a natural language processing perspective to these limitations, examining how they may be addressed in part by training deep contextual language models (LMs) to internalize and express relational knowledge in more flexible forms. We propose to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision. Our contributions are threefold: (1) We provide a high-level, extensible taxonomy for knowledge representation in LMs; (2) Within our taxonomy, we highlight notable models, evaluation tasks, and findings, in order to provide an up-to-date review of current knowledge representation capabilities in LMs; and (3) We suggest future research directions that build upon the complementary aspects of LMs and KBs as knowledge representations.},
  eventtitle = {{{EMNLP}} 2021},
  langid = {english},
  keywords = {LM,已整理,機器學習,知識詞譜},
  annotation = {6 citations (Crossref) [2024-03-26]\\
titleTranslation: 上下文語言模型中的關係世界知識表示：回顧\\
abstractTranslation:  關係知識庫 (KB) 通常用於表示機器中的世界知識。然而，雖然知識庫具有高度的精確性和可解釋性，但它通常是根據手動定義的模式進行組織的，這限制了它們的表達能力，並且需要大量的人力來設計和維護。在這篇綜述中，我們從自然語言處理的角度來看待這些限制，研究如何透過訓練深層情境語言模型（LM）來以更靈活的形式內化和表達關係知識來部分解決這些限制。我們建議根據提供的知識庫監督層級來組織語言模型中的知識表示策略，從根本沒有知識庫監督到實體和關係層級的監督。我們的貢獻有三：（1）我們為 LM 中的知識表示提供了一個高階的、可擴展的分類法； (2) 在我們的分類中，我們突出顯示值得注意的模型、評估任務和發現，以便對 LM 中當前的知識表示能力提供最新的審查； (3) 我們建議未來的研究方向，這些方向建立在 LM 和 KB 作為知識表示的互補面向。},
  file = {C:\Users\BlackCat\Zotero\storage\4WN6V8TM\Safavi 與 Koutra - 2021 - Relational World Knowledge Representation in Conte.pdf}
}

@online{tchangoDDXPlusNewDataset2022,
  title = {{{DDXPlus}}: {{A New Dataset For Automatic Medical Diagnosis}}},
  shorttitle = {{{DDXPlus}}},
  author = {Tchango, Arsene Fansi and Goel, Rishab and Wen, Zhi and Martel, Julien and Ghosn, Joumana},
  date = {2022-10-13},
  eprint = {2205.09148},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.09148},
  url = {http://arxiv.org/abs/2205.09148},
  urldate = {2024-01-13},
  abstract = {There has been a rapidly growing interest in Automatic Symptom Detection (ASD) and Automatic Diagnosis (AD) systems in the machine learning research literature, aiming to assist doctors in telemedicine services. These systems are designed to interact with patients, collect evidence about their symptoms and relevant antecedents, and possibly make predictions about the underlying diseases. Doctors would review the interactions, including the evidence and the predictions, collect if necessary additional information from patients, before deciding on next steps. Despite recent progress in this area, an important piece of doctors' interactions with patients is missing in the design of these systems, namely the differential diagnosis. Its absence is largely due to the lack of datasets that include such information for models to train on. In this work, we present a large-scale synthetic dataset of roughly 1.3 million patients that includes a differential diagnosis, along with the ground truth pathology, symptoms and antecedents for each patient. Unlike existing datasets which only contain binary symptoms and antecedents, this dataset also contains categorical and multi-choice symptoms and antecedents useful for efficient data collection. Moreover, some symptoms are organized in a hierarchy, making it possible to design systems able to interact with patients in a logical way. As a proof-of-concept, we extend two existing AD and ASD systems to incorporate the differential diagnosis, and provide empirical evidence that using differentials as training signals is essential for the efficiency of such systems or for helping doctors better understand the reasoning of those systems.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,OpenReview,已整理,西醫,資料集},
  annotation = {titleTranslation: DDXPlus：用於自動醫療診斷的新資料集},
  note = {一個法文辨證資料集。在github上有英文版的資料集。
\par
Comment: Camera ready. NeurIPS 2022 Datasets and Benchmarks Track},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\ND25CKPU\\Tchango 等。 - 2022 - DDXPlus A New Dataset For Automatic Medical Diagn.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\H94SAHWT\\2205.html}
}

@online{thakurBEIRHeterogenousBenchmark2021,
  title = {{{BEIR}}: {{A Heterogenous Benchmark}} for {{Zero-shot Evaluation}} of {{Information Retrieval Models}}},
  shorttitle = {{{BEIR}}},
  author = {Thakur, Nandan and Reimers, Nils and Rücklé, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
  date = {2021-10-20},
  eprint = {2104.08663},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.08663},
  url = {http://arxiv.org/abs/2104.08663},
  urldate = {2024-03-07},
  abstract = {Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir.},
  langid = {english},
  pubstate = {preprint},
  keywords = {benchmark,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,嵌入,機器學習,資料搜索,重要},
  annotation = {titleTranslation: BEIR：資訊檢索模型零樣本評估的異質基準},
  note = {Comment: Accepted at NeurIPS 2021 Dataset and Benchmark Track},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\4BT5FUMN\\Thakur 等。 - 2021 - BEIR A Heterogenous Benchmark for Zero-shot Evalu.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\FXKWNL7W\\2104.html}
}

@article{thanosExploratoryApproachData2023,
  title = {An Exploratory Approach to Data Driven Knowledge Creation},
  author = {Thanos, Costantino and Meghini, Carlo and Bartalesi, Valentina and Coro, Gianpaolo},
  date = {2023-03-06},
  journaltitle = {Journal of Big Data},
  shortjournal = {Journal of Big Data},
  volume = {10},
  number = {1},
  pages = {29},
  issn = {2196-1115},
  doi = {10.1186/s40537-023-00702-x},
  url = {https://doi.org/10.1186/s40537-023-00702-x},
  urldate = {2024-01-17},
  abstract = {This paper describes a new approach to knowledge creation that is instrumental for the emerging paradigm of data-intensive science. The proposed approach enables the acquisition of new insights from the data by exploiting existing relationships between diverse types of datasets acquired through various modalities. The value of data consistently improves when it can be linked to other data because linking multiple types of datasets allows creating novel data patterns within a scientific data space. These patterns enable the exploratory data analysis, an analysis strategy that emphasizes incremental and adaptive access to the datasets constituting a scientific data space while maintaining an open mind to alternative possibilities of data interconnectivity. A technology, the Linked Open data (LOD), was developed to enable the linking of datasets. We argue that the LOD technology presents several limitations that prevent the full exploitation of this technology to acquire new insights. In this paper, we outline a new approach that enables researchers to dynamically create data patterns in a research data space by exploiting explicit and implicit/hidden relationships between distributed research datasets. This dynamic creation of data patterns enables the exploratory data analysis strategy.},
  langid = {english},
  keywords = {Data analyzer,Data exploration,Data patterns,Data publication,Data relationships,未整理},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 數據驅動知識創造的探索性方法\\
abstractTranslation:  本文描述了一種新的知識創造方法，有助於新興的資料密集型科學範式。所提出的方法能夠透過利用透過各種方式取得的不同類型的資料集之間的現有關係，從資料中獲得新的見解。當資料可以連結到其他資料時，資料的價值會不斷提高，因為連結多種類型的資料集允許在科學資料空間內創建新穎的資料模式。這些模式支持探索性數據分析，這種分析策略強調對構成科學數據空間的數據集進行增量和自適應訪問，同時對數據互連的替代可能性保持開放的態度。開發了連結開放資料 (LOD) 技術來實現資料集的連結。我們認為，LOD 技術存在一些限制，阻礙了充分利用該技術來獲得新的見解。在本文中，我們概述了一種新方法，使研究人員能夠透過利用分散式研究資料集之間的顯式和隱式/隱藏關係，在研究資料空間中動態創建資料模式。這種資料模式的動態創建支援探索性資料分析策略。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\D89R5Z5F\\Thanos et al. - 2023 - An exploratory approach to data driven knowledge c.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\3254IWLT\\s40537-023-00702-x.html}
}

@inproceedings{thiliniwijesiriwardeneANALOGICALNovelBenchmark2023,
  title = {{{ANALOGICAL}} - {{A Novel Benchmark}} for {{Long Text Analogy Evaluation}} in {{Large Language Models}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2023},
  author = {{Thilini Wijesiriwardene} and {Ruwan Wickramarachchi} and {Bimal Gajera} and {Shreeyash Gowaikar} and {Chandan Gupta} and {Aman Chadha} and {Aishwarya Naresh Reganti} and {Amit Sheth} and {Amitava Das}},
  date = {2023-07},
  pages = {3534--3549},
  publisher = {Association for Computational Linguistics},
  location = {Toronto, Canada},
  doi = {10.18653/v1/2023.findings-acl.218},
  url = {https://aclanthology.org/2023.findings-acl.218},
  urldate = {2023-09-19},
  abstract = {Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity – (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space. Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.},
  eventtitle = {Findings 2023},
  langid = {english},
  keywords = {LLM,已整理,機器學習,測試框架},
  annotation = {1 citations (Crossref) [2024-04-27]\\
titleTranslation: ANALOGICAL - 大型語言模型中長文本類比評估的新基準\\
abstractTranslation:  在過去的十年中，詞級類比形式的類比作為評估詞嵌入方法（例如 word2vec）品質的內在衡量標準發揮了重要作用。然而，現代大語言模型（LLM）主要是根據基於 GLUE 和 SuperGLUE 等基準的外在測量進行評估，並且關於 LLM 是否可以在長文本之間進行類比的研究很少。在本文中，我們提出了ANALOGICAL，這是一個新的基準，用於對具有六個複雜程度的長文本類比分類進行內在評估法學碩士- (i) 單詞，(ii) 單詞與句子，(iii)句法，(iv)否定，(v) 蘊涵，(vi) 隱喻。使用十三個資料集和三個不同的距離度量，我們評估了八個法學碩士在語義向量空間中識別類比對的能力。我們的評估發現，法學碩士在進行類比分類時識別類比變得越來越具有挑戰性。},
  file = {C:\Users\BlackCat\Zotero\storage\BELWV3J3\Wijesiriwardene 等。 - 2023 - ANALOGICAL - A Novel Benchmark for Long Text Analo.pdf}
}

@article{thomasfritzDegreeofknowledgeModelingDeveloper2014,
  title = {Degree-of-Knowledge: {{Modeling}} a Developer's Knowledge of Code},
  shorttitle = {Degree-of-Knowledge},
  author = {{Thomas Fritz} and {Gail C. Murphy} and {Emerson Murphy-Hill} and {Jingwen Ou} and {Emily Hill}},
  year = {4 月 4, 2014},
  journaltitle = {ACM 軟件工程和方法學彙刊},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  volume = {23},
  number = {2},
  pages = {14:1--14:42},
  issn = {1049-331X},
  doi = {10.1145/2512207},
  url = {https://dl.acm.org/doi/10.1145/2512207},
  urldate = {2023-08-24},
  abstract = {As a software system evolves, the system's codebase constantly changes, making it difficult for developers to answer such questions as who is knowledgeable about particular parts of the code or who needs to know about changes made. In this article, we show that an externalized model of a developer's individual knowledge of code can make it easier for developers to answer such questions. We introduce a degree-of-knowledge model that computes automatically, for each source-code element in a codebase, a real value that represents a developer's knowledge of that element based on a developer's authorship and interaction data. We present evidence that shows that both authorship and interaction data of the code are important in characterizing a developer's knowledge of code. We report on the usage of our model in case studies on expert finding, knowledge transfer, and identifying changes of interest. We show that our model improves upon an existing expertise-finding approach and can accurately identify changes for which a developer should likely be aware. We discuss how our model may provide a starting point for knowledge transfer but that more refinement is needed. Finally, we discuss the robustness of the model across multiple development sites.},
  langid = {english},
  keywords = {Authorship,degree-of-interest,degree-of-knowledge,development environment,expertise,onboarding,recommendation,待讀,知識圖譜,知識本體,程式碼分析,重要},
  annotation = {48 citations (Crossref) [2024-03-26]\\
titleTranslation: 知識程度：對開發人員的代碼知識進行建模\\
abstractTranslation:  隨著軟件系統的發展，系統的代碼庫不斷變化，這使得開發人員很難回答諸如誰了解代碼的特定部分或誰需要了解所做的更改等問題。在本文中，我們展示了開發人員個人代碼知識的外部化模型可以使開發人員更輕鬆地回答此類問題。我們引入了一種知識程度模型，該模型可以根據開發人員的作者身份和交互數據，針對代碼庫中的每個源元素代碼自動計算一個真實的價值，該價值代表開發人員的元素的理解。我們提供了證據表明，我們的作者身份和交互數據對於表徵開發人員的代碼知識非常重要。我們報告了專家發現的模型、知識轉移表明和識別興趣變化的案例研究中的使用情況。我們的模型改進了我們現有的專業知識查找方法，並且可以準確地識別開發人員可能應該認識到的變化。討論了我們的模型如何獲取知識轉移提供了起點，但需要更多的改進。最後，我們討論了模型在多個開發站點上的可靠性。},
  file = {C:\Users\BlackCat\Zotero\storage\5UCSN5AE\Fritz 等。 - 2014 - Degree-of-knowledge Modeling a developer's knowle.pdf}
}

@inproceedings{tiagosantosLimitingTagsFosters2021,
  title = {Limiting {{Tags Fosters Efficiency}}},
  booktitle = {Proceedings of the 13th {{ACM Web Science Conference}} 2021},
  author = {{Tiago Santos} and {Keith Burghardt} and {Kristina Lerman} and {Denis Helic}},
  year = {6 月 22, 2021},
  series = {{{WebSci}} '21},
  pages = {46--55},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3447535.3462483},
  url = {https://dl.acm.org/doi/10.1145/3447535.3462483},
  urldate = {2023-09-03},
  abstract = {Tagging facilitates information retrieval in social media and other online communities by allowing users to organize and describe online content. Researchers found that the efficiency of tagging systems steadily decreases over time, because tags become less precise in identifying specific documents, i.e., they lose their descriptiveness. However, previous works did not answer how or even whether community managers can improve the efficiency of tags. In this work, we use information-theoretic measures to track the descriptive and retrieval efficiency of tags on Stack Overflow, a question-answering system that strictly limits the number of tags users can specify per question. We observe that tagging efficiency stabilizes over time, while tag content and descriptiveness both increase. To explain this observation, we hypothesize that limiting the number of tags fosters novelty and diversity in tag usage, two properties which are both beneficial for tagging efficiency. To provide qualitative evidence supporting our hypothesis, we present a statistical model of tagging that demonstrates how novelty and diversity lead to greater tag efficiency in the long run. Our work offers insights into policies to improve information organization and retrieval in online communities.},
  isbn = {978-1-4503-8330-1},
  langid = {english},
  keywords = {information retrieval,social tagging,tag efficiency},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 限制標籤可提高效率\\
abstractTranslation:  標籤允許用戶組織和描述在線內容，從而促進社交媒體和其他在線社區中的信息檢索。研究人員發現，標籤系統的效率隨著時間的推移穩步下降，因為標籤在識別特定文檔時變得不太精確，即它們失去了描述性。然而，之前的工作並沒有回答社區管理者如何甚至是否可以提高標籤的效率。在這項工作中，我們使用信息論方法來跟踪 Stack Overflow 上標籤的描述和檢索效率，Stack Overflow 是一個問答系統，嚴格限制用戶可以為每個問題指定的標籤數量。我們觀察到，標籤效率隨著時間的推移而穩定，而標籤內容和描述性都在增加。為了解釋這一觀察結果，我們假設限制標籤數量可以促進標籤使用的新穎性和多樣性，這兩個特性都有益於標籤效率。為了提供支持我們假設的定性證據，我們提出了一個標籤統計模型，該模型展示了新穎性和多樣性如何從長遠來看提高標籤效率。我們的工作提供了對改善在線社區信息組織和檢索的政策的見解。},
  file = {C:\Users\BlackCat\Zotero\storage\FFR52CXY\Santos 等。 - 2021 - Limiting Tags Fosters Efficiency.pdf}
}

@inproceedings{tianAPIBotQuestionAnswering2017,
  title = {{{APIBot}}: {{Question}} Answering Bot for {{API}} Documentation},
  shorttitle = {{{APIBot}}},
  author = {Tian, Y. and Thung, F. and Sharma, A. and Lo, D.},
  date = {2017},
  pages = {153--158},
  doi = {10.1109/ASE.2017.8115628},
  abstract = {As the carrier of Application Programming Interfaces (APIs) knowledge, API documentation plays a crucial role in how developers learn and use an API. It is also a valuable information resource for answering API-related questions, especially when developers cannot find reliable answers to their questions online/offline. However, finding answers to API-related questions from API documentation might not be easy because one may have to manually go through multiple pages before reaching the relevant page, and then read and understand the information inside the relevant page to figure out the answers. To deal with this challenge, we develop APIBot, a bot that can answer API questions given API documentation as an input. APIBot is built on top of SiriusQA, the QA system from Sirius, a state of the art intelligent personal assistant. To make SiriusQA work well under software engineering scenario, we make several modifications over SiriusQA by injecting domain specific knowledge. We evaluate APIBot on 92 API questions, answers of which are known to be present in Java 8 documentation. Our experiment shows that APIBot can achieve a Hit@5 score of 0.706. © 2017 IEEE.},
  eventtitle = {{{ASE}} 2017 - {{Proceedings}} of the 32nd {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  isbn = {978-1-5386-2684-9},
  langid = {english},
  keywords = {API Documentation,Bot,Question Answering,未整理},
  annotation = {28 citations (Crossref) [2024-03-26]\\
titleTranslation: APIBot：API 文件的問答機器人\\
abstractTranslation:  API文件作為API知識的載體，對於開發者學習和使用API\hspace{0pt}\hspace{0pt}起著至關重要的作用。它也是回答 API 相關問題的寶貴資訊資源，特別是當開發人員無法在線上/離線找到問題的可靠答案時。然而，從 API 文件中找到 API 相關問題的答案可能並不容易，因為人們可能需要手動瀏覽多個頁面才能到達相關頁面，然後閱讀並理解相關頁面內的資訊才能找到答案。為了應對這項挑戰，我們開發了 APIBot，這是一個可以以 API 文件作為輸入來回答 API 問題的機器人。 APIBot 建構在 SiriusQA 之上，SiriusQA 是最先進的智慧個人助理 Sirius 的 QA 系統。為了使 SiriusQA 在軟體工程場景下正常運作，我們透過注入特定領域的知識對 SiriusQA 進行了一些修改。我們根據 92 個 API 問題評估 APIBot，這些問題的答案已存在於 Java 8 文件中。我們的實驗顯示 APIBot 可以達到 0.706 的 Hit@5 分數。 © 2017 IEEE。},
  note = {被引用文獻:36}
}

@inproceedings{tianbaoxieUnifiedSKGUnifyingMultiTasking2022,
  title = {{{UnifiedSKG}}: {{Unifying}} and {{Multi-Tasking Structured Knowledge Grounding}} with {{Text-to-Text Language Models}}},
  shorttitle = {{{UnifiedSKG}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {{Tianbao Xie} and {Chen Henry Wu} and {Peng Shi} and {Ruiqi Zhong} and {Torsten Scholak} and {Michihiro Yasunaga} and {Chien-Sheng Wu} and {Ming Zhong} and {Pengcheng Yin} and {Sida I. Wang} and {Victor Zhong} and {Bailin Wang} and {Chengzu Li} and {Connor Boyle} and {Ansong Ni} and {Ziyu Yao} and {Dragomir Radev} and {Caiming Xiong} and {Lingpeng Kong} and {Rui Zhang} and {Noah A. Smith} and {Luke Zettlemoyer} and {Tao Yu}},
  date = {2022-02},
  pages = {602--631},
  publisher = {Association for Computational Linguistics},
  location = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.39},
  url = {https://aclanthology.org/2022.emnlp-main.39},
  urldate = {2023-09-21},
  abstract = {Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.},
  eventtitle = {{{EMNLP}} 2022},
  langid = {english},
  keywords = {問答系統,已整理,待讀,知識圖譜,重要},
  annotation = {26 citations (Crossref) [2024-03-26]\\
titleTranslation: UnifiedSKG：使用文本到文本語言模型統一和多任務結構化知識基礎\\
abstractTranslation:  結構化知識基礎（SKG）利用結構化知識來完成使用者請求，例如對資料庫進行語義解析、對知識庫進行問答等。由於SKG任務的輸入和輸出是異質的，不同社區分別對它們進行研究，這限制了SKG的系統性和相容性研究。在本文中，我們透過提出 UnifiedSKG 框架克服了這個限制，該框架將 21 個 SKG 任務統一為文本到文本的格式，旨在促進系統的 SKG 研究，而不是專屬於單一任務、領域或資料集。我們使用 UnifiedSKG 對不同大小的 T5 進行基準測試，結果表明，T5 在必要時進行簡單修改，以在幾乎所有 21 項任務上實現最先進的效能。我們進一步證明，多任務前綴調整可以提高大多數任務的效能，從而很大程度上提高整體效能。 UnifiedSKG 也促進了零樣本和少樣本學習的研究，我們表明 T0、GPT-3 和 Codex 在 SKG 的零樣本和少樣本學習中表現不佳。我們還使用 UnifiedSKG 對跨 SKG 任務的結構化知識編碼變體進行了一系列受控實驗。 UnifiedSKG 可以輕鬆擴展到更多任務，並且在 https://github.com/hkunlp/unifiedskg 上開源。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\8E88SU3W\\Xie 等。 - 2022 - UnifiedSKG Unifying and Multi-Tasking Structured .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\ALW4CI8Y\\UNIFIEDSKG： Unifying and Multi-Tasking Structured Knowledge.pdf}
}

@article{tianKnowledgeGraphKnowledge2022,
  title = {Knowledge Graph and Knowledge Reasoning: {{A}} Systematic Review},
  shorttitle = {Knowledge Graph and Knowledge Reasoning},
  author = {Tian, Ling and Zhou, Xue and Wu, Yan-Ping and Zhou, Wang-Tao and Zhang, Jin-Hao and Zhang, Tian-Shu},
  date = {2022-06-01},
  journaltitle = {Journal of Electronic Science and Technology},
  shortjournal = {Journal of Electronic Science and Technology},
  volume = {20},
  number = {2},
  pages = {100159},
  issn = {1674-862X},
  doi = {10.1016/j.jnlest.2022.100159},
  url = {https://www.sciencedirect.com/science/article/pii/S1674862X2200012X},
  urldate = {2023-11-28},
  abstract = {The knowledge graph (KG) that represents structural relations among entities has become an increasingly important research field for knowledge-driven artificial intelligence. In this survey, a comprehensive review of KG and KG reasoning is provided. It introduces an overview of KGs, including representation, storage, and essential technologies. Specifically, it summarizes several types of knowledge reasoning approaches, including logic rules-based, representation-based, and neural network-based methods. Moreover, this paper analyzes the representation methods of knowledge hypergraphs. To effectively model hyper-relational data and improve the performance of knowledge reasoning, a three-layer knowledge hypergraph model is proposed. Finally, it analyzes the advantages of three-layer knowledge hypergraphs through reasoning and update algorithms which could facilitate future research.},
  langid = {english},
  keywords = {Knowledge graph (KG),Knowledge graph applications,Knowledge hypergraph,Knowledge reasoning,Review,已整理,待讀,知識圖譜,知識推理,重要},
  annotation = {22 citations (Crossref) [2024-03-26]\\
titleTranslation: 知識圖與知識推理：系統回顧},
  file = {C:\Users\BlackCat\Zotero\storage\W3MMK42P\S1674862X2200012X.html}
}

@article{tiantianliuWavoiceMmWaveassistedNoiseresistant2023,
  title = {Wavoice: {{A mmWave-assisted Noise-resistant Speech Recognition System}}},
  shorttitle = {Wavoice},
  author = {{Tiantian Liu} and {Chao Wang} and {Zhengxiong Li} and {Ming-Chun Huang} and {Wenyao Xu} and {Feng Lin}},
  year = {5 月 18, 2023},
  journaltitle = {ACM Transactions on Sensor Networks},
  shortjournal = {ACM Trans. Sen. Netw.},
  issn = {1550-4859},
  doi = {10.1145/3597457},
  url = {https://dl.acm.org/doi/10.1145/3597457},
  urldate = {2023-09-04},
  abstract = {As automatic speech recognition evolves, the deployment of voice user interface has boomingly expanded. Especially since the COVID-19 pandemic, VUI has gained more attention in online communication owing to its non-contact property. However, VUI struggles to be applied in public scenes due to the degradation of received audio signals caused by various ambient noises. In this paper, we propose Wavoice, the first noise-resistant multi-modal speech recognition system that fuses two distinct voice sensing modalities, i.e., millimeter-wave (mmWave) signals and audio signals from a microphone, together. One key contribution is to model the inherent correlation between mmWave and audio signals. Based on it, Wavoice facilitates the real-time noise-resistant voice activity detection and user targeting from multiple speakers. Additionally, we elaborate on two novel modules for multi-modal fusion embedded into the neural network, leading to accurate speech recognition. Extensive experiments prove the effectiveness of Wavoice under adverse conditions, that is, the character recognition error rate below 1\%\%\textbackslash\% in a range of 7 meters. In terms of robustness and accuracy, Wavoice considerably outperforms existing audio-only speech recognition methods with lower character error rate and word error rate.},
  langid = {english},
  keywords = {biometrics,mmWave sensing,multimodal systems,回收,機器學習,訊號處理,語音辨識},
  annotation = {1 citations (Crossref) [2024-03-26]\\
Just Accepted\\
titleTranslation: Wavoice：毫米波輔助抗噪聲語音識別系統\\
abstractTranslation:  隨著自動語音識別的發展，語音用戶界面的部署蓬勃發展。特別是自COVID-19大流行以來，VUI因其非接觸性的特性在在線交流中獲得了更多關注。然而，由於各種環境噪聲導致接收到的音頻信號衰減，VUI很難在公共場景中應用。在本文中，我們提出了 Wavoice，這是第一個抗噪聲多模式語音識別系統，它將兩種不同的語音傳感模式融合在一起，即毫米波 (mmWave) 信號和來自麥克風的音頻信號。一項關鍵貢獻是對毫米波和音頻信號之間的固有相關性進行建模。基於此，Wavoice 有助於實時抗噪聲語音活動檢測和來自多個揚聲器的用戶定位。此外，我們還詳細闡述了嵌入神經網絡的兩個新穎的多模態融合模塊，從而實現準確的語音識別。大量實驗證明了Wavoice在惡劣條件下的有效性，即在7米範圍內字符識別錯誤率低於1\%\%\textbackslash\%。在魯棒性和準確性方面，Wavoice 大大優於現有的純音頻語音識別方法，並且字符錯誤率和單詞錯誤率更低。},
  note = {結合毫米波雷達和一般麥克風一起訓練模型，大幅提升抗噪能力。與研究較無關聯。},
  file = {C:\Users\BlackCat\Zotero\storage\IRUJIK9W\Liu 等。 - 2023 - Wavoice A mmWave-assisted Noise-resistant Speech .pdf}
}

@article{tiborkoltayBrightSideInformation2017,
  title = {The Bright Side of Information: Ways of Mitigating Information Overload},
  shorttitle = {The Bright Side of Information},
  author = {{Tibor Koltay}},
  date = {2017-07-10},
  journaltitle = {Journal of Documentation},
  shortjournal = {JD},
  volume = {73},
  number = {4},
  pages = {767--775},
  issn = {0022-0418},
  doi = {10.1108/JD-09-2016-0107},
  url = {https://www.emerald.com/insight/content/doi/10.1108/JD-09-2016-0107/full/html},
  urldate = {2023-10-23},
  abstract = {Purpose               The complex phenomenon of information overload (IO) is one of the pathologies in our present information environment, thus symbolically it signalizes the existence of a dark side of information. The purpose of this paper is to investigate the approaches on mitigating IO. Hence, it is an attempt to display the bright side.                                         Design/methodology/approach               Based on a literature review, the sources of IO are briefly presented, not forgetting about the role of information technology and the influence of the data-intensive world. The main attention is given to the possible ways of mitigating IO.                                         Findings               It is underlined that there are both technological and social approaches towards easing the symptoms of IO. While reducing IO by increasing search task delegation is a far away goal, solutions emerge when information is properly designed and tools of information architecture are applied to enable findability. A wider range of coping strategies is available when we interact with information. The imperative of being critical against information by exercising critical thinking and critical reading yields results if different, discipline-dependent literacies, first of all information literacy and data literacy are acquired and put into operation, slow principles are followed and personal information management (PIM) tools are applied.                                         Originality/value               The paper intends to be an add-on to the recent discussions and the evolving body of knowledge about the relationship between IO and information architecture, various literacies and PIM.},
  langid = {english},
  annotation = {23 citations (Crossref) [2024-03-26]\\
titleTranslation: 信息的光明面：减轻信息超载的方法\\
abstractTranslation:  目的 信息超载（IO）这一复杂现象是我们当前信息环境中的病态之一，因此它象征着信息黑暗面的存在。本文旨在研究减轻信息过载的方法。因此，本文试图展示光明的一面。                                         设计/方法/途径 在文献综述的基础上，简要介绍了 IO 的来源，同时不忘信息技术的作用和数据密集型世界的影响。主要关注减轻 IO 的可能方法。                                         研究结果 强调有技术和社会两种方法可以缓解 IO 的症状。虽然通过增加搜索任务委托来减少 IO 是一个遥远的目标，但如果信息设计得当，应用信息架构工具来实现可查找性，解决方案就会出现。当我们与信息互动时，可以采取更广泛的应对策略。通过批判性思维和批判性阅读对信息进行批判是当务之急，如果能够掌握并运用不同的、依赖于学科的素养，首先是信息素养和数据素养，遵循缓慢原则，并应用个人信息管理（PIM）工具，就能取得成效。                                         独创性/价值 本文旨在为近期关于信息获取与信息架构、各种素养和个人信息管理之间关系的讨论和不断发展的知识体系提供补充。},
  note = {[TLDR] It is underlined that there are both technological and social approaches towards easing the symptoms of IO, and solutions emerge when information is properly designed and tools of information architecture are applied to enable findability.}
}

@article{tieyuanliuReviewDeepLearningbased2022,
  title = {A Review of Deep Learning-Based Recommender System in e-Learning Environments},
  author = {{Tieyuan Liu} and {Qiong Wu} and {Liang Chang} and {Tianlong Gu}},
  date = {2022-12-01},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {55},
  number = {8},
  pages = {5953--5980},
  issn = {1573-7462},
  doi = {10.1007/s10462-022-10135-2},
  url = {https://doi.org/10.1007/s10462-022-10135-2},
  urldate = {2023-10-18},
  abstract = {While the recent emergence of a large number of online course resources has made life more convenient for many people, it has also caused information overload. According to a user’s situation and behavior, course recommendation systems can recommend courses of interest to the user, so that the user can quickly sift through a massive amount of information to find courses that meet his or her needs. This paper provide a systematic review of deep learning-based recommendation systems in e-learning environments. Firstly, the concept of recommendation systems is introduced in e-learning environments, and present a comprehensive survey and classification of deep learning techniques for course recommendation. And then, a detailed analysis of existing recommendation system is conducted based on the collected literature, and an overall course recommendation system framework is presented. Subsequently, this artical main focus is on multilayer perceptual machines, recurrent neural networks, convolutional neural networks, neural attention mechanisms, and deep reinforcement learning-based recommendation, and summarize the existing research on the use of the five techniques mentioned above in e-learning environments. The last section discusses seven flaws in the current recommendation systems used in e-learning environments and identify opportunities for future research.},
  langid = {english},
  keywords = {e-learning,Recommender system,Review,已整理,機器學習,深度學習},
  annotation = {11 citations (Crossref) [2024-03-26]\\
titleTranslation: 基于深度学习的电子学习环境推荐系统综述\\
abstractTranslation:  近年来，大量在线课程资源的出现为许多人的生活提供了便利，但同时也造成了信息过载。课程推荐系统可以根据用户的情况和行为，向用户推荐其感兴趣的课程，从而使用户能在海量信息中快速筛选出符合自己需求的课程。本文对电子学习环境中基于深度学习的推荐系统进行了系统综述。首先，介绍了网络学习环境中推荐系统的概念，并对用于课程推荐的深度学习技术进行了全面的调查和分类。然后，根据收集到的文献对现有的推荐系统进行了详细分析，并提出了一个整体的课程推荐系统框架。随后，本文主要介绍了多层感知机、递归神经网络、卷积神经网络、神经注意机制和基于深度强化学习的推荐，并总结了上述五种技术在网络学习环境中应用的现有研究。最后一节讨论了目前电子学习环境中使用的推荐系统存在的七个缺陷，并指出了未来研究的机会。},
  note = {關於如何使用深度學習推薦用戶適合的線上課程。}
}

@inproceedings{tjasajelovsekOnlineNotesSystemRealTime2023,
  title = {Online-{{Notes System}}: {{Real-Time Speech Recognition}} and~{{Translation}} of~{{Lectures}}},
  shorttitle = {Online-{{Notes System}}},
  booktitle = {Research {{Challenges}} in {{Information Science}}: {{Information Science}} and the {{Connected World}}},
  author = {{Tjaša Jelovšek} and {Marko Bajec} and {Iztok Lebar Bajec} and {Kaja Gantar} and {Slavko Žitnik}},
  editor = {{Selmin Nurcan} and {Andreas L. Opdahl} and {Haralambos Mouratidis} and {Aggeliki Tsohou}},
  date = {2023},
  series = {Lecture {{Notes}} in {{Business Information Processing}}},
  pages = {485--492},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-33080-3_29},
  abstract = {Student mobility gives students the opportunity to visit different universities across the world. Not all courses are offered in English or in other languages foreign students might understand, so they often face problems with following the lectures. To resolve these problems, we propose Online Notes (ON), which is a real-time speech recognition and translation system. The system is trained using existing course materials. During lectures, the lecturer wears a microphone, while the students can follow the lecture by using the ON system. After the lecture, the professor can edit and update the transcripts and translations, and students have the option to listen to the lecture and read the materials. We have conducted a series of one-time tests and currently, we are in the middle of two whole-semester pilot tests at the University of Ljubljana. In the tests, a speech recognition accuracy of up to 87.4\% was achieved. Preliminary results have shown that the tool is especially useful for students who either do not understand the language of the course or understand it to a limited extent. Additionally, the transcripts of the lectures have shown to be useful for creating additional learning materials.},
  isbn = {978-3-031-33080-3},
  langid = {english},
  keywords = {machine translation,real-time lecture translation,speech recognition,已整理,機器學習,語音辨識},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 線上筆記系統：即時語音辨識與講座翻譯},
  file = {C:\Users\BlackCat\Zotero\storage\MW6X78E2\Jelovšek et al. - 2023 - Online-Notes System Real-Time Speech Recognition .pdf}
}

@inproceedings{tokalayaswanthsrisaisantoshDAKEDocumentLevelAttention2020,
  title = {{{DAKE}}: {{Document-Level Attention}} for {{Keyphrase Extraction}}},
  shorttitle = {{{DAKE}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {{Tokala Yaswanth Sri Sai Santosh} and {Debarshi Kumar Sanyal} and {Plaban Kumar Bhowmick} and {Partha Pratim Das}},
  editor = {{Joemon M. Jose} and {Emine Yilmaz} and {João Magalhães} and {Pablo Castells} and {Nicola Ferro} and {Mário J. Silva} and {Flávio Martins}},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {392--401},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-45442-5_49},
  abstract = {Keyphrases provide a concise representation of the topical content of a document and they are helpful in various downstream tasks. Previous approaches for keyphrase extraction model it as a sequence labelling task and use local contextual information to understand the semantics of the input text but they fail when the local context is ambiguous or unclear. We present a new framework to improve keyphrase extraction by utilizing additional supporting contextual information. We retrieve this additional information from other sentences within the same document. To this end, we propose Document-level Attention for Keyphrase Extraction (DAKE), which comprises Bidirectional Long Short-Term Memory networks that capture hidden semantics in text, a document-level attention mechanism to incorporate document level contextual information, gating mechanisms which help to determine the influence of additional contextual information on the fusion with local contextual information, and Conditional Random Fields which capture output label dependencies. Our experimental results on a dataset of research papers show that the proposed model outperforms previous state-of-the-art approaches for keyphrase extraction.},
  isbn = {978-3-030-45442-5},
  langid = {english},
  keywords = {Document-level attention,Keyphrase extraction,LSTM,Sequence labelling,已整理,語意分析,重要},
  annotation = {10 citations (Crossref) [2024-03-26]\\
titleTranslation: DAKE：用於關鍵字提取的文檔級注意力\\
abstractTranslation:  關鍵字提供了文件主題內容的簡潔表示，它們對於各種下游任務很有幫助。先前的關鍵字詞提取方法將其建模為序列標記任務，並使用本地上下文資訊來理解輸入文字的語義，但當本地上下文不明確或不清楚時，它們會失敗。我們提出了一個新的框架，透過利用額外的支援上下文資訊來改進關鍵字詞提取。我們從同一文檔中的其他句子中檢索這些附加資訊。為此，我們提出了用於關鍵字詞提取的文檔級注意力（DAKE），它包括捕獲文本中隱藏語義的雙向長短期記憶網絡、結合文檔級上下文信息的文檔級注意力機制、幫助確定附加上下文資訊對與本地上下文資訊融合的影響，以及捕捉輸出標籤依賴性的條件隨機場。我們在研究論文資料集上的實驗結果表明，所提出的模型優於先前最先進的關鍵字詞擷取方法。},
  file = {C:\Users\BlackCat\Zotero\storage\GQ2YNZ67\Santosh 等。 - 2020 - DAKE Document-Level Attention for Keyphrase Extra.pdf}
}

@article{tongyuKnowledgeGraphTCM2017,
  title = {Knowledge Graph for {{TCM}} Health Preservation: {{Design}}, Construction, and Applications},
  shorttitle = {Knowledge Graph for {{TCM}} Health Preservation},
  author = {{Tong Yu} and {Jinghua Li} and {Qi Yu} and {Ye Tian} and {Xiaofeng Shun} and {Lili Xu} and {Ling Zhu} and {Hongjie Gao}},
  date = {2017-03},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artif Intell Med},
  volume = {77},
  eprint = {28545611},
  eprinttype = {pmid},
  pages = {48--52},
  issn = {1873-2860},
  doi = {10.1016/j.artmed.2017.04.001},
  abstract = {Traditional Chinese Medicine (TCM) is one of the important non-material cultural heritages of the Chinese nation. It is an important development strategy of Chinese medicine to collect, analyzes, and manages the knowledge assets of TCM health care. As a novel and massive knowledge management technology, knowledge graph provides an ideal technical means to solve the problem of "Knowledge Island" in the field of traditional Chinese medicine. In this study, we construct a large-scale knowledge graph, which integrates terms, documents, databases and other knowledge resources. This knowledge graph can facilitate various knowledge services such as knowledge visualization, knowledge retrieval, and knowledge recommendation, and helps the sharing, interpretation, and utilization of TCM health care knowledge.},
  langid = {english},
  keywords = {Databases Factual,Delivery of Health Care,Health preservation,Humans,Knowledge graph,Knowledge services,Medicine Chinese Traditional,Ontology,Pattern Recognition Automated,Traditional Chinese medicine,中醫,已整理,知識圖譜,重要},
  annotation = {105 citations (Crossref) [2024-03-26]\\
titleTranslation: 中醫養生知識圖譜：設計、建構與應用\\
abstractTranslation:  中醫藥是中華民族重要的非物質文化遺產之一。收集、分析、管理中醫保健知識資產是中醫藥的重要發展戰略。知識圖譜作為一種新穎的海量知識管理技術，為解決中醫藥領域的“知識島”問題提供了理想的技術手段。在本研究中，我們構建了一個大規模的知識圖譜，它集成了術語、文檔、數據庫和其他知識資源。該知識圖譜可以實現知識可視化、知識檢索、知識推薦等多種知識服務，幫助中醫養生知識的共享、解讀和利用。}
}

@inproceedings{tongyuOntologyBasedModelingClinical2015,
  title = {Ontology-{{Based Modeling}} of {{Clinical Reasoning}} in {{Traditional Chinese Medicine}}},
  booktitle = {2015 7th {{International Conference}} on {{Information Technology}} in {{Medicine}} and {{Education}} ({{ITME}})},
  author = {{Tong Yu} and {Jinghua Li} and {Hongjie Gao} and {Qi Yu} and {Meng Cui}},
  date = {2015-01},
  pages = {133--137},
  doi = {10.1109/ITME.2015.40},
  abstract = {The integration of Traditional Chinese Medicine (TCM) and Western Medicine (WM) requires extensive interdisciplinary and cross-cultural collaboration. The productivity of integrated studies is often limited by the lack of understanding between practitioners from different cultural backgrounds. Information technologies, especially the Semantic Web and ontologies, can facilitate the exchange of medical information across discipline boundaries. We use Semantic Web technologies to engineer a TCM domain ontology named TCMOnto, covering categories such as basic theories, diagnostics, diseases, therapeutics, and medicinal treatments. TCMOnto formalizes the meaning of TCM concepts, and captures the semantic relations between them. This ontology can be used to represent knowledge resources such as medical rules, semantic queries, and semantic annotations for medical documents, and facilitate a variety of applications such as information integration and exchange, clinical intelligence, and knowledge management.},
  eventtitle = {2015 7th {{International Conference}} on {{Information Technology}} in {{Medicine}} and {{Education}} ({{ITME}})},
  langid = {english},
  keywords = {Cognition,integrated medicine,Kidney,knowledge representation,Medical diagnostic imaging,Ontologies,ontology,Resource description framework,semantic web,Semantics,traditional Chiense medicine,中醫,已整理,待讀,知識推理,知識本體,辨證,重要},
  annotation = {0 citations (Crossref) [2024-03-26]\\
abstractTranslation:  中醫與西方醫學的融合需要廣泛的跨學科和跨文化合作。綜合研究的生產力常常因來自不同文化背景的實踐者之間缺乏理解而受到限制。資訊技術，特別是語義網和本體，可以促進跨學科邊界的醫學資訊交換。我們利用語意網技術建構了一個名為TCMOnto的中醫領域本體，涵蓋基礎理論、診斷、疾病、治療和藥物治療等類別。 TCMOnto 形式化了中醫概念的含義，並捕捉了它們之間的語義關係。此本體可用於表示醫療規則、語意查詢、醫療文件語意標註等知識資源，並促進資訊整合與交換、臨床智慧、知識管理等多種應用。\\
titleTranslation: 基於本體的中醫臨床推理建模},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\WFMJEHLG\\Yu 等。 - 2015 - Ontology-Based Modeling of Clinical Reasoning in T.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\9ZKACJFF\\7429114.html}
}

@inproceedings{tongyuSemanticWebKnowledge2015,
  title = {Semantic {{Web}} for {{Knowledge Integration}} between {{Traditional Chinese Medicine}} and {{Biomedicine}}},
  booktitle = {2015 7th {{International Conference}} on {{Information Technology}} in {{Medicine}} and {{Education}} ({{ITME}})},
  author = {{Tong Yu} and {Jing Liu} and {Shuo Yang} and {Jinghua Li} and {Lirong Jia}},
  date = {2015-01},
  pages = {229--233},
  doi = {10.1109/ITME.2015.42},
  abstract = {The World Wide Web connects a wide variety of communities in medical domain, and provides a platform for knowledge exchange and integration between Traditional Chinese Medicine (TCM) and biomedicine. However, the cultural gaps between TCM and Western Medicine hinder cross-cultural communication. We utilize Semantic Web technologies to build a knowledge base that integrates distributed and heterogeneous knowledge elements from both biomedicine and TCM, and provides various information retrieval and knowledge discovery services. We explain how this integrated knowledge base helps to bridge the linguistic, semantic, and ontological gaps between different communities, and fosters cross-cultural scientific collaboration.},
  eventtitle = {2015 7th {{International Conference}} on {{Information Technology}} in {{Medicine}} and {{Education}} ({{ITME}})},
  langid = {english},
  keywords = {knowledge base,Knowledge based systems,knowledge integration,knowledge services,Medical diagnostic imaging,Ontologies,ontology,Resource description framework,semantic web,Semantics,traditional Chiense medicine,中西合併,中醫,已整理,待讀,知識本體,重要},
  annotation = {2 citations (Crossref) [2024-03-26]\\
abstractTranslation:  萬維網連接了醫學領域的各種社區，並為傳統中醫學（TCM）和生物醫學之間的知識交流和融合提供了平台。然而，中西醫之間的文化差異阻礙了跨文化交流。我們利用語意Web技術建構整合生物醫學和中醫分散式異質知識元素的知識庫，並提供各種資訊檢索和知識發現服務。我們解釋了這個綜合知識庫如何幫助彌合不同社群之間的語言、語義和本體論差距，並促進跨文化科學合作。\\
titleTranslation: 中醫學與生物醫學知識整合的語意網},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\STX3S2FS\\Yu 等。 - 2015 - Semantic Web for Knowledge Integration between Tra.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\WQWWYDUF\\7429135.html}
}

@online{tonmoyComprehensiveSurveyHallucination2024,
  title = {A {{Comprehensive Survey}} of {{Hallucination Mitigation Techniques}} in {{Large Language Models}}},
  author = {Tonmoy, S. M. Towhidul Islam and Zaman, S. M. Mehedi and Jain, Vinija and Rani, Anku and Rawte, Vipula and Chadha, Aman and Das, Amitava},
  date = {2024-01-08},
  eprint = {2401.01313},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.01313},
  url = {http://arxiv.org/abs/2401.01313},
  urldate = {2024-04-28},
  abstract = {As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,LLM,Survey,幻覺,未整理,重要},
  annotation = {abstractTranslation:  隨著大型語言模型（LLM）在編寫類似人類文本的能力方面不斷進步，一個關鍵的挑戰仍然是它們傾向於產生看似事實但毫無根據的內容。這種幻覺問題可以說是將這些強大的法學碩士安全部署到影響人們生活的現實生產系統中的最大障礙。在實際環境中廣泛採用法學碩士的旅程在很大程度上依賴於解決和減輕幻覺。與專注於有限任務的傳統人工智慧系統不同，法學碩士在訓練期間接觸了大量線上文字資料。雖然這使他們能夠表現出令人印象深刻的語言流暢性，但這也意味著他們能夠從訓練數據的偏差中推斷出信息，誤解不明確的提示，或者修改信息以表面上與輸入保持一致。當我們依賴語言生成功能來實現敏感應用程式（例如總結醫療記錄、財務分析報告等）時，這就變得非常令人擔憂。全面調查。其中值得注意的是檢索增強生成（Lewis 等人，2021）、知識檢索（Varshney 等人，2023）、CoNLI（Lei 等人，2023）和 CoVe（Dhuliawala 等人，2023）。此外，我們引入了一個詳細的分類法，根據各種參數對這些方法進行分類，例如資料集利用率、常見任務、回饋機制和檢索器類型。這種分類有助於區分專門為解決法學碩士的幻覺問題而設計的不同方法。此外，我們分析了這些技術固有的挑戰和局限性，為法學碩士領域內解決幻覺和相關現象的未來研究奠定了堅實的基礎。\\
titleTranslation: 大型語言模型中幻覺緩解技術的綜合調查},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\4G2YW78M\\Tonmoy 等。 - 2024 - A Comprehensive Survey of Hallucination Mitigation.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\ZURNS949\\2401.html}
}

@article{ToolQADatasetLLM2023,
  title = {{{ToolQA}}: {{A Dataset}} for {{LLM Question Answering}} with {{External Tools}}},
  shorttitle = {{{ToolQA}}},
  date = {2023-06-23},
  journaltitle = {arXiv.org},
  volume = {abs/2306.13304},
  issn = {2331-8422},
  doi = {10.48550/arXiv.2306.13304},
  url = {https://typeset.io/papers/toolqa-a-dataset-for-llm-question-answering-with-external-3bgkq0y0},
  urldate = {2024-04-16},
  abstract = {Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning. To overcome these challenges, external tools can be used to enhance LLMs' question-answering abilities. However, current evaluation methods do not distinguish between questions that can be answered using LLMs' internal knowledge and those that require external information through tool use. To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs' ability to use external tools for question answering. Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions. Importantly, we strive to minimize the overlap between our benchmark data and LLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use reasoning abilities. We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements. Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements. Our data and code are freely available to the broader scientific community on GitHub.},
  langid = {english},
  keywords = {未整理},
  annotation = {titleTranslation: ToolQA：使用外部工具回答 LLM 問題的資料集\\
abstractTranslation:  大型語言模型（LLM）在各種 NLP 任務中表現出了令人印象深刻的性能，但它們仍然面臨著幻覺和弱數字推理等挑戰。為了克服這些挑戰，可以使用外部工具來增強法學碩士的問答能力。然而，目前的評估方法並沒有區分可以使用法學碩士內部知識回答的問題和需要透過工具使用外部資訊回答的問題。為了解決這個問題，我們引入了一個名為 ToolQA 的新資料集，該資料集旨在忠實評估法學碩士使用外部工具回答問題的能力。我們開發的 ToolQA 涉及一個可擴展的自動化資料集管理流程，以及 13 個專為與外部知識互動以回答問題而設計的專用工具。重要的是，我們努力最大限度地減少基準數據與法學碩士預訓練數據之間的重疊，從而能夠更精確地評估法學碩士的工具使用推理能力。我們對現有工具使用法學碩士進行了深入診斷，以突出他們的優勢、劣勢和潛在的改進。我們的研究結果為評估法學碩士設立了新的基準，並為未來的進步提出了新的方向。我們的數據和程式碼可在 GitHub 上免費提供給更廣泛的科學界。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\P9EM4NQ6\\2023 - ToolQA A Dataset for LLM Question Answering with .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\VW2EZ3B8\\toolqa-a-dataset-for-llm-question-answering-with-external-3bgkq0y0.html}
}

@inproceedings{tsung-hsienyangBERTbasedChineseMedicine2022,
  title = {{{BERT-based Chinese Medicine Named Entity Recognition Model Applied}} to {{Medication Reminder Dialogue System}}},
  booktitle = {2022 13th {{International Symposium}} on {{Chinese Spoken Language Processing}} ({{ISCSLP}})},
  author = {{Tsung-Hsien Yang} and {Matus Pleva} and {Daniel Hládek} and {Ming-Hsiang Su}},
  date = {2022-02},
  pages = {374--378},
  doi = {10.1109/ISCSLP57327.2022.10037867},
  abstract = {The general public in Taiwan generally believes that traditional Chinese medicine (TCM) is mild and has no side effects, but they ignore the safety of traditional Chinese medicine. If the Chinese medicine name and disease name can be correctly identified in the human-machine dialogue, it can help the dialogue system to give correct medication reminders. In this study, a named entity recognition was constructed and applied to the identification of Chinese medicine names and disease names, and the results could be further used in the human-computer dialogue system to provide people with correct Chinese medicine medication reminders. First, this study uses a web crawler to organize network resources to become a TCM named entity corpus, collecting 1097 articles, 1412 disease names and 38714 TCM names. Then we use the Chinese medicine name and BIO labeling method to label each article. Finally, this study trains and evaluates BERT, ALBERT, RoBERTa and GPT2 with biLSTM and CRF. The experimental results show that the NER system of RoBERTa combined with biLSTM and CRF achieves the best system performance, where the Precision is 0.96, the Recall is 0.96 and the F1-score is 0.96.},
  eventtitle = {2022 13th {{International Symposium}} on {{Chinese Spoken Language Processing}} ({{ISCSLP}})},
  langid = {english},
  keywords = {/unread,Bit error rate,Chinese medicine,Crawlers,Diseases,human-computer dialogue,Labeling,Man-machine systems,named entity recognition,Safety,System performance,中醫,實體識別,已整理,監督式學習},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於BERT的中醫命名實體辨識模型應用於服藥提醒對話系統\\
abstractTranslation:  台灣民眾普遍認為中藥性溫和、無副作用，但卻忽略了中藥的安全性。如果能夠在人機對話中正確辨識中藥名稱和疾病名稱，就可以幫助對話系統給予正確的用藥提醒。本研究建構了命名實體識別，並將其應用於中藥名稱和病名的識別，其結果可以進一步用於人機對話系統，為人們提供正確的中藥用藥提醒。首先，本研究利用網路爬蟲將網路資源整理成為中醫命名實體語料庫，收集文章1097篇、病名1412個、中醫名稱38714個。然後我們採用中藥名稱和BIO標註法對每篇文章進行標註。最後，本研究使用 biLSTM 和 CRF 來訓練和評估 BERT、ALBERT、RoBERTa 和 GPT2。實驗結果表明，RoBERTa結合biLSTM和CRF的NER系統取得了最佳的系統性能，其中Precision為0.96，Recall為0.96，F1-score為0.96。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\QZ25VRTZ\\Yang 等。 - 2022 - BERT-based Chinese Medicine Named Entity Recogniti.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\SBJZUXTT\\10037867.html}
}

@online{turneyLeveragingTermBanks2017,
  title = {Leveraging {{Term Banks}} for {{Answering Complex Questions}}: {{A Case}} for {{Sparse Vectors}}},
  shorttitle = {Leveraging {{Term Banks}} for {{Answering Complex Questions}}},
  author = {Turney, Peter D.},
  date = {2017-04-11},
  eprint = {1704.03543},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.03543},
  url = {http://arxiv.org/abs/1704.03543},
  urldate = {2024-03-13},
  abstract = {While open-domain question answering (QA) systems have proven effective for answering simple questions, they struggle with more complex questions. Our goal is to answer more complex questions reliably, without incurring a significant cost in knowledge resource construction to support the QA. One readily available knowledge resource is a term bank, enumerating the key concepts in a domain. We have developed an unsupervised learning approach that leverages a term bank to guide a QA system, by representing the terminological knowledge with thousands of specialized vector spaces. In experiments with complex science questions, we show that this approach significantly outperforms several state-of-the-art QA systems, demonstrating that significant leverage can be gained from continuous vector representations of domain terminology.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning,H.3.1,I.2.6,I.2.7,問答系統,已整理,待讀,術語庫,重要},
  annotation = {titleTranslation: 利用術語庫回答複雜問題：稀疏向量的案例},
  note = {Comment: Related datasets can be found at http://allenai.org/data.html},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\WI83MIEA\\Turney - 2017 - Leveraging Term Banks for Answering Complex Questi.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\GIAFPZ3A\\1704.html}
}

@inproceedings{tysssantoshSaSAKESyntaxSemantics2020,
  title = {{{SaSAKE}}: {{Syntax}} and Semantics Aware Keyphrase Extraction from Research Papers},
  shorttitle = {{{SaSAKE}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {{Tyss Santosh} and {Debarshi Kumar Sanyal} and {Plaban Kumar Bhowmick} and {Partha Pratim Das}},
  date = {2020},
  pages = {5372--5383},
  doi = {10.18653/v1/2020.coling-main.469},
  langid = {english},
  keywords = {已整理,待讀,機器學習,語意分析},
  annotation = {4 citations (Crossref) [2024-03-26]\\
titleTranslation: SaSAKE：從研究論文中提取語法和語義感知的關鍵字詞},
  file = {C:\Users\BlackCat\Zotero\storage\86PYYMPR\Santosh 等。 - 2020 - SaSAKE Syntax and semantics aware keyphrase extra.pdf}
}

@online{UnderstandingImpactEHRRelated,
  title = {Towards {{Understanding}} the {{Impact}} of {{EHR-Related Information Overload}} on {{Provider Cognition}} | {{Semantic Scholar}}},
  url = {https://www.semanticscholar.org/paper/Towards-Understanding-the-Impact-of-EHR-Related-on-Rand-Coleman/e89bd012227f4411bd064e85f7c49dd9c8832250},
  urldate = {2023-10-23},
  langid = {english},
  keywords = {已整理,資訊超載,醫療,電子病歷},
  annotation = {titleTranslation: 理解 EHR 相關資訊過載對提供者認知的影響 |語意學者},
  file = {C:\Users\BlackCat\Zotero\storage\MBVVNR3E\e89bd012227f4411bd064e85f7c49dd9c8832250.html}
}

@inproceedings{vargasUserInterfaceExploring2020,
  title = {A {{User Interface}} for {{Exploring}} and {{Querying Knowledge Graphs}} ({{Extended Abstract}})},
  booktitle = {Proceedings of the {{Twenty-Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Vargas, Hernán and Buil-Aranda, Carlos and Hogan, Aidan and López, Claudia},
  date = {2020-07},
  pages = {4785--4789},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  location = {Yokohama, Japan},
  doi = {10.24963/ijcai.2020/666},
  url = {https://www.ijcai.org/proceedings/2020/666},
  urldate = {2024-01-15},
  abstract = {As the adoption of knowledge graphs grows, more and more non-experts users need to be able to explore and query such graphs. These users are not typically familiar with graph query languages such as SPARQL, and may not be familiar with the knowledge graph's structure. In this extended abstract, we provide a summary of our work on a language and visual interface -- called RDF Explorer -- that help non-expert users to navigate and query knowledge graphs. A usability study over Wikidata shows that users successfully complete more tasks with RDF Explorer than with the existing Wikidata Query Helper interface.},
  eventtitle = {Twenty-{{Ninth International Joint Conference}} on {{Artificial Intelligence}} and {{Seventeenth Pacific Rim International Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-PRICAI-20}}\vphantom\{\}},
  isbn = {978-0-9992411-6-5},
  langid = {english},
  keywords = {知識圖譜},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 用於探索和查詢知識圖的使用者介面（擴展摘要）},
  file = {C:\Users\BlackCat\Zotero\storage\TEHBW9BE\Vargas 等。 - 2020 - A User Interface for Exploring and Querying Knowle.pdf}
}

@article{vasanthonavarSymbolicArtificialIntelligence1995,
  title = {Symbolic {{Artificial Intelligence And Numeric Artificial Neural Networks}}: {{Towards A Resolution Of The Dichotomy}}},
  shorttitle = {Symbolic {{Artificial Intelligence And Numeric Artificial Neural Networks}}},
  author = {{Vasant Honavar}},
  date = {1995-07-06},
  issn = {978-0-7923-9517-1},
  doi = {10.1007/978-0-585-29599-2_11},
  abstract = {This memory can take several forms based on the time scales at which such modifications are allowed. Some symbol structures might have the property of determining choice and the order of application of transformations to be applied on other symbol structures. These are essentially the programs. Programs when executed --- typically through the conventional process of compilation and interpretation and eventually --- when they operate on symbols that are linked through grounding to particular effectors --- produce behavior. Working memory holds symbol structures as they are being processed. Long-term memory, generally speaking, is the repository of programs and can be changed by addition, deletion, or modification of symbol structures that it holds. Such a system can compute any Turing-computable function provided it has sufficiently large memory and its primitive set of transformations are adequate for the composition of arbitrarily symbol structures (programs) and the interpreter is capable of interpreting any possible symbol structure. This also means that any particular set of symbolic processes can be carried out by an NANN --- provided it has potentially infinite memory, or finds a way to use its transducers and effectors to use the external physical environment to serve as its memory). 14 Chapter 12 Knowledge in SAI systems is typically embedded in complex symbol structures such as lists (Norvig, 1992), logical databases (Genesereth and Nilsson, 1987), semantic networks (Quillian, 1968), frames (Minsky, 1975), schemas (Arbib, 1972; 1994), and manipulated by (often serial) procedures or inferences (e.g., list processing, application of production rules (Waterman, 1985), or execution of logic programs (Kowalski, 1977) carried out by a central processor that accesse...},
  keywords = {經典},
  annotation = {10 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\8QE2KBTP\Honavar - 1995 - Symbolic Artificial Intelligence And Numeric Artif.pdf}
}

@inproceedings{vidyasetlurOlioSemanticSearch2023,
  title = {Olio: {{A Semantic Search Interface}} for {{Data Repositories}}},
  shorttitle = {Olio},
  booktitle = {Proceedings of the 36th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {{Vidya Setlur} and {Andriy Kanyuka} and {Arjun Srinivasan}},
  year = {10 月 29, 2023},
  series = {{{UIST}} '23},
  pages = {1--16},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3586183.3606806},
  url = {https://dl.acm.org/doi/10.1145/3586183.3606806},
  urldate = {2023-11-01},
  abstract = {Search and information retrieval systems are becoming more expressive in interpreting user queries beyond the traditional weighted bag-of-words model of document retrieval. For example, searching for a flight status or a game score returns a dynamically generated response along with supporting, pre-authored documents contextually relevant to the query. In this paper, we extend this hybrid search paradigm to data repositories that contain curated data sources and visualization content. We introduce a semantic search interface, Olio, that provides a hybrid set of results comprising both auto-generated visualization responses and pre-authored charts to blend analytical question-answering with content discovery search goals. We specifically explore three search scenarios - question-and-answering, exploratory search, and design search over data repositories. The interface also provides faceted search support for users to refine and filter the conventional best-first search results based on parameters such as author name, time, and chart type. A preliminary user evaluation of the system demonstrates that Olio’s interface and the hybrid search paradigm collectively afford greater expressivity in how users discover insights and visualization content in data repositories.},
  isbn = {9798400701320},
  langid = {english},
  keywords = {curated data sources.,design search,dynamic and static content,exploratory search,federated querying,Hybrid search,question and answering,visualizations,人機互動,使用者研究,問答系統,已整理,略讀,語意搜尋,資料可視化,資料庫查詢},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: Olio：資料儲存庫的語意搜尋介面\\
abstractTranslation:  搜尋和資訊檢索系統在解釋使用者查詢方面變得越來越具有表現力，超越了文件檢索的傳統加權詞袋模型。例如，搜尋航班狀態或遊戲分數會傳回動態產生的回應以及與查詢情境相關的支援性預創作文件。在本文中，我們將此混合搜尋範例擴展到包含精選資料來源和視覺化內容的資料儲存庫。我們引入了一個語義搜尋介面 Olio，它提供了一組混合結果，其中包括自動生成的視覺化回應和預先創建的圖表，以將分析性問答與內容發現搜尋目標相結合。我們特別探索了三種搜尋場景——問答、探索性搜尋和資料儲存庫的設計搜尋。該介面還為用戶提供分面搜尋支持，以根據作者姓名、時間和圖表類型等參數細化和過濾傳統的最佳優先搜尋結果。使用者對此系統的初步評估表明，Olio 的介面和混合搜尋範式共同為使用者如何在資料儲存庫中發現見解和視覺化內容提供了更大的表現力。},
  note = {從使用者調查開始，調查使用資料庫查詢的人期望獲得怎樣的資料。並開發對應的系統。有大量內容都在描述系統評估的部分。
\par
與研究較無相關，應該沒什麼有用的。
\par
頂多問題意圖、對語意搜尋的解釋有用，但還不如直接找關於LLM對於QA系統的幫助有用。},
  file = {C:\Users\BlackCat\Zotero\storage\E65D6UIE\Setlur 等。 - 2023 - Olio A Semantic Search Interface for Data Reposit.pdf}
}

@article{vietbachnguyenPatternbasedDetectionExtraction2023,
  title = {Pattern-Based Detection, Extraction and Analysis of Code Lists in Ontologies and Vocabularies},
  author = {{Viet Bach Nguyen} and {Vojtěch Svátek}},
  year = {7 月 13, 2023},
  journaltitle = {Web Semantics: Science, Services and Agents on the World Wide Web},
  shortjournal = {Web Semant.},
  volume = {77},
  number = {C},
  issn = {1570-8268},
  doi = {10.1016/j.websem.2023.100788},
  url = {https://doi.org/10.1016/j.websem.2023.100788},
  urldate = {2023-09-27},
  abstract = {While the early phase of the Semantic Web put emphasis on conceptual modeling through ontology classes, and the recent years saw the rise of loosely structured, instance-level knowledge graphs (used even for modeling concepts), in this paper, we focus on a third kind of concept modeling: via code lists, primarily those embedded in ontologies and vocabularies. We attempt to characterize the candidate structures for code lists based on our observations in OWL ontologies. Our main contribution is then an approach implemented as a series of SPARQL queries and a lightweight web application that can be used to browse and detect potential code lists in ontologies and vocabularies, in order to extract and enhance them, and to store them in a stand-alone knowledge base. The application allows inspecting query results coming from the Linked Open Vocabularies catalog dataset. In addition, we describe a complementary bottom-up analysis of potential code lists. We also provide in this paper a demonstration of the dominant nature of embedded codes from the aspect of ontological universals and their alternatives for modeling code lists.},
  keywords = {Code list,Knowledge base,Knowledge graph,Knowledge representation,Ontology,RDF,Semantic Web,未整理},
  annotation = {0 citations (Crossref) [2024-03-26]}
}

@inproceedings{vivekiyerFrameworkSyntacticSemantic2022,
  title = {A {{Framework}} for~{{Syntactic}} and~{{Semantic Quality Evaluation}} of~{{Ontologies}}},
  booktitle = {Secure {{Knowledge Management In The Artificial Intelligence Era}}},
  author = {{Vivek Iyer} and {Lalit Mohan Sanagavarapu} and {Y. Raghu Reddy}},
  editor = {{Ram Krishnan} and {H. Raghav Rao} and {Sanjay K. Sahay} and {Sagar Samtani} and {Ziming Zhao}},
  date = {2022},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {73--93},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-97532-6_5},
  abstract = {The increasing focus on Web 3.0 is leading to automated creation and enrichment of ontologies and other linked datasets. Alongside automation, quality evaluation of enriched ontologies can impact software reliability and reuse. Current quality evaluation approaches oftentimes seek to evaluate ontologies in either syntactic (degree of following ontology development guidelines) or semantic (degree of semantic validity of enriched concepts/relations) aspects. This paper proposes an ontology quality evaluation framework consisting of: (a) SynEvaluator and (b) SemValidator for evaluating syntactic and semantic aspects of ontologies respectively. SynEvaluator allows dynamic task-specific creation and updation of syntactic rules at run-time without any need for programming. SemValidator uses Twitter-based expertise of validators for semantic evaluation. The efficacy and validity of the framework is shown empirically on multiple ontologies.},
  isbn = {978-3-030-97532-6},
  langid = {english},
  keywords = {/unread,Crowdsourcing,已整理,本體驗證,知識本體},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 本體句法和語義品質評估框架\\
abstractTranslation:  對 Web 3.0 的日益關注正在導致本體和其他連結資料集的自動建立和豐富。除了自動化之外，豐富本體的品質評估也會影響軟體的可靠性和重複使用。目前的品質評估方法通常尋求在句法（遵循本體開發指南的程度）或語義（豐富概念/關係的語義有效性程度）方面評估本體。本文提出了一個本體質量評估框架，包括：（a）SynEvaluator 和（b）SemValidator 分別用於評估本體的句法和語義方面。 SynEvaluator 允許在執行時間動態建立和更新特定於任務的語法規則，而無需任何程式設計。 SemValidator 使用基於 Twitter 的驗證器專業知識進行語義評估。該框架的有效性和有效性在多個本體上得到了實證證明。},
  note = {太長了沒看完},
  file = {C:\Users\BlackCat\Zotero\storage\EN7GT27X\Iyer 等。 - 2022 - A Framework for Syntactic and Semantic Quality Eva.pdf}
}

@inproceedings{vladimirtarasovApplicationInferenceRules2023,
  title = {Application of {{Inference Rules}} to a {{Software Requirements Ontology}} to {{Generate Software Test Cases}}},
  booktitle = {{{OWL}}: {{Experiences}} and {{Directions}} – {{Reasoner Evaluation}}: 13th {{International Workshop}}, {{OWLED}} 2016, and 5th {{International Workshop}}, {{ORE}} 2016, {{Bologna}}, {{Italy}}, {{November}} 20, 2016, {{Revised Selected Papers}}},
  author = {{Vladimir Tarasov} and {He Tan} and {Muhammad Ismail} and {Anders Adlemo} and {Mats Johansson}},
  year = {4 月 25, 2023},
  pages = {82--94},
  publisher = {Springer-Verlag},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-319-54627-8_7},
  url = {https://doi.org/10.1007/978-3-319-54627-8_7},
  urldate = {2023-09-26},
  abstract = {Testing of a software system is resource-consuming activity. One of the promising ways to improve the efficiency of the software testing process is to use ontologies for testing. This paper presents an approach to test case generation based on the use of an ontology and inference rules. The ontology represents requirements from a software requirements specification, and additional knowledge about components of the software system under development. The inference rules describe strategies for deriving test cases from the ontology. The inference rules are constructed based on the examination of the existing test documentation and acquisition of knowledge from experienced software testers. The inference rules are implemented in Prolog and applied to the ontology that is translated from OWL functional-style syntax to Prolog syntax. The first experiments with the implementation showed that it was possible to generate test cases with the same level of detail as the existing, manually produced, test cases.},
  isbn = {978-3-319-54626-1},
  keywords = {Inference rules,Ontology,OWL,Prolog,Requirement specification,Test case generation,已整理,知識本體,軟體測試},
  annotation = {5 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\PNVWCIU2\Tarasov 等。 - 2023 - Application of Inference Rules to a Software Requi.pdf}
}

@online{volkertrespTensorBrainSemantic2020,
  title = {The {{Tensor Brain}}: {{Semantic Decoding}} for {{Perception}} and {{Memory}}},
  shorttitle = {The {{Tensor Brain}}},
  author = {{Volker Tresp} and {Sahand Sharifzadeh} and {Dario Konopatzki} and {Yunpu Ma}},
  date = {2020-02-10},
  eprint = {2001.11027},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2001.11027},
  url = {http://arxiv.org/abs/2001.11027},
  urldate = {2023-07-18},
  abstract = {We analyse perception and memory, using mathematical models for knowledge graphs and tensors, to gain insights into the corresponding functionalities of the human mind. Our discussion is based on the concept of propositional sentences consisting of \textbackslash textit\{subject-predicate-object\} (SPO) triples for expressing elementary facts. SPO sentences are the basis for most natural languages but might also be important for explicit perception and declarative memories, as well as intra-brain communication and the ability to argue and reason. A set of SPO sentences can be described as a knowledge graph, which can be transformed into an adjacency tensor. We introduce tensor models, where concepts have dual representations as indices and associated embeddings, two constructs we believe are essential for the understanding of implicit and explicit perception and memory in the brain. We argue that a biological realization of perception and memory imposes constraints on information processing. In particular, we propose that explicit perception and declarative memories require a semantic decoder, which, in a simple realization, is based on four layers: First, a sensory memory layer, as a buffer for sensory input, second, an index layer representing concepts, third, a memoryless representation layer for the broadcasting of information ---the "blackboard", or the "canvas" of the brain--- and fourth, a working memory layer as a processing center and data buffer. We discuss the operations of the four layers and relate them to the global workspace theory. In a Bayesian brain interpretation, semantic memory defines the prior for observable triple statements. We propose that ---in evolution and during development--- semantic memory, episodic memory, and natural language evolved as emergent properties in agents' process to gain a deeper understanding of sensory information.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning,待讀,未整理},
  annotation = {7 citations (Semantic Scholar/arXiv) [2023-07-20]\\
titleTranslation: 張量大腦：感知和記憶的語義解碼\\
abstractTranslation:  我們使用知識圖和張量的數學模型來分析感知和記憶，以深入了解人類思維的相應功能。我們的討論基於由 \textbackslash textit\{主謂賓\} (SPO) 三元組組成的命題句子的概念，用於表達基本事實。 SPO 句子是大多數自然語言的基礎，但對於外顯感知和陳述性記憶、腦內交流以及爭論和推理的能力也可能很重要。一組SPO句子可以描述為知識圖，可以將其轉化為鄰接張量。我們引入張量模型，其中概念具有作為索引和相關嵌入的雙重表示，我們認為這兩種結構對於理解大腦中的內隱和外顯感知和記憶至關重要。我們認為感知和記憶的生物學實現對信息處理施加了限制。特別是，我們提出顯性感知和陳述性記憶需要一個語義解碼器，在簡單的實現中，它基於四個層：第一，感覺記憶層，作為感覺輸入的緩衝區，第二，表示概念的索引層第三，用於信息廣播的無記憶表示層——大腦的“黑板”或“畫布”；第四，作為處理中心和數據緩衝區的工作記憶層。我們討論四層的操作並將它們與全局工作空間理論聯繫起來。在貝葉斯大腦解釋中，語義記憶定義了可觀察的三重陳述的先驗。我們提出，在進化和發展過程中，語義記憶、情景記憶和自然語言作為智能體過程中的新興屬性而進化，以獲得對感官信息的更深入理解。},
  file = {C:\Users\BlackCat\Zotero\storage\9SYST5WR\Tresp et al. - 2020 - The Tensor Brain Semantic Decoding for Perception and Memory.pdf}
}

@article{WangBuShengYiYuanZhongYiYaoXinXiHuaJianShe2019,
  title = {医院中医药信息化建设},
  author = {{汪步升}},
  date = {2019},
  journaltitle = {电脑编程技巧与维护},
  number = {5},
  pages = {2},
  abstract = {随着我国对中医药的大力发展,医院加强中医药信息化建设,根据实际使用情况对已有的中医电子病历系统和电子处方等进行优化和功能完善,提高医生工作效率,提升医院信息化水平.利用互联网技术,搭建自助签到取药系统,简化取药流程,减少等候时间,缓解医患矛盾,提高中药房信息管理.},
  keywords = {中醫,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\K8TDU2TP\汪步升 - 2019 - 医院中医药信息化建设.pdf}
}

@article{WangChunYuShuJuWaJueZaiJieGouHuaDianZiBingLiZhongDeYingYong2014,
  title = {数据挖掘在结构化电子病历中的应用},
  author = {{王春雨} and {王立准} and {魏瑜帅}},
  date = {2014},
  journaltitle = {医学信息学杂志},
  volume = {35},
  number = {3},
  pages = {31--33},
  doi = {10.3969/j.issn.1673-6036.2014.03.007},
  abstract = {介绍结构化电子病历数据挖掘的工具和过程,包括建立模型,设置关键绩效指标,分析器的使用,绘制控制面板等,在此基础上进行系统设计并投入使用.应用结果表明该系统能够为数据采集与挖掘提供强有力的辅助,提高运行效率.},
  file = {C:\Users\BlackCat\Zotero\storage\DL9HM9G3\王春雨 等。 - 2014 - 数据挖掘在结构化电子病历中的应用.pdf}
}

@inproceedings{wangDomainRepresentationKnowledge2019,
  title = {Domain {{Representation}} for {{Knowledge Graph Embedding}}},
  booktitle = {Natural {{Language Processing}} and {{Chinese Computing}}},
  author = {Wang, Cunxiang and Ren, Feiliang and Lin, Zhichao and Zhao, Chenxu and Xie, Tian and Zhang, Yue},
  editor = {Tang, Jie and Kan, Min-Yen and Zhao, Dongyan and Li, Sujian and Zan, Hongying},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {197--210},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-32233-5_16},
  abstract = {Embedding entities and relations into a continuous multi-dimensional vector space have become the dominant method for knowledge graph embedding in representation learning. However, most existing models ignore to represent hierarchical knowledge, such as the similarities and dissimilarities of entities in one domain. We proposed to learn a Domain Representations over existing knowledge graph embedding models, such that entities that have similar attributes are organized into the same domain. Such hierarchical knowledge of domains can give further evidence in link prediction. Experimental results show that domain embeddings give a significant improvement over the most recent state-of-art baseline knowledge graph embedding models.},
  isbn = {978-3-030-32233-5},
  langid = {english},
  keywords = {Domain,Representation learning,嵌入,微讀,數學,知識圖譜},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 知識圖嵌入的領域表示},
  note = {使用類似知識本體的方式來定義domain，使的KG預測具有更高的準確性。
\par
使用向量空間中的超橢圓來實現domain\\
domains are restricted using hyper-ellipsoids
\par
偏向數學與理論，與研究較無相關},
  file = {C:\Users\BlackCat\Zotero\storage\46UNRLMY\Wang 等。 - 2019 - Domain Representation for Knowledge Graph Embeddin.pdf}
}

@online{wangGPLGenerativePseudo2022,
  title = {{{GPL}}: {{Generative Pseudo Labeling}} for {{Unsupervised Domain Adaptation}} of {{Dense Retrieval}}},
  shorttitle = {{{GPL}}},
  author = {Wang, Kexin and Thakur, Nandan and Reimers, Nils and Gurevych, Iryna},
  date = {2022-04-25},
  eprint = {2112.07577},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.07577},
  url = {http://arxiv.org/abs/2112.07577},
  urldate = {2024-03-14},
  abstract = {Dense retrieval approaches can overcome the lexical gap and lead to significantly improved search results. However, they require large amounts of training data which is not available for most domains. As shown in previous work (Thakur et al., 2021b), the performance of dense retrievers severely degrades under a domain shift. This limits the usage of dense retrieval approaches to only a few domains with large training datasets. In this paper, we propose the novel unsupervised domain adaptation method Generative Pseudo Labeling (GPL), which combines a query generator with pseudo labeling from a cross-encoder. On six representative domain-specialized datasets, we find the proposed GPL can outperform an out-of-the-box state-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL requires less (unlabeled) data from the target domain and is more robust in its training than previous methods. We further investigate the role of six recent pre-training methods in the scenario of domain adaptation for retrieval tasks, where only three could yield improved results. The best approach, TSDAE (Wang et al., 2021) can be combined with GPL, yielding another average improvement of 1.4 points nDCG@10 across the six tasks. The code and the models are available at https://github.com/UKPLab/gpl.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,嵌入,已整理,待讀,檢索,資料集,重要,領域適應},
  annotation = {titleTranslation: GPL：用於密集檢索的無監督域適應的生成偽標籤\\
abstractTranslation:  密集檢索方法可以克服詞彙差距並顯著改善搜尋結果。然而，它們需要大量的訓練數據，而這對於大多數領域來說是不可用的。如同先前的工作（Thakur 等人，2021b）所示，密集檢索器的表現在域轉移下嚴重下降。這限制了密集檢索方法的使用僅限於具有大型訓練資料集的少數領域。在本文中，我們提出了新穎的無監督域適應方法來產生偽標籤（GPL），它將查詢產生器與來自交叉編碼器的偽標籤結合。在六個代表性領域專業資料集上，我們發現所提出的 GPL 可以比開箱即用的最先進的密集檢索方法高出高達 9.3 個點 nDCG@10。 GPL 需要來自目標域的較少（未標記）數據，其訓練比以前的方法更加穩健。我們進一步研究了最近六種預訓練方法在檢索任務領域適應情境中的作用，其中只有三種可以產生改進的結果。最好的方法 TSDAE（Wang 等人，2021）可以與 GPL 結合起來，在六個任務中又平均提高 1.4 點 nDCG@10。程式碼和模型可在 https://github.com/UKPLab/gpl 取得。},
  note = {Comment: Accepted at NAACL 2022},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\3PGXSWYH\\Wang 等。 - 2022 - GPL Generative Pseudo Labeling for Unsupervised D.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\RGL5LAEC\\2112.html}
}

@article{WangHuaZhen;LuBing;HongYanZhu;wanghua-zhen;lubing;hongyan-zhuJiYuYiZhiXingYuCeQiDeZhongYiZhengSuZuHeZhenDuanMoXing2014,
  title = {基于一致性预测器的中医证素组合诊断模型},
  author = {{王华珍; 吕兵; 洪燕珠; WANG Hua-zhen; L(U) Bing; HONG Yan-zhu}},
  date = {2014},
  journaltitle = {厦门大学学报（自然科学版）},
  volume = {53},
  number = {1},
  pages = {41--45},
  issn = {0438-0479},
  doi = {10.6043/j.issn.0438-0479.2014.01.009},
  abstract = {构建中医证素组合智能诊断模型需要特殊的域预测分类器而非传统的点预测分类器.引入一致性预测器(conformal predictor,CP),以算法随机性水平值为证素的重要性度量,以算法风险水平为阈值进行域预测输出,以中医慢性疲劳样本集为研究对象,随机森林(random forest,RF)等传统机器学习算法被嵌入到CP框架中计算样本奇异值.实验结果表明,CP-RF模型不仅拟合率比其他域预测分类器高,还对阈值具有很好的鲁棒性,克服了阈值对预测域的波动性,解决了中医多证素组合诊断关键的技术难题之一,同时CP-RF模型的预测域错误率能够被算法风险水平阈值所校准,表明其阈值具有明确的统计意义和可解释性,能够被中医医生所接受.},
  langid = {chi},
  keywords = {一致性预测器 证素 中医 慢性疲劳},
  annotation = {abstractTranslation:  構建中醫證素組合智能診斷模型需要特殊的域預測分類器而不是傳統的點預測分類器。以算法風險水平為閾值進行域預測輸出，以中醫慢性疲勞樣本集為研究對象，隨機森林（隨機森林，RF）等傳統機器學習算法被嵌入到CP框架中計算樣本奇異值。實驗結果證明，CP -RF模型不僅單一率比其他域預測分類器高，還對閾值具有很好的魯棒性，克服了閾值對預測域的波動性，解決了中醫多素組合診斷關鍵的技術難題之一,同時CP-RF模型的預測域錯誤率能夠被算法算法風險水平閾值所設計,表明其閾值具有明確的統計意義和可解釋性,能夠被中醫醫生所接受。\\
titleTranslation: 基於一致性預測器的中醫證素組合診斷模型},
  note = {作者个数:3;第一作者:王华珍
\par
JF Journal of Xiamen University(Natural Science)
\par
The following values have no corresponding Zotero field:\\
AD 华侨大学; 厦门大学\\
PP 中国\\
DS 万方数据 基金项目:福建省自然科学基金; 华侨大学高层次人才科研项目},
  file = {C:\Users\BlackCat\Zotero\storage\9GY3GF8Q\王华珍; 吕兵; 洪燕珠; WANG Hua-zhen; L(U) Bing; HONG Yan-zhu - 2014 - 基于一致性预测器的中医证素组合诊断模型.pdf}
}

@article{WangHuaZhenJiYuSuiJiSenLinDeZhongYiShuJuKeShiHuaYanJiu2014,
  title = {基于随机森林的中医数据可视化研究},
  author = {{王华珍} and {彭淑娟} and {缑锦} and {陈锻生}},
  date = {2014},
  journaltitle = {系统仿真学报},
  volume = {26},
  number = {11},
  pages = {2751--2756},
  url = {http://www.cqvip.com/qk/96569x/201411/663479197.html},
  urldate = {2022-08-05},
  abstract = {中医诊疗研究引入机器学习方法存在交互性差和特征值离散性两大缺陷。引入基于随机森林(Random Forest,RF)的可视化技术,对原始数据进行基于RF的特征变换,使样本在新特征空间的类可分性增强;采用主坐标分析法对变换后的...},
  keywords = {No DOI found}
}

@article{WangHuaZhenZhongYiNeiShengWuXieDeZhiNengZhengXingFenLei2011,
  title = {中医"内生五邪"的智能证型分类},
  author = {{王华珍} and {胡雪琴}},
  date = {2011},
  journaltitle = {计算机工程与应用},
  volume = {47},
  number = {6},
  pages = {156--160,163},
  issn = {1002-8331},
  doi = {10.3778/j.issn.1002-8331.2011.06.043},
  abstract = {采用随机森林算法对语料库中医"内生五邪"病证知识进行数据挖掘.采用随机森林对五邪病证的临床特征体系构建"内生五邪"智能诊断模型.采用改进的随机森林特征重要性度量方法针对各个类进行特征重要性度量.实验结果验证了该方法的有效性和优越性,能够胜任对疾病过程中所产生的类似于风、寒、湿、燥、火邪等五种病理状态进行深入细致的客观化研究.该研究将为中医临床医生提供一个诊疗决策的优良工具.},
  langid = {chi},
  keywords = {医案 "内生五邪" 分类 随机森林},
  annotation = {abstractTranslation:  採用隨機森林算法對語料庫中醫“內生五邪”病證知識進行數據挖掘。採用隨機森林對五邪病證的臨床特徵體系構建“內生五邪”智能診斷模型。採用改進的隨機森林特徵重要性聚焦方法針對每一類進行特徵聚焦。實驗結果驗證了該方法的有效性和相關性，能夠勝任對疾病過程中所產生的風、寒、濕、燥、火邪等五種病理狀態進行深入的調查研究。該研究將為中醫臨床醫生提供一個診斷決策的優良工具。\\
titleTranslation: 中醫"內生五邪"的智能證型分類},
  note = {作者个数:2;第一作者:王华珍
\par
JF COMPUTER ENGINEERING AND APPLICATIONS
\par
The following values have no corresponding Zotero field:\\
AD 华侨大学; 中国中医科学院中医药信息研究所\\
PP 中国\\
DS 万方数据 基金项目:厦门市科技计划; 福建省自然基金项目},
  file = {C:\Users\BlackCat\Zotero\storage\W9FRY28X\王华珍; 胡雪琴; WANG Huazhen; HU Xueqin - 2011 - 中医内生五邪的智能证型分类.pdf}
}

@thesis{WangJianLongZhiYuanZhenLieXingTaiCeShiAnLiDeHeiXiangHanShiCengJiDanYuanCeShi2019,
  title = {支援陣列型態測試案例的黑箱函式層級單元測試},
  author = {{王建瓏}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2019},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/h6b857},
  abstract = {陣列等集合型態的測試案例產生比純量型態的測試案例產生更複雜。對於這個問題，黑箱測試又比白箱測試更具有挑戰性，因為規格通常並沒有具體描述集合型態是如何被使用的。 本論文針對黑箱函式層級單元測試，提出一個基於限制式的框架，來自動產生陣列型態的測試案例。這個基於限制式的框架使用物件限制語言當作規格語言。本論文擴充物件限制語言，以支援陣列型態。本論文支援靜態陣列，動態固定大小陣列，及動態變動大小陣列。 這個基於限制式的框架接著將基於限制式的規格轉換成基於限制式的測試模型，稱為限制邏輯圖。本論文在限制邏輯圖上進行黑箱單元測試中的等價類別分割，邊界值分析，及測試覆蓋標準管理。這個基於限制式的框架接著將函式層級的測試案例產生問題定義成可從限制邏輯圖上產生的一組限制式滿足問題。 陣列型態的測試案例產生不僅牽涉到陣列元素值，也牽涉到陣列大小。這組限制式滿足問題可以使用限制邏輯程式去求解，以產生測試案例。 本論文實作了物件限制語言中每一個陣列運算所對應的限制邏輯述語。本論文也使用一組Java基準，針對這個基於限制式的框架，進行了初步的評估。 關鍵字：限制式測試；黑箱測試；函式層級單元測試；測試案例產生；陣列型態},
  pagetotal = {156},
  keywords = {實驗室}
}

@thesis{WangJunHongJavaTestYiGeJavaChengShiYuYanDeDanYuanCeShiGongJu2003,
  title = {{{JavaTest}}：{{一個Java程式語言的單元測試工具}}},
  author = {{王鈞鴻}},
  namea = {{林迺衛}},
  nameatype = {collaborator},
  date = {2003},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/m9rs95},
  abstract = {軟體測試在整個軟體系統開發的過程中扮演非常重要的角色，而單元測試則是軟體測試最基礎的一部份。本論文研製了一個軟體測試的工具JavaTest，以輔助Java程式的單元測試。這個工具可幫助我們加速軟體的開發及提昇軟體的品質。本論文將說明這個工具的研發過程，同時也展示它的使用方式。這個工具具備了下列特色： (1) 這套工具提供一個具有圖形介面的整合環境。 (2) 一個測試案件是用一組測試資料及預期結果來定義。 (3) 一個測試套件可以輕易地用一組測試案件或測試套件來組成。 (4) 測試驅動程式是根據測試案件或測試套件自動產生的。 (5) 單元測試的執行及驗證是自動完成的。 (6) 測試結果會被自動記錄下來，以供未來檢視及診斷之用。},
  pagetotal = {119}
}

@online{wangKnowledgeDrivenCoTExploring2023,
  title = {Knowledge-{{Driven CoT}}: {{Exploring Faithful Reasoning}} in {{LLMs}} for {{Knowledge-intensive Question Answering}}},
  shorttitle = {Knowledge-{{Driven CoT}}},
  author = {Wang, Keheng and Duan, Feiyu and Wang, Sirui and Li, Peiguang and Xian, Yunsen and Yin, Chuantao and Rong, Wenge and Xiong, Zhang},
  date = {2023-10-28},
  eprint = {2308.13259},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.13259},
  url = {http://arxiv.org/abs/2308.13259},
  urldate = {2024-04-10},
  abstract = {Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown impressive reasoning ability in various downstream tasks. Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA. To alleviate this issue, we propose a framework called Knowledge-Driven Chain-of-Thought (KD-CoT) to verify and modify reasoning traces in CoT via interaction with external knowledge, and thus overcome the hallucinations and error propagation. Concretely, we formulate the CoT rationale process of LLMs into a structured multi-round QA format. In each round, LLMs interact with a QA system that retrieves external knowledge and produce faithful reasoning traces based on retrieved precise answers. The structured CoT reasoning of LLMs is facilitated by our developed KBQA CoT collection, which serves as in-context learning demonstrations and can also be utilized as feedback augmentation to train a robust retriever. Extensive experiments on WebQSP and ComplexWebQuestion datasets demonstrate the effectiveness of proposed KD-CoT in task-solving reasoning generation, which outperforms the vanilla CoT ICL with an absolute success rate of 8.0\% and 5.1\%. Furthermore, our proposed feedback-augmented retriever outperforms the state-of-the-art baselines for retrieving knowledge, achieving significant improvement in Hit and recall performance. Our code and data are released on https://github.com/AdelWang/KD-CoT/tree/main.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,LLM,問答系統,已整理,經典,重要},
  annotation = {abstractTranslation:  配備思想鏈（CoT）的大型語言模型（LLM）在各種下游任務中展現了令人印象深刻的推理能力。即便如此，由於受到幻覺和無法獲取外部知識的困擾，法學碩士常常會出現不正確或更忠實的中間推理步驟，特別是在回答 KBQA 等知識密集型任務的情況下。為了緩解這個問題，我們提出了一個名為知識驅動思想鏈（KD-CoT）的框架，透過與外部知識的互動來驗證和修改CoT中的推理痕跡，從而克服幻覺和錯誤傳播。具體來說，我們將法學碩士的 CoT 基本原理流程製定為結構化的多輪 QA 格式。在每一輪中，法學碩士與 QA 系統進行交互，該系統檢索外部知識並根據檢索到的精確答案生成忠實的推理軌跡。我們開發的 KBQA CoT 集合促進了 LLM 的結構化 CoT 推理，該集合可用作上下文學習演示，也可用作反饋增強來訓練強大的檢索器。在 WebQSP 和 ComplexWebQuestion 資料集上的大量實驗證明了所提出的 KD-CoT 在任務解決推理生成中的有效性，其性能優於普通 CoT ICL，絕對成功率為 8.0\% 和 5.1\%。此外，我們提出的回饋增強檢索器的性能優於最先進的知識檢索基線，在命中和召回性能方面實現了顯著改善。我們的程式碼和資料發佈在 https://github.com/AdelWang/KD-CoT/tree/main 上。\\
titleTranslation: 知識驅動的 CoT：探索法學碩士中知識密集問答的忠實推理},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\YL6IIIQF\\Wang 等。 - 2023 - Knowledge-Driven CoT Exploring Faithful Reasoning.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\QRYE73M3\\2308.html}
}

@online{wangLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Source Planner}} for {{Personalized Knowledge-grounded Dialogue}}},
  author = {Wang, Hongru and Hu, Minda and Deng, Yang and Wang, Rui and Mi, Fei and Wang, Weichao and Wang, Yasheng and Kwan, Wai-Chung and King, Irwin and Wong, Kam-Fai},
  date = {2023-10-13},
  url = {https://arxiv.org/abs/2310.08840v1},
  urldate = {2024-04-11},
  abstract = {Open-domain dialogue system usually requires different sources of knowledge to generate more informative and evidential responses. However, existing knowledge-grounded dialogue systems either focus on a single knowledge source or overlook the dependency between multiple sources of knowledge, which may result in generating inconsistent or even paradoxical responses. To incorporate multiple knowledge sources and dependencies between them, we propose SAFARI, a novel framework that leverages the exceptional capabilities of large language models (LLMs) in planning, understanding, and incorporating under both supervised and unsupervised settings. Specifically, SAFARI decouples the knowledge grounding into multiple sources and response generation, which allows easy extension to various knowledge sources including the possibility of not using any sources. To study the problem, we construct a personalized knowledge-grounded dialogue dataset \textbackslash textit\{\textbackslash textbf\{K\}nowledge \textbackslash textbf\{B\}ehind \textbackslash textbf\{P\}ersona\}\textasciitilde (\textbackslash textbf\{KBP\}), which is the first to consider the dependency between persona and implicit knowledge. Experimental results on the KBP dataset demonstrate that the SAFARI framework can effectively produce persona-consistent and knowledge-enhanced responses.},
  langid = {english},
  organization = {arXiv.org},
  keywords = {LLM,RAG,個人化,問答系統,已整理,待讀},
  annotation = {titleTranslation: 大型語言模型作為個人化基於知識的對話的來源規劃器\\
abstractTranslation:  開放域對話系統通常需要不同的知識來源來產生更多資訊和證據的回應。然而，現有的基於知識的對話系統要么關注單一知識源，要么忽視多個知識源之間的依賴關係，這可能導致產生不一致甚至自相矛盾的回應。為了整合多個知識來源和它們之間的依賴關係，我們提出了 SAFARI，這是一個新穎的框架，它利用大型語言模型 (LLM) 在監督和無監督環境下規劃、理解和整合的卓越能力。具體來說，SAFARI 將知識基礎解耦為多個來源和回應生成，這允許輕鬆擴展到各種知識來源，包括不使用任何來源的可能性。為了研究這個問題，我們建立了一個個人化的知識為基礎的對話資料集\textbackslash textit\{\textbackslash textbf\{K\}nowledge \textbackslash textbf\{B\}ehind \textbackslash textbf\{P\}persona\}\textasciitilde (\textbackslash textbf\{KBP\})，這是第一個考慮人物角色和隱性知識之間的依賴關係。 KBP 資料集上的實驗結果表明，SAFARI 框架可以有效地產生角色一致且知識增強的反應。},
  file = {C:\Users\BlackCat\Zotero\storage\TB73VAMN\Wang 等。 - 2023 - Large Language Models as Source Planner for Person.pdf}
}

@article{WangPeiJiYuBERTMoXingDeZhongYiWenBenFenLeiYanJiu2021,
  title = {{{基于BERT模型的中医文本分类研究}}},
  author = {{王培} and {王亚文} and {卢苗苗}},
  date = {2021},
  journaltitle = {电脑知识与技术:学术版},
  volume = {17},
  number = {27},
  pages = {3},
  abstract = {文本分类是自然语言领域一个重要的研究方向和技术核心,一直受到研究者的热切关注.在医学领域,中医源远流长,在人类历史发展中发挥着不可磨灭的作用.中医语言包含了大量中医领域术语,且多为表述严谨和富含辩证思维的古文,上下文词语关联性较强,且大多是结构化,半结构化或非结构化数据的形式,这些特点给中医病案的智能分析分类造成了很大地困难.该文基于注意力机制的深度学习模型Bert模型实现中医深层全局语义的特征表示,并进行中医临床文本的分类研究.最后通过对中医临床文本分类实验的验证,该模型达到了非常可观的分类效果.},
  keywords = {中醫,機器學習},
  file = {C:\Users\BlackCat\Zotero\storage\4PYULW76\王培 等。 - 2021 - 基于BERT模型的中医文本分类研究.pdf}
}

@article{WangQiongJiYuMoShiZiDongHuoQuZhongYiLinChuangZhengZhuangShuYu2018,
  title = {基于模式自动获取中医临床症状术语},
  author = {{王琼} and {刘亮亮} and {张晓如} and {曹馨宇}},
  date = {2018},
  journaltitle = {中国数字医学},
  volume = {13},
  number = {3},
  pages = {3},
  url = {http://www.cnki.com.cn/Article/CJFDTotal-YISZ201803016.htm},
  urldate = {2022-10-18},
  abstract = {针对中医临床症状术语的构成特点,提出了一种基于词性模式的以《中医临床常见症状术语规范》中的标准术语为种子集的中医症状术语自动获取方法.首先,基于种子术语,获取种子术语中的特征词,构建种子词典;然后获取临床症状术语的词性模式,从语料中抽取所有满足词性模式并且包含种子词的词串;最后,基于规则和统计的方法,对获取的候选术语进行过滤,生成中医临床症状术语集.在测试集上抽取结果的准确率为81.1\%,召回率达到82.2\%,F值为81.6\%.},
  keywords = {中医症状术语,种子词典,词性模式},
  file = {C:\Users\BlackCat\Zotero\storage\7BRETU3H\王 等。 - 2018 - 基于模式自动获取中医临床症状术语.pdf}
}

@thesis{WangQiongZhongYiZhengZhuangShuYuZiDongHuoQuYanJiu2018,
  title = {中医症状术语自动获取研究},
  author = {{王琼}},
  date = {2018},
  institution = {江苏科技大学},
  url = {http://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CDFD&dbname=CMFD&filename=1018848039.nh},
  urldate = {2022-10-18},
  abstract = {中医症状信息是临床研究的基础数据资源,为了推动中医信息化进程,深入挖掘中医临床知识,需要大量的临床症状术语作为支持。术语的自动获取是自然语言处理的一个难点,由于中医语料的特殊性,从中医语料中自动获取中医术语更是一个研究难题。本文在分析了中医症状术语特征的基础上,利用自然语言处理技术从中医临床病历文本中对中医症状术语进行自动获取。针对现有中医临床术语资源缺乏,术语自动获取存在着技术难点等问题,本文借鉴了命名实体识别的方法理论,并结合了中医临床病历文本的特点,主要的研究内容有:1.参考现有中医语料库的构建方法和标注规范,构建了适用于症状术语获取的语料;2.通过分析中医临床病历文本中症状术语的构成模式,提出了一种利用术语构词模式来对中医症状术语进行自动获取的方法;3.针对于基于模式自动获取的低召回率问题,本文提出了一种基于泛化模式与统计特征相结合的方法,从中医临床病历中进行症状术语的自动获取。实验结果表明,该方法取得准确率81.1\%,召回率82.2\%,F值81.6\%;4.在获取了大量临床症状术语后,结合《中医临床常见症状术语规范》和《中医症状学研究》两部中医著作中收录的症状术语,利用本体构建方法,以中医四诊为顶层分类概念,构建了中医症状本体知识表示模型。},
  file = {C:\Users\BlackCat\Zotero\storage\M7LQMT9H\王琼 - 中医症状术语自动获取研究.pdf}
}

@article{WangRenShujen-shuwangYingYongJueCeShuLiLunYuZhongYiBianZhengYiManXingKeSouWeiLi2008,
  title = {應用決策樹理論於中醫辨證－以慢性咳嗽為例},
  author = {{王人澍(Jen-Shu Wang)} and {張寶源(Pao-Yuan Chang)} and {熊雅意(Ya-I Hsiung)}},
  date = {2008},
  journaltitle = {中西整合醫學雜誌},
  shortjournal = {中西整合醫學雜誌},
  volume = {10},
  number = {2},
  pages = {25--33},
  publisher = {臺灣中西整合醫學會},
  issn = {1607-2989},
  doi = {10.29613/jicwm.200812.0003},
  url = {http://dx.doi.org/10.29613/JICWM.200812.0003},
  abstract = {中醫治療疾病強調辨證論治，但由於臨床上病人症狀的表現往往並不典型，且存在醫師主觀認定之因素，因而影響了由「症」誰「證」的正確性。應用資料探勘技術能由大量的診療記錄中，有效的挖掘出辨證的重要屬性，進而提出客觀的診斷指導原則。 本研究將整理中榮中醫科歷年來的病歷資料，記錄其臨床症狀表現、治療原則、與治療成效，以決策樹分類理論對此資料庫進行知識挖掘，提出客觀化的辨證標準，並評估此標準的診斷正確率。},
  langid = {zh\_CN},
  keywords = {decision tree,identify patterns and determine treatment,knowledge discovering,決策樹,知識挖掘,辨證},
  annotation = {abstractTranslation:  中醫治療疾病強調辨證論治，但由於臨床上患者的症狀表現往往並不典型，且存在醫師優越認定之因素，從而影響了由“症”誰“證”的正確性。應用數據探查技術能夠通過大量的本有效研究將整理中榮中醫科歷年來的病歷資料，記錄其臨床症狀表現、治療原則、與治療的重要成效，以決策樹分類理論對此資料庫進行知識挖掘，進行偵查的辨證標準，並評估該標準的診斷正確率。\\
titleTranslation: 應用決策樹理論於中醫辨證-以慢性咳嗽為例},
  file = {C:\Users\BlackCat\Zotero\storage\8KGT3D2C\王人澍(Jen-Shu Wang) 等。 - 2008 - 應用決策樹理論於中醫辨證－以慢性咳嗽為例.pdf}
}

@inproceedings{wangSelfInstructAligningLanguage2023,
  title = {Self-{{Instruct}}: {{Aligning Language Models}} with {{Self-Generated Instructions}}},
  shorttitle = {Self-{{Instruct}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  date = {2023-07},
  pages = {13484--13508},
  publisher = {Association for Computational Linguistics},
  location = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.754},
  url = {https://aclanthology.org/2023.acl-long.754},
  urldate = {2024-04-22},
  abstract = {Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
  eventtitle = {{{ACL}} 2023},
  langid = {english},
  keywords = {LLM,未整理,機器學習},
  annotation = {49 citations (Crossref) [2024-04-27]\\
abstractTranslation:  大型「指令調整」語言模型（即經過微調以回應指令）已表現出將零樣本推廣到新任務的非凡能力。然而，它們嚴重依賴人工編寫的指令數據，而這些數據通常在數量、多樣性和創造力方面受到限制，因此阻礙了調整模型的通用性。我們引入了 Self-Instruct，這是一個透過引導自己的世代來提高預訓練語言模型的指令追蹤能力的框架。我們的管道從語言模型產生指令、輸入和輸出樣本，然後過濾無效或相似的樣本，然後使用它們來微調原始模型。將我們的方法應用於普通 GPT3，我們在 Super-NaturalInstructions 上展示了比原始模型 33\% 的絕對改進，與使用私人用戶資料和人工註釋進行訓練的 InstructGPT-001 的性能相當。為了進一步評估，我們為新任務策劃了一套專家編寫的指令，並透過人工評估表明，使用Self-Instruct 調整GPT3 的效能大幅優於使用現有公共指令資料集，僅落後InstructGPT 5\% 的絕對差距。 Self-Instruct 提供了一種幾乎無需註釋的方法，用於將預訓練的語言模型與指令對齊，並且我們發布了大型合成資料集，以促進未來指令調優的研究。},
  file = {C:\Users\BlackCat\Zotero\storage\AUYID9I6\Wang 等。 - 2023 - Self-Instruct Aligning Language Models with Self-.pdf}
}

@online{wangTheoreticalAnalysisNDCG2013,
  title = {A {{Theoretical Analysis}} of {{NDCG Type Ranking Measures}}},
  author = {Wang, Yining and Wang, Liwei and Li, Yuanzhi and He, Di and Liu, Tie-Yan and Chen, Wei},
  date = {2013-04-24},
  eprint = {1304.6480},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1304.6480},
  url = {http://arxiv.org/abs/1304.6480},
  urldate = {2024-03-13},
  abstract = {A central problem in ranking is to design a ranking measure for evaluation of ranking functions. In this paper we study, from a theoretical perspective, the widely used Normalized Discounted Cumulative Gain (NDCG)-type ranking measures. Although there are extensive empirical studies of NDCG, little is known about its theoretical properties. We first show that, whatever the ranking function is, the standard NDCG which adopts a logarithmic discount, converges to 1 as the number of items to rank goes to infinity. On the first sight, this result is very surprising. It seems to imply that NDCG cannot differentiate good and bad ranking functions, contradicting to the empirical success of NDCG in many applications. In order to have a deeper understanding of ranking measures in general, we propose a notion referred to as consistent distinguishability. This notion captures the intuition that a ranking measure should have such a property: For every pair of substantially different ranking functions, the ranking measure can decide which one is better in a consistent manner on almost all datasets. We show that NDCG with logarithmic discount has consistent distinguishability although it converges to the same limit for all ranking functions. We next characterize the set of all feasible discount functions for NDCG according to the concept of consistent distinguishability. Specifically we show that whether NDCG has consistent distinguishability depends on how fast the discount decays, and 1/r is a critical point. We then turn to the cut-off version of NDCG, i.e., NDCG@k. We analyze the distinguishability of NDCG@k for various choices of k and the discount functions. Experimental results on real Web search datasets agree well with the theory.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Information Retrieval,問答系統,基礎理論,已整理,排序任務,機器學習,評估},
  annotation = {titleTranslation: NDCG類型排序指標的理論分析},
  note = {Comment: COLT 2013},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\ZUHKHYJJ\\Wang 等。 - 2013 - A Theoretical Analysis of NDCG Type Ranking Measur.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\8MDUUQ8C\\1304.html}
}

@online{wangUniMSRAGUnifiedMultisource2024,
  title = {{{UniMS-RAG}}: {{A Unified Multi-source Retrieval-Augmented Generation}} for {{Personalized Dialogue Systems}}},
  shorttitle = {{{UniMS-RAG}}},
  author = {Wang, Hongru and Huang, Wenyu and Deng, Yang and Wang, Rui and Wang, Zezhong and Wang, Yufei and Mi, Fei and Pan, Jeff Z. and Wong, Kam-Fai},
  date = {2024-01-24},
  url = {https://arxiv.org/abs/2401.13256v1},
  urldate = {2024-04-11},
  abstract = {Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems.},
  langid = {english},
  organization = {arXiv.org},
  keywords = {RAG,已整理,待讀,文獻,未整理,重要},
  annotation = {titleTranslation: UniMS-RAG：個人化對話系統的統一多源檢索增強生成\\
abstractTranslation:  大型語言模型 (LLM) 在許多自然語言理解和生成任務中表現出了卓越的能力。然而，個人化問題仍然是一個令人垂涎的屬性，特別是當涉及對話系統中涉及的多個來源時。為了更好地規劃和整合多個來源的使用來產生個人化回應，我們首先將其分解為三個子任務：知識來源選擇、知識檢索和回應生成。然後，我們提出了一種新穎的統一多源檢索增強生成系統（UniMS-RAG）具體來說，我們在訓練期間將這三個具有不同公式的子任務統一到相同的序列到序列範式中，以自適應地檢索證據並評估使用特殊令牌（稱為代理令牌和評估令牌）按需關聯。使語言模型能夠產生動作令牌有助於與各種知識來源的交互，使它們能夠適應不同的任務要求。同時，評估標記衡量對話情境和檢索到的證據之間的相關性得分。此外，我們精心設計了一種自我細化機制，以迭代地細化產生的反應，考慮到：1）產生的反應與檢索到的證據之間的一致性分數； 2) 相關性分數。在兩個個人化資料集（DuLeMon 和 KBP）上的實驗表明，UniMS-RAG 以統一的方式將自身作為檢索器，在知識源選擇和回應生成任務上實現了最先進的性能。提供了廣泛的分析和討論，為個人化對話系統提供了一些新的視角。},
  note = {設計一個個人化的RAG框架，對於問題的定義非常詳細，值得一看},
  file = {C:\Users\BlackCat\Zotero\storage\DJ8UNCZ5\Wang 等。 - 2024 - UniMS-RAG A Unified Multi-source Retrieval-Augmen.pdf}
}

@thesis{WangXinHeJingLuoZhenDuanXiTongChuTan,
  title = {經絡診斷系統初探},
  author = {{王新賀}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/rbx35m},
  abstract = {近代以來，中醫科學化之路已有了不錯的進展。中醫的經絡理論亦有相當多的研究顯示其用於臨床之療效良好。本研究旨在穴道測量儀器上開發經絡診斷系統之雛形，以建立往後深入研究之基礎。本論文闡述了諸多此系統之建置與架構，並且詳細描寫如何從測量儀器上擷取經絡數值，以及如何分析經絡數值。},
  pagetotal = {35}
}

@thesis{WangYouQiJiYuZiLiaoKuGangYaoZhiZiLiaoKuWanZhengXingCeShiDeXianZhiShiCeShiAnLiChanSheng2022,
  title = {基於資料庫綱要之資料庫完整性測試的限制式測試案例產生},
  author = {{王佑齊}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2022},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/yj2gqy},
  abstract = {規格驅動開發框架使用測試案例自動產生技術，來同時提高測試案例產生的 品質，及降低測試案例產生的成本。本研究團隊已開發一個基於規格驅動開發框 架之 Java 單元測試的限制式測試案例產生系統。這個系統依據限制式軟體規格， 應用限制邏輯程式求解限制滿足問題，來自動產生測試案例。資料庫完整性測試 確保資料庫中資料的正確性及一致性。本論文延用規格驅動開發框架，開發一個 基於資料庫綱要之資料庫完整性測試的限制式測試案例產生系統。本系統產生的 測試案例集可以完全滿足資料庫的主動完整性限制覆蓋標準。雖然本論文使用 MySQL 資料庫管理系統，因為本論文產生的測試腳本使用 JBDC 標準應用程式 介面，系統可以很容易的擴充至其他資料庫管理系統。},
  pagetotal = {589},
  keywords = {實驗室}
}

@thesis{WangZhengYanZaiCeShiAnLiChanShengZhongYuXianQuChuBuKeShiXingLuJing2020,
  title = {在測試案例產生中預先去除不可實行路徑},
  author = {{王正諺}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2020},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/rghg3c},
  abstract = {在自動產生測試案例時，為了滿足測試覆蓋標準，會系統性的條列測試路徑及計算會執行該測試路徑的測試資料。然而有許多測試路徑是不可實行路徑，如果可以預先去除一些不可實行路徑，將可以提升測試路徑條列及測試資料計算的執行時間及記憶體空間的效率。本論文針對廣度優先搜尋的測試路徑條列，提出兩種有效率的方法：最短不可實行路徑偵測與衝突限制式組合偵測，來預先去除不可實行路徑。本論文也針對決策覆蓋標準、條件決策覆蓋標準、及多重條件覆蓋標準，做了這兩種方法的初步效率評估。評估結果顯示，對於本質上不可實行路徑數較多的多重條件覆蓋標準，使用衝突限制式組合偵測方法，執行時間及記憶體空間的效率都有顯著的提升。評估結果也顯示，對於本質上不可實行路徑數較少的決策覆蓋標準與條件決策覆蓋標準，使用衝突限制式組合偵測方法，造成的額外負擔是很小或可忽略的。},
  pagetotal = {338},
  keywords = {實驗室}
}

@article{WangZhiQuZhongYiHuLiBingLiBiaoZhunDeYanJiu1993,
  title = {中医护理病历标准的研究},
  author = {{王之渠} and {郭绍璋}},
  date = {1993},
  journaltitle = {护理研究},
  volume = {007},
  number = {004},
  pages = {1--2},
  abstract = {本文介绍了国家中医药管理局重点科研课题"中医护理病历标准化研究"的科研成果.翔实地阐述了该项研究的必要性以及研究过程.简要地介绍了该项成果的具体内容.同时从专家评价,推广应用,获奖情况等方面证实了该项成果的先进性,科学性,实用性和可行性.},
  keywords = {No DOI found,中醫,中醫標準化},
  file = {C:\Users\BlackCat\Zotero\storage\Z8N8C2SI\王之渠 與 郭绍璋 - 1993 - 中医护理病历标准的研究.pdf}
}

@thesis{WangZhiYuanAutoVoiceYiGeHTMLDaoVoiceXMLDeZhuanHuanQi2007,
  title = {{{AutoVoice}}：{{一個HTML到VoiceXML的轉換器}}},
  author = {{王至遠}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2007},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/83wdf3},
  abstract = {現今WWW可以藉由視覺裝置提供豐富的資訊和服務。隨著個人行動語音裝置的普及，如果讓WWW可以藉由個人行動語音裝置提供資訊和服務，將對世人產生重大的便利性。本論文介紹一個轉換器AutoVoice。它能夠自動或半自動地將HTML檔案轉換成VoiceXML檔案。結構樹用來維護WWW的資料架構。AutoVoice包含三個元件：結構樹產生器、結構樹編輯器、VoiceXML產生器。結構樹產生器可以自動的從HTML檔案抽取出資料的關連性而建構出一個結構樹；結構樹編輯器可以提供修改結構樹的彈性；VoiceXML產生器可以自動的將結構樹轉換成VoiceXML語音檔。使用AutoVoice能夠顯著地改善開發和維護語音網站的成本。},
  pagetotal = {58}
}

@article{WanXiaoNuoDuoMuBiaoDongTaiGuiHuaZaiDianZiBingLiJieGouHuaDeYingYong2012,
  title = {多目标动态规划在电子病历结构化的应用},
  author = {{万小娜} and {陈盛双} and {张卓}},
  date = {2012},
  journaltitle = {Ji suan ji gong cheng yu ying yong},
  volume = {48},
  number = {35},
  pages = {218--223},
  issn = {1002-8331},
  keywords = {No DOI found,知網},
  file = {C:\Users\BlackCat\Zotero\storage\ZTIPNWRQ\万小娜 等。 - 2012 - 多目标动态规划在电子病历结构化的应用.pdf}
}

@online{warmerdamGoingTSNEExposing2020,
  title = {Going {{Beyond T-SNE}}: {{Exposing}} \textbackslash texttt\{whatlies\} in {{Text Embeddings}}},
  shorttitle = {Going {{Beyond T-SNE}}},
  author = {Warmerdam, Vincent D. and Kober, Thomas and Tatman, Rachael},
  date = {2020-09-04},
  eprint = {2009.02113},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2009.02113},
  url = {http://arxiv.org/abs/2009.02113},
  urldate = {2024-03-22},
  abstract = {We introduce whatlies, an open source toolkit for visually inspecting word and sentence embeddings. The project offers a unified and extensible API with current support for a range of popular embedding backends including spaCy, tfhub, huggingface transformers, gensim, fastText and BytePair embeddings. The package combines a domain specific language for vector arithmetic with visualisation tools that make exploring word embeddings more intuitive and concise. It offers support for many popular dimensionality reduction techniques as well as many interactive visualisations that can either be statically exported or shared via Jupyter notebooks. The project documentation is available from https://rasahq.github.io/whatlies/.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,人機互動,嵌入,待讀,重要},
  annotation = {titleTranslation: 超越 T-SNE：在文字嵌入中暴露 \textbackslash texttt\{whatlies\}},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\6MC5BAFA\\Warmerdam et al. - 2020 - Going Beyond T-SNE Exposing texttt whatlies in .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\XTANMMJM\\2009.html}
}

@online{Web20ZhiShiPingTaiJiaZhiFenXiangJiZhiSheJiYuShiZuoYiWangLuoHuaGeRenZhiShiPingTaiWeiLi2009,
  title = {Web2.0知識平台加值分享機制設計與實作-以網絡化個人知識平台為例},
  date = {2009},
  url = {https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=baD9Yf/search?s=id=%22097TTU05716015%22.&openfull=1&setcurrent=0},
  urldate = {2024-03-22},
  abstract = {資訊通訊技術(ICT)發展打破了各個地域之間的實體疆界，也使得網路應用服務的取得越來越容易，Web 2.0、Web 3.0服務模式讓網路應用服務更多元，全球化、知識社會蓬勃的發展，加上行動工具的發展及廣泛應用，使得全球的知識工作者需要擁有迅速掌握資訊、應用知識的能力，個人化知識管理的趨勢更已逐漸明朗。惟個人知識管理的養成過程中，網路虛擬社群的經營佔著極重要的地位。在虛擬的網路世界中，社群活動的力量相當驚人，透過網路平台的應用，社群中的每個人自然地與其他人分享個人專門的知識技能；藉由群體的知識力量，讓個人在知識的廣度與深度上有提升的助益；因此，虛擬社群活動的過程中，完善的知識平台輔助更能達到知識分享的極大效益。 個人化知識管理平台除了提供知識訊息，還要能提供虛擬社群溝通的管道，知識產生的過程需要不斷的假設、推翻，從隱性到顯性，完成知識轉移，並達到最後知識具體化的結論。惟探索知識轉化過程所需的雙向即時溝通管道，在目前的個人化知識管理平台上似乎比較缺乏；因此本研究從社群的活動與管理，個人化知識管理平台的整合應用上探討著手，擬以「知識加值分享」之應用模組設計實作，充實網絡化個人知識管理加值平台，希望能藉由此Web 2.0 知識平台的實作來活絡知識產生過程中的效益。},
  langid = {chinese},
  keywords = {知識管理,重要},
  annotation = {titleTranslation: Web2.0知識平台加值分享機制設計與實務-以網路化個人知識平台為例},
  file = {D\:\\Paper\\Web2.0知識平台加值分享機制設計與實作-以網絡化個人知識平台為例.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\V9TLT85E\\search.html}
}

@online{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date = {2023-01-10},
  eprint = {2201.11903},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.11903},
  url = {http://arxiv.org/abs/2201.11903},
  urldate = {2024-02-22},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,COT,LLM,已整理,文獻,機器學習,經典},
  annotation = {titleTranslation: 思路鏈提示引發大型語言模型的推理\\
abstractTranslation:  我們探索如何產生一條思想鏈（一系列中間推理步驟）來顯著提高大型語言模型執行複雜推理的能力。特別是，我們展示了這種推理能力如何透過一種稱為思維鏈提示的簡單方法在足夠大的語言模型中自然出現，其中提供了一些思維鏈演示作為提示的範例。對三種大型語言模型的實驗表明，思維鏈提示可以提高一系列算術、常識和符號推理任務的表現。經驗收益可能是驚人的。例如，僅用八個思想鏈範例提示 540B 參數語言模型就可以在數學應用題的 GSM8K 基準上實現最先進的準確性，甚至超過帶有驗證器的微調 GPT-3。},
  note = {文獻中實驗結果可以發現，COT仍無法完全避免幻覺問題},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\NBZRUQXN\\Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\YIX8FNZF\\2201.html}
}

@article{weiminghuangAttractionDetailsQAAttractionDetails2022,
  title = {{{AttractionDetailsQA}}: {{An Attraction Details Focused}} on {{Chinese Question Answering Dataset}}},
  shorttitle = {{{AttractionDetailsQA}}},
  author = {{Weiming Huang} and {Shiting Xu} and {Wang Yuhan} and {Jin Fan} and {Qingling Chang}},
  date = {2022},
  journaltitle = {IEEE Access},
  volume = {10},
  pages = {86215--86221},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3181188},
  url = {https://ieeexplore.ieee.org/document/9861204},
  urldate = {2023-11-23},
  abstract = {With the increase in the number of domestic tourists and the popularity of digital upgrades in attractions, it is crucial to develop a question-answering(QA) system about the details of the attractions. However, there is little work on attractions QA, and the main bottleneck is the lack of available datasets. While previous QA datasets usually focus on news domain like CNN/DAILYMAIL and NewsQA, we present the first large-scale dataset for QA over attraction details. To ensure that the data we collected are useful, we only gather the data from public travel information website. Unlike other QA datasets like SQuAD, which is labeled manually, we formed the dataset by manual and question-answer pair generation(QAG) annotated model. Finally, we obtained a dataset covering 2,808 attractions with a total of 18,245 QA pairs, including seven types of attraction details: location, time, component, area, layout, rating, and character. The dataset is available at https://github.com/wyman130/AttractionDetailsQA. Considering that QAG has not been much studied in attraction details, we experimented some QAG models on this dataset and obtained the benchmark. This provides a basis for subsequent improvements to the dataset and research on QAG in attraction details.},
  eventtitle = {{{IEEE Access}}},
  langid = {english},
  keywords = {中文,問答系統,回收,已整理,機器學習},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: AttractionDetailsQA：專注於中文問答資料集的景點詳情},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\XHV28EFL\\Huang et al. - 2022 - AttractionDetailsQA An Attraction Details Focused.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\DHAXCR7Q\\9861204.html}
}

@thesis{WeiQiRenYiGeDianJiYuUMLJiASMLDeCeShiShenYu2008,
  title = {{{一個奠基於UML及ASML的測試神諭}}},
  author = {{魏啟仁}},
  namea = {{林迺衛} and {none}},
  nameatype = {collaborator},
  date = {2008},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/nj5u5s},
  abstract = {確認軟體是否符合軟體規格是一個困難的問題。其中一個困難是如何決定軟體的預期輸出。本論文將Unified Modeling Language的循序圖和狀態機圖的子集合及Abstract State Machine Language整合成為一個可執行的規格語言。本論文也開發一個可執行此可執行規格語言的測試神諭。此測試神諭可以用來產生符合規格的軟體的預期輸出。此測試神諭也可以用來實證規格是否符合客戶的需求。},
  pagetotal = {31}
}

@thesis{WengDaYuanZhongYiFangJiPeiWuXiTong2018,
  title = {中醫方劑配伍系統},
  author = {{翁大原}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2018},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/8c5a96},
  abstract = {本研究團隊已研製一個中醫辨證系統及一個中醫藥材配伍系統。本論文在前兩個系統的基礎上，研製一個中醫方劑配伍系統。這三個系統實踐中醫「辨證論治」的資訊化。中醫辨證系統實踐中醫「辨證」的資訊化，目前支援67個證候。中醫方劑配伍系統及中醫藥材配伍系統協力實踐中醫「論治」的兩階段資訊化。第一個階段，中醫方劑配伍系統根據辨證的證候，推薦治療效力最強的方劑，目前支援186帖方劑。第二個階段，中醫藥材配伍系統根據辨證的證候及推薦的方劑，進行加減藥材，推薦治療效力最強的藥材，目前支援334味藥材。本論文也進行新增藥材及方劑的標準化，並依標準化的藥材及方劑更新中醫藥材知識本體及中醫藥材辭庫。本論文也訂定各方劑治療各證候的效力值，運用多目標最佳化技術，推薦治療多證候效力最強的方劑。},
  pagetotal = {120},
  keywords = {實驗室}
}

@online{whitePromptPatternCatalog2023,
  title = {A {{Prompt Pattern Catalog}} to {{Enhance Prompt Engineering}} with {{ChatGPT}}},
  author = {White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C.},
  date = {2023-02-21},
  eprint = {2302.11382},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.11382},
  url = {http://arxiv.org/abs/2302.11382},
  urldate = {2024-05-06},
  abstract = {Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering,未整理},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\B4ZUPH9T\\White 等。 - 2023 - A Prompt Pattern Catalog to Enhance Prompt Enginee.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\VGX8WD59\\2302.html}
}

@inproceedings{wuAsdKBChineseKnowledge2023,
  title = {{{AsdKB}}: {{A Chinese Knowledge Base}} for~the~{{Early Screening}} and~{{Diagnosis}} of~{{Autism Spectrum Disorder}}},
  shorttitle = {{{AsdKB}}},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2023},
  author = {Wu, Tianxing and Cao, Xudong and Zhu, Yipeng and Wu, Feiyue and Gong, Tianling and Wang, Yuxiang and Jing, Shenqi},
  editor = {Payne, Terry R. and Presutti, Valentina and Qi, Guilin and Poveda-Villalón, María and Stoilos, Giorgos and Hollink, Laura and Kaoudi, Zoi and Cheng, Gong and Li, Juanzi},
  date = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {59--75},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-47243-5_4},
  abstract = {To easily obtain the knowledge about autism spectrum disorder and help its early screening and diagnosis, we create AsdKB, a Chinese knowledge base on autism spectrum disorder. The knowledge base is built on top of various sources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical descriptions on mental and behavioural disorders, 2) the diagnostic knowledge from DSM-5 and different screening tools recommended by social organizations and medical institutes, and 3) the expert knowledge on professional physicians and hospitals from the Web. AsdKB contains both ontological and factual knowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The potential applications of AsdKB are question answering, auxiliary diagnosis, and expert recommendation, and we illustrate them with a prototype which can be accessed at http://asdkb.org.cn/.},
  isbn = {978-3-031-47243-5},
  langid = {english},
  keywords = {Autism Spectrum Disorder,Knowledge Base,Ontology,問答系統,已整理,待讀,知識圖譜,重要},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: AsdKB：自閉症譜系障礙早期篩檢與診斷中文知識庫},
  file = {C:\Users\BlackCat\Zotero\storage\PK8DATZ9\Wu 等。 - 2023 - AsdKB A Chinese Knowledge Base for the Early Scre.pdf}
}

@online{wuLargeLanguageModels2023,
  title = {Large {{Language Models Perform Diagnostic Reasoning}}},
  author = {Wu, Cheng-Kuang and Chen, Wei-Lin and Chen, Hsin-Hsi},
  date = {2023-07-17},
  eprint = {2307.08922},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.08922},
  url = {http://arxiv.org/abs/2307.08922},
  urldate = {2024-01-13},
  abstract = {We explore the extension of chain-of-thought (CoT) prompting to medical reasoning for the task of automatic diagnosis. Motivated by doctors' underlying reasoning process, we present Diagnostic-Reasoning CoT (DR-CoT). Empirical results demonstrate that by simply prompting large language models trained only on general text corpus with two DR-CoT exemplars, the diagnostic accuracy improves by 15\% comparing to standard prompting. Moreover, the gap reaches a pronounced 18\% in out-domain settings. Our findings suggest expert-knowledge reasoning in large language models can be elicited through proper promptings.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,COT,LLM,OpenReview,台大自然語言處理實驗室,醫療},
  annotation = {titleTranslation: 大型語言模型執行診斷推理\\
abstractTranslation:  我們探索將思想鏈（CoT）提示擴展到自動診斷任務的醫學推理。受醫師基本推理過程的啟發，我們提出了診斷推理 CoT (DR-CoT)。實證結果表明，透過簡單地提示僅在通用文字語料庫上訓練的具有兩個 DR-CoT 樣本的大型語言模型，與標準提示相比，診斷準確性提高了 15\%。此外，在域外環境中，差距達 18\%。我們的研究結果表明，可以透過適當的提示來引發大型語言模型中的專家知識推理。},
  note = {通過COT技術增加LLM通過多輪詢問增加看診的準確度。但準確度只有30％，有待加強。
\par
在OpenReview中有相當多有價值的回饋。
\par
Comment: Accepted as a Tiny Paper at ICLR 2023 (10 pages, 5 figures)},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\JRYI3BDB\\Wu 等。 - 2023 - Large Language Models Perform Diagnostic Reasoning.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\D3S6BGJ3\\2307.html;C\:\\Users\\BlackCat\\Zotero\\storage\\S2RCFBWE\\forum.html}
}

@thesis{WuMingFengYiGeZhenDuiFreeBSDDeUSBHuaShuQuDongChengShiChanShengQi2004,
  title = {{{一個針對FreeBSD的USB滑鼠驅動程式產生器}}},
  author = {{吳明峰}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2004},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/e28x23},
  abstract = {USB是一個在個人電腦架構上很流行的工業標準的延伸。這是一個便宜的解決方案，而且最快傳輸速度可達480Mb/s。它同時也兼具熱拔插和省電模式的設計。而FreeBSD是一個支援USB裝置的UNIX-like作業系統，例如它支援USB滑鼠。 這篇論文描述一個針對FreeBSD的USB滑鼠驅動程式產生器。這個驅動程式產生器對所有的字元裝置提供了一套統一的介面，其支援的特性包括：載入不同的組態、切換不同的機制，而且還可以針對特定裝置指定它的輸入，所產生的USB滑鼠驅動程式由一些事前定義的巨集所構成，以利於程式碼的重複使用。目前支援的共享式巨集定義分成二種，一種是可以在USB裝置中共享的，另一種是可以在不同的滑鼠中共享的。},
  pagetotal = {94}
}

@thesis{WuQiRuiZhiYuanBuGuiZeZiLiaoFenPeiDeDuoChongYinXianTongXunHanShiKuDeYanZhi2001,
  title = {支援不規則資料分配的多重引線通訊函式庫的研製},
  author = {{吳啟瑞}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2001},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/65aphm},
  abstract = {這篇論文描述我們為了增進在對稱式多處理器群組上，撰寫平行程式的便利性而研製的一個多重引線通訊函式庫。對一個通訊函式庫而言，有效地支援不規則通訊是非常重要的。這篇論文探討兩種增進不規則通訊效率的方法。我們先提供一種軟體快取的機制來避免重複的資料傳送。我們接著再提供一種可以支援不規則資料分配的機制來改進資料的區域化及減少資料的傳送。對我們函式庫所做的初步效能評估,有非常正面的結果。},
  pagetotal = {77}
}

@thesis{WuShangYuJiYuXianZhiLuoJiTuDeCeShiFuGaiBiaoZhunGuanLiJiBianJieCeShiAnLiDeChanSheng2017,
  title = {基於限制邏輯圖的測試覆蓋標準管理及邊界測試案例的產生},
  author = {{吳尚諭}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2017},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/w8kx73},
  abstract = {軟體測試是確保軟體品質的主要方法，但自動產生測試案例的技術卻仍不成熟。限制式測試案例產生技術是一種重要的測試案例產生技術。限制式測試案例產生技術將測試案例產生問題制定為一個限制滿足問題。本論文提出使用限制邏輯圖作為測試模型，並說明限制邏輯圖如何被用來將測試案例產生問題轉換成一個限制滿足問題。限制邏輯圖以圖形的方式描述軟體的行為或輸入及輸出之間的限制邏輯關係。本論文將針對黑箱函式層級單元測試，詳細說明限制邏輯圖如何被用在行為模型的建構、等價行為類別的分割、類別邊界值的分析、及測試覆蓋標準的管理。},
  pagetotal = {86}
}

@thesis{WuSiYuanMoHuZhongYiJingLuoBianZheng2019,
  title = {模糊中醫經絡辨證},
  author = {{吳思遠}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2019},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/4wpdzu},
  abstract = {中醫辨證體系龐大複雜，臨床診斷充滿挑戰。本論文利用電腦的龐大記憶能力及迅速的推理能力，開發一個中醫經絡辨證系統，輔助中醫師的臨床診斷。本研究團隊曾經根據臟腑辨證及病因辨證，開發一個中醫辨證系統。經絡辨證和臟腑辨證及病因辨證相輔相成，可以建構更完整的中醫辨證系統。本論文根據四本中醫診斷學書籍的經絡辨證理論，整理經絡辨證的十二個證候及其對應的特徵症狀，並進行症狀標準化。中醫辨證是一個模糊分類問題，一個病人不是完全歸屬或完全不歸屬一個證候，而是有不同嚴重程度的歸屬。一個病人也可能同時歸屬多個證候。本論文利用模糊分類技術，實作十二個經絡辨證證候的歸屬函數。本論文也利用27個臨床病例，進行初步的系統評估。評估結果顯示辨證符合率為極強或強的比率佔74\%，辨證符合率為弱或極弱的比率僅佔11\%。},
  pagetotal = {80},
  keywords = {實驗室}
}

@thesis{WuZongLunYinDaoShiZhongYiBianZhengXiTong2017,
  title = {引導式中醫辨證系統},
  author = {{吳宗倫}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2017},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/6ww675},
  abstract = {中醫臨床診斷時，中醫師會先透過望、聞、問、切四診蒐集患者的症狀，再根據患者的症狀辨識患者的證候。由於中醫辨證體系龐大複雜，中醫師通常不易完整記憶辨證體系，臨床診斷也不易充分推理。本研究團隊先前利用電腦的龐大記憶能力及迅速的推理能力，開發一個中醫虛證辨證系統，做為中醫師臨床診斷的輔助工具。該系統提供近1000個標準化的症狀，將患者的標準症狀輸入，系統可以迅速地計算該患者罹患各虛證的歸屬度。 本論文將在該系統的基礎上，提供更有效的臨床診斷輔助功能。第一，引導式的症狀輸入功能。為免除中醫師記憶標準症狀的困擾，新系統允許中醫師輸入任意的症狀描述，系統會自動將其轉換成標準症狀，或自動提供可能的標準症狀，引導中醫師選擇適當的標準症狀。第二，引導式的症狀建議功能。新系統計算完患者罹患各虛證的歸屬度後，會建議中醫師可以繼續詢問歸屬度最高證候的症狀。此功能不僅提供即時的辨證結果，也提供即時的症狀建議，避免患者或中醫師因遺漏症狀而影響辨證的正確性。此功能也協助系統蒐集完整的症狀，以利後續的病歷分析及研究。},
  pagetotal = {90}
}

@thesis{WuZuHuanDianJiYuUMLZhuangTaiTuDeCeShiAnLiChanShengQi2007,
  title = {{{奠基於UML狀態圖的測試案例產生器}}},
  author = {{吳祖寰}},
  namea = {{林迺衛} and {熊博安} and {Nai-Wei Lin} and {none}},
  nameatype = {collaborator},
  date = {2007},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/5t96dq},
  abstract = {軟體測試是軟體開發過程中是非常重要的一環。由於軟體測試大約佔了軟體開發成本的一半。軟體測試自動化將可以顯著地節省軟體開發成本。在模型推動軟體開發中，通常會先根據軟體規格建立軟體的模型，然後軟體的實作及軟體的測試都可以依據這個統一的模型同時進行。因此如果能夠根據軟體的模型自動產生測試案例和執行測試案例，將可以顯著地節省開發成本。 本論文提出了一個奠基於UML狀態圖的測試案例產生器的實作，共分為三部分：狀態圖轉換器，資料流程分析器，測試路徑選擇器。藉由奠基於狀態圖的測試案例產生器，使用者在繪製狀態圖後，即可自動產生測試案例來進行測試。},
  pagetotal = {82}
}

@inproceedings{xianghuizhuFeatureBasedCoalitionGame2023,
  title = {A {{Feature-Based Coalition Game Framework}} with {{Privileged Knowledge Transfer}} for {{User-tag Profile Modeling}}},
  booktitle = {Proceedings of the 29th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {{Xianghui Zhu} and {Peng Du} and {Shuo Shao} and {Chenxu Zhu} and {Weinan Zhang} and {Yang Wang} and {Yang Cao}},
  year = {8 月 4, 2023},
  series = {{{KDD}} '23},
  pages = {5739--5749},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3580305.3599761},
  url = {https://dl.acm.org/doi/10.1145/3580305.3599761},
  urldate = {2023-09-11},
  abstract = {User-tag profiling is an effective way of mining user attributes in modern recommender systems. However, prior researches fail to extract users' precise preferences for tags in the items due to their incomplete feature-input patterns. To convert user-item interactions to user-tag preferences, we propose a novel feature-based framework named Coalition Tag Multi-View Mapping (CTMVM), which identifies and investigates two special features, Coalition Feature and Privileged Feature. The former indicates decisive tags in each click where relationships between tags in one item are treated as a coalition game. The latter represents highly informative features that only occur during training. For the coalition feature, we adopt Shapley Value based Empowerment (SVE) to model the tags in items with a game-theoretic paradigm and charge the network to straight master user preferences for essential tags. For the privileged feature, we present Privileged Knowledge Mapping (PKM) to explicitly distill privileged feature knowledge for each tag into one single embedding, which assists the model in predicting user-tag preferences at a more fine-grained level. However, the barren capacity of single embeddings limits the diverse relations between each tag and different privileged features. Therefore, we further propose Adaptive Multi-View Mapping (AMVM) model to enhance effect by handling multiple mapping networks. Excellent offline experiment results on two public and one private datasets show the out-standing performance of CTMVM. After the deployment on Alibaba large-scale recommendation systems, CTMVM achieved improvement by 10.81\% and 6.74\% in terms of Theme-CTR and Item-CTR respectively, which validates the effectiveness of taking in the two particular features for training.},
  isbn = {9798400701030},
  langid = {english},
  keywords = {回收},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於特徵的聯盟博弈框架，具有用於用戶標籤檔案建模的特權知識轉移\\
abstractTranslation:  用戶標籤分析是現代推薦系統中挖掘用戶屬性的有效方法。然而，由於特徵輸入模式不完整，先前的研究未能提取用戶對項目中標籤的精確偏好。為了將用戶-項目交互轉換為用戶標籤偏好，我們提出了一種新穎的基於特徵的框架，名為聯盟標籤多視圖映射（CTMVM），它識別並研究兩個特殊特徵：聯盟特徵和特權特徵。前者指示每次點擊中的決定性標籤，其中一個項目中的標籤之間的關係被視為聯盟遊戲。後者代表僅在訓練期間出現的信息豐富的特徵。對於聯盟功能，我們採用基於 Shapley 值的賦權（SVE）以博弈論範式對項目中的標籤進行建模，並讓網絡直接掌握用戶對基本標籤的偏好。對於特權特徵，我們提出了特權知識映射（PKM），將每個標籤的特權特徵知識顯式地提取到一個嵌入中，這有助於模型在更細粒度的級別上預測用戶標籤偏好。然而，單個嵌入的貧瘠容量限制了每個標籤和不同特權特徵之間的多樣化關係。因此，我們進一步提出自適應多視圖映射（AMVM）模型，通過處理多個映射網絡來增強效果。在兩個公共數據集和一個私有數據集上出色的離線實驗結果表明了 CTMVM 的出色性能。在阿里巴巴大規模推薦系統上部署後，CTMVM在Theme-CTR和Item-CTR方面分別取得了10.81\%和6.74\%的提升，驗證了引入這兩個特定特徵進行訓練的有效性。},
  note = {為用戶下標籤，非研究範圍},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\LF9VLYJN\\A Feature-Based Coalition Game Framework with Privileged Knowledge Transfer for User-tag Profile Modeling.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\UMM4Z3DU\\Zhu 等。 - 2023 - A Feature-Based Coalition Game Framework with Priv.pdf}
}

@article{xiao-qiangyueAnalysisStudiesPattern2004,
  title = {Analysis of Studies on Pattern Recognition of Tongue Image in Traditional {{Chinese}} Medicine by Computer Technology},
  author = {{Xiao-Qiang Yue}},
  date = {2004-09-15},
  journaltitle = {Journal of Chinese Integrative Medicine},
  shortjournal = {J Chin Integr Med},
  volume = {2},
  number = {5},
  pages = {326--329},
  issn = {16721977},
  doi = {10.3736/jcim20040503},
  url = {https://pubmed.ncbi.nlm.nih.gov/15383248/},
  urldate = {2022-09-19},
  annotation = {15 citations (Crossref) [2024-03-26]\\
16 citations (Semantic Scholar/DOI) [2022-10-26]},
  note = {舌象分析技術、中西醫聯合學報}
}

@online{xiaoCPackPackagedResources2023,
  title = {C-{{Pack}}: {{Packaged Resources To Advance General Chinese Embedding}}},
  shorttitle = {C-{{Pack}}},
  author = {Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighoff, Niklas},
  date = {2023-12-15},
  eprint = {2309.07597},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.07597},
  url = {http://arxiv.org/abs/2309.07597},
  urldate = {2024-03-07},
  abstract = {We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10\% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.},
  langid = {english},
  pubstate = {preprint},
  keywords = {benchmark,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,中文,嵌入,已整理,機器學習,重要},
  annotation = {titleTranslation: C-Pack：打包資源以推進通用中文嵌入\\
abstractTranslation:  我們推出 C-Pack，這是一個顯著推進通用中文嵌入領域發展的資源包。 C-Pack 包括三個關鍵資源。 1）C-MTEB是中文文本嵌入的綜合基準，涵蓋6個任務和35個資料集。 2) C-MTP 是一個海量文字嵌入資料集，由標記和未標記的中文語料庫整理而成，用於訓練嵌入模型。 3) C-TEM 是一系列涵蓋多種尺寸的嵌入模型。發佈時，我們的模型比 C-MTEB 上所有先前的中文文字嵌入效能高出 10\%。我們也整合和優化了 C-TEM 的整套培訓方法。除了我們關於一般中文嵌入的資源之外，我們還發布了英文文本嵌入的資料和模型。英國模型在 MTEB 基準上達到了最先進的性能；同時，我們發布的英文數據是中文數據的2倍。所有這些資源均在 https://github.com/FlagOpen/FlagEmbedding 上公開提供。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\B9E95GBL\\Xiao 等。 - 2023 - C-Pack Packaged Resources To Advance General Chin.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\CWQSMV44\\2309.html}
}

@article{XiaoJunZhongYiYiYuanDianZiBingLiXiTongJianSheDeShiJian2011,
  title = {中医医院电子病历系统建设的实践},
  author = {{肖军} and {顾一帅} and {顾国龙} and {沈海芹} and {章辰熙} and {孟涌}},
  date = {2011},
  journaltitle = {中国医药导报},
  volume = {8},
  number = {27},
  pages = {2},
  doi = {10.3969/j.issn.1673-7210.2011.27.062},
  abstract = {针对计算机技术在医院中应用的特点,选择技术力量强的专业商,采用合作策略,开发构架电子病历,并对项目背景,技术扶持及实施等问题展开讨论,随着中医电子病历的推进,分析中医院电子病历系统设计过程中应该注意的问题.},
  keywords = {中醫,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\V4WPVSDD\肖军 等。 - 2011 - 中医医院电子病历系统建设的实践.pdf}
}

@article{XiaoLiJiYuISOTC249GuoJiBiaoZhunDeZhongYiShuJuJieKouBiaoZhunHuaYanJiu2019,
  title = {{{基于ISO}}/{{TC249国际标准的中医数据接口标准化研究}}},
  author = {{肖丽} and {胡远樟} and {王金全} and {李虹丽} and {邓星月} and {李柯慧} and {温川飙}},
  date = {2019},
  journaltitle = {时珍国医国药},
  volume = {30},
  number = {8},
  pages = {3},
  abstract = {目的基于ISO/TC249界定中医信息数据接口标准化范畴.方法提出了质量量化控制,条件约束以及接口模式规范,并以中医临床电子病案数据为例,示范了在某一业务领域中中医数据在不同医疗信息系统之间的传输与共享.结果为突破信息孤岛,实现中医医疗数据分布式存储,互操作,互通共享提供了一种借鉴方法.结论通过ISO中医数据接口标准化,可以提升中医信息通信水平和互操作性,为实现中国中医药信息标准国际化,中医药信息与知识资源国际化奠定基础.},
  keywords = {中醫,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\G4V37Y89\肖丽 等。 - 2019 - 基于ISOTC249国际标准的中医数据接口标准化研究.pdf}
}

@article{xiaoliwangHowContextKnowledge2023,
  title = {How {{Context}} or {{Knowledge Can Benefit Healthcare Question Answering}}?},
  author = {{Xiaoli Wang} and {Feng Luo} and {Qingfeng Wu} and {Zhifeng Bao}},
  date = {2023-01},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {35},
  number = {1},
  pages = {575--588},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2021.3090253},
  url = {https://ieeexplore.ieee.org/document/9459504},
  urldate = {2023-11-23},
  abstract = {Healthcare question answering (HQA) is a challenging task as questions are generally non-factoid in nature. Traditional information retrieval techniques do not perform well on non-factoid questions. Recent neural question answering systems are reported to have performance gains over traditional methods. However, little attention has been given to HQA as datasets are generally too small to train a neural model from scratch. Recently, several systems have been proposed to learn context representations for HQA. Despite moderate progress, these systems have not been thoroughly compared with state-of-the-art neural models, and these neural models were tested only on self-created datasets. This makes it difficult for practitioners to decide which models should be used for various scenarios. To address the above challenges, we develop a new joint model to incorporate both context and knowledge embeddings into neural ranking architectures. First, we adapt context embedding pre-trained from large open-domain corpus to small healthcare datasets. Second, we learn knowledge embedding from knowledge graphs to provide external information for understanding non-factoid questions. To evaluate how context or knowledge embedding can benefit HQA, we adapt many state-of-the-art methods for general QA to HQA, by injecting the context or knowledge information only, or both of them. Extensive experiments are conducted to compare our approach with those adapted methods and current HQA systems. The results show that our approach achieves the state-of-the-art performance on both HealthQA and NFCorpus datasets.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  langid = {english},
  keywords = {問答系統,已整理,機器學習,知識圖譜,重要},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 背景或知識如何有益於醫療保健問答？},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\CFDDGVJ5\\Wang et al. - 2023 - How Context or Knowledge Can Benefit Healthcare Qu.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\QSTZWQK5\\9459504.html}
}

@article{xiaoqihanDivideConquerFramework2023,
  title = {A Divide and Conquer Framework for {{Knowledge Editing}}},
  author = {{Xiaoqi Han} and {Ru Li} and {Xiaoli Li} and {Jeff Z. Pan}},
  date = {2023-11-04},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowledge-Based Systems},
  volume = {279},
  pages = {110826},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2023.110826},
  url = {https://www.sciencedirect.com/science/article/pii/S0950705123005762},
  urldate = {2023-09-13},
  abstract = {As Pre-trained language models (LMs) play an important role in various Natural Language Processing (NLP) tasks, it is becoming increasingly important to make sure the knowledge learned from LMs is valid and correct. Unlike conventional knowledge bases, LMs implicitly memorize knowledge in their parameters, which makes it harder to correct if some knowledge is incorrectly inferred or obsolete. The task of Knowledge Editing is to correct errors in language models, avoiding the expensive overhead associated with retraining the model from scratch. While existing methods have shown some promising results, they fail on multi-edits as they ignore the conflicts between these edits. In the paper, we propose a novel framework to divide-and-conquer edits with parallel Editors. Specifically, we design explicit and implicit multi-editor models to learn diverse editing strategies in terms of dynamic structure and dynamic parameters respectively, which allows solving the conflict data in an efficient end-to-end manner. Our main findings are: (i) State of the art Knowledge Editing methods with multiple editing capability, such as MEND and ENN, can hardly outperform the fine-tuning method; (ii) Our proposed models outperform the fine-tuning method over the two widely used datasets for Knowledge Editing; (iii) Additional analytical experiments verify that our approach can learn diverse editing strategies, thus better adapting to multiple editing than state-of-the-art methods.},
  langid = {english},
  keywords = {Dynamantic interfence,已整理,數學,框架,模型編輯,預訓練},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 知識編輯的分而治之框架\\
abstractTranslation:  由於預訓練語言模型 (LM) 在各種自然語言處理 (NLP) 任務中發揮重要作用，因此確保從 LM 學到的知識有效且正確變得越來越重要。與傳統的知識庫不同，語言模型隱式地記憶其參數中的知識，如果某些知識被錯誤推斷或過時，則更難糾正。知識編輯的任務是糾正語言模型中的錯誤，避免與從頭開始重新訓練模型相關的昂貴開銷。雖然現有方法已經顯示出一些有希望的結果，但它們在多重編輯上失敗了，因為它們忽略了這些編輯之間的衝突。在本文中，我們提出了一種新穎的框架，用於使用平行編輯器進行分而治之的編輯。具體來說，我們設計了顯式和隱式多編輯器模型，分別在動態結構和動態參數方面學習不同的編輯策略，這使得能夠以有效的端到端方式解決衝突資料。我們的主要發現是：（i）具有多種編輯能力的最先進的知識編輯方法，例如 MEND 和 ENN，很難優於微調方法； (ii) 我們提出的模型在兩個廣泛使用的知識編輯資料集上優於微調方法； （iii）額外的分析實驗驗證了我們的方法可以學習不同的編輯策略，從而比最先進的方法更好地適應多重編輯。},
  note = {屬於修正模型中的錯誤而非使用模型查詢，因此與研究方向較無相關。
\par
研究貢獻在於能夠同時應用多種修改而不會害另一邊的表現不好。實作理論方面還沒細讀。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\D6G2RP7N\\Han 等。 - 2023 - A divide and conquer framework for Knowledge Editi.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\UNEZ2UTZ\\S0950705123005762.html}
}

@article{XiaoRuiJiYuShenDuShenJingWangLuoDeGanYingHuaZhongYiZhiLiaoYuCeYanJiu2019,
  title = {基于深度神经网络的肝硬化中医治疗预测研究},
  author = {{肖瑞} and {裴卫} and {胡冯菊} and {肖勇}},
  date = {2019},
  journaltitle = {医学信息学杂志},
  volume = {40},
  number = {5},
  pages = {5},
  url = {http://www.cnki.com.cn/Article/CJFDTotal-YXQB201905017.htm},
  urldate = {2022-10-18},
  abstract = {以中医电子病历中肝硬化数据为数据源,运用数据清洗,主成份分析技术构建致病指标与诊断结果二元组,通过训练神经网络和支持向量机分类器模型进行预测结果对比,结果表明该方法有效可行.},
  keywords = {中医,文本挖掘,电子病历,神经网络,肝硬化},
  file = {C:\Users\BlackCat\Zotero\storage\WWGP955T\基于深度神經网絡的肝硬化中醫治療預測研究.pdf}
}

@thesis{XiaoShiJieJiYuXunXuTuDeCeShiAnLiChanShengQi2012,
  title = {基於循序圖的測試案例產生器},
  author = {{蕭世杰}},
  namea = {{林迺衛} and {NaiWei Lin}},
  nameatype = {collaborator},
  date = {2012},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/7w7aqa},
  abstract = {軟體測試是確保軟體品質的主要方法之一，然而軟體測試純靠人力，曠日廢時且容易出錯。本篇論文實作一個整合測試的測試案例產生器，可以自動化地產生整合測試的測試案例，以降低軟體開發成本及提高軟體品質。 本工具使用循序圖、類別圖和物件限制語言作為規格輸入，其中循序圖用來描述物件之間函式呼叫的行為；類別圖和物件限制語言描述函式內部的行為。 此工具首先分析類別圖和物件限制語言規格並將這兩個規格轉換成函式限制圖，並將循序圖轉換成函式循序圖。接著在函式循序圖上系統化地條列測試路徑。藉由測試路徑和路徑上呼叫的函式的函式限制圖可以產生這條路徑相對應的限制邏輯敘述。一個限制邏輯敘述表示一條測試路徑所需滿足的限制式。執行這個限制邏輯敘述可解出滿足該測試路徑限制式的測試輸入與預期輸出。最後根據測試輸入與預期輸出可產生Java測試碼。使用者便可使用JUnit平台自動測試所產生出來的Java測試碼。},
  pagetotal = {74}
}

@thesis{XiaoZhiHaoZhongYiBianZhengXiTong2014,
  title = {中醫辨證系統},
  author = {{蕭志豪}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2014},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/22m7x7},
  abstract = {辨證是中醫臨床診斷的主要原則。辨證依據疾病的症狀及體徴，分析及辨別疾病的病因、病位、及病機。中醫辨證體系龐大複雜，掌握中醫辨證的精髓要旨非常困難。因此，借助計算機龐大的記憶能力及迅速的分析能力，研製一個中醫辨證系統，完整的實踐中醫辨證體系，對中醫的臨床診斷、研究及教學，都有很大的價值。 本論文根據馬建中的《中醫診斷學》和鄧鐵濤的《中醫診斷學》研製一個中醫辨證系統。這個系統包括八綱、氣血、病因及臟腑辨證。本論文也使用《中醫證候學》的症狀資料，來分析馬建中和鄧鐵濤的辨證體系的差異。這個系統提供中醫實證醫學化的基礎建設。},
  pagetotal = {73}
}

@inproceedings{xichenOWLReasoningBig2013,
  title = {{{OWL}} Reasoning over Big Biomedical Data},
  booktitle = {2013 {{IEEE International Conference}} on {{Big Data}}},
  author = {{Xi Chen} and {Huajun Chen} and {Ningyu Zhang} and {Jiaoyan Chen} and {Zhaohui Wu}},
  date = {2013-10},
  pages = {29--36},
  doi = {10.1109/bigdata.2013.6691755},
  abstract = {Recently, the emerging accumulation of biomedical data on the Web (e.g. vast amounts of protein sequences, genes, gene products, drugs, diseases and chemical compounds, etc.) has shaped a big network of isolated professional knowledge. Embedded with domain knowledge from different disciplines all regarding to human biological systems, the decentralized data repositories are implicitly connected by human expert knowledge. Lots of biomedical data sources are published separately in the form of semantic ontologies represented by Web Ontology Language (OWL) syntax, which is naturally based on linked graphs. When we are faced with such massive, disparate and interlinked data, biomedical data analysis becomes a challenge. In this paper, we present a general OWL reasoning framework for the analysis of big biomedical data and implement a MapReduce-based property chain reasoning prototype system. OWL reasoning method is ideally suitable for problems involved complex semantic associations because it is able to infer logical consequences based on a set of asserted rules or axioms. MapReduce framework is used to solve the problem of scalability. In our experiment, we focus on the discovery of associations between Traditional Chinese Medicine (TCM) and Western Medicine (WM). The results show the system achieves high performance, accuracy and scalability.},
  eventtitle = {2013 {{IEEE International Conference}} on {{Big Data}}},
  langid = {english},
  keywords = {Big Biomedical Data,Bioinformatics,Cognition,MapReduce,Ontologies,OWL,OWL Reasoning,Property Chain,Protein engineering,Proteins,Semantics},
  annotation = {5 citations (Crossref) [2024-03-26]\\
9 citations (Semantic Scholar/DOI) [2023-07-24]\\
titleTranslation: 生物醫學大數據的 OWL 推理\\
abstractTranslation:  近年來，網絡上不斷湧現的生物醫學數據（例如大量的蛋白質序列、基因、基因產物、藥物、疾病和化學化合物等）的積累，形成了一個孤立的專業知識的大網絡。分散的數據存儲庫嵌入了與人類生物系統有關的不同學科的領域知識，並通過人類專家知識隱式連接。許多生物醫學數據源以網絡本體語言（OWL）語法表示的語義本體的形式單獨發布，這自然是基於鏈接圖的。當我們面對如此海量、分散且相互關聯的數據時，生物醫學數據分析就成為一個挑戰。在本文中，我們提出了一個用於分析生物醫學大數據的通用OWL推理框架，並實現了一個基於MapReduce的屬性鏈推理原型系統。 OWL 推理方法非常適合涉及復雜語義關聯的問題，因為它能夠基於一組斷言的規則或公理來推斷邏輯結果。 MapReduce框架用於解決可擴展性問題。在我們的實驗中，我們專注於發現傳統中醫（TCM）和西醫（WM）之間的關聯。結果表明該系統具有高性能、準確性和可擴展性。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\S8SNEX4T\\Chen 等。 - 2013 - OWL reasoning over big biomedical data.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\2V7SVB8M\\6691755.html}
}

@article{xieEnablingBuildingDigital2022,
  title = {Enabling Building Digital Twin: {{Ontology-based}} Information Management Framework for Multi-Source Data Integration},
  shorttitle = {Enabling Building Digital Twin},
  author = {Xie, X and Moretti, N and Merino, J and Chang, J Y and Pauwels, P and Parlikad, A K},
  date = {2022-11-01},
  journaltitle = {IOP Conference Series: Earth and Environmental Science},
  shortjournal = {IOP Conf. Ser.: Earth Environ. Sci.},
  volume = {1101},
  number = {9},
  pages = {092010},
  issn = {1755-1307, 1755-1315},
  doi = {10.1088/1755-1315/1101/9/092010},
  url = {https://iopscience.iop.org/article/10.1088/1755-1315/1101/9/092010},
  urldate = {2024-04-16},
  abstract = {Abstract             The emergence of the digital twin concept can potentially change the way people manage built assets thoroughly. This is because the semantics-based model and linked data approach behind the digital twin, as the successor of classical BIM, provide strong capability in integrating data from fragmented and heterogeneous sources and thus enable better-informed decision-making. Taking buildings as the case, this paper demonstrates the ontology-based Information Management Framework and elaborates on the process to integrate data through a common data model. Specifically, the Foundation Data Model (FDM) representing the operation of buildings and embedded systems is developed and two patterns of integration architecture are compared. To conceptualise all the essential entities and relationships, the building topology ontology and BRICK ontology are reused and merged to serve as a feasible FDM. According to the characteristic of asset management services that digital twins support, two integration architectures are compared, including the data warehouse approach and the mediator approach. A case study is presented to elaborate on the implementation of these two approaches and their applicability. This work sets out the standardised and modularised paradigms for discovering, fetching, and integrating data from disparate sources with different data curation manners.},
  langid = {english},
  annotation = {titleTranslation: 賦能建構數位孿生：基於本體的多源資料整合資訊管理框架\\
abstractTranslation:  摘要 數位孿生概念的出現可能會改變人們徹底管理建築資產的方式。這是因為數位孿生背後基於語義的模型和關聯數據方法作為經典BIM的繼承者，提供了強大的能力來整合來自碎片和異質來源的數據，從而實現更明智的決策。本文以建築物為例，示範了基於本體的資訊管理框架，並詳細闡述了透過通用資料模型整合資料的過程。具體來說，開發了代表建築物和嵌入式系統操作的基礎資料模型（FDM），並對兩種整合架構模式進行了比較。為了概念化所有基本實體和關係，建築拓樸本體和 BRICK 本體被重複使用並合併以作為可行的 FDM。根據數位孿生支援的資產管理服務的特點，比較了資料倉儲方法和中介者方法兩種整合架構。提出了一個案例研究來詳細說明這兩種方法的實施及其適用性。這項工作提出了標準化和模組化的範例，用於透過不同的資料管理方式發現、獲取和整合來自不同來源的資料。},
  note = {[TLDR] This work sets out the standardised and modularised paradigms for discovering, fetching, and integrating data from disparate sources with different data curation manners and elaborates on the process to integrate data through a common data model.},
  file = {C:\Users\BlackCat\Zotero\storage\HCKZSNR5\Xie 等。 - 2022 - Enabling building digital twin Ontology-based inf.pdf}
}

@article{XieJianJiYuZhongWenFenCiDeDianZiBingLiShuJuWaJueJiShu2016,
  title = {基于中文分词的电子病历数据挖掘技术},
  author = {{谢剑} and {周小茜} and {童凌} and {罗凌云}},
  date = {2016},
  journaltitle = {Hu'nan Keji Xueyuan xuebao},
  volume = {37},
  number = {10},
  pages = {54--59},
  issn = {1673-2219},
  abstract = {电子病历中存在海量非结构化数据,其中隐含的信息对于医学研究与应用均具有重要的意义。文章通过比较各类中文分词器的优劣,结合词典扩充技术,挖掘出真实电子病历中的疾病与药品信息,并对疾病与疾病、疾病与药品进行关联分析,发掘有价值的信息。实验结果表明该方法是行之有效的。},
  langid = {chi},
  keywords = {No DOI found,医学数据,电子病历},
  annotation = {abstractTranslation:  電子病歷中存在海量非重構數據，其中隱含的信息對於醫學研究與應用均具有重要意義。文章通過比較通用中文分詞器的優劣，結合搜索補充技術，挖掘出真實的電子病歷中的疾病與藥品信息，將疾病與疾病、疾病與藥品進行關聯分析，挖掘有價值的信息。實驗結果表明該方法是行之有效的。\\
titleTranslation: 基於中文分詞的電子病歷數據挖掘技術},
  file = {C:\Users\BlackCat\Zotero\storage\39PWYDJ6\谢剑 等。 - 2016 - 基于中文分词的电子病历数据挖掘技术.pdf}
}

@thesis{XieJinWenZhongXiYiHeBingFeiAiZhiLiaoDeCunHuoFenXi2018,
  title = {中西醫合併肺癌治療的存活分析},
  author = {{葉錦文}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2018},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/35896b},
  abstract = {本論文分析中西醫合併治療的肺癌病患和純西醫治療的肺癌病患的存活時間是否有顯著差異。研究資料是基於大林慈濟及台北慈濟醫院的癌症登記資料庫及門診資料，總共收錄1871筆肺癌病患資料，其中包含228筆採用中西醫合併治療的資料及1643筆採用純西醫治療的資料。論文首先透過卡方檢定得知許多研究變項分布有顯著差異，因此無法直接運用Kaplan-Meier存活分析。論文接著採用Cox風險迴歸分析，將沒有通過等比例假設檢定的研究變項進行分層，並確定有交互作用的研究變項。分析結果顯示採用純西醫治療比採用中西醫合併治療約增加50\%的風險並且達到顯著，而中西醫合併治療的存活時間也明顯優於純西醫治療。而本篇論文也發現了8個單味藥與2個方劑在治療肺癌上有顯著的效果，也找出了7組有效的組合藥，約能降低80\textasciitilde 90\%的死亡風險。},
  pagetotal = {52},
  keywords = {實驗室}
}

@thesis{XieKunMingJiYuBianYiJiShuDeZhongYiZhengZhuangBiaoZhunHua2021,
  title = {基於編譯技術的中醫症狀標準化},
  author = {{謝昆明}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2021},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/ks4ssh},
  abstract = {中醫的診斷治療是以「辨證論治」的方式進行。透過四診—望、聞、問、切去蒐集症狀後，根據四診資料對疾病進行分析，這步驟為「辨證」；然後由辨證的資訊確立治療方式，為「論治」。但是在中醫發展的過程中會因為地區、氣候、朝代、個人習慣等因素，出現症狀一詞多義或是多詞一義的情形。所以我們要將這些描述不一致的症狀先經過標準化，使症狀達到一詞一義，才能降低後續的辨證難度。 本論文使用長詞優先斷詞法，搭配準備好的詞庫，把醫生的症狀敘述分割成更小部分，並轉換成編譯器看得懂的語彙。再使用上下文無關文法處理常見的症狀敘述，統整成文法規則之後，做出相對應的動作。與舊系統比較，新系統症狀標準化的辨識率從14.3\%顯著的提升到53.55\%。最後，上下文無關文法為可擴充的文法規則，未來如果觀察及分析更多的病歷資料，可以歸納出更多的文法規則，症狀標準化的辨識率就能持續地獲得提升。},
  pagetotal = {54},
  keywords = {實驗室},
  file = {C:\Users\BlackCat\Zotero\storage\9ITDNLNU\謝昆明 - 2021 - 基於編譯技術的中醫症狀標準化.pdf}
}

@thesis{XieMingKunYingYongBieMingZiXunZhiKuaChengXuDingYiShiYongGuanLianDeJianLi1996,
  title = {應用別名資訊之跨程序定義-使用關聯的建立},
  author = {{葉明坤}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {1996},
  journaltitle = {資訊工程學系},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/8748de},
  abstract = {在程序語言處理工具中, 分析程式變數的定義與使用之間的相依性, 是一項必要的工作. 在本篇論文中, 我們提供了對 C 語言程式, 建立跨 程序的定義－使用關聯的方法. 我們也探討如何利用定義－使用關聯來做 指令搬動的最佳化技巧. 在論文中我們也顯示了對指令搬動最佳化的基本 實驗的結果.},
  pagetotal = {63}
}

@article{xingzhaiTreatingDifferentDiseases2020,
  title = {Treating {{Different Diseases With}} the {{Same Method}}—{{A Traditional Chinese Medicine Concept Analyzed}} for {{Its Biological Basis}}},
  author = {{Xing Zhai} and {Xi Wang} and {Li Wang} and {Linlin Xiu} and {Weilu Wang} and {Xiaohan Pang}},
  date = {2020-06-26},
  journaltitle = {Frontiers in Pharmacology},
  shortjournal = {Front. Pharmacol.},
  volume = {11},
  pages = {946},
  issn = {1663-9812},
  doi = {10.3389/fphar.2020.00946},
  url = {https://www.frontiersin.org/article/10.3389/fphar.2020.00946/full},
  urldate = {2022-09-19},
  annotation = {17 citations (Crossref) [2024-03-26]\\
8 citations (Semantic Scholar/DOI) [2022-10-26]},
  file = {C:\Users\BlackCat\Zotero\storage\6RMKWGWL\Zhai 等。 - 2020 - Treating Different Diseases With the Same Method—A.pdf}
}

@inproceedings{xinyiheAnaMetaTableUnderstanding2023,
  title = {{{AnaMeta}}: {{A Table Understanding Dataset}} of {{Field Metadata Knowledge Shared}} by {{Multi-dimensional Data Analysis Tasks}}},
  shorttitle = {{{AnaMeta}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2023},
  author = {{Xinyi He} and {Mengyu Zhou} and {Mingjie Zhou} and {Jialiang Xu} and {Xiao Lv} and {Tianle Li} and {Yijia Shao} and {Shi Han} and {Zejian Yuan} and {Dongmei Zhang}},
  date = {2023-07},
  pages = {9471--9492},
  publisher = {Association for Computational Linguistics},
  location = {Toronto, Canada},
  doi = {10.18653/v1/2023.findings-acl.604},
  url = {https://aclanthology.org/2023.findings-acl.604},
  urldate = {2023-09-19},
  abstract = {Tabular data analysis is performed everyday across various domains. It requires an accurate understanding of field semantics to correctly operate on table fields and find common patterns in daily analysis. In this paper, we introduce the AnaMeta dataset, a collection of 467k tables with derived supervision labels for four types of commonly used field metadata: measure/dimension dichotomy, common field roles, semantic field type, and default aggregation function. We evaluate a wide range of models for inferring metadata as the benchmark. We also propose a multi-encoder framework, called KDF, which improves the metadata understanding capability of tabular models by incorporating distribution and knowledge information. Furthermore, we propose four interfaces for incorporating field metadata into downstream analysis tasks.},
  eventtitle = {Findings 2023},
  langid = {english},
  keywords = {未整理,資料集},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: AnaMeta：多維資料分析任務共享的領域元資料知識的表理解資料集\\
abstractTranslation:  表格數據分析每天都會在各個領域進行。需要對字段語義有準確的理解，才能正確操作表字段，並在日常分析中找到常見的模式。在本文中，我們介紹了 AnaMeta 資料集，這是一個 467k 表的集合，其中包含四種常用字段元資料的派生監督標籤：度量/維度二分法、公共欄位角色、語義欄位類型和預設聚合函數。我們評估了各種用於推斷元資料的模型作為基準。我們還提出了一種稱為 KDF 的多編碼器框架，它透過結合分佈和知識資訊來提高表格模型的元資料理解能力。此外，我們提出了四個接口，用於將字段元資料合併到下游分析任務中。},
  file = {C:\Users\BlackCat\Zotero\storage\7NQTDMVB\He 等。 - 2023 - AnaMeta A Table Understanding Dataset of Field Me.pdf}
}

@article{xixizhuApproximateReasoningLargeScale2023,
  title = {Approximate {{Reasoning}} for {{Large-Scale ABox}} in {{OWL DL Based}} on {{Neural-Symbolic Learning}}},
  author = {{Xixi Zhu} and {Bin Liu} and {Cheng Zhu} and {Zhaoyun Ding} and {Li Yao}},
  date = {2023-01},
  journaltitle = {Mathematics},
  volume = {11},
  number = {3},
  pages = {495},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-7390},
  doi = {10.3390/math11030495},
  url = {https://www.mdpi.com/2227-7390/11/3/495},
  urldate = {2023-07-28},
  abstract = {The ontology knowledge base (KB) can be divided into two parts: TBox and ABox, where the former models schema-level knowledge within the domain, and the latter is a set of statements of assertions or facts about instances. ABox reasoning is a process of discovering implicit knowledge in ABox based on the existing KB, which is of great value in KB applications. ABox reasoning is influenced by both the complexity of TBox and scale of ABox. The traditional logic-based ontology reasoning methods are usually designed to be provably sound and complete but suffer from long algorithm runtimes and do not scale well for ontology KB represented by OWL DL (Description Logic). In some application scenarios, the soundness and completeness of reasoning results are not the key constraints, and it is acceptable to sacrifice them in exchange for the improvement of reasoning efficiency to some extent. Based on this view, an approximate reasoning method for large-scale ABox in OWL DL KBs was proposed, which is named the ChunfyReasoner (CFR). The CFR introduces neural-symbolic learning into ABox reasoning and integrates the advantages of symbolic systems and neural networks (NNs). By training the NN model, the CFR approximately compiles the logic deduction process of ontology reasoning, which can greatly improve the reasoning speed while ensuring higher reasoning quality. In this paper, we state the basic idea, framework, and construction process of the CFR in detail, and we conduct experiments on two open-source ontologies built on OWL DL. The experimental results verify the effectiveness of our method and show that the CFR can support the applications of large-scale ABox reasoning of OWL DL KBs.},
  issue = {3},
  langid = {english},
  keywords = {approximate reasoning,CNN,large-scale ABox reasoning,neural-symbolic learning,待讀,數學,機器學習,理論基礎,知識挖掘,知識推理,知識本體},
  annotation = {0 citations (Crossref) [2024-03-26]\\
0 citations (Semantic Scholar/DOI) [2023-08-11]\\
titleTranslation: 基於神經符號學習的OWL DL大規模ABox近似推理\\
abstractTranslation:  本體知識庫（KB）可以分為兩部分：TBox和ABox，前者對領域內的模式級知識進行建模，後者是一組關於實例的斷言或事實的陳述。 ABox推理是基於現有知識庫發現ABox中隱含知識的過程，這在知識庫應用中具有重要價值。 ABox 推理受到 TBox 複雜度和 ABox 規模的影響。傳統的基於邏輯的本體推理方法通常被設計為可證明合理且完整，但算法運行時間長，並且對於以 OWL DL（描述邏輯）為代表的本體 KB 來說擴展性不佳。在某些應用場景中，推理結果的健全性和完整性並不是關鍵約束，犧牲它們來換取一定程度推理效率的提升也是可以接受的。基於這一觀點，提出了一種OWL DL KB中大規模ABox的近似推理方法，稱為ChunfyReasoner（CFR）。 CFR 將神經符號學習引入 ABox 推理中，並融合了符號系統和神經網絡（NN）的優點。 CFR通過訓練NN模型，近似編譯本體推理的邏輯推導過程，在保證較高推理質量的同時，可以大大提高推理速度。在本文中，我們詳細闡述了CFR的基本思想、框架和構建過程，並在兩個基於OWL DL的開源本體上進行了實驗。實驗結果驗證了我們方法的有效性，並表明CFR可以支持OWL DL KB的大規模ABox推理的應用。},
  note = {本篇論文在Knowledge Base、Ontology、Description Logic及相關數學描述有較完整的說明。},
  file = {C:\Users\BlackCat\Zotero\storage\NTTNXXCD\Zhu 等。 - 2023 - Approximate Reasoning for Large-Scale ABox in OWL .pdf}
}

@article{XuDongJiYuYuYinYunDeDianZiBingLiYanJiuYuShiJian2012,
  title = {基于语音云的电子病历研究与实践},
  author = {{徐冬} and {陶石} and {刘雨生}},
  date = {2012},
  journaltitle = {中国数字医学},
  volume = {007},
  number = {003},
  pages = {15--18},
  doi = {10.3969/j.issn.1673-7571.2012.03.005},
  abstract = {随着云计算概念的提出,基于云的语音识别技术得到快速发展.基于中文语音识别的云计算技术,结合模板化的电子病历应用实践,探索研究了中文语音识别技术在临床电子病历整合与集成的最佳应用实践.},
  keywords = {語音識別,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\I7QPL57F\徐冬 等。 - 2012 - 基于语音云的电子病历研究与实践.pdf}
}

@article{xuEffectOnlineResearch2022,
  title = {Effect of Online Research Data Management Instruction on Social Science Graduate Students’ {{RDM}} Skills},
  author = {Xu, Zhihong and Zhou, Xuan and Kogut, Ashlynn and Clough, Michael},
  date = {2022-10-01},
  journaltitle = {Library \& Information Science Research},
  shortjournal = {Library \& Information Science Research},
  volume = {44},
  number = {4},
  pages = {101190},
  issn = {0740-8188},
  doi = {10.1016/j.lisr.2022.101190},
  url = {https://www.sciencedirect.com/science/article/pii/S0740818822000536},
  urldate = {2024-03-22},
  abstract = {Prior research has suggested the value of and the need to provide consistent research data management (RDM) instruction specifically for graduate students. However, there is a lack of RDM instruction that is tailored for the social science disciplines. The effect of a four-hour, online RDM instruction intervention, designed based on the research data life cycle, on the RDM knowledge of graduate students in social science disciplines was investigated. A total of 84 students completed both pre/post knowledge assessments with 40 students randomly assigned into the intervention group receiving online instruction and 44 in the control group. A one-way ANCOVA was used for the data analysis. Results indicated that social science graduate students who received online RDM instruction had a significantly higher score in RDM knowledge than students in the control group. Moreover, the effect of the instruction on participants' RDM skills varied by their disciplines.},
  langid = {english},
  keywords = {Data literacy,Graduate students,Intervention,Online instruction,Research data management,使用者研究,未整理,研究流程},
  annotation = {5 citations (Crossref) [2024-03-26]\\
titleTranslation: 線上研究資料管理指導對社會科學研究生 RDM 技能的影響\\
abstractTranslation:  先前的研究表明專門為研究生提供一致的研究資料管理 (RDM) 指導的價值和必要性。然而，缺乏針對社會科學學科量身定制的 RDM 教學。根據研究資料生命週期設計的四小時線上 RDM 教學介入對社會科學學科研究生 RDM 知識的影響進行了調查。總共有 84 名學生完成了前/後知識評估，其中 40 名學生被隨機分配到接受線上指導的干預組，44 名學生被分配到對照組。使用單向ANCOVA 進行資料分析。結果表明，接受線上 RDM 指導的社會科學研究生的 RDM 知識得分顯著高於對照組學生。此外，指導對參與者 RDM 技能的影響因學科而異。},
  file = {C:\Users\BlackCat\Zotero\storage\2KLWX57G\S0740818822000536.html}
}

@inproceedings{xueliangzhaoKnowledgeGroundedDialogueGeneration2020,
  title = {Knowledge-{{Grounded Dialogue Generation}} with {{Pre-trained Language Models}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {{Xueliang Zhao} and {Wei Wu} and {Can Xu} and {Chongyang Tao} and {Dongyan Zhao} and {Rui Yan}},
  date = {2020-01},
  pages = {3377--3390},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.emnlp-main.272},
  url = {https://aclanthology.org/2020.emnlp-main.272},
  urldate = {2023-06-24},
  abstract = {We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues. Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.},
  eventtitle = {{{EMNLP}} 2020},
  langid = {english},
  keywords = {LLM,待讀,機器學習,知識本體},
  annotation = {42 citations (Crossref) [2024-04-27]\\
61 citations (Semantic Scholar/DOI) [2023-06-24]\\
titleTranslation: 使用預先訓練的語言模型生成基於知識的對話\\
abstractTranslation:  我們使用預先訓練的語言模型研究基於知識的對話生成。為了在容量限制下利用冗餘的外部知識，我們建議為由預先訓練的語言模型定義的響應生成配備知識選擇模塊，以及一種無監督方法，通過未標記的對話來聯合優化知識選擇和響應生成。兩個基準的實證結果表明，我們的模型在自動評估和人類判斷方面都可以顯著優於最先進的方法。},
  file = {C:\Users\BlackCat\Zotero\storage\TJZCLXK4\Zhao 等。 - 2020 - Knowledge-Grounded Dialogue Generation with Pre-tr.pdf}
}

@article{xuezhongzhouDevelopmentTraditionalChinese2010,
  title = {Development of Traditional {{Chinese}} Medicine Clinical Data Warehouse for Medical Knowledge Discovery and Decision Support},
  author = {{Xuezhong Zhou} and {Shibo Chen} and {Baoyan Liu} and {Runsun Zhang} and {Yinghui Wang} and {Ping Li} and {Yufeng Guo} and {Hua Zhang} and {Zhuye Gao} and {Xiufeng Yan}},
  date = {2010-02},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artificial Intelligence in Medicine},
  volume = {48},
  number = {2-3},
  pages = {139--152},
  issn = {09333657},
  doi = {10.1016/j.artmed.2009.07.012},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365709001055},
  urldate = {2022-09-19},
  langid = {english},
  annotation = {168 citations (Crossref) [2024-03-26]\\
193 citations (Semantic Scholar/DOI) [2022-10-26]\\
titleTranslation: 用於醫學知識發現和決策支持的中醫臨床數據倉庫開發}
}

@incollection{xuezhongzhouTextMiningClinical2005,
  title = {Text {{Mining}} for {{Clinical Chinese Herbal Medical Knowledge Discovery}}},
  booktitle = {Discovery {{Science}}},
  author = {{Xuezhong Zhou} and {Baoyan Liu} and {Zhaohui Wu}},
  editor = {{Achim Hoffmann} and {Hiroshi Motoda} and {Tobias Scheffer}},
  editora = {{David Hutchison} and {Takeo Kanade} and {Josef Kittler} and {Jon M. Kleinberg} and {Friedemann Mattern} and {John C. Mitchell} and {Moni Naor} and {Oscar Nierstrasz} and {C. Pandu Rangan} and {Bernhard Steffen} and {Madhu Sudan} and {Demetri Terzopoulos} and {Dough Tygar} and {Moshe Y. Vardi} and {Gerhard Weikum}},
  editoratype = {redactor},
  date = {2005},
  volume = {3735},
  pages = {396--398},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11563983_41},
  url = {http://link.springer.com/10.1007/11563983_41},
  urldate = {2022-09-19},
  isbn = {978-3-540-29230-2 978-3-540-31698-5}
}

@article{xuezhongzhouTextMiningTraditional2010,
  title = {Text Mining for Traditional {{Chinese}} Medical Knowledge Discovery: {{A}} Survey},
  shorttitle = {Text Mining for Traditional {{Chinese}} Medical Knowledge Discovery},
  author = {{Xuezhong Zhou} and {Yonghong Peng} and {Baoyan Liu}},
  date = {2010-08},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {Journal of Biomedical Informatics},
  volume = {43},
  number = {4},
  pages = {650--660},
  issn = {15320464},
  doi = {10.1016/j.jbi.2010.01.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046410000031},
  urldate = {2022-09-19},
  langid = {english},
  annotation = {93 citations (Crossref) [2024-03-26]\\
108 citations (Semantic Scholar/DOI) [2022-10-26]\\
titleTranslation: 中醫知識發現的文本挖掘：一項調查},
  file = {C:\Users\BlackCat\Zotero\storage\H7LF8HU7\Zhou 等。 - 2010 - Text mining for traditional Chinese medical knowle.pdf}
}

@article{XuTianMingMianXiangDianZiBingLiZhongWenYiXueXinXiDeKeShiZuZhiFangFa2015,
  title = {面向电子病历中文医学信息的可视组织方法},
  author = {{徐天明} and {樊银亭} and {马翠霞} and {滕东兴}},
  date = {2015},
  journaltitle = {计算机系统应用},
  volume = {24},
  number = {11},
  pages = {44--51},
  issn = {1003-3254},
  doi = {10.3969/j.issn.1003-3254.2015.11.007},
  abstract = {針對當前大量電子病歷信息無法充分利用的問題,研究了面向電子病歷中文醫學信息的主題建模及可視組織方法.首先基于電子病歷數據和醫療問答數據,進行預處理并轉換為純文本語料,然后采用基于Mallet的LDA主題模型訓練算法進行主題建模,并結合主題模型分析的需求進行可視組織與呈現,最后構建了面向中文醫學信息的可視分析系統.實例驗證表明該系統可以有效的輔助用戶進行主題模型的構建與分析,并有利于進一步的診斷.},
  keywords = {知網},
  file = {C:\Users\BlackCat\Zotero\storage\637ZLSC8\滕东兴徐天明 樊银亭 马翠霞 - 2015 - 面向电子病历中文医学信息的可视组织方法.pdf}
}

@article{XuYuanJiYuCRFYuRUTAGuiZeXiangJieHeDeZuZhongRuYuanJiLuYiXueShiTiShiBieJiYingYong2018,
  title = {基于CRF与RUTA规则相结合的卒中入院记录医学实体识别及应用},
  author = {{许源} and {葛艳秋} and {王强} and {熊刚} and {易应萍}},
  date = {2018},
  journaltitle = {中山大学学报（医学科学版）},
  shortjournal = {Journal of Sun Yat-sen University(Medical Sciences)},
  volume = {39},
  number = {3},
  pages = {455--462},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhF6c3lrZHh4YjIwMTgwMzAyMRoIOHY5b2JsejI%3D},
  urldate = {2022-08-14},
  abstract = {[目的]研究针对非结构化临床电子病历的自然语言处理模型的构建和优化,并利用该模型对江西省医疗大数据平台中卒中病人的病历进行结构化数据提取.[方法]从江西省医疗大数据平台中随机筛选500份2011-2016年的卒中病人入院记录,根据临床科研的实际需求构建了脑卒中专科病人的命名实体标注体系和命名实体标注语料库,利用该语料库构建基于CRF以及RUTA规则的命名实体抽取模型,并通过调整RUTA规则以及参数提升识别准确率.[结果]经五折交叉验证,该模型的医学命名实体的抽取准确率0.960,召回率0.916,Fscore 0.939,利用该抽取模型对大数据平台中10 295份脑卒中患者入院记录进行抽取,共},
  langid = {zh\_CN},
  keywords = {Chinese medical record,CRF,Journal of Sun Yat-sen University(Medical Sciences),named entity recognition,No DOI found,stroke,中山大学学报（医学科学版）,中文电子病历,命名实体识别,易应萍,条件随机场CRF,熊刚,脑卒中,许源},
  annotation = {南昌大学附属第二医院临床医疗大数据研究中心,江西南昌,330006南昌大学医学部公共卫生学院,江西南昌,330006赫博特医疗信息科技有限公司,江苏苏州,215000南昌大学附属第二医院临床医疗大数据研究中心,江西南昌330006;南昌大学第二附属医院科教处,江西南昌330006\\
江西省科技厅科技创新平台 国家自然科学基金\\
2018-06-21 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 基於CRF與RUTA規則相結合的卒中入院記錄醫學實體識別及應用\\
abstractTranslation:  [目的]研究針對非重構臨床電子病歷的自然語言處理模型的構建和優化，並利用該模型對江西省醫療大數據平台中卒中患者的病歷進行重構數據提取。[方法]來自江西省醫療大數據平台中隨機篩選500份2011-2016年的卒中患者入院記錄，根據臨床科研的實際需求構建了腦卒中專科患者的命名實體標記體系和命名實體標記語料庫，利用該語料庫構建基於CRF以及RUTA規則的抽取實體抽取模型，並通過調整RUTA規則以及參數提升識別準確率。[結果]經五折交叉驗證，該模型的醫學抽取實體的抽取準確率0.960，識別率0.916，Fscore 0.939，利用該抽取模型對大數據平台中10 295份腦卒中患者入院記錄進行抽取，共},
  file = {C:\Users\BlackCat\Zotero\storage\GUJF66DJ\许 等。 - 2018 - 基于CRF与RUTA规则相结合的卒中入院记录医学实体识别及应用.pdf}
}

@article{y.daradkehInformationOverloadHow2015,
  title = {Information {{Overload}}: {{How}} to Solve the Problem? {{Current}} Trends in Technology and Its Impacts to Individuals and Organizational Context},
  shorttitle = {Information {{Overload}}},
  author = {{Y. Daradkeh} and {Edona Thaqi Selimi} and {Luis Borges Gouveia}},
  date = {2015-03-04},
  journaltitle = {International Journal of Open Information Technologies},
  url = {https://www.semanticscholar.org/paper/Information-Overload%3A-How-to-solve-the-problem-in-Daradkeh-Selimi/0afcbae81b22f17eb5ae1da8fb268d4883842cde},
  urldate = {2023-10-23},
  abstract = {The Internet has had reflective effect on changing the life of each individual, not least by making consumers more involved in creating the content and offering products themselves using Internet. This paper reviews the problem of information overload, with particular reference to both individuals and business organizations. The paper tells that although the problem of information overload has existed for many years, in recent years the problem has become more widely recognized and practiced. The term ‘information overload’ has been recognized due to the rapid advances made by information and communication technology in the recent years, however, this does not make it clear whether this is making peoples life easier or more stressful. The paper reviews that even though there are hundreds and thousands of information available, it is still very difficult or even more stressful to find out the right information at the right time. Some solutions that are presented in order to solve the problem and reduce the information overload are: filtering the information, investing in knowledge and not information and the provision of value-added information (filtered by software or information curators). An emphasis is placed on technology as a tool and not the driver, while increased information literacy may provide the key to reducing information overload.},
  langid = {english},
  keywords = {No DOI found},
  annotation = {titleTranslation: 資訊過載：如何解決問題？當前技術趨勢及其對個人和組織的影響\\
abstractTranslation:  網路對改變每個人的生活產生了反思性的影響，尤其是讓消費者更參與利用網路創作內容和提供產品。本文回顧了資訊過載問題，特別是個人和商業組織。論文稱，儘管資訊過載問題已經存在多年，但近年來該問題已得到更廣泛的認識和實踐。近年來，由於資訊和通訊技術的快速發展，「資訊過載」一詞已得到認可，但這並不能明確這到底是讓人們的生活變得更輕鬆還是更有壓力。論文評論稱，儘管有成百上千的可用信息，但要在正確的時間找到正確的信息仍然非常困難，甚至壓力更大。為了解決問題並減少資訊過載而提出的一些解決方案是：過濾資訊、投資於知識而不是資訊以及提供增值資訊（由軟體或資訊管理者過濾）。重點強調技術作為工具而不是驅動力，而提高資訊素養可能是減少資訊過載的關鍵。},
  note = {[TLDR] It is still very difficult or even more stressful to find out the right information at the right time and some solutions that are presented are: filtering the information, investing in knowledge and not information and the provision of value-added information (filtered by software or information curators).}
}

@misc{YanAnZian-ziyenShengHuoJiLuTanKanYiGeRenZhiShiKuWeiBenDeDuoMoTaiZiXunZhaoHui2020,
  title = {生活紀錄探勘: 以個人知識庫為本的多模態資訊召回},
  shorttitle = {生活紀錄探勘},
  author = {{顏安孜(An-Zi Yen)}},
  year = {1 月 1, 2020},
  abstract = {由於人們經常忘記生活中所經歷的人、地、時、事、物，在本篇論文中，我們致力於研究建立個人生活知識庫和資訊召回系統，讓人們能夠以問答的方式，通過個人生活知識庫及其相關的世界知識，查詢個人生活經驗。首先，我們收集使用者分享在Twitter上的圖像和文本信息，從中擷取生活事件，並建立個人知識庫。其中，要解決的問題包括：(1)不是所有的文字描述都包含生活事件，(2)生活事件可以被明確地或是隱含地表達，(3) 隱含的生活事件通常不存在謂詞，(4)自然語言謂詞映射到知識庫關係的關聯可能是不明確的。我們提出多模態聯合學習的方法，該方法藉由使用者分享在Twitter上的文本和圖像進行訓練，用於偵測推文中的生活事件，並擷取事件元素。事件元素包括主題，謂語，賓語和時間。接著，我們將擷取的事件轉換為與知識庫格式相容的事實格式。為了連接個人生活事件和世界知識，我們提出了以對抗學習為本的實體連結模型，該實體連結模型不局限於特定領域。對抗學習網絡的選擇器神經網絡用於選擇高品質的負樣本訓練判別神經網絡，從而提高了判別神經網絡連結個人知識和世界知識實體的效能。 最後，我們研究如何建立資訊召回系統，幫助人們回想自己的生活經歷，例如她/他們去過的地方、遇到的人、事、物等等。但是，在現實世界的應用情境中，人們經常混淆曾經發生的事件，導致提出的問題無法根據知識庫回答。所以，我們建立了一個由問題回答模型和問題產生模型組成的資訊召回系統。系統不僅可以回答問題，還可以在必要時修正無法回答的問題，主動地幫助使用者回想她/他想查詢的生活經歷。我們的問答模型可以辨識問題是否與知識庫的事實不符，並建議可以建構出可行問題的事實。然後，通過基於強化學習的問題生成模型，將事實轉換為可回答的問題，並結合回收編輯的機制，產生更流暢的問題。 我們使用18個Twitter用戶的生活記錄和從維基百科擷取的世界知識評估方法的效能。實驗結果顯示，我們提出的方法能有效地擷取生活事件、建構個人知識庫，並且證明我們建立的資訊召回系統可以幫助使用者回想個人生活經歷。},
  langid = {chinese},
  organization = {國立台灣大學學位論文},
  keywords = {私人},
  annotation = {titleTranslation: 生活紀錄探查：以個人知識庫為主的多模態資訊認知}
}

@article{yangBrainInspiredSelfOrganizingEpisodic2022,
  title = {A {{Brain-Inspired Self-Organizing Episodic Memory Model}} for a {{Memory Assistance Robot}}},
  author = {Yang, Chiao-Yu and Gamborino, Edwinn and Fu, Li-Chen and Chang, Yu-Ling},
  date = {2022-06},
  journaltitle = {IEEE Transactions on Cognitive and Developmental Systems},
  volume = {14},
  number = {2},
  pages = {617--628},
  issn = {2379-8939},
  doi = {10.1109/TCDS.2021.3061659},
  url = {https://ieeexplore.ieee.org/document/9361701},
  urldate = {2024-01-14},
  abstract = {This article discusses the implementation of a brain-inspired episodic memory model, which provides memory assistance and tackles the modern public issue of memory impairment embedded as an end-to-end system on the robot companion, Pepper. Based on the fusion adaptive resonance theory, the proposed model can observe and memorize the content of daily events in five aspects: 1) people; 2) activities; 3) times; 4) places; and 5) objects. The model is based on the human memory pipeline, containing a working memory and a two-layer long-term memory model, which can effectively merge, cluster, and summarize past memories based on their context and relevance in a self-organizing manner. When providing memory assistance, the robot can analyze a user query and find the best matching memory cluster to generate verbal cues to stimulate recalling of the target event. Moreover, using reinforcement learning, the robot eventually learns the most effective mapping of cue types to event type through social interaction. Experiments show the feasibility of the proposed model, which can handle episodic events with elasticity and stability. Moreover, there is evidence that the robot is able to provide robust memory assistance from knowledge obtained through previous observations, with 99\% confidence, intervals in the participants’ mean recall percentage of the events increases 19.63\% after receiving memory assistance from the robot.},
  eventtitle = {{{IEEE Transactions}} on {{Cognitive}} and {{Developmental Systems}}},
  langid = {english},
  keywords = {問答,已整理,私人,記憶},
  annotation = {4 citations (Crossref) [2024-03-26]\\
titleTranslation: 用於記憶輔助機器人的受大腦啟發的自組織情景記憶模型},
  file = {C:\Users\BlackCat\Zotero\storage\Y5FP6332\9361701.html}
}

@article{YangFeiHongZhongWenDianZiBingLiDeMingMingShiTiShiBieYanJiuJinZhan2020,
  title = {中文电子病历的命名实体识别研究进展},
  author = {{杨飞洪} and {张宇} and {覃露} and {李姣}},
  date = {2020},
  journaltitle = {中国数字医学},
  shortjournal = {China Digital Medicine},
  volume = {15},
  number = {2},
  pages = {9--12},
  doi = {10.3969/j.issn.1673-7571.2020.02.003},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5Eg96Z3N6eXgyMDIwMDIwMDQaCGE5d3V5bmo2},
  urldate = {2022-08-14},
  abstract = {目的:了解命名实体识别技术在中文电子病历文本中的研究进展.方法:从电子病历与命名实体识别的基本概念、语料资源的获取、语料标注的相关工作、命名实体识别算法以及相关应用等多个角度进行文献调研.结果:综述了近五年中文电子病历命名实体识别的研究进展情况.结论:基于当前进展分析了中文电子病历命名实体识别的未来发展趋势.},
  langid = {zh\_CN},
  keywords = {China Digital Medicine,中国数字医学,命名实体识别,张宇,电子病历,综述},
  annotation = {中国医学科学院,北京协和医学院医学信息研究所,100020,北京市朝阳区雅宝路3号中国医学科学院,北京协和医学院医学信息研究所,100020,北京市朝阳区雅宝路3号中国医学科学院,北京协和医学院医学信息研究所,100020,北京市朝阳区雅宝路3号中国医学科学院,北京协和医学院医学信息研究所,100020,北京市朝阳区雅宝路3号\\
2020-05-18 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 中文電子疾病歷的命名實體識別研究進展\\
abstractTranslation:  目的：了解命名實體識別技術在中文電子疾病歷中的研究進展。方法：從電子病歷與命名實體識別的基本概念、語料資源的獲取、語料標記的相關工作、命名實體識別算法以及相關文本應用等多方面個角度進行文獻調研。結果：綜述了近五年中文電子病歷命名實體識別的研究進展情況。結論：基於當前進展分析了中文電子病歷命名實體識別的未來發展趨勢。},
  file = {C:\Users\BlackCat\Zotero\storage\ZK2BL94N\杨 等。 - 2020 - 中文电子病历的命名实体识别研究进展.pdf}
}

@article{YangHongMeiJiYuShuangXiangLSTMShenJingWangLuoDianZiBingLiMingMingShiTiDeShiBieMoXing2018,
  title = {基于双向LSTM神经网络电子病历命名实体的识别模型},
  author = {{杨红梅} and {李琳} and {杨日东} and {周毅}},
  date = {2018},
  journaltitle = {中国组织工程研究},
  shortjournal = {Chinese Journal of Tissue Engineering Research},
  volume = {22},
  number = {20},
  pages = {3237--3242},
  doi = {10.3969/j.issn.2095-4344.0302},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5Eg14ZGtmMjAxODIwMDIyGghhOXd1eW5qNg%3D%3D},
  urldate = {2022-08-14},
  abstract = {背景:电子病历数据是医疗领域大数据的重要源头,是医学知识的体现.电子病历是患者就医过程的记录,是临床辅助决策系统、精准医学研究和疾病监控等应用的重要数据支撑.目的:研究电子病历的信息抽取技术,提取中文电子病历中的重要医学实体,支持肝细胞癌的知识发现.方法:数据集来自广东省某三甲医院的电子病历数据库.共收集了240例患有肝细胞癌的病历记录(18542个句子),包括入院记录和出院小结.按照预先定义的标准进行标注.随机抽取180例患者病历(13839个句子)进行训练,并保留60个病例记录(4703个句子)作为测试集.利用双向的LSTM网络结合CRF训练命名实体识别模型.在测试数据集上评估NER系统的},
  langid = {zh\_CN},
  keywords = {BiLSTM,Chinese Journal of Tissue Engineering Research,CRF,中国组织工程研究,周毅,命名实体识别,电子病历,组织构建},
  annotation = {中山大学中山医学院,广东省广州市,510080新疆医科大学,新疆维吾尔自治区乌鲁木齐市,830011中山大学中山医学院,广东省广州市 510080;新疆医科大学,新疆维吾尔自治区乌鲁木齐市 830011\\
国家科技重大专项 广州市健康医疗协同创新重大专项~NSFC-广东大数据科学中心联合基金项目~国家重点研发计划~广东省前沿与关键技术创新专项基金项目\\
2018-08-14 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 基於LSTM神經網絡電子病歷命名實體的識別模型\\
abstractTranslation:  背景：電子病歷數據是醫療領域大數據的源頭，是醫學知識的體現。電子病歷是患者就醫過程的重要記錄，是臨床輔助決策系統、精準醫學研究和疾病監控等應用的重要數據支撐。研究電子病歷的信息抽取技術，提取中文電子病歷中的重要醫學實體，支持肝細胞癌的知識發現。方法：數據集來自廣東省某三甲醫院的電子病歷數據庫。共收集了240例肝細胞癌症的病歷記錄(18542個句子)，包括入院記錄和出院小結。按照事先規定的標准進行標註。隨機抽取180例病人病歷記錄(13839個句子)進行訓練，並保留60個病例記錄(4703個句子)作為測試集。利用集體的LSTM網絡結合CRF訓練命名實體識別模型。在測試數據集上評估NER系統的},
  file = {C:\Users\BlackCat\Zotero\storage\3P4D7UG8\杨 等。 - 2018 - 基于双向LSTM神经网络电子病历命名实体的识别模型.pdf}
}

@article{YangJinFengDianZiBingLiMingMingShiTiShiBieHeShiTiGuanXiChouQuYanJiuZongShu2014,
  title = {电子病历命名实体识别和实体关系抽取研究综述},
  author = {{杨锦锋} and {于秋滨} and {关毅} and {蒋志鹏}},
  date = {2014},
  journaltitle = {自动化学报},
  volume = {40},
  number = {8},
  pages = {1537--1562},
  keywords = {No DOI found},
  file = {C:\Users\BlackCat\Zotero\storage\JCA46IFS\杨锦锋 等。 - 2014 - 电子病历命名实体识别和实体关系抽取研究综述.pdf}
}

@thesis{YangJingHuaJiYuDuoMuBiaoZuiJiaHuaDeZhongYiXuZhengZhenJiuChuFangXiTongDeYanZhi2021,
  title = {基於多目標最佳化的中醫虛證針灸處方系統的研製},
  author = {{楊京樺}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2021},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/adygrw},
  abstract = {雖然針灸醫療已經累積了很多經驗，但是針灸處方仍然遠不如中藥處方成熟。尤其腧穴配伍法則尚未有很高的共識。本論文嘗試從已有的針灸處方著手，依據針灸處方的腧穴組成及採用的腧穴療效，來學習針灸處方的腧穴配伍法則。如同中藥處方常採用藥材的多個療效進行配伍，針灸處方也常採用腧穴的多個療效進行配伍，本論文運用多目標最佳化演算法，來實作多療效的針灸處方系統。本論文也進行針灸處方中腧穴與療效的標準化，並建構針灸知識本體，及腧穴與療效的查詢系統。本論文依據83帖虛證針灸處方學習，實作的多療效針灸處方系統，成功地將83帖虛證針灸處方的每個腧穴，都以首選腧穴配伍出來。期待這個多療效針灸處方系統可以做為智慧針灸醫療的基礎建設，提升針灸醫療的品質。},
  pagetotal = {46},
  keywords = {實驗室}
}

@online{yangLargeLanguageModels2024,
  title = {Large {{Language Models}} as {{Optimizers}}},
  author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
  date = {2024-04-15},
  eprint = {2309.03409},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.03409},
  url = {http://arxiv.org/abs/2309.03409},
  urldate = {2024-04-27},
  abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,LLM,未整理},
  annotation = {abstractTranslation:  優化無所不在。雖然基於導數的演算法一直是解決各種問題的強大工具，但梯度的缺失給許多現實世界的應用帶來了挑戰。在這項工作中，我們提出了 Optimization by PROmpting (OPRO)，這是一種利用大型語言模型 (LLM) 作為優化器的簡單而有效的方法，其中優化任務以自然語言描述。在每個最佳化步驟中，LLM 根據包含先前產生的解決方案及其值的提示產生新的解決方案，然後對新解決方案進行評估並將其新增至下一個最佳化步驟的提示中。我們首先展示 OPRO 在線性迴歸和旅行商問題上的表現，然後轉向我們在即時優化中的主要應用程序，其目標是找到最大化任務準確性的指令。透過各種法學碩士，我們證明了 OPRO 優化的最佳提示在 GSM8K 上比人工設計的提示高出高達 8\%，在 Big-Bench Hard 任務上比人工設計的提示高出高達 50\%。程式碼位於 https://github.com/google-deepmind/opro。\\
titleTranslation: 作為優化器的大型語言模型},
  note = {Comment: ICLR 2024; 42 pages, 26 figures, 15 tables. Code at https://github.com/google-deepmind/opro},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\Y53YGBK2\\Yang 等。 - 2024 - Large Language Models as Optimizers.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\SRDVPUA7\\2309.html}
}

@article{YangLingICD11ChuanTongYiXueMoKuaiFenLeiTiXiYuXinBanZhongYiGuoBiaoBianMaDeBiJiaoJiQiZaiFeiXiJiBingFenLeiZhongDeYingYong2022,
  title = {ICD-11传统医学模块分类体系与新版中医国标编码的比较及其在肺系疾病分类中的应用},
  author = {{杨玲} and {黄茜茜} and {姚放放} and {欧阳光}},
  date = {2022},
  journaltitle = {广州中医药大学学报},
  shortjournal = {Journal of Guangzhou University of Traditional Chinese Medicine},
  volume = {39},
  number = {6},
  pages = {1423--1428},
  doi = {10.13359/j.cnki.gzxbtcm.2022.06.034},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhJnenp5eWR4eGIyMDIyMDYwMzQaCGZ6ZWE2MW5z},
  urldate = {2022-08-14},
  abstract = {比较国际疾病分类第十一次修订本(International Classification of Diseases 11th Revision,ICD-11)的传统医学模块(Traditional Medicine Module 1,TM1)与国家卫生健康委员会和国家中医药管理局发布的《中医病证分类与代码》《中医临床诊疗术语第1部分:疾病》《中医临床诊疗术语第2部分:证候》《中医临床诊疗术语第3部分:治法》4项国标的修订版(简称"新版中医国标")的分类体系与编码方式,并对新版中医国标与ICD-11 TM1在肺系疾病的中医疾病与证候分类编码中的应用进行探讨.分析结果显示,ICD-11 TM1和新版},
  langid = {zh\_CN},
  keywords = {Journal of Guangzhou University of Traditional Chinese Medicine,中医疾病,中医证候,国际疾病分类第十一次修订本传统医学模块(ICD-11TM1),新版中医国标,黄茜茜},
  annotation = {广州中医药大学第二附属医院,广东广州 510120广州中医药大学,广东广州 510006\\
广东省中医药健康服务与产业发展研究中心项目\\
2022-06-06 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: ICD-11傳統醫學模塊分類體係與新版中醫國標編碼的比較及其在肺系疾病分類中的應用\\
abstractTranslation:  比較國際疾病分類第十一次修訂本（國際疾病分類第11版，ICD-11）的傳統醫學模塊（Traditional Medicine Module 1，TM1）與國家衛生健康委員會和國家中醫藥管理局發布的《中醫病》證分類與代碼》《中醫臨床診斷術語第1部分:疾病》《中醫臨床診斷術語第2部分:證候》《中醫臨床診斷術語第3部分:治療法》4項國標的修訂版(簡稱“新版”中醫國標")的分類與體系編碼方式,文學新版中醫國標與ICD-11 TM1在肺系疾病的中醫疾病與證候分類編碼中的應用進行探討。分析結果顯示,ICD-11 TM1和新版},
  file = {C:\Users\BlackCat\Zotero\storage\HTRTDQ4X\杨 等。 - 2022 - ICD-11传统医学模块分类体系与新版中医国标编码的比较及其在肺系疾病分类中的应用.pdf}
}

@inproceedings{yangsongRealtimeAutomaticTag2008,
  title = {Real-Time Automatic Tag Recommendation},
  booktitle = {Proceedings of the 31st Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {{Yang Song} and {Ziming Zhuang} and {Huajing Li} and {Qiankun Zhao} and {Jia Li} and {Wang-Chien Lee} and {C. Lee Giles}},
  year = {7 月 20, 2008},
  series = {{{SIGIR}} '08},
  pages = {515--522},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1390334.1390423},
  url = {https://doi.org/10.1145/1390334.1390423},
  urldate = {2023-09-05},
  abstract = {Tags are user-generated labels for entities. Existing research on tag recommendation either focuses on improving its accuracy or on automating the process, while ignoring the efficiency issue. We propose a highly-automated novel framework for real-time tag recommendation. The tagged training documents are treated as triplets of (words, docs, tags), and represented in two bipartite graphs, which are partitioned into clusters by Spectral Recursive Embedding (SRE). Tags in each topical cluster are ranked by our novel ranking algorithm. A two-way Poisson Mixture Model (PMM) is proposed to model the document distribution into mixture components within each cluster and aggregate words into word clusters simultaneously. A new document is classified by the mixture model based on its posterior probabilities so that tags are recommended according to their ranks. Experiments on large-scale tagging datasets of scientific documents (CiteULike) and web pages del.icio.us) indicate that our framework is capable of making tag recommendation efficiently and effectively. The average tagging time for testing a document is around 1 second, with over 88\% test documents correctly labeled with the top nine tags we suggested.},
  isbn = {978-1-60558-164-4},
  langid = {english},
  keywords = {graph partitioning,mixture model,tagging system,分類標籤},
  annotation = {152 citations (Crossref) [2024-03-26]\\
titleTranslation: 實時自動標籤推薦\\
abstractTranslation:  標籤是用戶為實體生成的標籤。現有的標籤推薦研究要么側重於提高其準確性，要么側重於過程自動化，而忽略了效率問題。我們提出了一種用於實時標籤推薦的高度自動化的新穎框架。帶標籤的訓練文檔被視為三元組（單詞、文檔、標籤），並以兩個二分圖表示，並通過譜遞歸嵌入 (SRE) 將其劃分為集群。每個主題簇中的標籤均通過我們新穎的排名算法進行排名。提出了一種雙向泊松混合模型（PMM），將文檔分佈建模為每個簇內的混合成分，並同時將單詞聚合到單詞簇中。混合模型根據後驗概率對新文檔進行分類，以便根據其排名推薦標籤。在科學文獻（CiteULike）和網頁（del.icio.us）的大規模標籤數據集上的實驗表明，我們的框架能夠高效且有效地進行標籤推薦。測試文檔的平均標記時間約為 1 秒，超過 88\% 的測試文檔正確標記了我們建議的前九個標籤。},
  file = {C:\Users\BlackCat\Zotero\storage\F786HU8S\Song 等。 - 2008 - Real-time automatic tag recommendation.pdf}
}

@article{YanLingJieJiYuDianZiBingLiDeDaShuJuPingTaiYingYongYanJiuYuTanSuo2020,
  title = {基于电子病历的大数据平台应用研究与探索},
  author = {{严灵杰}},
  date = {2020},
  journaltitle = {电子元器件与信息技术},
  doi = {10.19772/j.cnki.2096-4455.2020.10.020},
  url = {https://www.nstl.gov.cn/paper_detail.html?id=ce2fd824b723a8eef0ad2ad46d0419d2},
  urldate = {2022-10-23},
  abstract = {自二十一世纪开始以来,伴随着互联网以及通信产业的不断发展,人类生活的方方面面也都享受着其带来的种种便利,医疗行业也在其中,本文主要介绍一种基于电子病历的大数据平台,相比于传统的纸质病历,电子病历具有随时查看,方便保存等特性,该平台的出现也改变了以往患者对于纸质病历的过分依赖,电子病历也可用于远程会诊,异地就医,可以给患者节约大量的时间和金钱,大大方便了患者就医的过程,同样,对于医护人员而言,也可以在电脑上书写病历,依靠平台查看患者的信息,相较于传统的翻阅纸质病历,也可以大大提升医护人员的工作效率.本文对基于电子病历的大数据平台的应用展开了相关研究与探索,在大数据平台的基础之上,借助于大数据的手段医务人员可以进一步加强对病人信息的管理,医疗研究人员可以更加快速地获取到相关信息,利于对疾病开展研究,医院管理者能获取到具有参考价值的数据信息,患者也能获取到对应的专业知识,但在享受电子病历所带来的便利时也会承担相应的风险,病例对于患者而言也属于个人隐私,平台也需要保证患者病历的安全性,避免患者隐私的泄露.},
  keywords = {信息安全,信息管理,大数据平台,电子病历},
  file = {C:\Users\BlackCat\Zotero\storage\2VP5X7E7\严灵杰 - 2020 - 基于电子病历的大数据平台应用研究与探索.pdf}
}

@article{yanqiongzhangSoFDAIntegratedWeb2022,
  title = {{{SoFDA}}: An Integrated Web Platform from Syndrome Ontology to Network-Based Evaluation of Disease–Syndrome–Formula Associations for Precision Medicine},
  shorttitle = {{{SoFDA}}},
  author = {{Yanqiong Zhang} and {Ning Wang} and {Xia Du} and {Tong Chen} and {Zecong Yu} and {Yuewen Qin} and {Wenjia Chen} and {Meng Yu} and {Ping Wang} and {Huamin Zhang} and {Xuezhong Zhou} and {Luqi Huang} and {Haiyu Xu}},
  date = {2022-06-15},
  journaltitle = {Science Bulletin},
  shortjournal = {Science Bulletin},
  volume = {67},
  number = {11},
  pages = {1097--1101},
  issn = {2095-9273},
  doi = {10.1016/j.scib.2022.03.013},
  url = {https://www.sciencedirect.com/science/article/pii/S2095927322001037},
  urldate = {2023-10-19},
  keywords = {中醫,已整理,重要},
  annotation = {12 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\LYCEKSQF\S2095927322001037.html}
}

@thesis{YanZiXinShangZhiShangKeDeZhongYiMoHuBianZheng2022,
  title = {上肢傷科的中醫模糊辨證},
  author = {{顏子昕}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2022},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/pxygaw},
  abstract = {本論文整合中醫外傷學實務及西醫外傷學檢查，開發一個上肢傷科模糊辨證系統。本系統將辨證問題定義為一個模糊集合的歸屬問題，每個證候用一個模糊集合定義。本論文同時採用症狀權重指數遞減分配及症狀分群來優化模糊集合的歸屬函數。本論文採用兩種評估係數。本論文先使用基線鑑別係數來評估證候之間的差異度，基線鑑別係數愈接近1，表示證候之間的差異度愈大，愈容易達成精確的辨證。使用指數分權分群歸屬函數，本系統的基線鑑別係數可達0.97，顯示證候之間的差異度非常高。本論文接著使用臨床相似係數來評估系統臨床辨證的精確度，基於51個臨床病例，使用指數分權分群歸屬函數，在閥值為0.5時，臨床相似係數最高，可達0.95，顯示使用0.5的閥值，系統的臨床辨證精確度非常高。},
  pagetotal = {76},
  keywords = {實驗室}
}

@inproceedings{yaoliuAutomaticConstructionBasedNLP2008,
  title = {On Automatic Construction of Based-{{NLP Chinese}} Medicine Ontology Concept\&\#x2019;s Description Architacture},
  booktitle = {2008 {{International Conference}} on {{Audio}}, {{Language}} and {{Image Processing}}},
  author = {{Yao Liu} and {Xuefei Chen} and {Zhifang Sui} and {Huili Wang} and {Yang Zhou}},
  date = {2008-07},
  pages = {50--55},
  publisher = {IEEE},
  location = {Shanghai, China},
  doi = {10.1109/icalip.2008.4589992},
  url = {http://ieeexplore.ieee.org/document/4589992/},
  urldate = {2022-09-19},
  eventtitle = {2008 {{International Conference}} on {{Audio}}, {{Language}} and {{Image Processing}} ({{ICALIP}})},
  isbn = {978-1-4244-1723-0},
  annotation = {3 citations (Crossref) [2024-03-26]\\
6 citations (Semantic Scholar/DOI) [2022-10-26]},
  note = {和另一篇很像},
  file = {C:\Users\BlackCat\Zotero\storage\3LIKY3X9\Yao Liu 等。 - 2008 - On automatic construction of based-NLP Chinese med.pdf}
}

@inproceedings{yaoliuConstructionChineseMedicine2008,
  title = {On {{Construction}} of {{Chinese Medicine Ontology Concept}}'s {{Description Architecture}}},
  booktitle = {2008 3rd {{International Conference}} on {{Innovative Computing Information}} and {{Control}}},
  author = {{Yao Liu} and {Xuefei Chen} and {Yang Zhou} and {Huilin Wang} and {Chengzhi Zhang} and {Zhenguo Wang}},
  date = {2008-06},
  pages = {70--70},
  publisher = {IEEE},
  location = {Dalian, Liaoning},
  doi = {10.1109/icicic.2008.397},
  url = {https://ieeexplore.ieee.org/document/4603259/},
  urldate = {2022-09-19},
  eventtitle = {2008 3rd {{International Conference}} on {{Innovative Computing Information}} and {{Control}} ({{ICICIC}})},
  annotation = {1 citations (Crossref) [2024-03-26]\\
3 citations (Semantic Scholar/DOI) [2022-10-26]},
  note = {以NLP及au-learning system做中醫知識本體的自動構建},
  file = {C:\Users\BlackCat\Zotero\storage\H5TE4G2D\Liu 等。 - 2008 - On Construction of Chinese Medicine Ontology Conce.pdf}
}

@inproceedings{yaozhaoImplicitRelationLinking2022,
  title = {Implicit {{Relation Linking}} for {{Question Answering}} over {{Knowledge Graph}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {{Yao Zhao} and {Jiacheng Huang} and {Wei Hu} and {Qijin Chen} and {XiaoXia Qiu} and {Chengfu Huo} and {Weijun Ren}},
  date = {2022-05},
  pages = {3956--3968},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.findings-acl.312},
  url = {https://aclanthology.org/2022.findings-acl.312},
  urldate = {2023-09-19},
  abstract = {Relation linking (RL) is a vital module in knowledge-based question answering (KBQA) systems. It aims to link the relations expressed in natural language (NL) to the corresponding ones in knowledge graph (KG). Existing methods mainly rely on the textual similarities between NL and KG to build relation links. Due to the ambiguity of NL and the incompleteness of KG, many relations in NL are implicitly expressed, and may not link to a single relation in KG, which challenges the current methods. In this paper, we propose an implicit RL method called ImRL, which links relation phrases in NL to relation paths in KG. To find proper relation paths, we propose a novel path ranking model that aligns not only textual information in the word embedding space but also structural information in the KG embedding space between relation phrases in NL and relation paths in KG. Besides, we leverage a gated mechanism with attention to inject prior knowledge from external paraphrase dictionaries to address the relation phrases with vague meaning. Our experiments on two benchmark and a newly-created datasets show that ImRL significantly outperforms several state-of-the-art methods, especially for implicit RL.},
  eventtitle = {Findings 2022},
  langid = {english},
  keywords = {未整理,機器學習,語意分析},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 透過知識圖進行問答的隱式關係鏈接\\
abstractTranslation:  關係連結（RL）是基於知識的問答（KBQA）系統中的重要模組。它的目的是將自然語言（NL）表達的關係與知識圖譜（KG）中相應的關係連結起來。現有的方法主要依靠NL和KG之間的文本相似性來建立關係連結。由於NL的模糊性和KG的不完整性，NL中的許多關係是隱式表達的，可能無法連結到KG中的單一關係，這對目前的方法提出了挑戰。在本文中，我們提出了一種名為 ImRL 的隱式強化學習方法，它將 NL 中的關係短語連結到 KG 中的關係路徑。為了找到正確的關係路徑，我們提出了一種新穎的路徑排序\hspace{0pt}\hspace{0pt}模型，該模型不僅可以對齊單字嵌入空間中的文字訊息，還可以對齊NL 中的關係短語和KG 中的關係路徑之間的KG 嵌入空間中的結構資訊。此外，我們利用門控機制，注意從外部釋義詞典中註入先驗知識來處理含義模糊的關係短語。我們在兩個基準和新創建的資料集上進行的實驗表明，ImRL 顯著優於幾種最先進的方法，尤其是對於隱式強化學習。},
  file = {C:\Users\BlackCat\Zotero\storage\J8HK8DMQ\Zhao 等。 - 2022 - Implicit Relation Linking for Question Answering o.pdf}
}

@article{yaswanthsrisaisantoshtokalaLabelInformedHierarchical2023,
  title = {Label Informed Hierarchical Transformers for Sequential Sentence Classification in Scientific Abstracts},
  author = {{Yaswanth Sri Sai Santosh Tokala} and {Sai Saketh Aluru} and {Anoop Vallabhajosyula} and {Debarshi Kumar Sanyal} and {Partha Pratim Das}},
  date = {2023},
  journaltitle = {Expert Systems},
  volume = {40},
  number = {6},
  pages = {e13238},
  issn = {1468-0394},
  doi = {10.1111/exsy.13238},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.13238},
  urldate = {2023-09-18},
  abstract = {Segmenting scientific abstracts into discourse categories like background, objective, method, result, and conclusion is useful in many downstream tasks like search, recommendation and summarization. This task of classifying each sentence in the abstract into one of a given set of discourse categories is called sequential sentence classification. Existing machine learning-based approaches to this problem consider the content of only the abstract to obtain the neural representation of each sentence, which is then labelled with a discourse category. But this ignores the semantic information offered by the discourse labels themselves. In this paper, we propose LIHT, Label Informed Hierarchical Transformers – a method for sequential sentence classification that explicitly and hierarchically exploits the semantic information in the labels to learn label-aware neural sentence representations. The hierarchical model helps to capture not only the fine-grained interactions between the discourse labels and the words in the abstract at the sentence level but also the potential dependencies that may exist in the label sequence. Thus, LIHT generates label-aware contextual sentence representations that are then labelled with a conditional random field. We evaluate LIHT on three publicly available datasets, namely, PUBMED-RCT, NICTA-PIBOSO and CSAbstract. The incremental gain in F1-score in all the three cases over the respective state-of-the-art approaches is around 1\%. Though the gains are modest, LIHT establishes a new performance benchmark for this task and is a novel technique of independent interest. We also perform an ablation study to identify the contribution of each component of LIHT in the observed performance, and a case study to visualize the roles of the different components of our model.},
  langid = {english},
  keywords = {discourse segmentation,hierarchical transformers,scholarly data,scientific abstracts,sequential sentence classification,已整理,文字處理,語意分析},
  annotation = {0 citations (Crossref) [2024-03-26]\\
titleTranslation: 標籤知情分層轉換器，用於科學摘要中的順序句子分類\\
abstractTranslation:  將科學摘要細分為背景、目標、方法、結果和結論等話語類別對於許多下游任務（如搜尋、推薦和總結）很有用。將摘要中的每個句子分類為一組給定的語篇類別之一的任務稱為順序句子分類。現有的基於機器學習的解決此問題的方法僅考慮摘要的內容以獲得每個句子的神經表示，然後用話語類別進行標記。但這忽略了話語標籤本身所提供的語意資訊。在本文中，我們提出了LIHT，即標籤知情分層變換器（Label Informed Hierarchical Transformers）——一種用於順序句子分類的方法，它明確地、分層地利用標籤中的語義資訊來學習標籤感知的神經句子表示。層次模型不僅有助於捕捉句子層級的話語標籤與摘要中的單字之間的細粒度交互，還有助於捕捉標籤序列中可能存在的潛在依賴關係。因此，LIHT 產生標籤感知的上下文句子表示，然後用條件隨機場進行標記。我們在三個公開可用的資料集（即 PUBMED-RCT、NICTA-PIBOSO 和 CSAbstract）上評估 LIHT。在所有三種情況下，相對於各自最先進的方法，F1 分數的增量增益約為 1\%。儘管收益不大，但 LIHT 為這項任務建立了新的效能基準，並且是一項獨立感興趣的新技術。我們還進行了消融研究，以確定 LIHT 每個組件在觀察到的表現中的貢獻，並進行了一個案例研究，以可視化我們模型的不同組件的作用。},
  file = {C:\Users\BlackCat\Zotero\storage\C4U9FF7X\exsy.html}
}

@online{yeCognitiveMirageReview2023,
  title = {Cognitive {{Mirage}}: {{A Review}} of {{Hallucinations}} in {{Large Language Models}}},
  shorttitle = {Cognitive {{Mirage}}},
  author = {Ye, Hongbin and Liu, Tong and Zhang, Aijia and Hua, Wei and Jia, Weiqiang},
  date = {2023-09-13},
  eprint = {2309.06794},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.06794},
  url = {http://arxiv.org/abs/2309.06794},
  urldate = {2024-05-02},
  abstract = {As large language models continue to develop in the field of AI, text generation systems are susceptible to a worrisome phenomenon known as hallucination. In this study, we summarize recent compelling insights into hallucinations in LLMs. We present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. Based on this, future research directions are proposed. Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future. As hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,LLM,Review,幻覺},
  note = {Comment: work in progress; 21 pages},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\SRREQ28F\\Ye et al. - 2023 - Cognitive Mirage A Review of Hallucinations in La.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\7PXEJVZL\\2309.html}
}

@article{YeFengDianZiBingLiZhongMingMingShiTiDeZhiNengShiBie2011,
  title = {电子病历中命名实体的智能识别},
  author = {{叶枫} and {陈莺莺} and {周根贵} and {李昊旻} and {李莹}},
  date = {2011},
  journaltitle = {中国生物医学工程学报},
  shortjournal = {CHINESE JOURNAL OF BIOMEDICAL ENGINEERING},
  volume = {30},
  number = {2},
  pages = {256--262},
  doi = {10.3969/j.issn.0258-8021.2011.02.014},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhN6Z3N3eXhnY3hiMjAxMTAyMDE0Ggg4djlvYmx6Mg%3D%3D},
  urldate = {2022-08-14},
  abstract = {电子病历中命名实体的识别对于构建和挖掘大型临床数据库以服务于临床决策具有重要意义,而我国目前对此的研究相对较少.在比较现有的实体识别方法和模型后,采用条件随机场模型(CRF)机器学习的方法,对疾病、临床症状、手术操作3类中文病历中常见的命名实体进行智能识别.首先,通过分析电子病历的数据特征,选择以语言符号、词性、构词特征、词边界、上下文为特征集.然后,基于随机抽取的来自临床医院多个科室的电子病历数据,构建小规模语料库并进行标注.最后,利用条件随机场算法执行工具CRF++进行3次对照实验.通过逐步分析特征集中的多种特征对CRF自动识别的影响,提出在中文病历环境下CRF特征选择和模板设计的一些基本},
  langid = {zh\_CN},
  keywords = {命名實體,條件隨機場(CRF),機器學習,電子病歷},
  annotation = {浙江工业大学经贸管理学院,杭州,310023浙江大学生物医学工程与仪器科学学院,杭州,310027\\
2011-07-04 （万方平台首次上网日期，不代表论文的发表时间）\\
abstractTranslation:  電子病歷中命名實體的識別對於構建和挖掘大型臨床數據庫以服務於臨床決策具有重要意義，而目前這一研究相對較少。在比較現有的實體識別方法和模型後，採用條件隨機場模型（ CRF）機器學習的方法，對疾病、臨床症狀、手術操作3類中文病歷中常見的命名實體進行智能識別。首先，通過分析電子病歷的數據特徵，以符號、詞性、構詞特徵選擇語言然後，根據隨機抽取的來自臨床醫院科室的多個電子病歷數據，構建小規模語料庫並進行標註。最後，利用實驗條件隨機場算法執行工具CRF++進行3次對照。通過逐步分析特徵集中的多種特徵對CRF自動識別的影響，提出在中文病歷環境下CRF特徵選擇和模板設計的一些基本內容\\
titleTranslation: 電子病歷中命名實體的智能識別},
  file = {C:\Users\BlackCat\Zotero\storage\4GXGDWWQ\叶 等。 - 2011 - 电子病历中命名实体的智能识别.pdf}
}

@article{yi-huichenEfficientSPARQLQueries2022,
  title = {Efficient {{SPARQL Queries Generator}} for {{Question Answering Systems}}},
  author = {{Yi-Hui Chen} and {Eric Jui-Lin Lu} and {Ying-Yen Lin}},
  date = {2022},
  journaltitle = {IEEE Access},
  volume = {10},
  pages = {99850--99860},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3206794},
  url = {https://ieeexplore.ieee.org/document/9893129},
  urldate = {2023-11-23},
  abstract = {Much like traditional database querying, the question answering process in a Question Answering (QA) system involves converting a user’s question input into query grammar, querying the knowledge base through the query grammar, and finally returning the query result (i.e., the answer) to the user. The accuracy of query grammar generation is therefore important in determining whether a Question Answering system can produce a correct answer. Generally speaking, incorrect query grammar will never find the right answer. SPARQL is the most frequently used query language in question answering systems. In the past, SPARQL was generated based on graph structures, such as dependency trees, syntax trees and so on. However, the query cost of generating SPARQL is high, which creates long processing times to answer questions. To reduce the query cost, this work proposes a low-cost SPARQL generator named Light-QAWizard, which integrates multi-label classification into a recurrent neural network (RNN), builds a template classifier, and generates corresponding query grammars based on the results of template classifier. Light-QAWizard reduces query frequency to DBpedia by aggregating multiple outputs into a single output using multi-label classification. In the experimental results, Light-QAWizard’s performance on Precision, Recall and F-measure metrics were evaluated on the QALD-7, QALD8 and QALD-9 datasets. Not only did Light-QAWizard outperform all other models, but it also had a lower query cost that was nearly half that of QAWizard.},
  eventtitle = {{{IEEE Access}}},
  langid = {english},
  keywords = {SPARQL,問答系統,問題分析,已整理,機器學習},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 用於問答系統的高效 SPARQL 查詢產生器},
  note = {開發了一個新的模型來使用很低的成本做到自然問題轉SPARQL，但是該研究用於英文問答，不知道有沒有做成中文的?
\par
關於SPARQL轉換問題方面描述的相當清楚，可能會對撰寫論文有幫助。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\KIK5SPJF\\Chen et al. - 2022 - Efficient SPARQL Queries Generator for Question An.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\EWNRGJRU\\9893129.html}
}

@book{yi-peichenLEDDatasetLife2023,
  title = {{{LED}}: {{A Dataset}} for {{Life Event Extraction}} from {{Dialogs}}},
  shorttitle = {{{LED}}},
  author = {{Yi-Pei Chen} and {An-Zi Yen} and {Hen-Hsen Huang} and {Hideki Nakayama} and {Hsin-Hsi Chen}},
  date = {2023-04-17},
  abstract = {Lifelogging has gained more attention due to its wide applications, such as personalized recommendations or memory assistance. The issues of collecting and extracting personal life events have emerged. People often share their life experiences with others through conversations. However, extracting life events from conversations is rarely explored. In this paper, we present Life Event Dialog, a dataset containing fine-grained life event annotations on conversational data. In addition, we initiate a novel conversational life event extraction task and differentiate the task from the public event extraction or the life event extraction from other sources like microblogs. We explore three information extraction (IE) frameworks to address the conversational life event extraction task: OpenIE, relation extraction, and event extraction. A comprehensive empirical analysis of the three baselines is established. The results suggest that the current event extraction model still struggles with extracting life events from human daily conversations. Our proposed life event dialog dataset and in-depth analysis of IE frameworks will facilitate future research on life event extraction from conversations.},
  langid = {english},
  keywords = {/unread,已整理,資料集},
  annotation = {titleTranslation: LED：從對話中提取生活事件的資料集\\
abstractTranslation:  生活記錄因其廣泛的應用而受到越來越多的關注，例如個人化推薦或記憶輔助。收集和提取個人生活事件的問題已經出現。人們經常透過對話與他\hspace{0pt}\hspace{0pt}人分享他們的生活經驗。然而，很少有人探索從對話中提取生活事件。在本文中，我們提出了生活事件對話，這是一個包含對話資料的細粒度生活事件註釋的資料集。此外，我們發起了一種新穎的會話生活事件提取任務，並將該任務與公共事件提取或來自微博等其他來源的生活事件提取區分開來。我們探索了三種資訊提取 (IE) 框架來解決會話生活事件提取任務：OpenIE、關係提取和事件提取。對三個基線進行了全面的實證分析。結果表明，目前的事件提取模型仍然難以從人類日常對話中提取生活事件。我們提出的生活事件對話資料集和對 IE 框架的深入分析將有助於未來從對話中提取生活事件的研究。}
}

@article{yifengKnowledgeDiscoveryTraditional2006,
  title = {Knowledge Discovery in Traditional {{Chinese}} Medicine: {{State}} of the Art and Perspectives},
  shorttitle = {Knowledge Discovery in Traditional {{Chinese}} Medicine},
  author = {{Yi Feng} and {Zhaohui Wu} and {Xuezhong Zhou} and {Zhongmei Zhou} and {Weiyu Fan}},
  date = {2006-11},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artificial Intelligence in Medicine},
  volume = {38},
  number = {3},
  pages = {219--236},
  issn = {09333657},
  doi = {10.1016/j.artmed.2006.07.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365706001047},
  urldate = {2022-09-19},
  langid = {english},
  annotation = {153 citations (Crossref) [2024-03-26]\\
182 citations (Semantic Scholar/DOI) [2022-10-26]\\
titleTranslation: 中醫知識發現：最新技術和前景},
  file = {C:\Users\BlackCat\Zotero\storage\VUH3XR9H\Feng 等。 - 2006 - Knowledge discovery in traditional Chinese medicin.pdf}
}

@inproceedings{yihongxieTCMQuestionAnswer2020,
  title = {A {{TCM Question}} and {{Answer System Based}} on {{Medical Records Knowledge Graph}}},
  booktitle = {2020 {{International Conference}} on {{Computing}} and {{Data Science}} ({{CDS}})},
  author = {{Yihong Xie}},
  date = {2020-08},
  pages = {373--376},
  doi = {10.1109/CDS49703.2020.00078},
  url = {https://ieeexplore.ieee.org/document/9276000},
  urldate = {2023-09-25},
  abstract = {In recent years, the combination of knowledge map and question and answer system are more and more applied in professional fields. In this paper, we firstly constructed a knowledge graph of traditional Chinese medical records which contains four entities: herbs, prescriptions, diseases, symptoms and their relationships; Secondly, we use Node2vec algorithm to represent the graph; Finally, we match questions and answers based on KNN. Experiment result shows that with the output of five prescriptions, our system's accuracy reaches 93.4\%, which achieves good results.},
  eventtitle = {2020 {{International Conference}} on {{Computing}} and {{Data Science}} ({{CDS}})},
  langid = {english},
  keywords = {中醫,問答系統,病歷分析,知識圖譜,重要},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於病案知識圖譜的中醫問答系統\\
abstractTranslation:  近年來，知識圖譜與問答系統的結合越來越多地應用於專業領域。本文首先建構了一個中醫病案知識圖譜，包含四個實體：藥材、方劑、疾病、症狀及其關係；其次，我們使用Node2vec演算法來表示圖；最後，我們基於KNN來匹配問題和答案。實驗結果表明，在輸出5個處方時，我們的系統準確率達到93.4\%，取得了良好的效果。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\6AY65DEH\\Xie - 2020 - A TCM Question and Answer System Based on Medical .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\WMKJB3ZG\\9276000.html}
}

@article{yinghuiwangNovelChineseTraditional2020,
  title = {A {{Novel Chinese Traditional Medicine Prescription Recommendation System}} Based on {{Knowledge Graph}}},
  author = {{Yinghui Wang}},
  date = {2020-03-01},
  journaltitle = {Journal of Physics: Conference Series},
  shortjournal = {J. Phys.: Conf. Ser.},
  volume = {1487},
  number = {1},
  pages = {012019},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/1487/1/012019},
  url = {https://iopscience.iop.org/article/10.1088/1742-6596/1487/1/012019},
  urldate = {2022-09-19},
  abstract = {As the traditional Chinese medical approach, Chinese medicine plays an extremely important role in the field of medical treatment. With the development of computers, people tend to acquire medical knowledge from the Internet in daily life. Because of the complexity of online Chinese medicine knowledge, nowadays there is no good way to organize the existing knowledge to provide convenience for doctors and patients. This paper introduces the study on the recommendation system based on the Knowledge Graph (KG). Firstly, it conducts extraction of entities such as Traditional Chinese Medicine (TCM) diseases, prescription, Chinese herbal medicine, symptoms, etc. Secondly, it transforms KG into vector space using Node2vec. At last, based on the similarities between vectors the provided system recommends prescription by adopting the diagnostic process of Traditional Chinese Medicine. The result shows that the Hit Ratio (HR) of the recommend system is as high as 80\%.},
  langid = {english},
  annotation = {8 citations (Crossref) [2024-03-26]\\
5 citations (Semantic Scholar/DOI) [2022-10-26]\\
titleTranslation: 一種基於知識圖譜的新型中藥處方推薦系統\\
abstractTranslation:  中醫藥作為中醫的傳統醫學手段，在醫療領域發揮著極其重要的作用。隨著計算機的發展，人們在日常生活中傾向於通過互聯網獲取醫學知識。由於在線中醫知識的複雜性，目前還沒有很好的方法來整理現有的知識來為醫生和患者提供方便。本文介紹了基於知識圖譜（KG）的推薦系統的研究。首先，對中醫疾病、方劑、中草藥、症狀等實體進行提取。其次，利用Node2vec將KG轉換到向量空間。最後，基於向量之間的相似性，所提供的系統採用中醫診斷過程來推薦處方。結果表明，推薦系統的命中率(HR)高達80\%。},
  note = {配藥系統，準確度HR},
  file = {C:\Users\BlackCat\Zotero\storage\BGC9TY8F\Wang - 2020 - A Novel Chinese Traditional Medicine Prescription .pdf}
}

@inproceedings{yingjiangOntoLTCnChineseText2008,
  title = {{{OntoLTCn}}: {{A Chinese Text Oriented Semi-auto Ontology Knowledge Discovery Tool}}},
  shorttitle = {{{OntoLTCn}}},
  booktitle = {2008 {{International Conference}} on {{Computer Science}} and {{Software Engineering}}},
  author = {{Ying Jiang} and {Hui Dong} and {Haoyi Xiong}},
  date = {2008-02},
  volume = {1},
  pages = {662--665},
  doi = {10.1109/csse.2008.788},
  abstract = {Ontology (RDF/OWL) plays a foundational role of semantic Web for knowledge representation. But nowadays there are few Chinese ontology bases available, which hinders the research and development of Chinese Semantic Web applications. This paper introduces an ontology knowledge discovery tool, named OntoLTCn, which supports semi-auto domain ontology acquisition from Chinese corpus. In brief, OntoLTCn is a Protege plug-in based on OntoLT platform, which integrates Chinese NLP and XML pattern mapping technologies for knowledge discovery. A case study in Chinese e-government domain is discussed as well, which shows the usability of OntoLTCn for ontology construction from digital archives.},
  eventtitle = {2008 {{International Conference}} on {{Computer Science}} and {{Software Engineering}}},
  langid = {english},
  keywords = {已發表,待讀,知識挖掘,知識本體},
  annotation = {0 citations (Crossref) [2024-03-26]\\
0 citations (Semantic Scholar/DOI) [2023-07-23]\\
titleTranslation: OntoLTCn：面向中文文本的半自動本體知識發現工具\\
abstractTranslation:  本體（RDF/OWL）在語義網知識表示方面發揮著基礎作用。但目前可用的中文本體庫很少，這阻礙了中文語義網應用的研究和開發。本文介紹了一種本體知識發現工具OntoLTCn，它支持從中文語料庫中半自動獲取領域本體。簡而言之，OntoLTCn是基於OntoLT平台的Protege插件，集成了中文NLP和XML模式映射技術用於知識發現。還討論了中國電子政務領域的案例研究，展示了 OntoLTCn 在數字檔案本體構建中的可用性。},
  note = {介绍了一個稱為OntoLTCn的本體知識發現工具，可以從中文語料庫中進行半自動領域本體獲取。OntoLTCn是基于OntoLT平台的Protégé插件，它集成了中文自然語言處理和XML模式匹配技術用於知識發現。通過在中國電子政務領域進行的案例研究，展示了OntoLTCn在從數位檔案中建構本體方面的可用性。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\AUKK25CS\\Jiang 等。 - 2008 - OntoLTCn A Chinese Text Oriented Semi-auto Ontolo.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\IJ8HPWHG\\4721836.html}
}

@online{YiWenTiJieFa,
  title = {以「問題–解法」為基礎的軟體知識分類法\_\_臺灣博碩士論文知識加值系統},
  url = {https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/login?o=dnclcdr&s=id=%22092NCTU5392004%22.&searchmode=basic},
  urldate = {2023-10-05},
  file = {C:\Users\BlackCat\Zotero\storage\66YAJ8SU\login.html}
}

@article{yixiaomaIncorporatingStructuralInformation2023,
  title = {Incorporating {{Structural Information}} into {{Legal Case Retrieval}}},
  author = {{Yixiao Ma} and {Yueyue Wu} and {Qingyao Ai} and {Yiqun Liu} and {Yunqiu Shao} and {Min Zhang} and {Shaoping Ma}},
  year = {7 月 19, 2023},
  journaltitle = {ACM Transactions on Information Systems},
  shortjournal = {ACM Trans. Inf. Syst.},
  issn = {1046-8188},
  doi = {10.1145/3609796},
  url = {https://dl.acm.org/doi/10.1145/3609796},
  urldate = {2023-09-09},
  abstract = {Legal case retrieval has received increasing attention in recent years. However, compared to ad-hoc retrieval tasks, legal case retrieval has its unique challenges. First, case documents are rather lengthy and contain complex legal structures. Therefore, it is difficult for most existing dense retrieval models to encode an entire document and capture its inherent complex structure information. Most existing methods simply truncate part of the document content to meet the input length limit of PLMs, which will lead to information loss. Additionally, the definition of relevance in the legal domain differs from that in the general domain. Previous semantic-based or lexical-based methods fail to provide a comprehensive understanding of the relevance of legal cases. In this paper, we propose a Structured Legal case Retrieval (SLR) framework, which incorporates internal and external structural information to address the above two challenges. Specifically, to avoid the truncation of long legal documents, the internal structural information, which is the organization pattern of legal documents, can be utilized to split a case document into segments. By dividing the document-level semantic matching task into segment-level subtasks, SLR can separately process segments using different methods based on the characteristic of each segment. In this way, the key elements of a case document can be highlighted without losing other content information. Secondly, towards a better understanding of relevance in the legal domain, we investigate the connections between criminal charges appearing in large-scale case corpus to generate a charge-wise relation graph. Then, the similarity between criminal charges can be pre-computed as the external structural information to enhance the recognition of relevant cases. Finally, a learning-to-rank algorithm integrates the features collected from internal and external structures to output the final retrieval results. Experimental results on public legal case retrieval benchmarks demonstrate the superior effectiveness of SLR over existing state-of-the-art baselines, including traditional bag-of-words and neural-based methods. Furthermore, we conduct a case study to visualize how the proposed model focuses on key elements and improves retrieval performance.},
  langid = {english},
  keywords = {legal case retrieval,relevance,structural information,已整理,摘要,文字處理,預訓練},
  annotation = {0 citations (Crossref) [2024-03-26]\\
Just Accepted\\
titleTranslation: 將結構信息納入法律案例檢索\\
abstractTranslation:  近年來，法律案件檢索越來越受到人們的關注。然而，與臨時檢索任務相比，法律案件檢索有其獨特的挑戰。首先，案件文件較長，法律結構複雜。因此，大多數現有的密集檢索模型很難對整個文檔進行編碼並捕獲其固有的複雜結構信息。現有的大多數方法只是簡單地截斷部分文檔內容以滿足PLM的輸入長度限制，這將導致信息丟失。此外，法律領域中相關性的定義與一般領域中的相關性定義不同。以前基於語義或基於詞彙的方法無法提供對法律案件相關性的全面理解。在本文中，我們提出了一種結構化法律案例檢索（SLR）框架，該框架結合了內部和外部結構信息來解決上述兩個挑戰。具體來說，為了避免長法律文件被截斷，可以利用內部結構信息，即法律文件的組織模式，將案件文件分割成多個片段。通過將文檔級語義匹配任務劃分為片段級子任務，SLR可以根據每個片段的特徵，採用不同的方法分別處理片段。這樣既可以突出案例文檔的關鍵要素，又不會丟失其他內容信息。其次，為了更好地理解法律領域的相關性，我們調查了大規模案件語料庫中出現的刑事指控之間的聯繫，以生成指控關係圖。然後，可以預先計算刑事指控之間的相似度作為外部結構信息，以增強對相關案件的識別。最後，學習排序算法整合從內部和外部結構收集的特徵，輸出最終的檢索結果。公共法律案例檢索基準的實驗結果表明，SLR 比現有最先進的基準（包括傳統的詞袋和基於神經的方法）具有卓越的有效性。此外，我們還進行了一個案例研究，以可視化所提出的模型如何關注關鍵元素並提高檢索性能。},
  note = {以長文件內的結構做分段
\par
以外部文件的結構學習找關鍵字
\par
結合兩者，在預訓練模型上微調出長法律文本摘要。
\par
超出研究範圍?},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\GQHGW9FZ\\Ma 等。 - 2023 - Incorporating Structural Information into Legal Ca.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\KFXRF8V7\\Incorporating Structural Information into Legal Case Retrieval.pdf}
}

@inproceedings{yizhouzhangChineseMedicalConcept2018,
  title = {Chinese {{Medical Concept Normalization}} by {{Using Text}} and {{Comorbidity Network Embedding}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {{Yizhou Zhang} and {Xiaojun Ma} and {Guojie Song}},
  date = {2018-01},
  pages = {777--786},
  issn = {2374-8486},
  doi = {10.1109/icdm.2018.00093},
  abstract = {Chinese medical concept normalization, which maps non-standard medical concepts to standard expressions, is a NLP task with wide-ranging applications in medical big data research and clinical statistic. Many previous works apply supervised methods which require a lot of annotated data. However, they can not address the challenge brought by the high cost of medical data annotation, which requires sufficient professional knowledge and experience. Meanwhile, existing unsupervised methods perform poorly facing the various non-standard expression from different data sources. In this paper, we propose DUNE, Disease Unsupervised Normalization by Embedding, an unsupervised Chinese medical concept normalization framework by applying denoising auto-encoder (DAE) and network embedding. We formulate this task as finding mention-entity pairs with great text and comorbidity similarity. To handle the noise in text, we design a multi-view attention based denoising auto-encoder (MADAE) to capture text information from multiple views, reduce the influence of noise, and transform text to denoised vectors. To introduce comorbidity information, we construct a comorbidity network with both standard and non-standard disease names as nodes from medical records. Because of the diversity of nonstandard expressions, one disease perhaps corresponds to several different nodes, which causes noise in comorbidity network. To handle such network structure noise, we propose a denoising network embedding framework, which reduce the structure noise with the help of text information, to embed the nodes to vectors for comorbidity similarity measurement. Convincing experiment results show that our method performs better than existing unsupervised baselines and approaches the performance of classical supervised machine learning model on this task.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  langid = {english},
  keywords = {Diseases,medical concept normalization comorbidity network embedding denoising auto-encoder,Medical diagnostic imaging,Noise reduction,Standards,Task analysis},
  annotation = {6 citations (Crossref) [2024-03-26]\\
6 citations (Semantic Scholar/DOI) [2023-08-11]\\
titleTranslation: 使用文本和合併症網絡嵌入進行中醫概念規範化\\
abstractTranslation:  中醫概念規範化是將非標準醫學概念映射為標準表達的自然語言處理任務，在醫學大數據研究和臨床統計中有著廣泛的應用。以前的許多工作都應用了需要大量註釋數據的監督方法。然而，它們無法解決醫療數據標註成本高昂帶來的挑戰，這需要足夠的專業知識和經驗。同時，現有的無監督方法在面對來自不同數據源的各種非標準表達時表現不佳。在本文中，我們提出了 DUNE（Disease Unsupervised Normalization by Embedding），這是一種應用去噪自動編碼器（DAE）和網絡嵌入的無監督中醫概念標準化框架。我們將這個任務表述為尋找具有良好文本和共病相似性的提及實體對。為了處理文本中的噪聲，我們設計了一種基於多視圖注意的去噪自動編碼器（MADAE）來從多個視圖捕獲文本信息，減少噪聲的影響，並將文本轉換為去噪向量。為了引入合併症信息，我們構建了一個合併症網絡，其中標準和非標準疾病名稱作為醫療記錄中的節點。由於非標準表達的多樣性，一種疾病可能對應多個不同的節點，這會導致共病網絡中出現噪聲。為了處理這種網絡結構噪聲，我們提出了一種去噪網絡嵌入框架，它藉助文本信息來減少結構噪聲，將節點嵌入到向量中以進行共病相似性測量。令人信服的實驗結果表明，我們的方法比現有的無監督基線表現更好，並且接近經典監督機器學習模型在此任務上的性能。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\662NTKUS\\Zhang 等。 - 2018 - Chinese Medical Concept Normalization by Using Tex.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\RNMR9DBB\\8594902.html}
}

@inproceedings{yonghaoliuTimeAwareMultiwayAdaptive2023,
  title = {Time-{{Aware Multiway Adaptive Fusion Network}} for {{Temporal Knowledge Graph Question Answering}}},
  booktitle = {{{ICASSP}} 2023 - 2023 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {{Yonghao Liu} and {Di Liang} and {Fang Fang} and {Sirui Wang} and {Wei Wu} and {Rui Jiang}},
  date = {2023-06},
  pages = {1--5},
  issn = {2379-190X},
  doi = {10.1109/ICASSP49357.2023.10095395},
  url = {https://ieeexplore.ieee.org/document/10095395},
  urldate = {2023-11-23},
  abstract = {Knowledge graphs (KGs) have received increasing attention due to its wide applications on natural language processing. However, its use case on temporal question answering (QA) has not been well-explored. Most of existing methods are developed based on pre-trained language models, which might not be capable to learn temporal-specific presentations of entities in terms of temporal KGQA task. To alleviate this problem, we propose a novel Time-aware Multiway Adaptive (TMA) fusion network. Inspired by the step-by-step reasoning behavior of humans. For each given question, TMA first extracts the relevant concepts from the KG, and then feeds them into a multiway adaptive module to produce a temporal-specific representation of the question. This representation can be incorporated with the pre-trained KG embedding to generate the final prediction. Empirical results verify that the proposed model achieves better performance than the state-of-the-art models in the benchmark dataset. Notably, the Hits@1 and Hits@10 results of TMA on the CronQuestions dataset’s complex questions are absolutely improved by 24\% and 10\% compared to the best-performing baseline. Furthermore, we also show that TMA employing an adaptive fusion mechanism can provide interpretability by analyzing the proportion of information in question representations.},
  eventtitle = {{{ICASSP}} 2023 - 2023 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  langid = {english},
  keywords = {問答系統,回收,已整理,機器學習,知識圖譜},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 用於時態知識圖問答的時間感知多路自適應融合網絡},
  note = {注重在如何解決問答中的時間相關問題，例如before、after等。
\par
和研究的影響較小，暫時放入回收桶。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\BB2N99LR\\Liu et al. - 2023 - Time-Aware Multiway Adaptive Fusion Network for Te.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\TXZPDL68\\10095395.html}
}

@thesis{YouKunMingZhongYiBingLiXiTong2019,
  title = {中醫病歷系統},
  author = {{尤崑名}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2019},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/du6968},
  abstract = {目前中醫診斷治療系統僅提供文字記錄功能。這種簡易的病歷記錄，缺乏中醫症狀、證候、療效、及方劑等的標準化及量化，這些缺陷非常不利於臨床即時的電腦輔助病勢分析及視覺化呈現，也非常不利於後續透過資料探勘及機器學習的診斷治療研究。本篇論文根據本研究團隊所開發的中醫診斷治療系統，研製一個中醫病歷系統，提供標準化的中醫症狀、證候、療效、及方劑等的病歷記錄。本篇論文也根據病人連續看診的病歷記錄，實作量化的證候趨勢分析功能，輔助中醫師臨床即時病勢監控及方劑療效評估。},
  pagetotal = {39},
  keywords = {實驗室}
}

@thesis{YouShuQingZhongYiXuZhengBianZhengXiTong2014,
  title = {中醫虛證辨證系統},
  author = {{游淑晴}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2014},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/y7dg6r},
  abstract = {《中醫證候學》是蒐集及整理古今中醫臨床診斷文獻的巨著。它先依病因為綱將疾病分為15個證門，又依病因的不同性質將15個證門再細分為45個證類。接著，它依病位為目將45個證類再分為281證型。最後，它再依病機為科將281個證型分為2344個證候。《中醫證候學》的辨證體系非常龐雜，一般醫生難以掌握，我們希望運用計算機龐大的記憶能力及迅速的分析能力，研製一個中醫自動辨證系統，提供研究中醫辨證體系的平台。 本論文根據《中醫證候學》虛證門的症狀資訊研製一個中醫自動辨證系統，分成臟腑辨證、證門辨證、證類辨證、證型辨證及證候辨證五個部分，逐步判斷疾病的病因、病位及病機。本論文以兩種不同的方式進行辨證：平權辨證及加權辨證。平權辨證將每個症狀之權重視為相同，亦即假設證候中每個症狀之發生機率相同。實驗顯示平權辨證在許多情況的辨識率偏低。加權辨證依症狀之發生機率給予不同的權重。實驗顯示加權辨證在大多數的情況都可以提高辨識率，改善平權辨證在許多情況辨識率偏低的問題。},
  langid = {chinese},
  pagetotal = {208},
  keywords = {中醫,實驗室,辨證},
  annotation = {abstractTranslation:  《中醫證候學》是蒐集及整理古中醫臨床診斷文獻的巨著。它先依病因為綱將疾病分為15個證門，又依今的不同性質將15個證門再剖析為45個證繼續，它依病位為目將45個證類再分為281個證型。最後，它再依病機為科將281個證型分為2344個候。《中醫證候學》的辨證體系非常龐雜，一般醫生難以掌握，我們希望利用計算機龐大的記憶能力及迅速的分析能力，標註一個中醫自動辨證體系，提供研究辨證體系的平台。 本論文根據《中醫證候學》虛證門的症狀資訊一個中醫自動辨證系統，分為內部裝修辨證、證門辨證、證類辨證、證型辨證及證候辨證五個部分，逐步判斷疾病的新生、病位及病機。多種不同的方式進行鑑別證：平權鑑別證及加權鑑別證數量。平權鑑別證將每個症狀之權重視為相同，亦即假設證候中每個症狀之發生機率相同。實驗顯示平權鑑別證在情況的清除率偏實驗顯示加權簽證在大多數情況下都可以提高抽籤率，改善了平權簽證在許多情況下抽籤率偏低的問題。\\
titleTranslation: 中醫虛證辨證系統},
  file = {C:\Users\BlackCat\Zotero\storage\5P3UNT8B\游淑晴 - 2014 - 中醫虛證辨證系統.pdf}
}

@article{YuanXueLiDianZiBingLiDeXianZhuangYuNanDianFenXi2010,
  title = {电子病历的现状与难点分析},
  author = {{袁雪莉}},
  date = {2010},
  journaltitle = {计算机与现代化},
  shortjournal = {COMPUTER AND MODERNIZATION},
  number = {10},
  pages = {198--200,204},
  doi = {10.3969/j.issn.1006-2475.2010.10.055},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhBqc2p5eGRoMjAxMDEwMDU1GggxOWtleHI1dQ%3D%3D},
  urldate = {2022-08-14},
  abstract = {探讨电子病历的发展现状及难点问题.查阅相关文献,比较分析电子病历的国内外现状,总结其实施过程中的难点问题.电子病历的法律、技术、管理方面的难点问题有待进一步解决.电子病历的实施是一个复杂的系统工程,需要各个方面的努力.},
  langid = {zh\_CN},
  keywords = {電子病歷},
  annotation = {重庆医科大学附属第一医院网络信息中心,重庆,400016\\
2010-11-10 （万方平台首次上网日期，不代表论文的发表时间）\\
abstractTranslation:  探討電子病歷的發展現狀及難點問題。查閱相關文獻，比較分析電子病歷的現狀，總結其實施過程中的難點問題。電子病歷的法律、技術、管理方面的難點問題有待進一步解決。實施是一個複雜的系統工程，需要各個方面的努力。\\
titleTranslation: 電子病歷的現狀與難點分析},
  file = {C:\Users\BlackCat\Zotero\storage\8PIDKQ5S\袁 - 2010 - 电子病历的现状与难点分析.pdf}
}

@inproceedings{yubomaMMEKGMultimodalEvent2022,
  title = {{{MMEKG}}: {{Multi-modal Event Knowledge Graph}} towards {{Universal Representation}} across {{Modalities}}},
  shorttitle = {{{MMEKG}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {{Yubo Ma} and {Zehao Wang} and {Mukai Li} and {Yixin Cao} and {Meiqi Chen} and {Xinze Li} and {Wenqi Sun} and {Kunquan Deng} and {Kun Wang} and {Aixin Sun} and {Jing Shao}},
  date = {2022-05},
  pages = {231--239},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-demo.23},
  url = {https://aclanthology.org/2022.acl-demo.23},
  urldate = {2023-09-19},
  abstract = {Events are fundamental building blocks of real-world happenings. In this paper, we present a large-scale, multi-modal event knowledge graph named MMEKG. MMEKG unifies different modalities of knowledge via events, which complement and disambiguate each other.Specifically, MMEKG incorporates (i) over 990 thousand concept events with 644 relation types to cover most types of happenings, and (ii) over 863 million instance events connected through 934 million relations, which provide rich contextual information in texts and/or images. To collect billion-scale instance events and relations among them, we additionally develop an efficient yet effective pipeline for textual/visual knowledge extraction system. We also develop an induction strategy to create million-scale concept events and a schema organizing all events and relations in MMEKG. To this end, we also provide a pipeline enabling our system to seamlessly parse texts/images to event graphs and to retrieve multi-modal knowledge at both concept- and instance-levels.},
  langid = {english},
  keywords = {已整理,待讀,知識圖譜,重要},
  annotation = {3 citations (Crossref) [2024-03-26]\\
titleTranslation: MMEKG：跨模態通用表示的多模態事件知識圖\\
abstractTranslation:  事件是現實世界中發生的事件的基本組成部分。在本文中，我們提出了一個名為MMEKG的大規模、多模態事件知識圖。 MMEKG 透過事件統一不同的知識模態，相互補充和消除歧義。具體來說，MMEKG 包含(i) 超過99 萬個概念事件和644 種關係類型，涵蓋大多數類型的事件，以及(ii) 超過8.63億個實例事件透過9.34 億個關係，提供豐富的文字和/或圖像上下文資訊。為了收集數十億規模的實例事件及其之間的關係，我們還開發了一種高效且有效的文字/視覺知識提取系統管道。我們也制定了一個歸納策略來建立百萬級概念事件，以及一個組織 MMEKG 中所有事件和關係的模式。為此，我們還提供了一個管道，使我們的系統能夠將文字/圖像無縫解析為事件圖，並在概念和實例層級檢索多模態知識。},
  file = {C:\Users\BlackCat\Zotero\storage\NIDCSYS8\Ma 等。 - 2022 - MMEKG Multi-modal Event Knowledge Graph towards U.pdf}
}

@online{yuChainofNoteEnhancingRobustness2023,
  title = {Chain-of-{{Note}}: {{Enhancing Robustness}} in {{Retrieval-Augmented Language Models}}},
  shorttitle = {Chain-of-{{Note}}},
  author = {Yu, Wenhao and Zhang, Hongming and Pan, Xiaoman and Ma, Kaixin and Wang, Hongwei and Yu, Dong},
  date = {2023-11-15},
  eprint = {2311.09210},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.09210},
  url = {http://arxiv.org/abs/2311.09210},
  urldate = {2024-04-10},
  abstract = {Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with "unknown" when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,LLM,RAG,RALM,問答系統,已整理,微讀},
  annotation = {abstractTranslation:  檢索增強語言模型（RALM）代表了大型語言模型能力的重大進步，特別是在透過利用外部知識來源來減少事實幻覺方面。然而，檢索到的信息的可靠性並不總是得到保證。檢索不相關的資料可能會導致錯誤的回應，並可能導致模型忽略其固有知識，即使它擁有足夠的資訊來解決查詢。此外，標準 RALM 經常難以評估它們是否擁有足夠的知識（內在的和檢索到的）以提供準確的答案。在缺乏知識的情況下，當答案無法獲得時，這些系統理想地應該回應「未知」。為了應對這些挑戰，我們引入了 Chain-of-Noting (CoN)，這是一種新穎的方法，旨在提高 RALM 在面對嘈雜、不相關的文件和處理未知場景時的穩健性。 CoN 的核心思想是為檢索到的文件生成順序閱讀筆記，從而能夠全面評估它們與給定問題的相關性，並整合這些資訊以製定最終答案。我們使用 ChatGPT 為 CoN 建立訓練數據，然後在 LLaMa-2 7B 模型上進行訓練。我們在四個開放域 QA 基準測試中進行的實驗表明，配備 CoN 的 RALM 的性能顯著優於標準 RALM。值得注意的是，在完全吵雜的檢索文件的情況下，CoN 的 EM 分數平均提高了 +7.9，而對於超出預訓練知識範圍的即時問題，CoN 的拒絕率平均提高了 +10.5。\\
titleTranslation: Chain-of-Note：增強檢索增強語言模型的穩健性},
  note = {通過先讓語言模型進行資料的整理後再回答問題來增加RALM的能力
\par
Comment: Preprint},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\C83G7T8L\\Yu 等。 - 2023 - Chain-of-Note Enhancing Robustness in Retrieval-A.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\SIY7RCBG\\2311.html}
}

@article{yuchenSubgraphGuidedKnowledgeGraph2023,
  title = {Toward {{Subgraph-Guided Knowledge Graph Question Generation With Graph Neural Networks}}},
  author = {{Yu Chen} and {Lingfei Wu} and {Mohammed J. Zaki}},
  date = {2023},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--12},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2023.3264519},
  url = {https://ieeexplore.ieee.org/document/10107656},
  urldate = {2023-11-23},
  abstract = {Knowledge graph (KG) question generation (QG) aims to generate natural language questions from KGs and target answers. Previous works mostly focus on a simple setting that is to generate questions from a single KG triple. In this work, we focus on a more realistic setting where we aim to generate questions from a KG subgraph and target answers. In addition, most previous works built on either RNN-or Transformer-based models to encode a linearized KG subgraph, which totally discards the explicit structure information of a KG subgraph. To address this issue, we propose to apply a bidirectional Graph2Seq model to encode the KG subgraph. Furthermore, we enhance our RNN decoder with a node-level copying mechanism to allow direct copying of node attributes from the KG subgraph to the output question. Both automatic and human evaluation results demonstrate that our model achieves new state-of-the-art scores, outperforming existing methods by a significant margin on two QG benchmarks. Experimental results also show that our QG model can consistently benefit the question-answering (QA) task as a means of data augmentation.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  langid = {english},
  keywords = {RNN,問答系統,回收,已整理,機器學習,知識圖譜},
  annotation = {7 citations (Crossref) [2024-03-26]\\
titleTranslation: 使用圖神經網路生成子圖引導的知識圖問題},
  note = {本研究的貢獻在於使用新的編碼方式來ˊ將KG像量化，和研究較無相關。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\GC25J5BH\\Chen et al. - 2023 - Toward Subgraph-Guided Knowledge Graph Question Ge.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\4VKBHCM5\\10107656.html}
}

@article{yugyungleeSemanticEnrichmentMedical2006,
  title = {Semantic Enrichment for Medical Ontologies},
  author = {{Yugyung Lee} and {James Geller}},
  year = {4 月 1, 2006},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {J. of Biomedical Informatics},
  volume = {39},
  number = {2},
  pages = {209--226},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2005.08.001},
  url = {https://doi.org/10.1016/j.jbi.2005.08.001},
  urldate = {2023-09-27},
  abstract = {The Unified Medical Language System (UMLS) contains two separate but interconnected knowledge structures, the Semantic Network (upper level) and the Metathesaurus (lower level). In this paper, we have attempted to work out better how the use of such a two-level structure in the medical field has led to notable advances in terminologies and ontologies. However, most ontologies and terminologies do not have such a two-level structure. Therefore, we present a method, called semantic enrichment, which generates a two-level ontology from a given one-level terminology and an auxiliary two-level ontology. During semantic enrichment, concepts of the one-level terminology are assigned to semantic types, which are the building blocks of the upper level of the auxiliary two-level ontology. The result of this process is the desired new two-level ontology. We discuss semantic enrichment of two example terminologies and how we approach the implementation of semantic enrichment in the medical domain. This implementation performs a major part of the semantic enrichment process with the medical terminologies, with difficult cases left to a human expert.},
  keywords = {controlled medical vocabularies,ontology,semantic enrichment,semantic web,semantics,terminology,two-level ontology,unified medical language system,未整理},
  annotation = {13 citations (Crossref) [2024-03-26]},
  file = {C:\Users\BlackCat\Zotero\storage\VAD9D4CZ\Lee 與 Geller - 2006 - Semantic enrichment for medical ontologies.pdf}
}

@article{YuJiaQiYiZhongXinDeZhongWenDianZiBingLiWenBenJianSuoMoXing2022,
  title = {一种新的中文电子病历文本检索模型},
  author = {{于家畦} and {康晓东} and {白程程} and {刘汉卿}},
  date = {2022},
  journaltitle = {计算机科学},
  issue = {049-0z1},
  keywords = {BERT,BiLSTM,擴展搜索主題詞,文本檢索,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\B2BG28GQ\于家畦 等。 - 2022 - 一种新的中文电子病历文本检索模型.pdf}
}

@inproceedings{yukigatoRetrievalandClassificationApproachFactchecking2022,
  title = {A {{Retrieval-and-Classification Approach}} for {{Fact-checking Grounded}} by {{Assembly Minutes}}},
  booktitle = {2022 9th {{International Conference}} on {{Advanced Informatics}}: {{Concepts}}, {{Theory}} and {{Applications}} ({{ICAICTA}})},
  author = {{Yuki Gato} and {Tomoyoshi Akiba}},
  date = {2022-09},
  pages = {1--5},
  doi = {10.1109/ICAICTA56449.2022.9932924},
  url = {https://ieeexplore.ieee.org/document/9932924},
  urldate = {2023-11-23},
  abstract = {In this paper, we propose a method of the fact-verification task, which is one of the subtasks evaluated in the NTCIR-16 QA Lab-PoliInfo-3 task. The fact verification subtask aims at determining whether a given claim is actually said in a given assembly minutes, and if so, locating its corresponding sentences in the minutes as its evidence. Our proposed method consists of two steps, passage retrieval and textual entailment. In the passage retrieval step, it retrieves a relevant passage using the claim as a query. In the textual entailment step, it determines whether the claim entails the retrieved passage by employing a classifier. We experimentally compared two types of the passage and two IR metrics for the passage retrieval, and three classifiers for textual entailment. The best performed system was evaluated in the NTCIR-16 and achieved the highest score in the formal evaluation.},
  eventtitle = {2022 9th {{International Conference}} on {{Advanced Informatics}}: {{Concepts}}, {{Theory}} and {{Applications}} ({{ICAICTA}})},
  langid = {english},
  keywords = {事實查核,回收,已整理,機器學習},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 以大會紀要為基礎的事實查核的檢索與分類方法},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\64ZIG9AF\\Gato and Akiba - 2022 - A Retrieval-and-Classification Approach for Fact-c.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\ZMVVNQ6G\\9932924.html}
}

@article{yulinwangArtificialIntelligencedirectedAcupuncture2022,
  title = {Artificial Intelligence-Directed Acupuncture: A Review},
  shorttitle = {Artificial Intelligence-Directed Acupuncture},
  author = {{Yulin Wang} and {Xiuming Shi} and {Thomas Efferth} and {Dong Shang}},
  date = {2022-06-28},
  journaltitle = {Chinese Medicine},
  shortjournal = {Chinese Medicine},
  volume = {17},
  number = {1},
  pages = {80},
  issn = {1749-8546},
  doi = {10.1186/s13020-022-00636-1},
  url = {https://doi.org/10.1186/s13020-022-00636-1},
  urldate = {2023-09-15},
  abstract = {Acupuncture is widely used around the whole world nowadays and exhibits significant efficacy against many chronic diseases, especially in pain-related diseases. With the rapid development of artificial intelligence (AI), its implementation into acupuncture has achieved a series of significant breakthroughs in many areas of acupuncture practice, such as acupoints selection and prescription, acupuncture manipulation identification, acupuncture efficacy prediction, and so on. The paper will discuss the significant theoretical and technical achievements in AI-directed acupuncture. AI-based data mining methods uncovered crucial acupoint combinations for treating various diseases, which provide a scientific basis for acupoints prescription in clinical practice. Furthermore, the rapid development of modern TCM instruments facilitates the integration of modern medical instruments, AI techniques, and acupuncture. This integration significantly improves the quantification, objectification, and standardization of acupuncture as well as the delivery of clinical personalized acupuncture therapy. Machine learning-based clinical efficacy prediction of acupuncture can help doctors screen patients who may benefit from acupuncture treatment. However, the existing challenges require additional work for developing AI-directed acupuncture. Some include a better understanding of ancient Chinese philosophy for AI researchers, TCM acupuncture theory-based explanation of the knowledge discoveries, construction of acupuncture databases, and clinical trials for novel knowledge validation. This review aims to summarize the major contribution of AI techniques to the discovery of novel acupuncture knowledge, the improvement for acupuncture safety and efficacy, the development and inheritance of acupuncture, and the major challenges for the further development of AI-directed acupuncture. The development of acupuncture can progress with the help of AI.},
  langid = {english},
  keywords = {Acupuncture,Artificial intelligence,Machine learning,Review,Traditional Chinese medicine,中醫,已整理,機器學習,針灸},
  annotation = {8 citations (Crossref) [2024-03-26]\\
titleTranslation: 人工智慧引導針灸：綜述\\
abstractTranslation:  針灸目前在世界各地廣泛使用，對許多慢性疾病，特別是與疼痛相關的疾病顯示出顯著的療效。隨著人工智慧（AI）的快速發展，其在針灸治療中的應用在選穴處方、針灸手法識別、針灸療效預測等針灸實踐的多個領域取得了一系列重大突破。本文將討論人工智慧引導針灸領域的重大理論和技術成果。基於人工智慧的資料探勘方法揭示了治療各種疾病的關鍵穴位組合，為臨床實踐中的腧穴處方提供了科學依據。此外，現代中醫儀器的快速發展，促進了現代醫療儀器、人工智慧技術和針灸的融合。這種整合顯著提高了針灸的量化、客觀化和標準化以及臨床個人化針灸治療的實施。基於機器學習的針灸臨床療效預測可以幫助醫生篩選可能受益於針灸治療的患者。然而，現有的挑戰需要額外的工作來開發人工智慧指導的針灸。其中包括讓人工智慧研究人員更能理解中國古代哲學、基於中醫針灸理論對知識發現的解釋、針灸資料庫的建構以及新知識驗證的臨床試驗。本文旨在總結人工智慧技術對針灸新知識的發現、針灸安全性和療效的提高、針灸的發展和傳承的主要貢獻，以及人工智慧指導針灸進一步發展面臨的主要挑戰。針灸的發展可以在人工智慧的幫助下取得進步。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\S2H3UHQQ\\Wang 等。 - 2022 - Artificial intelligence-directed acupuncture a re.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\SJR2TDM8\\s13020-022-00636-1.html}
}

@article{yunshilanComplexKnowledgeBase2023,
  title = {Complex {{Knowledge Base Question Answering}}: {{A Survey}}},
  shorttitle = {Complex {{Knowledge Base Question Answering}}},
  author = {{Yunshi Lan} and {Gaole He} and {Jinhao Jiang} and {Jing Jiang} and {Wayne Xin Zhao} and {Ji-Rong Wen}},
  date = {2023-01},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {35},
  number = {11},
  pages = {11196--11215},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2022.3223858},
  url = {https://ieeexplore.ieee.org/document/9960856/citations?tabFilter=papers#citations},
  urldate = {2023-10-12},
  abstract = {Knowledge base question answering (KBQA) aims to answer a question over a knowledge base (KB). Early studies mainly focused on answering simple questions over KBs and achieved great success. However, their performances on complex questions are still far from satisfactory. Therefore, in recent years, researchers propose a large number of novel methods, which looked into the challenges of answering complex questions. In this survey, we review recent advances in KBQA with the focus on solving complex questions, which usually contain multiple subjects, express compound relations, or involve numerical operations. In detail, we begin with introducing the complex KBQA task and relevant background. Then, we present two mainstream categories of methods for complex KBQA, namely semantic parsing-based (SP-based) methods and information retrieval-based (IR-based) methods. Specifically, we illustrate their procedures with flow designs and discuss their difference and similarity. Next, we summarize the challenges that these two categories of methods encounter when answering complex questions, and explicate advanced solutions as well as techniques used in existing work. After that, we discuss the potential impact of pre-trained language models (PLMs) on complex KBQA. To help readers catch up with SOTA methods, we also provide a comprehensive evaluation and resource about complex KBQA task. Finally, we conclude and discuss several promising directions related to complex KBQA for future research.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  annotation = {15 citations (Crossref) [2024-03-26]},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\J8XLUJWD\\Lan 等。 - 2023 - Complex Knowledge Base Question Answering A Surve.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\LECMME87\\Lan 等。 - 2023 - Complex Knowledge Base Question Answering A Surve.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\HAPZPV72\\9960856.html}
}

@article{yuqitangResearchInsomniaTraditional2021,
  title = {Research of Insomnia on Traditional {{Chinese}} Medicine Diagnosis and Treatment Based on Machine Learning},
  author = {{Yuqi Tang} and {Zechen Li} and {Dongdong Yang} and {Yu Fang} and {Shanshan Gao} and {Shan Liang} and {Tao Liu}},
  date = {2021-01-06},
  journaltitle = {Chinese Medicine},
  shortjournal = {Chin. Med.},
  volume = {16},
  number = {1},
  pages = {2},
  publisher = {Bmc},
  location = {London},
  issn = {1749-8546},
  doi = {10.1186/s13020-020-00409-8},
  url = {https://www.webofscience.com/api/gateway?GWVersion=2&SrcAuth=DOISource&SrcApp=WOS&KeyAID=10.1186%2Fs13020-020-00409-8&DestApp=DOI&SrcAppSID=EUW1ED0DDDirKeRaF78tUdqIiyCyX&SrcJTitle=CHINESE+MEDICINE&DestDOIRegistrantName=Springer+%28Biomed+Central+Ltd.%29},
  urldate = {2022-11-13},
  abstract = {BackgroundInsomnia as one of the dominant diseases of traditional Chinese medicine (TCM) has been extensively studied in recent years. To explore the novel approaches of research on TCM diagnosis and treatment, this paper presents a strategy for the research of insomnia based on machine learning.MethodsFirst of all, 654 insomnia cases have been collected from an experienced doctor of TCM as sample data. Secondly, in the light of the characteristics of TCM diagnosis and treatment, the contents of research samples have been divided into four parts: the basic information, the four diagnostic methods, the treatment based on syndrome differentiation and the main prescription. And then, these four parts have been analyzed by three analysis methods, including frequency analysis, association rules and hierarchical cluster analysis. Finally, a comprehensive study of the whole four parts has been conducted by random forest.ResultsResearches of the above four parts revealed some essential connections. Simultaneously, based on the algorithm model established by the random forest, the accuracy of predicting the main prescription by the combinations of the four diagnostic methods and the treatment based on syndrome differentiation was 0.85. Furthermore, having been extracted features through applying the random forest, the syndrome differentiation of five zang-organs was proven to be the most significant parameter of the TCM diagnosis and treatment.ConclusionsThe results indicate that the machine learning methods are worthy of being adopted to study the dominant diseases of TCM for exploring the crucial rules of the diagnosis and treatment.},
  langid = {english},
  keywords = {Association rules,Cluster   analysis,Diagnosis,Insomnia,Machine learning,Random forest,tcm},
  annotation = {14 citations (Crossref) [2024-03-26]\\
7 citations (Semantic Scholar/DOI) [2022-11-18]\\
WOS:000608236200002\\
titleTranslation: 基於機器學習的失眠中醫診療研究\\
abstractTranslation:  研究背景失眠作為中醫優勢病種之一，近年來得到廣泛研究。為了探索中醫診療研究的新途徑，本文提出了基於機器學習的失眠研究策略。方法首先收集經驗豐富的中醫醫師的654例失眠病例作為樣本數據。其次，針對中醫診治特點，將研究樣本內容分為基本信息、四診法、辨證論治、主方四個部分。然後，利用頻率分析、關聯規則和層次聚類分析三種分析方法對這四個部分進行了分析。最後利用隨機森林對整個四個部分進行了綜合研究。結果上述四個部分的研究揭示了一些本質上的聯繫。同時，基於隨機森林建立的算法模型，四種診斷方法和辨證論治組合預測主方的準確率為0.85。此外，通過應用隨機森林提取特徵，五臟辨證被證明是中醫診斷和治療最重要的參數。結論結果表明機器學習方法值得採用來研究探討中醫優勢病種的診斷和治療的關鍵規律。},
  file = {C:\Users\BlackCat\Zotero\storage\VLZ7D23K\Tang 等。 - 2021 - Research of insomnia on traditional Chinese medici.pdf}
}

@article{YuXiaoLingJiYuLiangWeiYiTiDeZhongWenDianZiBingLiMingMingShiTiShiBie2017,
  title = {基于两位一体的中文电子病历命名实体识别},
  author = {{郁小玲} and {张铁山} and {吴彤} and {方明哲} and {黄建一} and {胡长军}},
  date = {2017},
  journaltitle = {中国卫生信息管理杂志},
  shortjournal = {Chinese Journal of Health Informatics and Management},
  volume = {14},
  number = {4},
  pages = {552--556},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhN6Z3dzeHhnbHp6MjAxNzA0MDEzGghhOXd1eW5qNg%3D%3D},
  urldate = {2022-08-14},
  abstract = {命名实体识别是信息抽取中的一项重要任务.在医疗研究领域,从电子病历中自动识别命名实体形成结构化的文本为医疗决策提供数据支持,已经成为重要的研究课题.分词和实体识别分步进行容易造成下层错误向上累加传递且不能充分利用融合信息.针对这一问题,本文提出一种两位一体字标注方法,该方法将识别过程看做是序列的字标注过程,采用条件随机场模型经过标注实现病历的命名实体识别.实验结果表明,两位一体字标注方法在命名实体识别中性能得到很大的提升.},
  langid = {zh\_CN},
  keywords = {Binity,Character_based tagging,Chinese Journal of Health Informatics and Management,Conditions random field model,Information extraction,Named entity recognition,No DOI found,两位一体,信息抽取,命名实体识别,字标注,条件随机场},
  annotation = {北京科技大学计算机与通信工程学院,北京市,100083中日友好医院,北京市,100029\\
2017-09-20 （万方平台首次上网日期，不代表论文的发表时间）\\
abstractTranslation:  實體命名是信息抽取中的一項重要任務。在醫療研究領域，從電子病歷中自動識別實體形成構成的文本為醫療決策提供數據支持，已經成為重要的研究課題。步進行容易下層錯誤向上加累交且不能充分利用融合信息。針對這一問題，本文提出了一種三位一體字標記形成方法，該方法將識別過程看做是序列的字標記過程，採用條件隨機場模型經過標記實現疾病歷的命名實體識別。實驗結果表明，這兩個一體字標記方法在命名實體識別中性能得到了很大的提升。\\
titleTranslation: 基於它們一體的中文電子病歷命名實體識別},
  file = {C:\Users\BlackCat\Zotero\storage\3FWBZ693\郁 等。 - 2017 - 基于两位一体的中文电子病历命名实体识别.pdf}
}

@inproceedings{yuxinluoSurveyComplexKnowledge2022,
  title = {A {{Survey}}: {{Complex Knowledge Base Question Answering}}},
  shorttitle = {A {{Survey}}},
  booktitle = {2022 {{IEEE}} 2nd {{International Conference}} on {{Information Communication}} and {{Software Engineering}} ({{ICICSE}})},
  author = {{Yuxin Luo} and {Bailong Yang} and {Donghui Xu} and {Luogeng Tian}},
  date = {2022-03},
  pages = {46--52},
  doi = {10.1109/ICICSE55337.2022.9828967},
  url = {https://ieeexplore.ieee.org/document/9828967},
  urldate = {2023-10-16},
  abstract = {Knowledge base question answering(KBQA) is a technique that utilizes the rich semantic information in the knowledge base and fully understands the question to obtain the answer. At present, scholars put more energy into solving complex relationship problems. This paper first outlines the background and core challenges of complex KBQA. Second, two mainstream complex KBQA methods are introduced, namely, semantic parsing (SP-based) and information retrieval (IR-based) methods. Finally, future research trends are analyzed.},
  eventtitle = {2022 {{IEEE}} 2nd {{International Conference}} on {{Information Communication}} and {{Software Engineering}} ({{ICICSE}})},
  keywords = {Survey,問答系統,已整理,概讀,機器學習,知識圖譜},
  annotation = {2 citations (Crossref) [2024-03-26]},
  note = {關於基於KB的問答系統的概述。說明這個領域的特點、資料集及常見的解決方法，包含語意規則、神經網路、問題生成等。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\ZULUTBQU\\Luo et al. - 2022 - A Survey Complex Knowledge Base Question Answerin.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\MDTAI6UE\\9828967.html}
}

@thesis{YuXinXianYiGeZiLiaoPingXingYuYanZaiDuiChengShiDuoChuLiQiShangDeYanZhi2002,
  title = {一個資料平行語言在對稱式多處理器上的研製},
  author = {{余信賢}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2002},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/63xguu},
  abstract = {一般而言，撰寫平行程式比撰寫循序程式困難許多，所以提供一個高階的平行程式語言幫助程式設計師更容易地撰寫平行程式是非常重要的。我們所發展的高階平行程式語言CCC是C的一個簡單延伸。CCC提供三個主要的特性來支援資料平行程式的平行計算環境，分別是虛擬處理器、同步執行模式、全域命名。依CCC的發展始意而言，我們希望用CCC撰寫平行程式可以比用來發展平行程式的多執行緒訊息傳遞函式庫(如Pthread)更具有生產力，並且能產生比隱性的平行程式語言(如HPF和HPC++)更有效率的程式碼。 CCC所擁有的三種特性讓程式設計師可以不需考慮太多與硬體相關的細節問題。從虛擬計算環境至實體計算環境的轉換過程是由編譯器來處理，此轉換過程包含兩個主要步驟：虛擬處理器的模擬以及目的碼的生成。其中虛擬處理器的模擬只與實體計算環境中處理器的總數有關，是以CCC至CCC的轉換方式來實作。因此我們對虛擬處理器的模擬幾乎完全與底層的硬體細節無關，大部分與底層硬體相關的程式碼是在目的碼的生成步驟中。此編譯結構讓我們的編譯器保有極高的可重標的性。},
  pagetotal = {152}
}

@article{yuxinyaoInfluenceDigitalTechnologies2023,
  title = {The {{Influence}} of {{Digital Technologies}} on {{Knowledge Management}} in {{Engineering}}: {{A Systematic Literature Review}}},
  shorttitle = {The {{Influence}} of {{Digital Technologies}} on {{Knowledge Management}} in {{Engineering}}},
  author = {{Yuxin Yao} and {Eann A. Patterson} and {Richard J. Taylor}},
  date = {2023},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--15},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2023.3285952},
  url = {https://ieeexplore.ieee.org/document/10159469},
  urldate = {2023-10-12},
  abstract = {Digital technologies are gaining widespread acceptance in engineering and offer opportunities for collating and curating knowledge during and beyond the life cycle of engineering products. Knowledge is central to strategy and operations in most engineering organizations and digital technologies have been employed in attempts to improve current knowledge management practices. A systematic literature review was undertaken to address the question: how do digital technologies influence knowledge management in the engineering sector? Twenty-seven primary studies were identified from 3097 papers on these topics within the engineering literature published between 2010 and 2022. Four knowledge management processes supported by digital technologies were recognized: knowledge creation, storage and retrieval, sharing and application. In supporting knowledge management, digital technologies were found to have been acting in five roles: repositories, transactive memory systems, communication spaces, boundary objects and non-human actors. However, the ability of digital technologies to perform these roles simultaneously had not been considered and similarly knowledge management had not been addressed as a holistic process. Hence, it was concluded that a holistic approach to knowledge management combined with the deployment of digital technologies in multiple roles simultaneously would likely yield significant competitive advantage and organizational value for organizations in the engineering sector.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  annotation = {1 citations (Crossref) [2024-03-26]},
  file = {D:\Paper\Yao et al. - 2023 - The Influence of Digital Technologies on Knowledge.pdf}
}

@article{YuYangZhongYiDianZiBingLiDeYanJiuXianZhuangYuZhanWang2006,
  title = {中医电子病历的研究现状与展望},
  author = {{于洋} and {凌昌全}},
  date = {2006},
  journaltitle = {解放军医院管理杂志},
  volume = {13},
  number = {7},
  pages = {612--614},
  issn = {1008-9985},
  doi = {10.3969/j.issn.1008-9985.2006.07.042},
  abstract = {中医电子病历是伴随着国内医院信息化进程而发展起来的新生事物.中医电子病历是中医临床信息系统的采集系统,是中医现代化研究主要工具.当前国内中医电子病历的研究的重点主要集中在标准化、结构化、集成化、质量监控四个方面.随着中医临床科研的广泛深入和现代信息技术的日新月异,临床科研一体化、专科专病化、智能化、网络化,将是未来中医电子病历发展的必由之路.},
  langid = {chi},
  keywords = {中醫,電子病歷},
  annotation = {abstractTranslation:  中醫電子病歷是伴隨著國內醫院信息化進程而發展起來的新生事物。中醫電子病歷是中醫臨床信息系統的採集系統，是中醫現代化研究的主要工具。當前國內中醫電子病歷的研究重點主要集中在標準化隨著中醫臨床科研的廣泛研究和現代信息技術的日新月異、臨床科研一體化、專科專病化、標準化、網絡化，將是未來中醫電子病歷發展的必由之路。\\
titleTranslation: 中醫電子病歷的研究現狀與展望},
  file = {C:\Users\BlackCat\Zotero\storage\5YQNNA5P\于洋 與 凌昌全 - 2006 - 中医电子病历的研究现状与展望.pdf}
}

@inproceedings{yuzhangEffectMetadataScientific2023,
  title = {The {{Effect}} of {{Metadata}} on {{Scientific Literature Tagging}}: {{A Cross-Field Cross-Model Study}}},
  shorttitle = {The {{Effect}} of {{Metadata}} on {{Scientific Literature Tagging}}},
  booktitle = {Proceedings of the {{ACM Web Conference}} 2023},
  author = {{Yu Zhang} and {Bowen Jin} and {Qi Zhu} and {Yu Meng} and {Jiawei Han}},
  year = {4 月 30, 2023},
  series = {{{WWW}} '23},
  pages = {1626--1637},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3543507.3583354},
  url = {https://doi.org/10.1145/3543507.3583354},
  urldate = {2023-09-11},
  abstract = {Due to the exponential growth of scientific publications on the Web, there is a pressing need to tag each paper with fine-grained topics so that researchers can track their interested fields of study rather than drowning in the whole literature. Scientific literature tagging is beyond a pure multi-label text classification task because papers on the Web are prevalently accompanied by metadata information such as venues, authors, and references, which may serve as additional signals to infer relevant tags. Although there have been studies making use of metadata in academic paper classification, their focus is often restricted to one or two scientific fields (e.g., computer science and biomedicine) and to one specific model. In this work, we systematically study the effect of metadata on scientific literature tagging across 19 fields. We select three representative multi-label classifiers (i.e., a bag-of-words model, a sequence-based model, and a pre-trained language model) and explore their performance change in scientific literature tagging when metadata are fed to the classifiers as additional features. We observe some ubiquitous patterns of metadata’s effects across all fields (e.g., venues are consistently beneficial to paper tagging in almost all cases), as well as some unique patterns in fields other than computer science and biomedicine, which are not explored in previous studies.},
  isbn = {978-1-4503-9416-1},
  langid = {english},
  keywords = {metadata,scientific literature tagging,text classification,回收,機器學習,知識分類},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: 元數據對科學文獻標註的影響：跨領域跨模型研究\\
abstractTranslation:  由於網絡上科學出版物的指數級增長，迫切需要為每篇論文添加細粒度的主題標籤，以便研究人員能夠跟踪他們感興趣的研究領域，而不是淹沒在整個文獻中。科學文獻標記超出了純粹的多標籤文本分類任務，因為網絡上的論文普遍附有元數據信息，例如地點、作者和參考文獻，這些信息可以作為推斷相關標籤的附加信號。儘管已經有研究在學術論文分類中使用元數據，但它們的重點通常僅限於一兩個科學領域（例如計算機科學和生物醫學）和一種特定模型。在這項工作中，我們系統地研究了元數據對 19 個領域的科學文獻標記的影響。我們選擇三個代表性的多標籤分類器（即詞袋模型、基於序列的模型和預訓練的語言模型），並探索當元數據輸入分類器時它們在科學文獻標記中的性能變化：附加功能。我們觀察到元數據在所有領域的影響普遍存在的模式（例如，在幾乎所有情況下，場所都始終有利於紙張標記），以及計算機科學和生物醫學以外的領域中的一些獨特模式，這些模式在以前的研究中沒有探討過。},
  note = {這篇論文的貢獻在於分析了除醫學與資訊以外的領域的分類表現，超出研究範圍。但仍然有部分可取之處(可參考引用)},
  file = {C:\Users\BlackCat\Zotero\storage\E9U3MXKG\The Effect of Metadata on Scientific Literature Tagging： A Cross-Field Cross-Model Study.pdf}
}

@article{yuzhuowangReviewMethodEntities2022,
  title = {A Review on Method Entities in the Academic Literature: Extraction, Evaluation, and Application},
  shorttitle = {A Review on Method Entities in the Academic Literature},
  author = {{Yuzhuo Wang} and {Chengzhi Zhang} and {Kai Li}},
  date = {2022-05-01},
  journaltitle = {Scientometrics},
  shortjournal = {Scientometrics},
  volume = {127},
  number = {5},
  pages = {2479--2520},
  issn = {1588-2861},
  doi = {10.1007/s11192-022-04332-7},
  url = {https://doi.org/10.1007/s11192-022-04332-7},
  urldate = {2023-10-25},
  abstract = {In scientific research, the method is an indispensable means to solve scientific problems and a critical research object. With the advancement of sciences, many scientific methods are being proposed, modified, and used in academic literature. The authors describe details of the method in the abstract and body text, and key entities in academic literature reflecting names of the method are called method entities. Exploring diverse method entities in a tremendous amount of academic literature helps scholars understand existing methods, select the appropriate method for research tasks, and propose new methods. Furthermore, the evolution of method entities can reveal the development of a discipline and facilitate knowledge discovery. Therefore, this article offers a systematic review of methodological and empirical works focusing on extracting method entities from full-text academic literature and efforting to build knowledge services using these extracted method entities. Definitions of key concepts involved in this review were first proposed. Based on these definitions, we systematically reviewed the approaches and indicators to extract and evaluate method entities, with a strong focus on the pros and cons of each approach. We also surveyed how extracted method entities were used to build new applications. Finally, limitations in existing works as well as potential next steps were discussed.},
  langid = {english},
  keywords = {Entity evaluation,Entity extraction,Entity platform,Method entities,Review,實體抽取,研究流程},
  annotation = {10 citations (Crossref) [2024-03-26]\\
titleTranslation: 學術文獻中方法實體的回顧：提取、評估和應用\\
abstractTranslation:  在科學研究中，方法是解決科學問題不可或缺的手段，也是關鍵的研究對象。隨著科學的進步，許多科學方法被提出、修改並在學術文獻中使用。作者在摘要和正文中描述了該方法的細節，學術文獻中反映該方法名稱的關鍵實體稱為方法實體。在大量學術文獻中探索不同的方法實體有助於學者理解現有方法，選擇適合研究任務的方法，並提出新方法。此外，方法實體的演化可以揭示學科的發展並促進知識發現。因此，本文對方法論和實證研究進行了系統性回顧，重點在於從全文學術文獻中提取方法實體，並努力利用這些提取的方法實體來建構知識服務。首次提出了本次審查涉及的關鍵概念的定義。基於這些定義，我們系統地回顧了提取和評估方法實體的方法和指標，並專注於每種方法的優缺點。我們還調查瞭如何使用提取的方法實體來建立新應用程式。最後，討論了現有工作的局限性以及潛在的後續步驟。},
  file = {C:\Users\BlackCat\Zotero\storage\2EPDYSZY\Wang et al. - 2022 - A review on method entities in the academic litera.pdf}
}

@article{z.m.maFormalSemanticspreservingTranslation2010,
  title = {Formal Semantics-Preserving Translation from Fuzzy {{ER}} Model to Fuzzy {{OWL DL}} Ontology},
  author = {{Z. M. Ma} and {Fu Zhang} and {Li Yan} and {Yanhui Lv}},
  year = {12 月 1, 2010},
  journaltitle = {Web Intelligence and Agent Systems},
  shortjournal = {Web Intelli. and Agent Sys.},
  volume = {8},
  number = {4},
  pages = {397--412},
  issn = {1570-1263},
  doi = {10.3233/WIA-2010-0199},
  abstract = {Ontology is an important part of the W3C standards for the Semantic Web, and how to quickly and cheaply construct Web ontologies has become a key technology to enable the Semantic Web. However, information imprecision and uncertainty exist in many real-world applications, thus constructing fuzzy ontology by extracting domain knowledge from fuzzy database models (e.g., fuzzy ER model) can profitably support fuzzy ontology development. In this paper, we propose an approach for constructing fuzzy ontology from fuzzy ER model, in which the fuzzy ontology consists of fuzzy ontology structure and instances. Firstly, we give the formal definition and the semantics of fuzzy ER models. Then, we introduce the fuzzy extension of ontology language OWL DL, i.e., fuzzy OWL DL. Based on the fuzzy OWL DL, a kind of fuzzy ontology called fuzzy OWL DL ontology is presented. Furthermore, we consider the fuzzy ER schema and the corresponding database instances, and translate them into the fuzzy ontology structure and the fuzzy ontology instances, respectively. Finally, since a fuzzy OWL DL ontology is equivalent to a fuzzy Description Logic f-SHOIN(D) knowledge base, how the reasoning problems of fuzzy ER models (e.g., satisfiability, subsumption, and redundancy) may be reduced to reasoning on f-SHOIN(D) knowledge bases is investigated, which will further contribute to constructing fuzzy OWL DL ontology that exactly meet application's needs. Of course, the correctness of the translation and reasoning problems are proved completely.},
  langid = {english},
  keywords = {/unread,description logic,Fuzzy database,fuzzy ER model,fuzzy OWL DL ontology,ontology learning,reasoning,回收,無法取得,知識本體},
  annotation = {8 citations (Crossref) [2024-03-26]\\
abstractTranslation:  本體是語意Web W3C標準的重要組成部分，如何快速、廉價地建構Web本體已成為實現語意Web的關鍵技術。然而，許多現實應用中存在資訊不精確和不確定性，因此透過從模糊資料庫模型（例如模糊ER模型）中提取領域知識來建立模糊本體可以有效地支援模糊本體的開發。在本文中，我們提出了一種從模糊ER模型建立模糊本體的方法，其中模糊本體由模糊本體結構和實例組成。首先，我們給了模糊ER模型的形式定義和語意。然後，我們介紹了本體語言OWL DL的模糊擴展，即模糊OWL DL。在模糊OWL DL的基礎上，提出了一種模糊本體，稱為模糊OWL DL本體。此外，我們考慮模糊ER模式和對應的資料庫實例，並將它們分別轉換為模糊本體結構和模糊本體實例。最後，由於模糊OWL DL 本體相當於模糊描述邏輯f-SHOIN(D) 知識庫，因此模糊ER 模型的推理問題（例如可滿足性、包含性和冗餘性）如何可以簡化為f-SHOIN(D ) 上的推理對SHOIN(D)知識庫進行了研究，這將進一步有助於建立完全滿足應用需求的模糊OWL DL本體。當然，翻譯和推理問題的正確性都完全證明了。\\
titleTranslation: 從模糊 ER 模型到模糊 OWL DL 本體的形式語意保留翻譯}
}

@article{z.q.zhangUnderstandingZHENGTraditional2007,
  title = {Understanding {{ZHENG}} in Traditional {{Chinese}} Medicine in the Context of Neuro-Endocrine-Immune Network},
  author = {{Z.Q. Zhang} and {S. Li} and {L.J. Wu} and {X.G. Zhang} and {Y.Y. Wang} and {Y.D. Li}},
  date = {2007-01-01},
  journaltitle = {IET Systems Biology},
  volume = {1},
  number = {1},
  pages = {51--60},
  issn = {1751-8849, 1751-8857},
  doi = {10.1049/iet-syb:20060032},
  url = {https://digital-library.theiet.org/content/journals/10.1049/iet-syb_20060032},
  urldate = {2022-09-19},
  langid = {english},
  annotation = {286 citations (Crossref) [2024-03-26]\\
307 citations (Semantic Scholar/DOI) [2022-10-26]\\
titleTranslation: 從神經內分泌免疫網絡的角度理解中醫“正”},
  file = {C:\Users\BlackCat\Zotero\storage\KPUGACKZ\Zhang 等。 - 2007 - Understanding ZHENG in traditional Chinese medicin.pdf}
}

@article{zaibConversationalQuestionAnswering2022,
  title = {Conversational Question Answering: A Survey},
  shorttitle = {Conversational Question Answering},
  author = {Zaib, Munazza and Zhang, Wei Emma and Sheng, Quan Z. and Mahmood, Adnan and Zhang, Yang},
  date = {2022-12-01},
  journaltitle = {Knowledge and Information Systems},
  shortjournal = {Knowl Inf Syst},
  volume = {64},
  number = {12},
  pages = {3151--3195},
  issn = {0219-3116},
  doi = {10.1007/s10115-022-01744-y},
  url = {https://doi.org/10.1007/s10115-022-01744-y},
  urldate = {2024-04-11},
  abstract = {Question answering (QA) systems provide a way of querying the information available in various formats including, but not limited to, unstructured and structured data in natural languages. It constitutes a considerable part of conversational artificial intelligence (AI) which has led to the introduction of a special research topic on conversational question answering (CQA), wherein a system is required to understand the given context and then engages in multi-turn QA to satisfy a user’s information needs. While the focus of most of the existing research work is subjected to single-turn QA, the field of multi-turn QA has recently grasped attention and prominence owing to the availability of large-scale, multi-turn QA datasets and the development of pre-trained language models. With a good amount of models and research papers adding to the literature every year recently, there is a dire need of arranging and presenting the related work in a unified manner to streamline future research. This survey is an effort to present a comprehensive review of the state-of-the-art research trends of CQA primarily based on reviewed papers over the recent years. Our findings show that there has been a trend shift from single-turn to multi-turn QA which empowers the field of Conversational AI from different perspectives. This survey is intended to provide an epitome for the research community with the hope of laying a strong foundation for the field of CQA.},
  langid = {english},
  keywords = {Conversational agents,Conversational AI,Conversational machine reading comprehension,Knowledge base,Question answering,Survey,問答系統,已整理,略讀,重要},
  annotation = {titleTranslation: 對話式問答：一項調查\\
abstractTranslation:  問答（QA）系統提供了一種查詢各種格式的可用資訊的方法，包括但不限於自然語言的非結構化和結構化資料。它構成了對話式人工智慧（AI）的重要組成部分，從而引發了對話式問答（CQA）的特殊研究主題，其中系統需要理解給定的上下文，然後進行多輪問答以滿足用戶的資訊需求。雖然大多數現有研究工作的重點是單輪 QA，但由於大規模、多輪 QA 資料集的可用性以及預驗證技術的發展，多輪 QA 領域最近引起了人們的關注和重視。的語言模型。近年來，隨著每年大量的模型和研究論文被添加到文獻中，迫切需要以統一的方式整理和呈現相關工作，以簡化未來的研究。本次調查主要基於近年來審查的論文，對 CQA 的最新研究趨勢進行全面回顧。我們的研究結果表明，從單輪問答到多輪問答的趨勢已經發生轉變，這從不同的角度為對話式人工智慧領域提供了支持。本次調查旨在為研究界提供一個縮影，希望為CQA領域奠定堅實的基礎。},
  note = {著重在多輪問答CQA，但其他的描述也很詳細
\par
檢查關鍵字markdown及note，沒有關於筆記問答的內容。},
  file = {C:\Users\BlackCat\Zotero\storage\DAD6TCXW\Zaib 等。 - 2022 - Conversational question answering a survey.pdf}
}

@thesis{ZhangChongYanYiGeZhiYuanGuiGeQuDongKaiFaDeZhengHeHuanJing2020,
  title = {一個支援規格驅動開發的整合環境},
  author = {{張崇彥}},
  namea = {{林迺衛} and {LIN NAI-WEI}},
  nameatype = {collaborator},
  date = {2020},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/deez7q},
  abstract = {軟體測試是確保軟體品質的主要方法。傳統軟體開發流程會先實作程式，再實作測試案例，來測試程式。測試驅動開發流程則先實作測試案例，再透過自動執行測試案例，來驅動程式的實作。Eclipse開發環境結合JUnit測試架構組成一個支援測試驅動開發流程的整合環境。這個整合環境雖然支援測試案例的自動執行，但是使用者仍需根據軟體規格，手動實作測試案例。本論文結合黑箱測試案例自動產生及測試驅動開發流程，提出規格驅動開發流程。本論文也在Eclipse上結合XML模型工具Papyrus、OCL處理器、黑箱測試案例自動產生工具CBTCG、及JUnit，開發一個支援規格驅動開發流程的整合環境CBSDD。這個整合環境以專案的形式，完整支援軟體規格的制定，根據軟體規格自動產生測試案例，並透過自動執行測試案例，來驅動程式的實作與重構。規格驅動開發流程可以讓我們更注重軟體規格的制定，並可以同時提高軟體品質及降低開發成本。},
  pagetotal = {57},
  keywords = {實驗室}
}

@thesis{ZhangChunHuaYueJingBingMoHuZhongYiBianZheng2021,
  title = {月經病模糊中醫辨證},
  author = {{張純華}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2021},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/8298x8},
  abstract = {女性的生理及病理均不同於男性。婦科疾病亦有顯著的特殊性。因此，將中醫婦科從中醫內科中獨立出來是必要的。本論文運用電腦強大的記憶能力及快速的分析能力，研製一個基於模糊集合的中醫婦科月經病辨證系統。本論文根據《中醫婦科學》，進行中醫證候標準化，整理且歸納出93個中醫婦科月經病證候及其特徵症狀。本論文為每一個月經病證候定義一個模糊集合，並使用兩個技術來設計模糊集合的歸屬函數。第一個是症狀分群技術，將證候的特徵症狀分為婦科症狀群、全身症狀群、舌診症狀群、及脈診症狀群等四個症狀群。第二個是症狀分權技術，依指數遞減地方式，將權重依序分配給症狀群中的症狀。本論文也使用三個指標：基線鑑別係數、絕對臨床鑑別係數、與相對臨床鑑別係數，來評估系統的鑑別率。評估結果顯示症狀分群技術及症狀分權技術達成非常良好的鑑別能力。},
  pagetotal = {95},
  keywords = {實驗室}
}

@article{zhangInformationExtractionText2022,
  title = {Information {{Extraction}} from the {{Text Data}} on {{Traditional Chinese Medicine}}: {{A Review}} on {{Tasks}}, {{Challenges}}, and {{Methods}} from 2010 to 2021},
  shorttitle = {Information {{Extraction}} from the {{Text Data}} on {{Traditional Chinese Medicine}}},
  author = {Zhang, Tingting and Huang, Zonghai and Wang, Yaqiang and Wen, Chuanbiao and Peng, Yangzhi and Ye, Ying},
  editor = {Zhou, Xuezhong},
  date = {2022-05-13},
  journaltitle = {Evidence-Based Complementary and Alternative Medicine},
  shortjournal = {Evidence-Based Complementary and Alternative Medicine},
  volume = {2022},
  pages = {1--19},
  issn = {1741-4288, 1741-427X},
  doi = {10.1155/2022/1679589},
  url = {https://www.hindawi.com/journals/ecam/2022/1679589/},
  urldate = {2023-11-28},
  abstract = {Background. The practice of traditional Chinese medicine (TCM) began several thousand years ago, and the knowledge of practitioners is recorded in paper and electronic versions of case notes, manuscripts, and books in multiple languages. Developing a method of information extraction (IE) from these sources to generate a cohesive data set would be a great contribution to the medical field. The goal of this study was to perform a systematic review of the status of IE from TCM sources over the last 10\,years. Methods. We conducted a search of four literature databases for articles published from 2010 to 2021 that focused on the use of natural language processing (NLP) methods to extract information from unstructured TCM text data. Two reviewers and one adjudicator contributed to article search, article selection, data extraction, and synthesis processes. Results. We retrieved 1234 records, 49 of which met our inclusion criteria. We used the articles to (i) assess the key tasks of IE in the TCM domain, (ii) summarize the challenges to extracting information from TCM text data, and (iii) identify effective frameworks, models, and key findings of TCM IE through classification. Conclusions. Our analysis showed that IE from TCM text data has improved over the past decade. However, the extraction of TCM text still faces some challenges involving the lack of gold standard corpora, nonstandardized expressions, and multiple types of relations. In the future, IE work should be promoted by extracting more existing entities and relations, constructing gold standard data sets, and exploring IE methods based on a small amount of labeled data. Furthermore, fine-grained and interpretable IE technologies are necessary for further exploration.},
  langid = {english},
  keywords = {Review,中醫,資訊抽取},
  annotation = {7 citations (Crossref) [2024-03-26]\\
titleTranslation: 中醫文本資料資訊擷取：2010-2021年任務、挑戰與方法回顧},
  note = {[TLDR] It was showed that IE from TCM text data has improved over the past decade, but the extraction ofTCM text still faces some challenges involving the lack of gold standard corpora, nonstandardized expressions, and multiple types of relations.},
  file = {C:\Users\BlackCat\Zotero\storage\UIRYWZ6J\Zhang et al. - 2022 - Information Extraction from the Text Data on Tradi.pdf}
}

@thesis{ZhangJieMianXiangZhongWenDianZiBingLiDeMingMingShiTiShiBieFangFaYanJiu2022,
  type = {[] AB K1},
  title = {面向中文电子病历的命名实体识别方法研究},
  author = {{张杰}},
  date = {2022},
  institution = {浙江科技大学},
  location = {中国},
  langid = {chi},
  annotation = {titleTranslation: 面向中文電子疾病歷的命名實體識別方法研究},
  note = {应用统计
\par
The following values have no corresponding Zotero field:\\
A3 [万健]\\
ED 硕士\\
DS 万方数据},
  file = {C:\Users\BlackCat\Zotero\storage\86VQNSI2\张杰 - 2022 - 面向中文电子病历的命名实体识别方法研究.pdf}
}

@online{ZhangKunLiJiYuZiRanYuYanChuLiDeZhongWenChanKeDianZiBingLiYanJiu2017,
  title = {基于自然语言处理的中文产科电子病历研究},
  author = {{张坤丽} and {ZHANG Kunli} and {马鸿超} and {M. A. Hongchao} and {赵悦淑} and {ZHAO Yueshu} and {昝红英} and {Z. A. N. Hongying} and {庄雷} and {ZHUANG Lei}},
  date = {2017-12-15},
  publisher = {郑州大学学报(理学版)},
  issn = {1671-6841},
  url = {http://www.xml-data.org/ZZDXXBLXB/html/c95b7123-2803-492d-9bcd-64b071796bf1.htm},
  urldate = {2022-07-22},
  abstract = {电子病历中蕴含着大量的医疗知识和患者的健康信息，而产科电子病历的结构化及信息抽取对临床决策支持及提高人口的生育健康水平具有重要意义.首先对中文产科电子病历的结构特点及内容进行了分析，并采用基于规则的方法对电子病历数据进行了清洗和结构化；其次采用最大熵(ME)模型及基于规则方法按治疗类型对电子病历进行分类，分类的\emph{F}值达到88.16\%；最后，为了进一步利用电子病历进行信息抽取和知识挖掘，以短句为单位，相似度为衡量标准，采用支持向量机(SVM)模型对首次病程记录进行去重处理及自动差异化分析，从分析的结果中筛选出68.6\%的重复及相似短句.},
  langid = {cn},
  annotation = {abstractTranslation:  電子病歷中蘊藏著大量的醫療知識和患者的健康信息，而產科電子病歷的成型及信息抽取對臨床決策支持及人口生育健康水平的提高具有重要意義。內容進行了分析，並採用基於規則的方法對電子病歷數據進行了清理和塑造；其次採用最大熵（ME）模型及基於規則方法按治療類型對電子病歷數據進行分類、分類的\emph{F{$<$} /i{$>$}值達到88.16\%;最後，為了進一步利用電子病歷進行信息抽取和知識挖掘，以短句為單位，近似度為簡化標準，採用支持支持機（SVM）模型對首次病程記錄進行去重處理及自動差異化分析，從分析的結果中篩選出68.6\%的重複及相似短句。\\
titleTranslation: 基於自然語言處理的中文產科電子病歷研究}},
  note = {\begin{quotation}
张坤丽, 马鸿超, 赵悦淑, 等. 基于自然语言处理的中文产科电子病历研究[J]. 郑州大学学报(理学版), 2017, 49(4): 40-45.

\end{quotation}},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\EJKC9AC8\\张坤丽 等。 - 2017 - 基于自然语言处理的中文产科电子病历研究.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\2B5WZQ8H\\c95b7123-2803-492d-9bcd-64b071796bf1.html}
}

@article{ZhangLiBangJiYuWuJianDuXueXiDeZhongWenDianZiBingLiFenCi2014,
  title = {基于无监督学习的中文电子病历分词},
  author = {{张立邦} and {关毅} and {杨锦峰}},
  date = {2014},
  journaltitle = {智能计算机与应用},
  shortjournal = {INTELLIGENT COMPUTER AND APPLICATIONS},
  number = {2},
  pages = {68--71},
  doi = {10.3969/j.issn.2095-2163.2014.02.017},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5Eg1kbnh4MjAxNDAyMDE3GghneXk4dTQxbg%3D%3D},
  urldate = {2022-08-14},
  abstract = {电子病历中包含大量有用的医疗知识，抽取这些知识对于构建临床决策支持系统和个性化医疗健康信息服务具有重要意义。自动分词是分析和挖掘中文电子病历的关键基础。为了克服获取标注语料的困难，提出了一种基于无监督学习的中文电子病历分词方法。首先，使用通用领域的词典对电子病历进行初步的切分，为了更好地解决歧义问题，引入概率模型，并通过 EM算法从生语料中估计词的出现概率。然后，利用字串的左右分支信息熵构建良度，将未登录词识别转化为最优化问题，并使用动态规划算法进行求解。最后，在3000来自神经内科的中文电子病历上进行实验，证明了该方法的有效性。},
  langid = {zh\_CN},
  keywords = {Branching Entropy,Chinese EMRs,Dynamic Programming,EM Algorithm,EM算法,INTELLIGENT COMPUTER AND APPLICATIONS,Unsupervised Segmentation,中文电子病历,分支信息熵,动态规划,张立邦,无监督分词,智能计算机与应用},
  annotation = {哈尔滨工业大学 计算机科学与技术学院,哈尔滨,150001哈尔滨工业大学 计算机科学与技术学院,哈尔滨,150001哈尔滨工业大学 计算机科学与技术学院,哈尔滨,150001\\
2014-05-28 （万方平台首次上网日期，不代表论文的发表时间）\\
abstractTranslation:  電子病歷中包含大量有用的醫療知識，吸納這些知識對於構建臨床決策支持系統和個性化醫療健康信息服務具有重要意義。自動分詞是分析和挖掘中文電子病歷的關鍵基礎。為克服獲取標註語料的困難，提出了一種基於無監督學習的中文電子病歷分詞方法。首先，利用通用領域的字典對電子病歷進行初步的切分，為了更好地解決歧義問題，引入概率模型，並通過EM算法從生語料中估計詞的出現概率。然後，利用字串的左右分支信息熵構建良度，將未登錄詞識別轉化為最優化問題，並利用動態規划算法進行活動。最後，在3000來自神經內科的中文電子病歷上進行了實驗，證明了該方法的有效性。\\
titleTranslation: 基於無監督學習的中文電子病歷分詞},
  file = {C:\Users\BlackCat\Zotero\storage\4QRAINRG\张 等。 - 2014 - 基于无监督学习的中文电子病历分词.pdf}
}

@online{zhangMakingMIRACLMultilingual2022,
  title = {Making a {{MIRACL}}: {{Multilingual Information Retrieval Across}} a {{Continuum}} of {{Languages}}},
  shorttitle = {Making a {{MIRACL}}},
  author = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},
  date = {2022-10-18},
  eprint = {2210.09984},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.09984},
  url = {http://arxiv.org/abs/2210.09984},
  urldate = {2024-03-14},
  abstract = {MIRACL (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual dataset we have built for the WSDM 2023 Cup challenge that focuses on ad hoc retrieval across 18 different languages, which collectively encompass over three billion native speakers around the world. These languages have diverse typologies, originate from many different language families, and are associated with varying amounts of available resources -- including what researchers typically characterize as high-resource as well as low-resource languages. Our dataset is designed to support the creation and evaluation of models for monolingual retrieval, where the queries and the corpora are in the same language. In total, we have gathered over 700k high-quality relevance judgments for around 77k queries over Wikipedia in these 18 languages, where all assessments have been performed by native speakers hired by our team. Our goal is to spur research that will improve retrieval across a continuum of languages, thus enhancing information access capabilities for diverse populations around the world, particularly those that have been traditionally underserved. This overview paper describes the dataset and baselines that we share with the community. The MIRACL website is live at http://miracl.ai/.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,問答系統,嵌入,已整理,資料檢索,資料集},
  annotation = {titleTranslation: 創造奇蹟：跨語言連續體的多語言資訊檢索\\
abstractTranslation:  MIRACL（跨語言連續體的多語言資訊檢索）是我們為 WSDM 2023 Cup 挑戰賽構建的多語言資料集，重點關注 18 種不同語言的臨時檢索，這些語言總共涵蓋了全球超過 30 億的母語人士。這些語言有不同的類型，源自許多不同的語系，並且與不同數量的可用資源相關——包括研究人員通常描述的高資源和低資源語言。我們的資料集旨在支援單語言檢索模型的建立和評估，其中查詢和語料庫使用相同的語言。總的來說，我們針對維基百科上這 18 種語言的約 77,000 個查詢收集了超過 700,000 個高品質的相關性判斷，所有評估都是由我們團隊聘請的母語人士進行的。我們的目標是推動研究，改善跨語言的檢索，從而增強世界各地不同人群的資訊獲取能力，特別是傳統上服務不足的人群。這篇概述論文描述了我們與社區共享的資料集和基準。 MIRACL 網站現已上線：http://miracl.ai/。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\7RZ4BC4C\\Zhang 等。 - 2022 - Making a MIRACL Multilingual Information Retrieva.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\U23HGXJF\\2210.html}
}

@inproceedings{zhangMultimodalDialogSystem2021,
  title = {Multimodal {{Dialog System}}: {{Relational Graph-based Context-aware Question Understanding}}},
  shorttitle = {Multimodal {{Dialog System}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Multimedia}}},
  author = {Zhang, Haoyu and Liu, Meng and Gao, Zan and Lei, Xiaoqiang and Wang, Yinglong and Nie, Liqiang},
  year = {10 月 17, 2021},
  series = {{{MM}} '21},
  pages = {695--703},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3474085.3475234},
  url = {https://doi.org/10.1145/3474085.3475234},
  urldate = {2024-01-14},
  abstract = {Multimodal dialog system has attracted increasing attention from both academia and industry over recent years. Although existing methods have achieved some progress, they are still confronted with challenges in the aspect of question understanding (i.e., user intention comprehension). In this paper, we present a relational graph-based context-aware question understanding scheme, which enhances the user intention comprehension from local to global. Specifically, we first utilize multiple attribute matrices as the guidance information to fully exploit the product-related keywords from each textual sentence, strengthening the local representation of user intentions. Afterwards, we design a sparse graph attention network to adaptively aggregate effective context information for each utterance, completely understanding the user intentions from a global perspective. Moreover, extensive experiments over a benchmark dataset show the superiority of our model compared with several state-of-the-art baselines.},
  isbn = {978-1-4503-8651-7},
  langid = {english},
  keywords = {attribute-enhanced text representation,multimodal dialog system,sparse relational context modeling,未整理},
  annotation = {17 citations (Crossref) [2024-03-26]\\
titleTranslation: 多模態對話系統：基於關係圖的上下文感知問題理解}
}

@thesis{ZhangTingGuangLiYongSuoJianSouSuoKongJianJiaSuZiDongBianYiQiZuiJiaHuaXuanXiangXuanZe2011,
  title = {利用縮減搜索空間加速自動編譯器最佳化選項選擇},
  author = {{張廷光}},
  namea = {{林迺衛} and {naiwei Lin}},
  nameatype = {collaborator},
  date = {2011},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/6253y9},
  abstract = {現今的編譯器都會提供大量的最佳化選項，讓使用者可以自由的來針對所編寫的程式作最佳化，但是對大部分的使用者而言，要熟悉這些最佳化選項的作用並且很好的運用是一件很困難的事情。基因演算法雖然適合用來處理此類的問題並且可以有不錯的效果，但是演化所需的時間是一大挑戰。在本研究中我們提出使用UROM將對程式可能沒有作用的選項先行排除，平均可以減少46.62\%左右的搜索空間，進而得以降低演化時的族群規模。並提出依照GCC最佳化執行的順序將最佳化選項大致分為四群，以多階段的方式依序找出每一階段的最佳解。平均可以減少71.53\%的演化時間。},
  pagetotal = {59}
}

@article{ZhangXiangJuJuYouZhongYiTeSeDeZhengTiHuaDianZiBingLiXiTongGouJian2011,
  title = {具有中医特色的整体化电子病历系统构建},
  author = {{张湘菊} and {魏丹蕾}},
  date = {2011},
  journaltitle = {医学信息学杂志},
  volume = {32},
  number = {004},
  pages = {22--24},
  doi = {10.3969/j.issn.1673-6036.2011.04.006},
  abstract = {从中医院实际情况出发,介绍中医医嘱,中医病历等具有中医特色的整体化电子病历系统的建设内容,优势,特点和实际应用情况,利用该系统实现病历信息的自动复制,结构化处理和无纸化传递,有助于提高临床各科室人员工作效率.},
  keywords = {中醫,電子病歷},
  file = {C:\Users\BlackCat\Zotero\storage\ZFCV3L52\张湘菊 與 魏丹蕾 - 2011 - 具有中医特色的整体化电子病历系统构建.pdf}
}

@thesis{ZhangXiuJuanYiGeKongZhiPingXingYuYanZaiDuiChengShiDuoChuLiQiShangDeYanZhi2001,
  title = {一個控制平行語言在對稱式多處理器上的研製},
  author = {{張秀娟}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2001},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/92qecz},
  abstract = {一般而言，撰寫平行程式要比撰寫循序程式來得困難許多，所以發展一個高階的平行程式語言來幫助程式設計師能夠更容易撰寫平行程式是非常必要的。在這篇論文中，我們發展出一高階平行程式語言CCC，它是程式語言C的簡單延伸。CCC除了保留傳統C原有的功能外，還提供了支援平行執行的機制來幫助我們撰寫出控制平行程式。在平行機制方面，CCC提供可以被平行執行的函式。並且用結構化的方式產生平行執行的工作。在溝通機制方面，CCC同時提供了訊息傳遞與共享變數兩種溝通機制，讓使用者依據應用程式選擇最適當的溝通方式。 CCC編譯器的後端分成兩部分：語法樹轉換器以及目的碼生成器。語法樹轉換器會將前端所產生的CCC語法樹轉換成多執行緒的C語法樹當中間碼。目的碼生成器再利用此轉換過的語法樹來產生平行程式的目的碼。針對不同的系統，這樣的架構只需改變目的碼生成器，不需改變語法樹轉換器。因此，讓我們的CCC編譯器保有較高的可重標的性。在論文中，我們也用CMU大學所提出的控制平行程式組來評估我們編譯器的效能。實驗結果顯示CCC編譯器產生的程式和手寫的程式大約相差在10\%以內。},
  pagetotal = {144}
}

@article{ZhangYanQiongSoFDAMianXiangJingZhunYiXueDeZhongYiZhengHouBenTiXinXiCunChuJiBingZhengFang2022,
  title = {{{SoFDA}}——面向精准医学的中医证候本体信息存储及"病-证-方"多维关联计算的整合平台},
  author = {{张彦琼} and {王宁} and {杜霞} and {陈同} and {于泽从} and {秦月雯} and {陈文佳} and {于梦} and {王萍} and {张华敏}},
  date = {2022},
  journaltitle = {中国科学通报：英文版},
  number = {011},
  pages = {067},
  url = {https://www.nstl.gov.cn/paper_detail.html?id=4e7642d93eb85350835c8f329185a086},
  urldate = {2023-10-16},
  abstract = {Clinical manifestations of symptoms play a crucial role in the diagnosis and appropriate treatment of diseases and are considered one of the main clinical features for contemporary disease taxonomy(i.e.,international classification of diseases,ICD)[1].Deep investigation on molecular connections among symptoms is one of the key tasks for developing a disease-specific knowledge network and thus promoting the refinement of disease taxonomy toward precision medicine[2].},
  keywords = {No DOI found,信息存储,多维关联,精准医学}
}

@article{ZhangYiZhuoBianZhengLunZhiSiXiangZhiDaoXiaDeZhongYiZhuTiCiZiDongBiaoYinMoXingGouJian2022,
  title = {辨证论治思想指导下的中医主题词自动标引模型构建},
  author = {{张异卓} and {周璐} and {孙燕} and {郑丰杰} and {徐凤芹} and {李宇航}},
  date = {2022},
  journaltitle = {中国中医药信息杂志},
  shortjournal = {Chinese Journal of Information on Traditional Chinese Medicine},
  volume = {29},
  number = {8},
  pages = {18--23},
  doi = {10.19879/j.cnki.1005-5304.202109202},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhJ6Z3p5eXh4enoyMDIyMDgwMDQaCGgyMXpraGJx},
  urldate = {2022-08-14},
  abstract = {目的 在辨证论治思想指导下构建中医主题词自动标引模型,为相关研究提供参考.方法 收集2019年12月-2020年12月中国中医科学院"名医名家传承"项目管理平台记录的22位名老中医电子病历,在辨证论治思想指导下对病历中的症状和证候进行主题词标引,并采用Tensorflow人工智能模型构建工具、双向编码表示(BERT)语言处理模型、Sigmoid函数及统一计算架构(CUDA)技术构建中医主题词自动标引模型,以准确率、精确率、召回率、F1得分为指标对模型进行评价.结果 在对症状和证候的主题词标引中,基于BERT的中医主题词自动标引模型各项指标表现最优,精确率与召回率均达87％以上.结论 本研究在辨},
  langid = {zh\_CN},
  keywords = {中醫,主題詞,機器學習,症狀標準化,自动标引模型,辨證},
  annotation = {北京中医药大学中医学院,北京 100029中国中医科学院西苑医院,北京 100091\\
国家重点研发计划\\
2022-08-05 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 辨證論治思想指導下的中醫主題詞自動標引模型構建\\
abstractTranslation:  目的在辨證論治思想指導下構建中醫主題詞自動標引模型，為相關研究提供參考。方法收集2019年12月-2020年12月中國科學院中醫“名醫名家傳承”項目管理平台記錄的22位名老中醫電子病歷，在辨證論治思想指導下對病歷中的症狀和證候進行主題詞引，並採用Tensorflow人工智能模型構建工具、結構編碼表示(BERT)語言處理模型、Sigmoid函數及統一計算架構(CUDA)技術構建中醫主題詞自動標引模型，以準確率、準確率、召回率、F1得分為指標對模型進行評價。結果在對症狀和證候的主題詞標引中，基於BERT的中醫主題詞自動標引模型指標引模型各項指標表現最優，準確率與召回率均達到87\%以上。結論本研究在明確},
  note = {將原始病歷經過人工標記後交給BERT學習症狀標準化的方式},
  file = {C:\Users\BlackCat\Zotero\storage\LAY9UB7S\张 等。 - 2022 - 辨证论治思想指导下的中医主题词自动标引模型构建.pdf}
}

@thesis{ZhangYongShenShiZuoYiGeJianJinShiYuFaFenXiQiDeChanShengQi2005,
  title = {實作一個漸進式語法分析器的產生器},
  author = {{張永伸}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2005},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/7pjzr6},
  abstract = {快速地找出程式中含有的語法錯誤，一直是軟體開發過程中，很重要的問題。在現在應用程式日趨複雜的情況下，盡快找出程式中含有的語法錯誤顯得更為重要。漸進式語法分析是一個可以幫助使用者在程式編輯時期便找到語法錯誤並修正的方法。藉由漸進式語法分析器的幫助，使用者不必等到完成程式編輯，再交由編譯器處理，才可得知程式中的語法錯誤，如此可縮短軟體開發的時間。 本篇論文提出了一個漸進式語法分析器產生器之實作，以及一個結合漸進式語法分析器與編輯器的應用程式介面。本論文實作之漸進式語法分析器產生器奠基於目前使用最多的Bison 語法分析器產生器。藉由漸進式語法分析器產生器和應用程式介面的幫助，使用者可以自行製作一個可針對自定語言進行漸進式語法分析的編輯器。},
  pagetotal = {102}
}

@article{ZhangYunZhongJiYuDuoShenJingWangLuoXieZuoDeDianZiBingLiMingMingShiTiShiBieFangFa2021,
  title = {基于多神经网络协作的电子病历命名实体识别方法},
  author = {{张运中} and {纪斌} and {余杰} and {刘慧君}},
  date = {2021},
  journaltitle = {计算机应用与软件},
  shortjournal = {Computer Applications and Software},
  volume = {38},
  number = {2},
  pages = {179--184},
  doi = {10.3969/j.issn.1000-386x.2021.02.030},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5EhFqc2p5eXlyajIwMjEwMjAzMRoIYTl3dXluajY%3D},
  urldate = {2022-08-14},
  abstract = {随着电子病历在医疗领域的推广应用,越来越多的研究者关注如何高效地从电子病历中抽取高价值科研信息.CHIP2018将中文电子病历临床医疗命名实体识别作为评测任务,即从中文电子病历中抽取三种恶性肿瘤相关的实体.结合三种实体的特点和实体间的依赖关系,提出基于多神经网络协作的复杂医疗命名实体识别方法,并实现了句子级别的模型迁移,解决了训练数据集数量和质量问题,最终获得了该评测任务的第二名.此外,该方法的改进方法取得了CCKS2019评测任务一的第一名,印证了其有效性和泛化能力.},
  langid = {zh\_CN},
  keywords = {BiLSTM-CRF,Computer Applications and Software,中文电子病历,余杰,命名实体识别,张运中,模型迁移,泛化,神经网络,纪斌,计算机应用与软件},
  annotation = {湖南省电子口岸服务中心 湖南 长沙 410001国防科技大学计算机学院 湖南 长沙 410073中国工程物理研究院计算机应用研究所 四川 绵阳 621999\\
装备预研项目\\
2021-03-09 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 基於多神經網絡協作的電子病歷命名實體識別方法\\
abstractTranslation:  隨著電子病歷在醫療領域的推廣應用，越來越多的研究者關注如何高效地從電子病歷中提取高價值科研信息。CHIP2018將中文電子病歷臨床醫療命名實體作為營養任務，即來自中文電子病歷結合突變關係實體的特徵和實體間的依賴，提出了基於多神經網絡協作的複雜醫療命名實體識別方法，並實現了句子級別的模型遷移，解決了數據集數量和質量問題，最終獲得了該體育任務的第二名。此外，該方法的改進方法取得了CCKS2019體育任務的第一名，印證了其有效性和泛化能力。},
  file = {C:\Users\BlackCat\Zotero\storage\CKJX2GDM\张 等。 - 2021 - 基于多神经网络协作的电子病历命名实体识别方法.pdf}
}

@thesis{ZhangZhaoXiangCeShiWuJianChuShiHuaChengXuDeZiDongChanSheng2019,
  title = {測試物件初始化程序的自動產生},
  author = {{張朝翔}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2019},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/qaurua},
  abstract = {軟體測試的測試案例包含測試輸入物件及預期輸出物件。這些測試輸入物件在執行測試前必須先初始化到適當的狀態。一個測試物件的初始化序列是一串創建及擴充測試物件的函式呼叫。本論文將測試物件的初始化序列產生問題定義成一個限制滿足問題。測試物件初始化序列的限制集合是從狀態圖規格及物件限制語言規格推導出來的，這個限制集合接著再轉成限制邏輯程式求解。換句話說，本論文將不可執行的狀態圖規格及物件限制語言規格自動轉成可執行的限制邏輯程式規格。給予一串初始化序列，這個可執行的規格可以模擬狀態圖，推導出這串初始化序列導致的物件目的狀態。根據限制邏輯程式的一致化機制，反過來，給予一個物件的目的狀態，這個可執行的規格也可以模擬狀態圖，推導出導致這個目的狀態的一串初始化序列。 關鍵字: 軟體測試;測試物件初始化程序;限制滿足問題;限制邏輯程式;狀態圖;物件限制語言},
  pagetotal = {124},
  keywords = {實驗室}
}

@thesis{ZhangZhengMingYiGeCCCZiLiaoPingXingChengShiYuYanDeKeChongBiaoDeBianYiQi2001,
  title = {{{一個CCC資料平行程式語言的可重標的編譯器}}},
  author = {{張正銘}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2001},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/f2p358},
  abstract = {一般而言,撰寫平行程式比撰寫循序程式困難,所以提供一個高階的平行程式語言幫助程式設計師更容易撰寫平行程式是非常重要的,而我們所發展的高階平行程式語言CCC是C的延伸。CCC提供三個主要的性質來支援資料平行程式的平行計算環境,分別是虛擬處理器、同步執行模式、全域命名.依CCC的發展始意而言,我們希望用CCC撰寫平行程式可以比用來發展平行程式的低階訊息傳遞函式庫(如MPI和PVM)更具有生產力,並且能產生比隱性的平行程式語言(如HPF和HPC++)更有效率的程式碼。 CCC所擁有的三種性質讓程式設計師可以不需考慮太多與硬體相關的細節問題。從虛擬計算環境至實際的計算環境的轉換過程是由編譯器來處理,此轉換過程包含兩個主要步驟：虛擬處理器的模擬以及目的碼的生成。其中虛擬處理器的模擬只與實際計算環境中處理器個數的總和有關,是以CCC至CCC的轉換方式來實作。因此我們對虛擬處理器的模擬幾乎完全與底層的硬體細節無關,大部分與底層硬體相關的程式碼是在目的碼產生中編譯的。進一步來說,此編譯結構讓我們的編譯器保有極高的可重標的性。},
  pagetotal = {179}
}

@thesis{ZhangZhenHongXianZhiShiBaiXiangHanShiCengJiDanYuanCeShiAnLiChanShengQi2017,
  title = {限制式白箱函式層級單元測試案例產生器},
  author = {{張振鴻}},
  namea = {{林迺衛} and {NAI-WEI LIN}},
  nameatype = {collaborator},
  date = {2017},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/36b93f},
  abstract = {本論文將測試案例產生問題定義成限制滿足問題，並實作一個限制式白箱函式層級單元測試案例產生器。一個測試案例包含一個測試輸入及其對應的預期輸出。針對測試輸入，這個工具先將程式碼轉換成測試模型，限制邏輯圖。限制邏輯圖以圖形的方式描述函式的實際行為或函式輸入及輸出的限制邏輯關係。在限制邏輯圖上，這個工具接著用圖形的方式進行函式行為的等價類別分割及測試覆蓋標準管理，以篩選數量少但覆蓋廣的代表性行為來測試。產生每個代表性行為的輸入可以被定義成一個限制滿足問題。這個限制滿足問題可以用限制邏輯程式來描述並求解。針對預期輸出，物件限制語言所定義的函式預期行為也可以被視為一個限制滿足問題。因為物件限制語言不能執行，這個工具會將物件限制語言的描述轉換成可執行的限制邏輯程式的描述，並依代表性行為的輸入解出對應的預期輸出。這個工具的特色是使用一致的技術來分別產生白箱函式層級單元測試案例的測試輸入及預期輸出。 關鍵詞-單元測試、白箱測試、限制式測試測試案例產生、函式層級測試、限制邏輯程式},
  pagetotal = {80}
}

@thesis{ZhangZhenYuanZhongYiZhenDuanXiTongDeSheJiYuShiZuo2020,
  title = {中醫診斷系統的設計與實作},
  author = {{張鎮遠}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2020},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/cnahar},
  abstract = {本研究團隊已開發一個中醫診斷系統。經過數年的開發，系統逐漸龐雜，效率不高且維護不易。本論文描述重構這個中醫診斷系統的成果。這個系統是一個基於模型-視圖-控制器框架的網頁應用程式。本論文根據中醫自動診斷的特色，詳細描述模型模組、視圖模組、及控制器模組重構的成果。重構方式包含模組化降低程式碼耦合度，物件化提高程式碼重複使用率，資料庫簡化減少資料表大小，整數比對取代字串比對降低系統執行時間等。效能評估顯示資料庫資料表大小減少25\%，程式碼大小減少84\%，系統執行時間減少68\%。系統重構後，不僅效率提高且易於維護。},
  pagetotal = {52},
  keywords = {實驗室},
  file = {C:\Users\BlackCat\Zotero\storage\FH8SQNBH\張鎮遠 - 2020 - 中醫診斷系統的設計與實作.pdf}
}

@thesis{ZhangZhiGuangHanShiCengJiDanYuanCeShiZhiXianZhiShiHeiXiangCeShiAnLiChanShengQiYanZhi2015,
  title = {函式層級單元測試之限制式黑箱測試案例產生器研製},
  author = {{張智光}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2015},
  journaltitle = {資訊工程研究所},
  volume = {博士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/57ss76},
  abstract = {進行討論，諸如：訂定軟體行為的規格、於軟體行為中區隔出具代表性的行為區塊、管理測試案例的產生，以達到預訂之軟體品質、如何於軟體行為區塊中，產生出具代表性的測試輸入與輸出、如何準備適合的測試前置環境，以利測試案例的進行。 在這個研究裡，我們設計並實作了一個「函式層級單元測試之限制式黑箱測試案例產生器」，我們將測試案例的產生轉換成「滿足限制式之求解問題」。軟體的規格與行為採用 UML 與 OCL 來描述；透過將 OCL 轉換成「限制邏輯圖」（constraint logic graph），可自動化地產生具代表性的軟體行為區塊，並可依照所訂定之軟體品質約束行為區塊產生的數量。「限制邏輯圖」可視為將限制式的 DNF（disjunctive normal form）格式，以結構化的方式呈現出來。每一個「限制邏輯圖」的完整路徑，即為一個具代表性的行為區塊，可用來作產生測試案例的產生。我們將「滿足限制式之求解問題」，以「限制邏輯語言」來表達；透過「限制邏輯語言」的求解系統，測試案例所需的測試輸入、測試輸出與測試前置環境可以同時自動地產生出來。 我們所實作的測試案例產生器，提供了console mode 與 Eclipse plugin. Console mode 可提供純文字介面的測試案例產生與驗證；Eclipse plugin 與Eclipse IDE 無縫整合，提供可完整而便利的使用方式。經過初步的效能評估，此一測試案例產生器，可於短時間內（幾十秒）產生數十筆測試案例。 基於這份研究，未來可朝以下幾個方向進行研究：支援白箱的測試案例產生、支援類別層級的單元測試案例之產生與支援不同平台的測試案例的產生。},
  pagetotal = {148}
}

@online{zhaoExplainabilityLargeLanguage2023,
  title = {Explainability for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Explainability for {{Large Language Models}}},
  author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  date = {2023-11-28},
  eprint = {2309.01029},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.01029},
  url = {http://arxiv.org/abs/2309.01029},
  urldate = {2024-04-27},
  abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,LLM,Survey,未整理,重要},
  annotation = {titleTranslation: 大型語言模型的可解釋性：調查\\
abstractTranslation:  大型語言模型 (LLM) 在自然語言處理方面展示了令人印象深刻的能力。然而，它們的內部機制仍不清楚，缺乏透明度為下游應用帶來了不必要的風險。因此，理解和解釋這些模型對於闡明它們的行為、限制和社會影響至關重要。在本文中，我們介紹了可解釋性技術的分類，並提供了用於解釋基於 Transformer 的語言模型的方法的結構化概述。我們根據法學碩士的培訓範式對技術進行分類：傳統的基於微調的範式和基於提示的範式。對於每個範式，我們總結了產生個體預測的局部解釋和整體模型知識的全局解釋的目標和主要方法。我們還討論了評估產生的解釋的指標，並討論瞭如何利用解釋來調試模型和提高性能。最後，我們與傳統機器學習模型相比，研究了法學碩士時代解釋技術的主要挑戰和新興機會。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\UHGIY4TV\\Zhao 等。 - 2023 - Explainability for Large Language Models A Survey.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\MRL47VUQ\\2309.html}
}

@thesis{ZhaoFangFangMianXiangZhongWenDianZiBingLiDeCiXingBiaoZhuJiShuYanJiu2014,
  type = {硕士},
  title = {面向中文电子病历的词性标注技术研究},
  author = {{赵芳芳}},
  namea = {{关毅}},
  nameatype = {collaborator},
  date = {2014},
  institution = {哈尔滨工业大学},
  doi = {10.7666/d.D593472},
  url = {http://d.wanfangdata.com.cn/thesis/ChJUaGVzaXNOZXdTMjAyMjA1MjYSB0Q1OTM0NzIaCHF0ZTVwZDhi},
  urldate = {2022-08-14},
  abstract = {随着大数据时代的到来，“智慧医疗”已经成为全球医疗服务产业的发展趋势。作为医疗信息化的载体，电子病历蕴含大量的医疗健康知识。电子病历中的知识可以为医疗诊断、用户健康管理及医疗协调等领域提供服务。挖掘电子病历中的知识离不开自然语言处理及信息抽取技术。词性标注是自然处理技术的基础，对其进行研究有助于后续句法分析及信息抽取任务的展开。　　由于标注语料的匮乏，目前面向中文电子病历的分词和词性标注研究还处于空白阶段。与开放领域语料不同，中文电子病历含有大量的专业术语、缩略词和模式。因此，开放领域的词性标注模型并不能直接用于中文电子病历的标注。　　为了更好地进行词性标注模型的研究，本文首先构建了中文电子病},
  langid = {zh\_CN},
  keywords = {中文电子病历,联合模型,词性标注,语料构建,赵芳芳},
  annotation = {2015-08-17 （万方平台首次上网日期，不代表论文的发表时间）\\
abstractTranslation:  隨著大數據時代的到來，“智慧醫療”已成為全球醫療服務產業的發展趨勢。作為醫療信息化的載體，電子病歷中蘊藏著大量的醫療健康知識。電子病歷中的知識可以為醫療診斷、用戶健康提供服務管理及醫療協調等領域提供服務。挖掘電子病歷中的知識抽取自然語言處理及信息抽取技術。詞性標籤是自然處理技術的基礎，由此進行的研究有助於後續句法分析及信息抽取任務的由於標註語料的匱乏，目前針對中文電子病歷的分詞和詞性標註研究還處於空白階段。與開放領域語料不同，中文電子病歷含有大量的專業術語、縮略詞和模式。因此，開放領域的詞性標註模型並不能直接用於中文電子病歷的標註。為了更好地進行詞性標註模型的研究，本文首先構建了中文電子病歷\\
titleTranslation: 針對中文電子病歷的詞性標註技術研究},
  file = {C:\Users\BlackCat\Zotero\storage\DH66S2KD\赵 - 2014 - 面向中文电子病历的词性标注技术研究.pdf}
}

@incollection{zhaohuiwuTextMiningFinding2004,
  title = {Text {{Mining}} for {{Finding Functional Community}} of {{Related Genes Using TCM Knowledge}}},
  booktitle = {Knowledge {{Discovery}} in {{Databases}}: {{PKDD}} 2004},
  author = {{Zhaohui Wu} and {Xuezhong Zhou} and {Baoyan Liu} and {Junli Chen}},
  editor = {{Jean-François Boulicaut} and {Floriana Esposito} and {Fosca Giannotti} and {Dino Pedreschi}},
  editora = {{David Hutchison} and {Takeo Kanade} and {Josef Kittler} and {Jon M. Kleinberg} and {Friedemann Mattern} and {John C. Mitchell} and {Moni Naor} and {Oscar Nierstrasz} and {C. Pandu Rangan} and {Bernhard Steffen} and {Madhu Sudan} and {Demetri Terzopoulos} and {Dough Tygar} and {Moshe Y. Vardi} and {Gerhard Weikum}},
  editoratype = {redactor},
  date = {2004},
  volume = {3202},
  pages = {459--470},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-30116-5_42},
  url = {http://link.springer.com/10.1007/978-3-540-30116-5_42},
  urldate = {2022-09-19},
  isbn = {978-3-540-23108-0 978-3-540-30116-5},
  file = {C:\Users\BlackCat\Zotero\storage\GEBYIEZZ\Wu 等。 - 2004 - Text Mining for Finding Functional Community of Re.pdf}
}

@online{zhengHelpfulAssistantBest2023,
  title = {Is "{{A Helpful Assistant}}" the {{Best Role}} for {{Large Language Models}}? {{A Systematic Evaluation}} of {{Social Roles}} in {{System Prompts}}},
  shorttitle = {Is "{{A Helpful Assistant}}" the {{Best Role}} for {{Large Language Models}}?},
  author = {Zheng, Mingqian and Pei, Jiaxin and Jurgens, David},
  date = {2023-11-16},
  eprint = {2311.10054},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.10054},
  url = {http://arxiv.org/abs/2311.10054},
  urldate = {2024-05-04},
  abstract = {Prompting serves as the major way humans interact with Large Language Models (LLM). Commercial AI systems commonly define the role of the LLM in system prompts. For example, ChatGPT uses "You are a helpful assistant" as part of the default system prompt. But is "a helpful assistant" the best role for LLMs? In this study, we present a systematic evaluation of how social roles in system prompts affect model performance. We curate a list of 162 roles covering 6 types of interpersonal relationships and 8 types of occupations. Through extensive analysis of 3 popular LLMs and 2457 questions, we show that adding interpersonal roles in prompts consistently improves the models' performance over a range of questions. Moreover, while we find that using gender-neutral roles and specifying the role as the audience leads to better performances, predicting which role leads to the best performance remains a challenging task, and that frequency, similarity, and perplexity do not fully explain the effect of social roles on model performances. Our results can help inform the design of system prompts for AI systems. Code and data are available at https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,LLM,Prompt,未整理},
  annotation = {titleTranslation: 「得力助手」是大型語言模型的最佳角色嗎？系統提示中社會角色的系統性評價\\
abstractTranslation:  提示是人類與大型語言模型（LLM）互動的主要方式。商業人工智慧系統通常在系統提示中定義法學碩士的角色。例如，ChatGPT 使用「你是一個有用的助手」作為預設系統提示的一部分。但「樂於助人的助手」是法學碩士的最佳角色嗎？在這項研究中，我們對系統提示中的社會角色如何影響模型表現進行了系統性評估。我們整理了 162 個角色列表，涵蓋 6 種人際關係和 8 種職業。透過對 3 個熱門法學碩士和 2457 個問題的廣泛分析，我們表明，在提示中添加人際角色可以持續提高模型在一系列問題上的表現。此外，雖然我們發現使用性別中立的角色並將角色指定為觀眾會帶來更好的表演，但預測哪個角色會帶來最佳表演仍然是一項具有挑戰性的任務，並且頻率、相似性和困惑度並不能完全解釋效果社會角色對模特兒表演的影響。我們的結果可以幫助為人工智慧系統的系統提示設計提供資訊。程式碼和資料可在 https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles 取得。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\WFKZBR7D\\Zheng 等。 - 2023 - Is A Helpful Assistant the Best Role for Large L.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\JUU764MI\\2311.html}
}

@thesis{ZhengMingHuiRuanShiJiSuanYuShiXunChaoJieXiDuZhiYingYong2013,
  title = {軟式計算於視訊超解析度之應用},
  author = {{鄭茗徽}},
  namea = {{黃國勝} and {林迺衛} and {Kao-Shing Hwang} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2013},
  journaltitle = {資訊工程研究所},
  volume = {博士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/b9e39m},
  abstract = {隨著多媒體與通訊裝置的蓬勃發展，顯示器解析度的增加已成為未來必然的趨勢，舊有低解析度視訊影片，在新興高解析度顯示裝置中撥放，已無法滿足使用者的視覺需求。視訊超解析度提升(video super-resolution)，即在此科技需求中孕育而生。視訊超解析度提升方法，是將低解析度視訊中的影像，與前後多張影像一併考慮處理後放大為高解析度影像。視訊超解析度提升時，會考慮該視訊中前後多張具有時間關聯性的低解度影像，彼此在空間與時間上的特徵資訊進行影像的放大，因此在影像提升後會具有較佳的視覺影像品質。 軟式計算(soft computing)可區分為最佳化演算法(optimization algorithm)和機器學習(machine learning)兩類。常見的最佳化演算法有粒子群最佳化(particle swarm optimization, PSO)與基因演算法(genetic algorithm, GA)等。機器學習方面最常用的有類神經網路(artificial neural network, ANN)與支撐向量機(support vector machine, SVM)等。粒子群最佳化演算法是一種以族群為基礎的迭代演算法，可以在較短的訓練時間內獲得可靠的近似解，其可以較低的運算來解決高複雜性問題。類神經網路是模擬生物神經元訊息處理，與傳導結構的數學模型，可以利用統計學中的統計模型加以詮釋，因此，成為數學統計學習方法中可應用於實際問題處理的模型。 本論文中，我們首先提出一種採用影像融合方式的視訊超解析度(super-resolution)提升方法。在本方法中，我們使用視訊運動補償(motion compensation)與影像內插方法，分別產生四張解析度較佳的視訊影像。隨後，利用時間與空間上的特徵資訊，對要處理的視訊影像進行分類，依據分類的結果，使用粒子群最佳化演算法來找尋可靠且有效的融合參數，對四張解析度較佳的視訊影像進行融合成為超解析度視訊影像(super-resolved frame)。 第二個部分，我們以nonlocal-means (NLM)視訊超解析度強化方法為基礎，提出可移動式之視訊運動搜尋(motion search)方法，在減低視訊運動搜尋計算量的同時，進一步保持運動搜尋效果。此外，一個適應性的patch大小調整方法，則使用來提升超解析度視訊影像的視覺效果。 第三個部分，我們提出一個以類神經網路學習方法為基礎的視訊超解析度提升方法。在本方法中，我們利用視訊運動搜尋方法，對視訊影像收集適當的訓練資料供ANN進行訓練，依據訓練結果所得之參數與權重可以有效提升視訊影像的解析度。我們也在此基礎下，加入適當的分類方法，對要提升的視訊影像進行分類處理，以改善超解析度視訊影像中物體邊緣的視覺影像品質。此外，一個以雙向濾波器為基礎的方法則被運用來對收集的訓練資料進行前處理，以進一步改善類神經網路的學習效果，並提升超解析度視訊影像的品質。實驗結果顯示，本論文所提出的三個方法皆可有效改善視訊超解析度於影像提升後的品質。 本論文研究之主要貢獻，在利用所提之方法提取出空間與時間特徵，並與軟式計算方法加以結合運用在視訊超解析提升上，最後以實驗結果證明所提之概念的可行性與有效性。},
  pagetotal = {99}
}

@thesis{ZhengXiYouYingWenBanZhongYiZhengZhuangZhiShiBenTiZhiYanZhi2018,
  title = {英文版中醫症狀知識本體之研製},
  author = {{鄭璽佑}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2018},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/y6xg6a},
  abstract = {知識本體是某一領域相關概念的知識庫，包含這些概念具共識的術語及其定義，也包含概念與概念之間的關聯。一個領域知識本體的建構是該領域資訊化的基石。該領域的資訊系統可以透過知識本體自動分析及分享該領域具共識的知識。本研究團隊先前已進行中醫症狀的標準化，並研製一個中醫症狀知識本體。該症狀知識本體用四個性質來定義一個症狀：症狀物件、症狀屬性、症狀程度、症狀期間，這種方式可以定義大部分的中醫症狀。 本論文在中文版中醫症狀知識本體的基礎上，研製一個英文版中醫症狀知識本體。本論文在中醫症狀知識本體中，為每一個症狀、症狀物件、症狀屬性、症狀程度、及症狀期間，增加英文名稱及英文定義這兩個資料性質。運用中文名稱及英文名稱的對應，英文版中醫症狀知識本體的搜尋可以透過中文版中醫症狀知識本體的搜尋來完成。 關鍵字: 中醫症狀標準化；中醫症狀知識本體；英文版中醫症狀知識本體。},
  pagetotal = {35},
  keywords = {中醫,實驗室,標準化,病歷分析,知識本體},
  file = {C:\Users\BlackCat\Zotero\storage\8ZJBCPHA\鄭璽佑 - 2018 - 英文版中醫症狀知識本體之研製.pdf}
}

@thesis{ZhengZongQiYiGeJavaSwingDeCeShiAnLiChanShengQi2008,
  title = {{{一個JavaSwing的測試案例產生器}}},
  author = {{鄭宗其}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2008},
  journaltitle = {資訊工程所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/wmnpxz},
  abstract = {軟體測試的目標在於確保軟體的品質。現在的應用程式中圖形使用者介面的程式碼所佔的比例很高，所以圖形使用者介面的測試相當重要。圖形使用者介面測試大多利用錄製播放的技術，需要龐大的人為工作。 本篇論文實作一個 Java Swing 的測試案例產生器，這個工具使用一個異於錄製播放的技術，奠基於 UML 的使用案例圖及活動圖。這個工具由三個部份組成，第一部份，UML 圖形嬝知飽A讀取 UML 的使用案例圖與活動圖。第二部份，測試路徑產生器，根據三種覆遞郱ョA走訪 UML 圖形來產生測試路徑。第三部份，測試程式產生器，依照測試路徑與測試者給予的測試資料，來產生測試程式碼。},
  pagetotal = {54},
  keywords = {實驗室,軟測}
}

@article{zhenhaoliProvidingAutomatedSupports2020,
  title = {Towards Providing Automated Supports to Developers on Writing Logging Statements},
  author = {{Zhenhao Li}},
  date = {2020-06-27},
  journaltitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
  pages = {198--201},
  publisher = {ACM},
  location = {Seoul South Korea},
  doi = {10.1145/3377812.3381385},
  url = {https://dl.acm.org/doi/10.1145/3377812.3381385},
  urldate = {2023-10-20},
  abstract = {Developers write logging statements to generate logs and record system execution behaviors. Such logs are widely used for a variety of tasks, such as debugging, testing, program comprehension, and performance analysis. However, there exists no practical guidelines on how to write logging statements; hence, making the logging decision a very challenging task. There are two main challenges that developers are facing while making logging decisions: 1) Difficult to accurately and succinctly record execution behaviors; and 2) Hard to decide where to write logging statements. This thesis proposes a series of approaches to address the problems and help developers make logging decisions in two aspects: assist in making decisions on logging contents and on logging locations. Through case studies on large-scale open source and commercial systems, we anticipate that our study will provide useful suggestions and supports to developers for writing better logging statements.},
  eventtitle = {{{ICSE}} '20: 42nd {{International Conference}} on {{Software Engineering}}},
  isbn = {9781450371223},
  langid = {english},
  keywords = {已整理,待讀,研究流程/程式,重要},
  annotation = {7 citations (Crossref) [2024-03-26]\\
titleTranslation: 为开发人员编写日志语句提供自动支持\\
abstractTranslation:  开发人员编写日志语句是为了生成日志并记录系统执行行为。这些日志被广泛用于各种任务，如调试、测试、程序理解和性能分析。然而，目前还没有关于如何编写日志语句的实用指南；因此，日志记录决策是一项非常具有挑战性的任务。开发人员在做出日志记录决定时面临两大挑战：1) 难以准确、简洁地记录执行行为；2) 难以决定在何处编写日志语句。本论文提出了一系列方法来解决这些问题，并从两个方面帮助开发人员做出日志记录决策：协助决策日志记录内容和日志记录位置。通过对大型开源系统和商业系统的案例研究，我们预计本研究将为开发人员编写更好的日志语句提供有用的建议和支持。},
  note = {[TLDR] This thesis proposes a series of approaches to address the problems and help developers make logging decisions in two aspects: assist in making decisions on logging contents and on logging locations.}
}

@article{zhichenOPALOntologyAwarePretrained2023,
  title = {{{OPAL}}: {{Ontology-Aware Pretrained Language Model}} for {{End-to-End Task-Oriented Dialogue}}},
  shorttitle = {{{OPAL}}},
  author = {{Zhi Chen} and {Yuncong Liu} and {Lu Chen} and {Su Zhu} and {Mengyue Wu} and {Kai Yu}},
  date = {2023-01-12},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {68--84},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00534},
  url = {https://transacl.org/index.php/tacl/article/view/4067},
  urldate = {2023-09-15},
  abstract = {This paper presents an ontology-aware pretrained language model (OPAL) for end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models, task-oriented dialogue models fulfill at least two task-specific modules: dialogue state tracker (DST) and response generator (RG). The dialogue state consists of the domain-slot-value triples, which are regarded as the user's constraints to search the domain-related databases. The large-scale task-oriented dialogue data with the annotated structured dialogue state usually are inaccessible. It prevents the development of the pretrained language model for the task-oriented dialogue. We propose a simple yet effective pretraining method to alleviate this problem, which consists of two pretraining phases. The first phase is to pretrain on large-scale contextual text data, where the structured information of the text is extracted by the information extracting tool. To bridge the gap between the pretraining method and downstream tasks, we design two pretraining tasks: ontology-like triple recovery and next-text generation, which simulates the DST and RG, respectively. The second phase is to fine-tune the pretrained model on the TOD data. The experimental results show that our proposed method achieves an exciting boost and get competitive performance even without any TOD data on CamRest676 and MultiWOZ benchmarks.},
  langid = {english},
  keywords = {已讀,機器學習,監督式學習,知識本體},
  annotation = {2 citations (Crossref) [2024-03-26]\\
titleTranslation: OPAL：用於端到端任務導向對話的本體感知預訓練語言模型\\
abstractTranslation:  本文提出了一種用於端到端任務導向對話（TOD）的本體感知預訓練語言模型（OPAL）。與閒聊對話模型不同，以任務為導向的對話模型至少實現兩個特定於任務的模組：對話狀態追蹤器（DST）和回應產生器（RG）。對話狀態由領域-插槽-值三元組組成，被視為使用者搜尋領域相關資料庫的限制。帶有註釋的結構化對話狀態的大規模面向任務的對話資料通常是無法存取的。它阻止了面向任務的對話的預訓練語言模型的發展。我們提出了一種簡單而有效的預訓練方法來緩解這個問題，該方法由兩個預訓練階段組成。第一階段是對大規模情境文字資料進行預先訓練，其中透過資訊擷取工具提取文字的結構化資訊。為了彌合預訓練方法和下游任務之間的差距，我們設計了兩個預訓練任務：類本體三元組恢復和下文本生成，分別模擬 DST 和 RG。第二階段是在 TOD 資料上微調預訓練模型。實驗結果表明，即使在 CamRest676 和 MultiWOZ 基準上沒有任何 TOD 數據，我們提出的方法也能實現令人興奮的提升並獲得有競爭力的性能。}
}

@inproceedings{zhiqingsunRotatEKnowledgeGraph2018,
  title = {{{RotatE}}: {{Knowledge Graph Embedding}} by {{Relational Rotation}} in {{Complex Space}}},
  shorttitle = {{{RotatE}}},
  author = {{Zhiqing Sun} and {Zhi-Hong Deng} and {Jian-Yun Nie} and {Jian Tang}},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=HkgEQnRqYQ},
  urldate = {2023-06-25},
  abstract = {We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  annotation = {titleTranslation: RotatE：複雜空間中關係旋轉的知識圖嵌入\\
abstractTranslation:  我們研究學習知識圖中實體和關係的表示以預測缺失鍊接的問題。此類任務的成功在很大程度上依賴於建模和推斷關係（或關係之間）模式的能力。在本文中，我們提出了一種稱為 RotatE 的知識圖嵌入新方法，它能夠建模和推斷各種關係模式，包括：對稱/反對稱、反轉和組合。具體來說，RotatE模型將每個關係定義為複向量空間中從源實體到目標實體的旋轉。此外，我們提出了一種新穎的自對抗負採樣技術，用於高效且有效地訓練 RotatE 模型。多個基準知識圖譜上的實驗結果表明，所提出的 RotatE 模型不僅具有可擴展性，而且能夠推斷和建模各種關係模式，並且顯著優於現有的最先進的鏈接預測模型。},
  file = {C:\Users\BlackCat\Zotero\storage\ETGB8IIH\Sun et al. - 2018 - RotatE Knowledge Graph Embedding by Relational Rotation in Complex Space.pdf}
}

@article{zhixinqiDualStoreStructureKnowledge2023,
  title = {A {{Dual-Store Structure}} for {{Knowledge Graphs}}},
  author = {{Zhixin Qi} and {Hongzhi Wang} and {Haoran Zhang}},
  date = {2023-02},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {35},
  number = {2},
  pages = {1104--1118},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2021.3093200},
  url = {https://ieeexplore.ieee.org/document/9468312},
  urldate = {2023-11-23},
  abstract = {To effectively manage increasing knowledge graphs in various domains, a hot research topic, knowledge graph storage management, has emerged. Existing methods are classified to relational stores and native graph stores. Relational stores are able to store large-scale knowledge graphs and convenient in updating knowledge, but the query performance weakens obviously when the selectivity of a knowledge graph query is large. Native graph stores are efficient in processing complex knowledge graph queries due to its index-free adjacent property, but they are inapplicable to manage a large-scale knowledge graph due to limited storage budgets or inflexible updating process. Motivated by this, we propose a dual-store structure which leverages a graph store to accelerate the complex query process in the relational store. However, it is challenging to determine what data to transfer from relational store to graph store at what time. To address this problem, we formulate it as a Markov Decision Process and derive a physical design tuner \textbackslash sf DOTILDOTIL based on reinforcement learning. With \textbackslash sf DOTILDOTIL, the dual-store structure is adaptive to dynamic changing workloads. Experimental results on real knowledge graphs demonstrate that our proposed dual-store structure improves query performance up to average 50.11 percent compared with the most commonly used relational stores.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  langid = {english},
  keywords = {儲存方式,已整理,知識圖譜},
  annotation = {6 citations (Crossref) [2024-03-26]\\
titleTranslation: 知識圖的雙儲存結構},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\7WWDBB8F\\Qi 等。 - 2023 - A Dual-Store Structure for Knowledge Graphs.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\7ZTDAFGB\\9468312.html}
}

@article{ZhongHuiRuhui-juchungZhongYiZhuYuanHuLiBingLiBiaoZhunHuaZhiYanJiu2007,
  title = {中醫住院護理病歷標準化之研究},
  author = {{鍾蕙如(Hui-Ju Chung)} and {林淑瓊(Shuv-Chung Lin)} and {李秀茹(Hsiu-Ju Lin)} and {李采芬(Tsae-Fen Lee)} and {林胤谷(Yin-Ku Lin)} and {楊建中(Chein-Chung Yang)} and {楊賢鴻(Sien-Hung Yang)}},
  date = {2007},
  journaltitle = {中醫藥雜誌},
  shortjournal = {中醫藥雜誌},
  volume = {18},
  pages = {129--146},
  publisher = {衛生福利部國家中醫藥研究所},
  issn = {1017-6446},
  doi = {10.6940/jcm.200712_18(3_4).04},
  url = {http://dx.doi.org/10.6940/JCM.200712_18(3_4).04},
  abstract = {本研究目的係為建構中醫住院辨證評估施護後之入院護理病歷紀錄及護理病歷書寫指引標準，並針對設立中醫護理標準後臨床中醫護理人員執行的成效進行相關探討，藉以提升我國中醫護理照護的品質與提昇中醫護理人員的專業能力。 先收集國內外及過去中醫護理記錄及護理計畫的內容與標準，以專家會議德菲法(Delphi Method)進行專家內容效度針對「中醫護理病歷記錄」之內容及指引標準進行專家意見修正。研究結果顯示，經專家效度鑑定後，依專家的意見結果求取內容效度指數(Content Validity Index, CVI)，皆達到0.8。完整的中醫住院護理病歷內容應涵蓋：(1)病人基本資料、家族史、四診辨證評估資料、理學檢查、施護內容及評值。(2)一般性的辨證施護記錄：包含西醫一般檢查、生命徵象（體溫、脈搏、呼吸、血壓），運用中醫望、聞、問、切的四診評估記錄病人的脈象、舌苔、神志、汗、嘔吐、二便，患者現存的中醫健康護理問題、記錄醫囑中涉及的護理問題、對患者告知內容的記錄以及重要環節中護理問題的記錄。(3)病重患者護理記錄：除應有一般性的護理記錄外，更應詳細記錄病情的變化過程經過及採取的具體措施。研究發現，護理人員的中醫護理專業知識與照護能力會影響其紀錄執行之品質，建議中醫病房護理人員應接受中醫基礎護理七科目九學分的訓練並通過能力認證，以確保病患照護品質。},
  issue = {3\&4},
  langid = {zh\_CN},
  keywords = {Traditional Chinese Medicine Nursing,Traditional Chinese Medicine Nursing records,Traditional Chinese Medicine of Inpatient care,中醫住院,中醫護理,中醫護理記錄},
  annotation = {abstractTranslation:  本研究目的系針對目前中醫住院辨證評估施護後之入院護理病歷記錄及護理病曆書寫指引標準，並針對設立中醫護理標准後臨床中醫護理人員執行的成果進行相關探討，藉以提升我國中醫護理照護的質量與提升中醫護理人員的專業能力。先收集常規及過去中醫護理記錄及護理計劃的內容與標準，以專家會議德菲法（德爾菲法）針對「護理病歷記錄」進行專家內容有效性度之內容及指引標准進行專家修改。研究結果顯示，經專家效度鑑定後，依專家意見結果意見求取內容效度指數（Content Validity Index，CVI），皆達到0.8。完整的中醫住院護理病歷內容內容主題：(1)患者基本資料、家族史、四診辨證評估資料、理學檢查、施護內容及評價值。(2)一般性的辨證施護記錄：包含西醫一般檢查、生命徵象(體溫、脈搏、呼吸、高血壓），運用中醫望、聞、問、切的四診評估記錄患者的脈象、舌苔、神誌、汗、嘔吐、二便，患者的中醫健康護理問題、記錄醫囑中涉及的(3 )病重患者護理記錄：除應有一般性的護理記錄外，更應詳細記錄病情的過程及採取的具體措施研究發現，護理人員的中醫護理專業知識與照護能力會影響其臨床執行之質量，建議中醫病房護理人員應接受中醫基礎護理七學期九學分的訓練並通過認證，以確保病患照護能力。\\
titleTranslation: 中醫住院護理病歷標準化之研究},
  file = {C:\Users\BlackCat\Zotero\storage\YY6DTP2G\鍾蕙如(Hui-Ju Chung) 等。 - 2007 - 中醫住院護理病歷標準化之研究.pdf}
}

@article{ZhouJiaoMeiZhongYiHuLiJieGouHuaDianZiBingLiXiTongDeYingYongYuFenXi2019,
  title = {中医护理结构化电子病历系统的应用与分析},
  author = {{周姣媚} and {张素秋} and {樊艳美} and {刘新影} and {陈丽丽}},
  date = {2019},
  journaltitle = {中国护理管理},
  shortjournal = {Chinese Nursing Management},
  volume = {19},
  number = {10},
  pages = {1441--1444},
  doi = {10.3969/j.issn.1672-1756.2019.10.001},
  url = {http://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNzE5Eg96Z2hsZ2wyMDE5MTAwMDEaCG1tMnc0OWQ3},
  urldate = {2022-08-14},
  abstract = {目的:评价中医护理结构化电子病历系统的应用效果.方法:采集2018年10月至2019年6月我院风湿科病房第一诊断为"尪痹"的211例患者的数据,从辨证分型、临床常见症状、中医护理技术应用情况等角度进行分析.结果:结构化电子病历系统的应用利于数据的规范、完整及客观采集,为中医护理方案的优化提供数据支持.结论:结构化电子病历系统的应用促进了中医护理方案的有效落实,为方案的优化提供了依据,提升中医护理服务质量.},
  langid = {zh\_CN},
  keywords = {中文電子病歷CEMR,中醫},
  annotation = {中国中医科学院广安门医院护理部,100053 北京市中国中医科学院广安门医院风湿、泌尿科,100053 北京市\\
北京中医药科技发展资金项目\\
2019-11-28 （万方平台首次上网日期，不代表论文的发表时间）\\
titleTranslation: 中醫護理形成電子病歷系統的應用與分析\\
abstractTranslation:  目的：評價中醫護理形成電子病歷系統的應用效果。方法：採集2018年10月至2019年6月我院風濕科病房首次診斷為“尪痺”的211例患者的數據，從辨證分型結果：塑造電子病歷系統的應用有利於數據的規範、完整及偵查，為中醫護理方案的優化提供數據支持。 結論：塑造電子病歷系統系統的應用促進了中醫護理方案的有效落地，為方案的優化提供了借鑒，提升了中醫護理服務質量。},
  file = {C:\Users\BlackCat\Zotero\storage\DLUVNAS7\周 等。 - 2019 - 中医护理结构化电子病历系统的应用与分析.pdf}
}

@thesis{ZhouLiangMaoJiYuXianZhiLuoJiChengShiZhiLeiBieCengJiCeShiAnLiChanShengQi2011,
  title = {基於限制邏輯程式之類別層級測試案例產生器},
  author = {{周良懋}},
  namea = {{林迺衛}},
  nameatype = {collaborator},
  date = {2011},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/4zpdu8},
  abstract = {軟體測試是確定軟體品質最主要的方法，本論文使用黑箱測試的技術來自動 產生Java程式的測試案例。本論文使用UML類別圖、UML狀態圖和OCL做為 規格語言，使用UML類別圖和OCL來定義函式層級的規格，使用UML狀態圖 和OCL來定義類別層級的規格。 本論文先依據規格利用限制邏輯程式強大的限制求解能力來自動地產生測 試案例的測試輸入及預期輸出，再依據測試輸入及預期輸出自動地產生可以 在JUnit測試架構上自動執行的Java測試類別。},
  pagetotal = {81},
  keywords = {實驗室,軟測}
}

@online{zhouLLMEnhancedDataManagement2024,
  title = {{{LLM-Enhanced Data Management}}},
  author = {Zhou, Xuanhe and Zhao, Xinyang and Li, Guoliang},
  date = {2024-02-04},
  eprint = {2402.02643},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.02643},
  url = {http://arxiv.org/abs/2402.02643},
  urldate = {2024-04-29},
  abstract = {Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years. However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context). Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning). However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks. To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high cost of LLMs by vector databases which provide semantic search and caching abilities. LLMDB improves the task accuracy by LLM agent which provides multiple-round inference and pipeline executions. We showcase three real-world scenarios that LLMDB can well support, including query rewrite, database diagnosis and data analytics. We also summarize the open research challenges of LLMDB.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Machine Learning,未整理,重要},
  annotation = {abstractTranslation:  近五年來，用於優化資料管理問題的機器學習（ML）技術已被廣泛研究和廣泛部署了。然而，傳統的機器學習方法在泛化性（適應不同場景）和推理能力（理解上下文）方面存在局限性。幸運的是，大型語言模型（LLM）在理解上下文方面表現出了高度的通用性和人類競爭能力，這對於資料管理任務（例如資料庫診斷、資料庫調優）來說是有希望的。然而，現有的法學碩士有幾個限制：幻覺、成本高、複雜任務的準確性低。為了應對這些挑戰，我們設計了LLMDB，一種LLM增強的資料管理範式，它具有通用性和高推理能力，同時避免幻覺，降低LLM成本，並實現高精度。 LLMDB嵌入了特定領域的知識，透過LLM微調和提示工程來避免產生幻覺。 LLMDB 透過提供語意搜尋和快取功能的向量資料庫降低了 LLM 的高成本。 LLMDB透過LLM代理提高了任務準確性，LLM代理提供多輪推理和管道執行。我們展示了 LLMDB 可以很好支援的三個現實場景，包括查詢重寫、資料庫診斷和資料分析。我們也總結了 LLMDB 的開放研究挑戰。\\
titleTranslation: 法學碩士-增強資料管理},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\Y72FTDMT\\Zhou 等。 - 2024 - LLM-Enhanced Data Management.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\EI9MVF6S\\2402.html}
}

@online{zhouOasisDataCuration2023,
  title = {Oasis: {{Data Curation}} and {{Assessment System}} for {{Pretraining}} of {{Large Language Models}}},
  shorttitle = {Oasis},
  author = {Zhou, Tong and Chen, Yubo and Cao, Pengfei and Liu, Kang and Zhao, Jun and Liu, Shengping},
  date = {2023-11-21},
  eprint = {2311.12537},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.12537},
  url = {http://arxiv.org/abs/2311.12537},
  urldate = {2024-05-04},
  abstract = {Data is one of the most critical elements in building a large language model. However, existing systems either fail to customize a corpus curation pipeline or neglect to leverage comprehensive corpus assessment for iterative optimization of the curation. To this end, we present a pretraining corpus curation and assessment platform called Oasis -- a one-stop system for data quality improvement and quantification with user-friendly interactive interfaces. Specifically, the interactive modular rule filter module can devise customized rules according to explicit feedback. The debiased neural filter module builds the quality classification dataset in a negative-centric manner to remove the undesired bias. The adaptive document deduplication module could execute large-scale deduplication with limited memory resources. These three parts constitute the customized data curation module. And in the holistic data assessment module, a corpus can be assessed in local and global views, with three evaluation means including human, GPT-4, and heuristic metrics. We exhibit a complete process to use Oasis for the curation and assessment of pretraining data. In addition, an 800GB bilingual corpus curated by Oasis is publicly released.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,LLM,可視化,未整理,資料集},
  annotation = {titleTranslation: Oasis：大型語言模型預先訓練的資料管理和評估系統\\
abstractTranslation:  數據是建構大型語言模型最關鍵的元素之一。然而，現有系統要么無法自訂語料庫管理流程，要么忽視利用全面的語料庫評估來迭代最佳化管理。為此，我們推出了一個名為 Oasis 的預訓練語料庫管理和評估平台，這是一個透過使用者友善的互動介面進行資料品質改進和量化的一站式系統。具體地，互動式模組化規則過濾模組可以根據顯式回饋設計自訂規則。除偏神經過濾模組以負為中心的方式建立品質分類資料集，以消除不必要的偏差。自適應文件重複資料刪除模組可以在有限的記憶體資源下執行大規模的重複資料刪除。這三個部分構成了客製化的資料管理模組。在整體資料評估模組中，可以從局部和全局角度對語料庫進行評估，採用人類、GPT-4和啟發式度量三種評估手段。我們展示了使用 Oasis 管理和評估預訓練資料的完整流程。此外，綠洲策劃的800GB雙語語料庫也已公開發布。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\A45T23LQ\\Zhou 等。 - 2023 - Oasis Data Curation and Assessment System for Pre.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\64DX2IIJ\\2311.html}
}

@thesis{ZhouYanQuanLiYongYanHuaShiYanSuanFaZhenDuiZhiXingShiJianYuMuDeMaDaXiaoZuiJiaHuaZhiDieDaiShiBianYi2012,
  title = {利用演化式演算法針對執行時間與目的碼 大小最佳化之迭代式編譯},
  author = {{周彥全}},
  namea = {{林迺衛} and {Naiwei Lin}},
  nameatype = {collaborator},
  date = {2012},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/gvw63n},
  abstract = {現代的編譯器通常提供大量的編譯最佳化選項，協助使用者微調程式發揮最大的效能。然而要妥善應用這些最佳化選項，必須具備編譯最佳化方面的知識，所以大多數的使用者並沒有足夠的能力使用這些最佳化選項。Iterative Compilation是目前常用於為一個程式搜尋最佳編譯選項組合的方法。在編譯最佳化方面有幾個令人感興趣的目標，如：執行時間、編譯時間、目的碼大小、記憶體用量、能源消耗及其他計算資源。本研究將目前廣為使用的多目標演化式演算法 NSGA-II和MOEA/D，針對執行時間以及目的碼大小進行迭代式編譯。實驗結果顯示，兩個演算法所選擇的最佳化組合皆優於隨機搜尋獲得的結果，也優於編譯器內建的最佳化等級。},
  langid = {chinese-traditional},
  pagetotal = {42},
  keywords = {實驗室,編譯器},
  annotation = {abstractTranslation:  現代的編譯器通常提供大量的編譯最佳化選項，幫助用戶調試程序發揮最大的作用。然而，要理解應用這些最佳化選項，必須具備編譯最佳化方面的知識，因此大多數用戶並沒有足夠的能力使用這些最佳化選項。迭代編譯目前常用於為一個程序尋找最佳編譯選項組合的方法。在編譯最佳化方面有幾個令人感興趣的目標，例如：執行時間、編譯時間、目標碼大小、記憶體佔用、能耗及其他計算資源。本研究將目前廣為使用的多目標適應式演算法NSGA-II和MOEA/D，針對執行時間以及碼目標大小進行迭代實驗結果顯示，兩個演算法所選擇的最佳化組合既可以隨機搜索得到的結果，也可以通過編譯器內建的最佳化等級。\\
titleTranslation: 利用迭代式演算法針對執行時間與目的碼大小最優化之迭代式編譯}
}

@thesis{ZhuangShengChaoYiGeCCCPingXingChengShiYuYanDeXiaoNengFenXiGongJu2004,
  title = {{{一個CCC平行程式語言的效能分析工具}}},
  author = {{莊勝超}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2004},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/dtj6be},
  abstract = {CCC是一個高階的平行程式語言，它同時支援資料平行與控制平行。在CCC語言中，資料平行採用的是「單一指令多重資料」的設計模式，而控制平行則採用的是「多重指令多重資料」的設計模式。在CCC語言中，控制平行在語法上同時支援訊息傳遞溝通機制與共享變數同步機制。 這篇論文主要是講述設計和實作一個針對CCC語言的效能分析工具。效能分析會包含效能監測與效能視覺化兩部分，效能監測意指靜態地安插效能事件追蹤函式到受測程式中，並且動態的進行效能量測與記錄收集到的效能事件。效能視覺化的部分是分析效能事件資料檔，並且將分析結果同時呈現出數值化的效能統計資料及圖形式的效能視覺化兩種分析結果。這個工具所提供的資訊可以有效的幫助程式設計師瞭解CCC平行程式的效能行為。},
  pagetotal = {183},
  keywords = {實驗室,編譯器}
}

@thesis{ZhuangYaoJiaZhongYiBingLiBaoGaoZiLiaoKu2018,
  title = {中醫病例報告資料庫},
  author = {{莊曜嘉}},
  namea = {{林迺衛} and {Nai-Wei Lin}},
  nameatype = {collaborator},
  date = {2018},
  journaltitle = {資訊工程研究所},
  volume = {碩士},
  institution = {國立中正大學},
  location = {嘉義縣},
  url = {https://hdl.handle.net/11296/dz8b26},
  abstract = {診斷治療資訊化是提升診斷治療品質的基石。我們可以根據系統化整理的醫學典籍及名醫經驗，開發電腦輔助診斷治療系統，並運用大量的嚴謹審核的病例報告驗證系統。我們也可以根據大量的嚴謹審核的病例報告，運用資料探勘及機器學習技術，開發電腦輔助診斷治療系統。因此，蒐集及整理大量嚴謹審核的病例報告非常重要。 本論文蒐集台灣 13 種中醫期刊雜誌的中醫病例報告，總計整理了 336 例的病例，並研製一個中醫病例報告資料庫。此資料庫系統提供病例檢索、病例維護、及病例匯出的功能。此中醫病例報告資料庫未來可用於電腦輔助中醫診斷治療系統的開發及驗證。},
  pagetotal = {48},
  keywords = {中醫,實驗室,病歷分析},
  file = {C:\Users\BlackCat\Zotero\storage\MWDWBBRV\莊曜嘉 - 2018 - 中醫病例報告資料庫.pdf}
}

@inproceedings{zhuComputationalModelingIndividualized2016,
  title = {Computational {{Modeling}} of the {{Individualized Knowledge Building}} in a {{Constructivist Way}}},
  booktitle = {2016 12th {{International Conference}} on {{Semantics}}, {{Knowledge}} and {{Grids}} ({{SKG}})},
  author = {Zhu, Mengmeng and Zhao, Defang and Yang, Juan},
  date = {2016-08},
  pages = {25--31},
  doi = {10.1109/SKG.2016.012},
  url = {https://ieeexplore.ieee.org/document/7815073},
  urldate = {2023-12-03},
  abstract = {Knowledge management becomes increasingly more important for individuals since it would make the best use of knowledge through helping learners to better understand, administer and transfer the knowledge. Externalize what the learners already know is one of the most important aspects of knowledge management. Although mind tools, ontology and knowledge graph are the most popular methodologies to interpret and organize knowledge, they still have defects to simulate individuals' mental images of the knowledge, especially lack the necessary supporting mechanisms on sophisticated semantic reasoning and related cognitive simulation required by the personal knowledge maps' producing. This paper proposed a computational model to simulate the personal knowledge building process which not only enriches semantic relations and inferring, but also is supported by the general cognitive intelligence theory about knowledge integration and the constructivism learning theory. Finally, the experiment result shows the personalized map has a good fidelity to the changing of the learners' mental images which consistently evolve along with the learning progress.},
  eventtitle = {2016 12th {{International Conference}} on {{Semantics}}, {{Knowledge}} and {{Grids}} ({{SKG}})},
  langid = {english},
  keywords = {使用者研究,學習流程,已整理,知識圖譜},
  annotation = {1 citations (Crossref) [2024-03-26]\\
titleTranslation: 以建構主義方式建構個人化知識的計算模型\\
abstractTranslation:  知識管理對個人來說變得越來越重要，因為它可以透過幫助學習者更好地理解、管理和轉移知識來充分利用知識。將學習者已知的知識具體化是知識管理最重要的面向之一。儘管思考工具、本體論和知識圖譜是最受歡迎的解釋和組織知識的方法論，但它們在模擬個體對知識的心理圖像方面仍然存在缺陷，特別是缺乏人類所需的複雜語義推理和相關認知模擬的必要支持機制。個人知識圖譜的製作。本文提出了一個模擬個人知識建構過程的計算模型，不僅豐富了語意關係和推理，也得到了關於知識整合的一般認知智能理論和建構主義學習理論的支持。最後，實驗結果表明，個人化地圖對學習者心理形象的變化具有良好的保真度，並且隨著學習的進展而不斷變化。},
  note = {建立一個知識圖譜用於分析學習者的學習過程，並期望用這樣的結果作到防患未然。
\par
可以作為未來展望使用。},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\DB9QIPZ5\\Zhu et al. - 2016 - Computational Modeling of the Individualized Knowl.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\WBN8B5TU\\7815073.html}
}

@online{zhuFanOutQAMultiHopMultiDocument2024,
  title = {{{FanOutQA}}: {{Multi-Hop}}, {{Multi-Document Question Answering}} for {{Large Language Models}}},
  shorttitle = {{{FanOutQA}}},
  author = {Zhu, Andrew and Hwang, Alyssa and Dugan, Liam and Callison-Burch, Chris},
  date = {2024-02-21},
  eprint = {2402.14116},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.14116},
  url = {http://arxiv.org/abs/2402.14116},
  urldate = {2024-03-22},
  abstract = {One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset and open-source tools to run models to encourage evaluation at https://fanoutqa.com},
  langid = {english},
  pubstate = {preprint},
  keywords = {ChatGPT,Claude,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,LLama2,LLM,Mistral,問答系統,多跳問答,已整理,數據集},
  annotation = {titleTranslation: FanOutQA：大型語言模型的多跳、多重文件問答\\
abstractTranslation:  日常場景中常見的一類問題是「扇出」問題，即複雜的多跳、多文檔推理問題，需要尋找大量實體的資訊。然而，很少有資源來評估大型語言模型中的此類問答能力。為了更全面地評估法學碩士中的複雜推理，我們提出了 FanOutQA，這是一個以英語維基百科為知識庫的扇出問答對和人工註釋分解的高質量數據集。我們在資料集和基準 7 個 LLM 中製定了三個基準設置，包括 GPT-4、LLaMA 2、Claude-2.1 和 Mixtral-8x7B，發現當代模型仍然有空間改進長上下文中文檔間依賴關係的推理。我們提供資料集和開源工具來運行模型，以鼓勵在 https://fanoutqa.com 上進行評估},
  note = {Comment: 18 pages, 2 figures. In review at ACL 2024},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\7DNUXK6U\\Zhu 等。 - 2024 - FanOutQA Multi-Hop, Multi-Document Question Answe.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\2BEE3ZXS\\2402.html}
}

@article{ZhuYanXinJiYuFAIRYuanZeDeXunZhengYiXueWenXianBenTiGouJianYiXiaoChuanYaoWuZhiLiaoWenXianBenTiWeiLi2022,
  title = {{{基于FAIR原则的循证医学文献本体构建}}——以哮喘药物治疗文献本体为例},
  author = {{朱妍昕} and {徐维} and {王霞} and {李栎}},
  date = {2022},
  journaltitle = {情报理论与实践},
  volume = {45},
  number = {1},
  pages = {9},
  doi = {10.16353/j.cnki.1000-7490.2022.01.024},
  url = {https://www.nstl.gov.cn/paper_detail.html?id=d75e48b0a71c2f795d3e97625ef565a1},
  urldate = {2023-10-16},
  abstract = {[目的/意义]遵循FAIR原则构建标准化,结构化的循证医学文献数据本体表达模式.[方法/过程]采用NeOn本体建立方法,通过本体构建工具Protégé4.0进行本体的设计与构建.以科学证据及来源信息本体(SEPIO)为主要框架,同时参考已有生物医学本体,如副作用本体(OAE),药物本体(DRON),疾病本体(DO)等,构建了循证医学文献本体(Evidence-Based Medicine Ontology, EBMO).[结果/结论]EBMO目前包括58个类,13个对象属性和11个数据属性.以哮喘药物治疗随机对照试验文献为实例进行录入验证,证实依据EBMO著录的实例可实现文献证据强度以及诊疗信息的推理.研究构建的循证医学文献本体能够更好地组织,利用和呈现循证医学文献信息的内部特征,为循证医学数据库和语义网的建立奠定良好的基础.},
  keywords = {FAIR原则,关联数据,循证医学,本体}
}

@online{zifengdingFewShotInductiveLearning2022,
  title = {Few-{{Shot Inductive Learning}} on {{Temporal Knowledge Graphs}} Using {{Concept-Aware Information}}},
  author = {{Zifeng Ding} and {Jingpei Wu} and {Bailan He} and {Yunpu Ma} and {Zhen Han} and {Volker Tresp}},
  date = {2022-11-15},
  eprint = {2211.08169},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.08169},
  url = {http://arxiv.org/abs/2211.08169},
  urldate = {2023-07-18},
  abstract = {Knowledge graph completion (KGC) aims to predict the missing links among knowledge graph (KG) entities. Though various methods have been developed for KGC, most of them can only deal with the KG entities seen in the training set and cannot perform well in predicting links concerning novel entities in the test set. Similar problem exists in temporal knowledge graphs (TKGs), and no previous temporal knowledge graph completion (TKGC) method is developed for modeling newly-emerged entities. Compared to KGs, TKGs require temporal reasoning techniques for modeling, which naturally increases the difficulty in dealing with novel, yet unseen entities. In this work, we focus on the inductive learning of unseen entities' representations on TKGs. We propose a few-shot out-of-graph (OOG) link prediction task for TKGs, where we predict the missing entities from the links concerning unseen entities by employing a meta-learning framework and utilizing the meta-information provided by only few edges associated with each unseen entity. We construct three new datasets for TKG few-shot OOG link prediction, and we propose a model that mines the concept-aware information among entities. Experimental results show that our model achieves superior performance on all three datasets and our concept-aware modeling component demonstrates a strong effect.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,回收,已發表,知識挖掘,知識推理},
  annotation = {6 citations (Semantic Scholar/arXiv) [2023-07-20]\\
6 citations (Semantic Scholar/DOI) [2023-07-20]\\
titleTranslation: 使用概念感知信息對時態知識圖進行少樣本歸納學習\\
abstractTranslation:  知識圖補全（KGC）旨在預測知識圖（KG）實體之間缺失的鏈接。儘管針對 KGC 已經開發了各種方法，但大多數方法只能處理訓練集中看到的 KG 實體，而不能很好地預測測試集中新實體的鏈接。時序知識圖（TKG）中也存在類似的問題，並且之前沒有開發出時序知識圖補全（TKGC）方法來對新出現的實體進行建模。與 KG 相比，TKG 需要時間推理技術進行建模，這自然增加了處理新的、未見過的實體的難度。在這項工作中，我們重點關注 TKG 上不可見實體表示的歸納學習。我們為 TKG 提出了少鏡頭圖外 (OOG) 鏈接預測任務，其中我們通過採用元學習框架並利用僅少數邊提供的元信息，從涉及未見實體的鏈接中預測丟失的實體與每個看不見的實體相關聯。我們為 TKG 少樣本 OOG 鏈接預測構建了三個新數據集，並提出了一個挖掘實體之間概念感知信息的模型。實驗結果表明，我們的模型在所有三個數據集上都取得了優異的性能，並且我們的概念感知建模組件表現出了很強的效果。},
  note = {解決大多數知識圖譜補全難以對新的實體做補全的問題。剩下的不是很懂，但暫時跟研究無關。},
  file = {C:\Users\BlackCat\Zotero\storage\HCM32UTE\Ding et al. - 2022 - Few-Shot Inductive Learning on Temporal Knowledge Graphs using Concept-Aware Information.pdf}
}

@online{zifengdingImprovingFewShotInductive2023,
  title = {Improving {{Few-Shot Inductive Learning}} on {{Temporal Knowledge Graphs}} Using {{Confidence-Augmented Reinforcement Learning}}},
  author = {{Zifeng Ding} and {Jingpei Wu} and {Zongyue Li} and {Yunpu Ma} and {Volker Tresp}},
  date = {2023-06-11},
  eprint = {2304.00613},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.00613},
  url = {http://arxiv.org/abs/2304.00613},
  urldate = {2023-07-18},
  abstract = {Temporal knowledge graph completion (TKGC) aims to predict the missing links among the entities in a temporal knwoledge graph (TKG). Most previous TKGC methods only consider predicting the missing links among the entities seen in the training set, while they are unable to achieve great performance in link prediction concerning newly-emerged unseen entities. Recently, a new task, i.e., TKG few-shot out-of-graph (OOG) link prediction, is proposed, where TKGC models are required to achieve great link prediction performance concerning newly-emerged entities that only have few-shot observed examples. In this work, we propose a TKGC method FITCARL that combines few-shot learning with reinforcement learning to solve this task. In FITCARL, an agent traverses through the whole TKG to search for the prediction answer. A policy network is designed to guide the search process based on the traversed path. To better address the data scarcity problem in the few-shot setting, we introduce a module that computes the confidence of each candidate action and integrate it into the policy for action selection. We also exploit the entity concept information with a novel concept regularizer to boost model performance. Experimental results show that FITCARL achieves stat-of-the-art performance on TKG few-shot OOG link prediction.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,待讀,未整理},
  annotation = {1 citations (Semantic Scholar/arXiv) [2023-07-20]\\
1 citations (Semantic Scholar/DOI) [2023-07-20]\\
titleTranslation: 使用置信度增強強化學習改進時態知識圖上的少樣本歸納學習\\
abstractTranslation:  時態知識圖補全（TKGC）旨在預測時態知識圖（TKG）中實體之間缺失的鏈接。大多數以前的TKGC方法只考慮預測訓練集中看到的實體之間缺失的鏈接，而對於新出現的不可見實體的鏈接預測無法取得良好的性能。最近，提出了一項新任務，即 TKG 少鏡頭圖外（OOG）鏈接預測，其中需要 TKGC 模型對僅具有少鏡頭觀察示例的新出現實體實現出色的鏈接預測性能。在這項工作中，我們提出了一種 TKGC 方法 FITCARL，它將小樣本學習與強化學習相結合來解決此任務。在 FITCARL 中，智能體遍歷整個 TKG 來搜索預測答案。策略網絡旨在根據遍歷的路徑指導搜索過程。為了更好地解決少樣本設置中的數據稀缺問題，我們引入了一個模塊來計算每個候選動作的置信度並將其集成到動作選擇策略中。我們還通過新穎的概念正則化器來利用實體概念信息來提高模型性能。實驗結果表明，FITCARL 在 TKG 少樣本 OOG 鏈路預測上實現了最先進的性能。},
  file = {C:\Users\BlackCat\Zotero\storage\C6Z7Z87X\Ding et al. - 2023 - Improving Few-Shot Inductive Learning on Temporal Knowledge Graphs using Confidence-Augmented Reinforcement Learning.pdf}
}

@online{ZiRanYuYanWenDaXiTongJiYuDBpediaGeiYuDaAnYuJianLiXinDeGuanXi__TaiWanBoShuoShiLunWenZhiShiJiaZhiXiTong,
  title = {自然語言問答系統基於DBpedia給予答案與建立新的關係\_\_臺灣博碩士論文知識加值系統},
  url = {https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=U1i_Zp/search?s=id=%22104NCHU5396048%22.&openfull=1&setcurrent=0},
  urldate = {2023-09-22},
  langid = {chinese},
  keywords = {問答系統,知識管理},
  annotation = {titleTranslation: 基於DBpedia的自然語言問答系統給出答案並建立新的關係\_\_台灣博碩士論文知識加值系統},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\MAV4RHUK\\自然語言問答系統基於DBpedia給予答案與建立新的關係__臺灣博碩士論文知識加值系統.pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\MIPDBCXX\\search.html}
}

@article{ziweijiSurveyHallucinationNatural2023,
  title = {Survey of {{Hallucination}} in {{Natural Language Generation}}},
  author = {{Ziwei Ji} and {Nayeon Lee} and {Rita Frieske} and {Tiezheng Yu} and {Dan Su} and {Yan Xu} and {Etsuko Ishii} and {Ye Jin Bang} and {Andrea Madotto} and {Pascale Fung}},
  year = {3 月 3, 2023},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {55},
  number = {12},
  pages = {248:1--248:38},
  issn = {0360-0300},
  doi = {10.1145/3571730},
  url = {https://dl.acm.org/doi/10.1145/3571730},
  urldate = {2023-10-25},
  abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
  langid = {english},
  keywords = {consistency in NLG,extrinsic hallucination,factuality in NLG,faithfulness in NLG,intrinsic hallucination,NLG,NLP,Survey,問答系統,已整理,幻覺,機器學習,重要},
  annotation = {290 citations (Crossref) [2024-03-26]\\
titleTranslation: 自然語言生成中的幻覺調查},
  file = {C:\Users\BlackCat\Zotero\storage\XBWNGGQE\Ji et al. - 2023 - Survey of Hallucination in Natural Language Genera.pdf}
}

@article{zonghaihuangTraditionalChineseMedicine2022,
  title = {A {{Traditional Chinese Medicine Syndrome Classification Model Based}} on {{Cross-Feature Generation}} by {{Convolution Neural Network}}: {{Model Development}} and {{Validation}}},
  shorttitle = {A {{Traditional Chinese Medicine Syndrome Classification Model Based}} on {{Cross-Feature Generation}} by {{Convolution Neural Network}}},
  author = {{Zonghai Huang} and {Jiaqing Miao} and {Ju Chen} and {Yanmei Zhong} and {Simin Yang} and {Yiyi Ma} and {Chuanbiao Wen}},
  date = {2022-04-06},
  journaltitle = {JMIR Medical Informatics},
  shortjournal = {JMIR Med Inform},
  volume = {10},
  number = {4},
  eprint = {35384854},
  eprinttype = {pmid},
  pages = {e29290},
  issn = {2291-9694},
  doi = {10.2196/29290},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9021949/},
  urldate = {2023-04-18},
  abstract = {Background Nowadays, intelligent medicine is gaining widespread attention, and great progress has been made in Western medicine with the help of artificial intelligence to assist in decision making. Compared with Western medicine, traditional Chinese medicine (TCM) involves selecting the specific treatment method, prescription, and medication based on the dialectical results of each patient’s symptoms. For this reason, the development of a TCM-assisted decision-making system has lagged. Treatment based on syndrome differentiation is the core of TCM treatment; TCM doctors can dialectically classify diseases according to patients’ symptoms and optimize treatment in time. Therefore, the essence of a TCM-assisted decision-making system is a TCM intelligent, dialectical algorithm. Symptoms stored in electronic medical records are mostly associated with patients’ diseases; however, symptoms of TCM are mostly subjectively identified. In general electronic medical records, there are many missing values. TCM medical records, in which symptoms tend to cause high-dimensional sparse data, reduce algorithm accuracy. Objective This study aims to construct an algorithm model compatible for the multidimensional, highly sparse, and multiclassification task of TCM syndrome differentiation, so that it can be effectively applied to the intelligent dialectic of different diseases. Methods The relevant terms in electronic medical records were standardized with respect to symptoms and evidence-based criteria of TCM. We structuralized case data based on the classification of different symptoms and physical signs according to the 4 diagnostic examinations in TCM diagnosis. A novel cross-feature generation by convolution neural network model performed evidence-based recommendations based on the input embedded, structured medical record data. Results The data set included 5273 real dysmenorrhea cases from the Sichuan TCM big data management platform and the Chinese literature database, which were embedded into 60 fields after being structured and standardized. The training set and test set were randomly constructed in a ratio of 3:1. For the classification of different syndrome types, compared with 6 traditional, intelligent dialectical models and 3 click-through-rate models, the new model showed a good generalization ability and good classification effect. The comprehensive accuracy rate reached 96.21\%. Conclusions The main contribution of this study is the construction of a new intelligent dialectical model combining the characteristics of TCM by treating intelligent dialectics as a high-dimensional sparse vector classification task. Owing to the standardization of the input symptoms, all the common symptoms of TCM are covered, and the model can differentiate the symptoms with a variety of missing values. Therefore, with the continuous improvement of disease data sets, this model has the potential to be applied to the dialectical classification of different diseases in TCM.},
  langid = {english},
  pmcid = {PMC9021949},
  annotation = {8 citations (Crossref) [2024-03-26]\\
1 citations (Semantic Scholar/DOI) [2023-04-18]\\
titleTranslation: 基於卷積神經網絡交叉特徵生成的中醫證候分類模型：模型開發與驗證\\
abstractTranslation:  背景如今，智能醫療受到廣泛關注，西醫借助人工智能輔助決策也取得了長足進步。與西醫相比，中醫需要根據每個患者的症狀辯證結果來選擇具體的治療方法、方劑和藥物。因此，中醫輔助決策系統的發展相對滯後。辨證論治是中醫治療的核心；中醫可以根據患者的症狀辨證分類，及時優化治療。因此，中醫輔助決策系統的本質是中醫智能辯證算法。電子病歷中存儲的症狀大多與患者的疾病相關；然而，中醫症狀大多是主觀識別的。一般的電子病歷中，存在很多缺失值。中醫病歷中的症狀往往會造成高維稀疏數據，降低了算法的準確性。目的 構建兼容中醫辨證多維、高度稀疏、多分類任務的算法模型，使其能夠有效應用於不同疾病的智能辯證。方法根據症狀和中醫循證標準對電子病歷中的相關術語進行規範。我們根據中醫診斷的四種診斷檢查，根據不同症狀和體徵的分類，對病例數據進行結構化。卷積神經網絡模型的新型交叉特徵生成基於輸入的嵌入式結構化醫療記錄數據執行基於證據的建議。結果數據集包含來自四川省中醫大數據管理平台和中醫文獻數據庫的5273例痛經真實病例，經結構化和標準化後嵌入到60個領域。訓練集和測試集按照3:1的比例隨機構建。對於不同證型的分類，與6種傳統智能辯證模型和3種點擊率模型相比，新模型表現出良好的泛化能力和良好的分類效果。綜合準確率達到96.21\%。結論本研究的主要貢獻是將智能辯證視為高維稀疏向量分類任務，構建了結合中醫特點的新型智能辯證模型。由於輸入症狀的標準化，覆蓋了所有中醫常見症狀，模型可以區分各種缺失值的症狀。因此，隨著疾病數據集的不斷完善，該模型有潛力應用於中醫不同疾病的辯證分類。},
  file = {C:\Users\BlackCat\Zotero\storage\NEKTK5DH\Huang 等。 - 2022 - A Traditional Chinese Medicine Syndrome Classifica.pdf}
}

@inproceedings{zongliangchenResearchImplementationQA2020,
  title = {Research and {{Implementation}} of {{QA System Based}} on the {{Knowledge Graph}} of {{Chinese Classic Poetry}}},
  booktitle = {2020 {{IEEE}} 5th {{International Conference}} on {{Cloud Computing}} and {{Big Data Analytics}} ({{ICCCBDA}})},
  author = {{Zongliang Chen} and {Shiqun Yin} and {Xu Zhu}},
  date = {2020-04},
  pages = {495--499},
  doi = {10.1109/ICCCBDA49378.2020.9095587},
  url = {https://ieeexplore.ieee.org/document/9095587},
  urldate = {2023-11-23},
  abstract = {With the rapid development of the Internet, intelligent QA (Question Answering) system has been widely used in telecom operators, financial services, e-commerce shopping and other industries, but there are few researches and applications of intelligent QA system in the field of Chinese classical poetry. In view of the above situation, this paper aims to implement an automatic QA system based on the knowledge graph of Chinese classical poetry by combining natural language processing technology. In terms of the construction of knowledge graph, the common triads of Chinese classical poetry knowledge was extracted from the classical poetry websites and the knowledge graph of Chinese classical poetry stored with Neo4j was constructed. In the aspect of question recognition and multi-round dialogue, the Rasa framework was adopted to extract the entity and identify the intention of the user's questions in Chinese classical poetry, so as to realize multi-round dialogue.},
  eventtitle = {2020 {{IEEE}} 5th {{International Conference}} on {{Cloud Computing}} and {{Big Data Analytics}} ({{ICCCBDA}})},
  langid = {english},
  keywords = {中文,問答系統,回收,已整理,知識圖譜},
  annotation = {5 citations (Crossref) [2024-03-26]\\
titleTranslation: 基於中國古典詩詞知識圖譜的問答系統的研究與實現},
  file = {C\:\\Users\\BlackCat\\Zotero\\storage\\SYW236BB\\Chen et al. - 2020 - Research and Implementation of QA System Based on .pdf;C\:\\Users\\BlackCat\\Zotero\\storage\\Y3CKRMEC\\9095587.html}
}
